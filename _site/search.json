[
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html",
    "href": "unrefined file/r_practice/Regression Analysis.html",
    "title": "R Notebook",
    "section": "",
    "text": "options(repr.plot.width = 15, repr.plot.height = 8)  ## 플롯 크기 설정\n\n\nx &lt;- rnorm(100) ## rnorm() / dnorm() / pnorm() / qnorm()\nhist(x)\n\n\n\n\n\ndefault는 표준정규분포이다. (n, mean = 0, sd = 1)\n\n\nx &lt;- seq(-5, 5, 0.01)\nplot(x, dnorm(x), type = 'l', lwd = 2,\n     main = \"표준정규분포의 확률밀도함수\",\n     cex.main = 1,  ## character expansion.main : title\n     cex.lab = 1)   ## character expansion.lab : label\n\n\n\n\n\npnorm(q, mean, sd) | probability, cdf\n\n\nprint(pnorm(1.96))\n\n[1] 0.9750021\n\nprint(pnorm(4, 3, 1))\n\n[1] 0.8413447\n\nprint(pnorm(4, mean = 3, sd = 1))\n\n[1] 0.8413447\n\n\n\np_value를 구할 수 없다면 백분위수(quantile value)를 구해야 한다.\n\n\nprint(qnorm(0.95))\n\n[1] 1.644854\n\nprint(qnorm(0.975))\n\n[1] 1.959964\n\nprint(qnorm(0.995))\n\n[1] 2.575829\n\n\n\n\n\n\n자유도가 중요한 분포이다. &gt; 자유도 : t분포의 모양을 결정하는 것. parameter는 아니다. 그냥 아는 값.\n\n\n표본의 분포가 아닌 표본 통계량의 분포에 해당한다.\n\n\nx &lt;- rt(1000, 3)  ## 자유도가 3인 t분포에서의 샘플 1000개\nhist(x, cex.main = 1, cex.lab = 1)\n\n\n\n\n\n원리는 정규분포와 동일하다. rt(), pt(), dt(), qt()\n\n\nt분포의 확률밀도함수(probability Density function)\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dt(x, 3), type = 'l',\n     main = \"t분포의 확률밀도함수\")\n\n\n\n\n\n여러 개의 확률밀도함수를 같이 그려보자.\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dnorm(x), type = 'l', main = 't분포와 정규분포의 확률밀도함수'  ## , lty = 1\n     )\n\nlines(x, dt(x, 1), col = 'red', lty = 2)\nlines(x, dt(x, 3), col = 'blue', lty = 3)\nlines(x, dt(x, 30), col = 'green', lty = 4)\n\nlegend('topleft', c('N(0,1)', 't(1)', 't(3)', 't(30)'), ## query 문법처럼 사용가능한듯.\n       lty = 1:4, col = c('black', 'red', 'blue', 'green'))\n\n\n\n\n\n자유도 n이 커질 수록 표준정규분포의 확률밀도함수와 유사해진다.\n\n\\(T\\) ~ \\(t(df), ~~~ P(T≤t)\\)\n\\(T_0\\) ~ \\(t(df), ~~~ t_a(df) : P(T &gt; t_a(df)) = a\\)\n\nprint(pt(2, 3))  ## probability distribution function\n\n[1] 0.930337\n\nprint(qt(0.95, 3))  ## quantile\n\n[1] 2.353363\n\nprint(dt(0, 3))   ## probability density function\n\n[1] 0.3675526\n\n\n\n우측검정 시, 오른쪽 영역의 확률을 알고 싶을 경우\n\nStudent T distribution practice\n\npt(1,3, lower.tail = FALSE)   ## 낮은 값의 꼬리쪽으로 산출?\n\n[1] 0.1955011\n\nqt(0.05, 3, lower.tail = FALSE) ## 오른쪽에서의 a = 0.05인 quantile 값\n\n[1] 2.353363\n\nqt(0.95, 3)\n\n[1] 2.353363\n\n\n\n\n\n\nx &lt;- rchisq(1000, 4)\nhist(x)\n\n\n\n\n\\(x &gt; 0\\) 에서의 오른쪽으로 치우쳐진 분포가 나온다.\n\nx &lt;- seq(0, 20, 0.01)\nplot(x, dchisq(x, 5), type = 'l', lwd = 2,\n     main = '자유도가 5인 카이제곱분포의 확률밀도함수',\n     cex.main = 1, cex.lab = 1)\n\n\n\n\n\nx &lt;- seq(0, 200, 0.01)\nplot(x, dchisq(x, 100), type = 'l', lwd = 2,\n     main = '자유도가 100인 카이제곱분포의 확률밀도함수')\n\n\n\n\n자유도가 높을 수록 정규분포와 비슷한 모양을 띈다.\n- 이외 코드는 위와 유사하다…(t분포)\n\npchisq(100, 100)\n\n[1] 0.5188083\n\nqchisq(0.95, 100)\n\n[1] 124.3421\n\ndchisq(50, 50)\n\n[1] 0.03976148\n\n\n\n\n\n\nx &lt;- rf(1000, 4, 6)  ## n=100, 분자의 자유도(df1) = 4, 분모의 자유도(df2) = 6\nhist(x, breaks = 50)\n\n\n\n\n\nx &lt;- seq(0, 15, 0.01)\nplot(x, df(x, 4, 6), type = 'l', lwd = 2,\n     main = \"F분포의 확률밀도함수\")\nlines(x, df(x, 6, 4), col = 'red')\n\nlegend(\"topright\", c(\"F(4,6)\", \"F(6,4)\"), lty = 1:2, col = c('black', 'red'))\n\n\n\n\n\n활용 방법은 비슷하다.\n\n\nqf(0.95, 4, 6)\n\n[1] 4.533677\n\n\n\n\n\n\n\n예제) 비누공장데이터 : 9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6 (n=9)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\n\nbar_x = mean(x); bar_x\n\n[1] 9\n\nS_x = var(x); S_x\n\n[1] 0.0575\n\ns_x = sd(x); s_x\n\n[1] 0.2397916\n\n\n\nlower_x = bar_x - qt(0.975, 8)*s_x/sqrt(9)  ## 자료의 수가 9이므로 자유도 8인 t분포\nupper_x = bar_x + qt(0.975, 8)*s_x/sqrt(9)\n\nc(lower_x, upper_x)\n\n[1] 8.81568 9.18432\n\n\n\n95% 신뢰구간이다.\nquantile 값이 0.975면 우측 확률이 0.025인 t_a와 같으니까…\n\n\n\n\nls(t.test(x))\n\n [1] \"alternative\" \"conf.int\"    \"data.name\"   \"estimate\"    \"method\"     \n [6] \"null.value\"  \"p.value\"     \"parameter\"   \"statistic\"   \"stderr\"     \n\n\n\nt.test(x)$conf.int  ## 해당 모듈의 하위 모듈인 conf.int : 신뢰구간 산출\n\n[1] 8.81568 9.18432\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\nt.test(x, conf.level = 0.99)$conf.int ## 99% 신뢰구간, 디폴트는 95%\n\n[1] 8.731802 9.268198\nattr(,\"conf.level\")\n[1] 0.99\n\n\n\n\n\n\n\n\n\n예제) 공정온도에서의 제품의 강도(\\(N(73.7, 1)\\))\n\n두 가지 방법 : \\(p-value\\)와 \\(\\alpha\\)값 비교, 기각역과 관측값 비교\n\n가설 : \\(H_0 : \\mu = 73.7 ~ vs. ~ H_1 :\\mu &gt; 73.7\\)\n\n\\(\\bar{x} = 75.1, ~ n = 16, ~ \\alpha = 0.05\\)\n\nmu &lt;- 73.7; xbar &lt;- 75.1; sigma &lt;- 1; n &lt;- 16\n\n\n검정통계량의 관측값 : \\(z_0 = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{n}}\\)\n\n\nz_0 &lt;- (xbar - mu)/(sigma/sqrt(n)); z_0\n\n[1] 5.6\n\n\n\n기각역\n\n\nqnorm(0.95)\n\n[1] 1.644854\n\n\n\n기각역보다 검정통계량의 관측값이 휠씬 크므로 귀무가설 기각 가능\n\n\n\n\n\n예제 2\n\n\nmu &lt;- 2; x_bar &lt;- 1.96; s &lt;- 0.05; n &lt;- 50\n\n\nt_0 &lt;- (x_bar - mu)/(s/sqrt(50)); t_0\n\n[1] -5.656854\n\n\n\n기각역\n\n\n-qt(0.95, n - 1)\n\n[1] -1.676551\n\n\n\n유의확률(p-value)\n\n\npt(t_0, n-1)\n\n[1] 3.93525e-07\n\n\n\n\n\n\n예제) 일전의 비누공장 데이터, 모분산은 알리가 없음.\n\n\n가설 : \\(H_0 : \\mu = 9.2 ~ vs. ~ H_1 : \\mu ≠ 9.2\\)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\nxbar = mean(x)\nS_x = var(x)  ## r은 데이터 분석 툴이기 때문에 기본적으로 n-1로 나누어준다.\ns_x = sd(x)\n\n\n유의수준 \\(\\alpha = 0.05\\)\n검정통계량의 관측값 : \\(t_0 = \\frac{}{}, \\alpha = 0.05\\)\n\n\nt_0 &lt;- (xbar - 9.2)/(s_x/sqrt(9)); t_0\n\n[1] -2.502173\n\n\n\n기각역\n\n\nqt(0.975, 9-1)\n\n[1] 2.306004\n\n\n\n유의확률(p-balue)\n\n\npt(t_0, 8) + pt(-t_0, 8, lower.tail = FALSE)\n\n[1] 0.03681717\n\n\n\nt.test()\n\n\nt.test(x)\n\n\n    One Sample t-test\n\ndata:  x\nt = 112.6, df = 8, p-value = 4.325e-14\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n\n\ndefault는 \\(\\mu = 0\\)이기 때문에 귀무가설을 입력해줘햐 한다.\n\n6.1\n\nt.test(x, mu = 9.2, conf.level = 0.95, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.5022, df = 8, p-value = 0.03682\nalternative hypothesis: true mean is not equal to 9.2\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n## two.sided(de), less, greater"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html#분포",
    "href": "unrefined file/r_practice/Regression Analysis.html#분포",
    "title": "R Notebook",
    "section": "",
    "text": "options(repr.plot.width = 15, repr.plot.height = 8)  ## 플롯 크기 설정\n\n\nx &lt;- rnorm(100) ## rnorm() / dnorm() / pnorm() / qnorm()\nhist(x)\n\n\n\n\n\ndefault는 표준정규분포이다. (n, mean = 0, sd = 1)\n\n\nx &lt;- seq(-5, 5, 0.01)\nplot(x, dnorm(x), type = 'l', lwd = 2,\n     main = \"표준정규분포의 확률밀도함수\",\n     cex.main = 1,  ## character expansion.main : title\n     cex.lab = 1)   ## character expansion.lab : label\n\n\n\n\n\npnorm(q, mean, sd) | probability, cdf\n\n\nprint(pnorm(1.96))\n\n[1] 0.9750021\n\nprint(pnorm(4, 3, 1))\n\n[1] 0.8413447\n\nprint(pnorm(4, mean = 3, sd = 1))\n\n[1] 0.8413447\n\n\n\np_value를 구할 수 없다면 백분위수(quantile value)를 구해야 한다.\n\n\nprint(qnorm(0.95))\n\n[1] 1.644854\n\nprint(qnorm(0.975))\n\n[1] 1.959964\n\nprint(qnorm(0.995))\n\n[1] 2.575829\n\n\n\n\n\n\n자유도가 중요한 분포이다. &gt; 자유도 : t분포의 모양을 결정하는 것. parameter는 아니다. 그냥 아는 값.\n\n\n표본의 분포가 아닌 표본 통계량의 분포에 해당한다.\n\n\nx &lt;- rt(1000, 3)  ## 자유도가 3인 t분포에서의 샘플 1000개\nhist(x, cex.main = 1, cex.lab = 1)\n\n\n\n\n\n원리는 정규분포와 동일하다. rt(), pt(), dt(), qt()\n\n\nt분포의 확률밀도함수(probability Density function)\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dt(x, 3), type = 'l',\n     main = \"t분포의 확률밀도함수\")\n\n\n\n\n\n여러 개의 확률밀도함수를 같이 그려보자.\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dnorm(x), type = 'l', main = 't분포와 정규분포의 확률밀도함수'  ## , lty = 1\n     )\n\nlines(x, dt(x, 1), col = 'red', lty = 2)\nlines(x, dt(x, 3), col = 'blue', lty = 3)\nlines(x, dt(x, 30), col = 'green', lty = 4)\n\nlegend('topleft', c('N(0,1)', 't(1)', 't(3)', 't(30)'), ## query 문법처럼 사용가능한듯.\n       lty = 1:4, col = c('black', 'red', 'blue', 'green'))\n\n\n\n\n\n자유도 n이 커질 수록 표준정규분포의 확률밀도함수와 유사해진다.\n\n\\(T\\) ~ \\(t(df), ~~~ P(T≤t)\\)\n\\(T_0\\) ~ \\(t(df), ~~~ t_a(df) : P(T &gt; t_a(df)) = a\\)\n\nprint(pt(2, 3))  ## probability distribution function\n\n[1] 0.930337\n\nprint(qt(0.95, 3))  ## quantile\n\n[1] 2.353363\n\nprint(dt(0, 3))   ## probability density function\n\n[1] 0.3675526\n\n\n\n우측검정 시, 오른쪽 영역의 확률을 알고 싶을 경우\n\nStudent T distribution practice\n\npt(1,3, lower.tail = FALSE)   ## 낮은 값의 꼬리쪽으로 산출?\n\n[1] 0.1955011\n\nqt(0.05, 3, lower.tail = FALSE) ## 오른쪽에서의 a = 0.05인 quantile 값\n\n[1] 2.353363\n\nqt(0.95, 3)\n\n[1] 2.353363\n\n\n\n\n\n\nx &lt;- rchisq(1000, 4)\nhist(x)\n\n\n\n\n\\(x &gt; 0\\) 에서의 오른쪽으로 치우쳐진 분포가 나온다.\n\nx &lt;- seq(0, 20, 0.01)\nplot(x, dchisq(x, 5), type = 'l', lwd = 2,\n     main = '자유도가 5인 카이제곱분포의 확률밀도함수',\n     cex.main = 1, cex.lab = 1)\n\n\n\n\n\nx &lt;- seq(0, 200, 0.01)\nplot(x, dchisq(x, 100), type = 'l', lwd = 2,\n     main = '자유도가 100인 카이제곱분포의 확률밀도함수')\n\n\n\n\n자유도가 높을 수록 정규분포와 비슷한 모양을 띈다.\n- 이외 코드는 위와 유사하다…(t분포)\n\npchisq(100, 100)\n\n[1] 0.5188083\n\nqchisq(0.95, 100)\n\n[1] 124.3421\n\ndchisq(50, 50)\n\n[1] 0.03976148\n\n\n\n\n\n\nx &lt;- rf(1000, 4, 6)  ## n=100, 분자의 자유도(df1) = 4, 분모의 자유도(df2) = 6\nhist(x, breaks = 50)\n\n\n\n\n\nx &lt;- seq(0, 15, 0.01)\nplot(x, df(x, 4, 6), type = 'l', lwd = 2,\n     main = \"F분포의 확률밀도함수\")\nlines(x, df(x, 6, 4), col = 'red')\n\nlegend(\"topright\", c(\"F(4,6)\", \"F(6,4)\"), lty = 1:2, col = c('black', 'red'))\n\n\n\n\n\n활용 방법은 비슷하다.\n\n\nqf(0.95, 4, 6)\n\n[1] 4.533677"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html#신뢰구간",
    "href": "unrefined file/r_practice/Regression Analysis.html#신뢰구간",
    "title": "R Notebook",
    "section": "",
    "text": "예제) 비누공장데이터 : 9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6 (n=9)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\n\nbar_x = mean(x); bar_x\n\n[1] 9\n\nS_x = var(x); S_x\n\n[1] 0.0575\n\ns_x = sd(x); s_x\n\n[1] 0.2397916\n\n\n\nlower_x = bar_x - qt(0.975, 8)*s_x/sqrt(9)  ## 자료의 수가 9이므로 자유도 8인 t분포\nupper_x = bar_x + qt(0.975, 8)*s_x/sqrt(9)\n\nc(lower_x, upper_x)\n\n[1] 8.81568 9.18432\n\n\n\n95% 신뢰구간이다.\nquantile 값이 0.975면 우측 확률이 0.025인 t_a와 같으니까…\n\n\n\n\nls(t.test(x))\n\n [1] \"alternative\" \"conf.int\"    \"data.name\"   \"estimate\"    \"method\"     \n [6] \"null.value\"  \"p.value\"     \"parameter\"   \"statistic\"   \"stderr\"     \n\n\n\nt.test(x)$conf.int  ## 해당 모듈의 하위 모듈인 conf.int : 신뢰구간 산출\n\n[1] 8.81568 9.18432\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\nt.test(x, conf.level = 0.99)$conf.int ## 99% 신뢰구간, 디폴트는 95%\n\n[1] 8.731802 9.268198\nattr(,\"conf.level\")\n[1] 0.99"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html#가설검정",
    "href": "unrefined file/r_practice/Regression Analysis.html#가설검정",
    "title": "R Notebook",
    "section": "",
    "text": "예제) 공정온도에서의 제품의 강도(\\(N(73.7, 1)\\))\n\n두 가지 방법 : \\(p-value\\)와 \\(\\alpha\\)값 비교, 기각역과 관측값 비교\n\n가설 : \\(H_0 : \\mu = 73.7 ~ vs. ~ H_1 :\\mu &gt; 73.7\\)\n\n\\(\\bar{x} = 75.1, ~ n = 16, ~ \\alpha = 0.05\\)\n\nmu &lt;- 73.7; xbar &lt;- 75.1; sigma &lt;- 1; n &lt;- 16\n\n\n검정통계량의 관측값 : \\(z_0 = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{n}}\\)\n\n\nz_0 &lt;- (xbar - mu)/(sigma/sqrt(n)); z_0\n\n[1] 5.6\n\n\n\n기각역\n\n\nqnorm(0.95)\n\n[1] 1.644854\n\n\n\n기각역보다 검정통계량의 관측값이 휠씬 크므로 귀무가설 기각 가능\n\n\n\n\n\n예제 2\n\n\nmu &lt;- 2; x_bar &lt;- 1.96; s &lt;- 0.05; n &lt;- 50\n\n\nt_0 &lt;- (x_bar - mu)/(s/sqrt(50)); t_0\n\n[1] -5.656854\n\n\n\n기각역\n\n\n-qt(0.95, n - 1)\n\n[1] -1.676551\n\n\n\n유의확률(p-value)\n\n\npt(t_0, n-1)\n\n[1] 3.93525e-07\n\n\n\n\n\n\n예제) 일전의 비누공장 데이터, 모분산은 알리가 없음.\n\n\n가설 : \\(H_0 : \\mu = 9.2 ~ vs. ~ H_1 : \\mu ≠ 9.2\\)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\nxbar = mean(x)\nS_x = var(x)  ## r은 데이터 분석 툴이기 때문에 기본적으로 n-1로 나누어준다.\ns_x = sd(x)\n\n\n유의수준 \\(\\alpha = 0.05\\)\n검정통계량의 관측값 : \\(t_0 = \\frac{}{}, \\alpha = 0.05\\)\n\n\nt_0 &lt;- (xbar - 9.2)/(s_x/sqrt(9)); t_0\n\n[1] -2.502173\n\n\n\n기각역\n\n\nqt(0.975, 9-1)\n\n[1] 2.306004\n\n\n\n유의확률(p-balue)\n\n\npt(t_0, 8) + pt(-t_0, 8, lower.tail = FALSE)\n\n[1] 0.03681717\n\n\n\nt.test()\n\n\nt.test(x)\n\n\n    One Sample t-test\n\ndata:  x\nt = 112.6, df = 8, p-value = 4.325e-14\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n\n\ndefault는 \\(\\mu = 0\\)이기 때문에 귀무가설을 입력해줘햐 한다.\n\n6.1\n\nt.test(x, mu = 9.2, conf.level = 0.95, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.5022, df = 8, p-value = 0.03682\nalternative hypothesis: true mean is not equal to 9.2\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n## two.sided(de), less, greater"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html",
    "title": "Kaggle | Autogluon",
    "section": "",
    "text": "자동 예측 프로그램인 Autogluon을 활용하여 titanic data를 적합해보자!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#라이브러리-imports",
    "title": "Kaggle | Autogluon",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#pip install autogluon\n\n\nimport pandas as pd\nimport numpy as np\n\n## tabular(테이블) 형식의 데이터를 다루는 모듈을 다운로드한다.\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#분석",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#분석",
    "title": "Kaggle | Autogluon",
    "section": "2. 분석",
    "text": "2. 분석\n\nA. 데이터 입력\n\n\n문제를 받아오는 과정으로 비유할 수 있다.\n\n\ntr = TabularDataset('./data/train.csv')  ## 학습할 데이터\ntst = TabularDataset('./data/test.csv')\n\n## tr = TabularDataset('/kaggle/input/titanic/train.csv')  ## 학습할 데이터\n## tst = TabularDataset('/kaggle/input/titanic/test.csv')\n\n## tr = pd.read_csv('/kaggle/input/titanic/train.csv')\n## tst"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#b.-predictor-생성",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#b.-predictor-생성",
    "title": "Kaggle | Autogluon",
    "section": "### B. Predictor 생성",
    "text": "### B. Predictor 생성\n\n문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\n\npredictr = TabularPredictor('Survived') ## target variable이 들어있는 데이터프레임, 변수 철자는 임의로 틀리게 설정\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_130536\"\n\n\n\npredictr는 뭔데?\n\n\ntype(predictr)\n\nautogluon.tabular.predictor.predictor.TabularPredictor\n\n\n\n대충 autogluon에서의 class인듯.\n\n\nC. 적합(fit)\n\n\n학습 과정에 해당한다.\n\n\npredictr.fit(tr) ## 학생(predictr)에게 문제(tr)를 주어 학습을 시킴(predictr.fit(tr))\n##tr 그 자체로 학습할 수 있는 건 다 시킨다. sklearn의 모델과는 차이가 있음\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_130536\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.71 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1930.98 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.3s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.36s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    1.83s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    1.08s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    7.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8324   = Validation score   (accuracy)\n    3.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    1.32s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8212   = Validation score   (accuracy)\n    7.74s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.93s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.67s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 28.23s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_130536\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20df4525d50&gt;\n\n\n학습 완료, 이에 따라 리더보드를 확인한다. (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.009000  0.928348                0.009000           0.928348            1       True         13\n1       NeuralNetFastAI   0.832402       0.028001  3.279027                0.028001           3.279027            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.029001  3.949707                0.001000           0.670681            2       True         14\n3              CatBoost   0.826816       0.006002  7.468165                0.006002           7.468165            1       True          7\n4              LightGBM   0.821229       0.006004  0.432719                0.006004           0.432719            1       True          4\n5        NeuralNetTorch   0.821229       0.031999  7.740229                0.031999           7.740229            1       True         12\n6            LightGBMXT   0.815642       0.005002  1.084200                0.005002           1.084200            1       True          3\n7        ExtraTreesGini   0.815642       0.061763  0.516426                0.061763           0.516426            1       True          8\n8      RandomForestEntr   0.815642       0.064586  0.538568                0.064586           0.538568            1       True          6\n9      RandomForestGini   0.815642       0.064718  0.637898                0.064718           0.637898            1       True          5\n10              XGBoost   0.810056       0.013002  1.323482                0.013002           1.323482            1       True         11\n11       ExtraTreesEntr   0.810056       0.062742  0.519159                0.062742           0.519159            1       True          9\n12       KNeighborsDist   0.653631       0.005996  0.012003                0.005996           0.012003            1       True          2\n13       KNeighborsUnif   0.653631       0.215770  1.826697                0.215770           1.826697            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.009000\n0.928348\n0.009000\n0.928348\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.028001\n3.279027\n0.028001\n3.279027\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.029001\n3.949707\n0.001000\n0.670681\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.006002\n7.468165\n0.006002\n7.468165\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.006004\n0.432719\n0.006004\n0.432719\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.031999\n7.740229\n0.031999\n7.740229\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.005002\n1.084200\n0.005002\n1.084200\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.061763\n0.516426\n0.061763\n0.516426\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.064586\n0.538568\n0.064586\n0.538568\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.064718\n0.637898\n0.064718\n0.637898\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.013002\n1.323482\n0.013002\n1.323482\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.062742\n0.519159\n0.062742\n0.519159\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.005996\n0.012003\n0.005996\n0.012003\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.215770\n1.826697\n0.215770\n1.826697\n1\nTrue\n1\n\n\n\n\n\n\n\nscore_val이 의미하는 것 * 실제로 predictr가 학습한 것은? &gt; predictor와 train set이 있고, train set에 데이터가 1000개 있다고 하면 해당 데이터를 전부 가용하지 않는다. &gt; * 800개를 사용한다고 하면 200개는 학습하지 않고 답을 맞춰 보는 식이다. &gt; &gt; * 200개는 왜 남겨두지? &gt; &gt; 문제에서 답을 찾는 규칙이 맞는지, 다른 데이터들에 대해서도 일반화시킬 수 있는 지 테스트 해보면 좋을 것 같다. 따라서 나머지 데이터셋에서 분석을 해본다. &gt; &gt; 실제 테스트에서 잘하기 위한 자체적 테스트셋에 해당, 200개의 나머지 테스트용 데이터셋을 validation set이라 일컫는다.\n\n\n\n\ntrain\nval\n\n\n\n\n학생1\n95%\n72%\n\n\n학생2\n80%\n80%\n\n\n…\n…\n…\n\n\n\ntrain(연습문제)만 계속 푼 것 보다, val(모의고사)에서 가장 높은 점수를 받은 것이 유의미할 것.\n\n그러니까 score_val는 모의고사 점수라고 보면 된다.\n\n- 따라서 가장 높은 점수를 받은 WeightedEnsemble_L2모델을 사용해보자.[1]\n\n[1] 처음 실습할 땐 분명 이게 제일 높았었는데…"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#d.-예측predict",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#d.-예측predict",
    "title": "Kaggle | Autogluon",
    "section": "### D. 예측(predict)",
    "text": "### D. 예측(predict)\n\n학습 이후에 문제를 푸는 과정으로 비유.\n\n기존에 했던 분석들\n\n무조건 남자는 죽고, 여자는 사는 형식 0.7x / 0.76555\nRandomForestClassifier를 사용한 형식 0.8x / 0.77511\nRandomForestClassifier에서 하이퍼파라미터를 조정한 형식 0.8x / 0.76555 (트레인 셋에서의 분석에서는 더 높았는데 실제 결과는 오히려 더 낮았다.)\n\n4. WeightedEnsemble_L2모델 사용(알아서 사용하긴 함)\ntrain set을 일단 풀어보자(predict)\n\ntype(tr) ## 처음 보는 것으로 저장되는데 데이터프레임에서 쓸 수 있는 모든 기능들을 다 사용할 수 있다.\n\nautogluon.core.dataset.TabularDataset\n\n\n\ntr.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.8810325476992144\n\n\n\n정확도가 0.9349나 된다. 상당히 기대가 되는 부분\n\n\npredictr.predict(tst)\n\n0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n413    0\n414    1\n415    0\n416    0\n417    0\nName: Survived, Length: 418, dtype: int64\n\n\n\ntst.assign(Survived = predictr.predict(tst)).loc[:, ['PassengerId', 'Survived']]\\\n.to_csv('autogluon_submission.csv', index = False)\n\n\n제출 결과 정확도는 0.78947로 지금껏 가장 높은 수치가 나왔다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#개선",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#개선",
    "title": "Kaggle | Autogluon",
    "section": "3. 개선",
    "text": "3. 개선\n\n결과를 좀 더 개선할 수 있지 않을까?\n\n\nA. Fsize로 feature engeenering\n\n1) 데이터\n\ntr = TabularDataset('./data/train.csv')  ## 학습할 데이터\ntst = TabularDataset('./data/test.csv')\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./data/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n-피쳐 엔지니어링\n\ntr.assign(Fsize = tr.SibSp + tr.Parch)\ntst.assign(Fsize = tst.SibSp + tst.Parch)\n\n#tr.eval('Fsize = SibSp + Parch')\n#tst.eval('Fsize = SibSp + Parch')\n\ntr.head()  ## 원본 데이터를 손상시키지 않음, Fsize 열이 추가되지 않은 것을 알 수 있음\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n2) Predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132447\"\n\n\n3) 적합(fit)\n\npredictr.fit(tr.assign(Fsize = tr.SibSp + tr.Parch))  ## 새로운 데이터셋을 추가하여 학습\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132447\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.59 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1923.41 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.3s = Fit runtime\n    12 features in original data used to generate 25 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.37s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6425   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8268   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8492   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.7989   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    6.8s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8045   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    2.76s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.68s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    9.58s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.838    = Validation score   (accuracy)\n    0.83s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8492   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 25.25s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132447\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d8e852770&gt;\n\n\n-리더보드 확인\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0              LightGBM   0.849162       0.010999  0.554687                0.010999           0.554687            1       True          4\n1   WeightedEnsemble_L2   0.849162       0.012000  1.201853                0.001000           0.647166            2       True         14\n2         LightGBMLarge   0.837989       0.005001  0.827996                0.005001           0.827996            1       True         13\n3       NeuralNetFastAI   0.832402       0.024005  2.761039                0.024005           2.761039            1       True         10\n4        NeuralNetTorch   0.832402       0.034000  9.577353                0.034000           9.577353            1       True         12\n5            LightGBMXT   0.826816       0.004981  0.426716                0.004981           0.426716            1       True          3\n6              CatBoost   0.826816       0.005996  6.798872                0.005996           6.798872            1       True          7\n7               XGBoost   0.821229       0.008010  0.680577                0.008010           0.680577            1       True         11\n8      RandomForestEntr   0.815642       0.063459  0.504724                0.063459           0.504724            1       True          6\n9        ExtraTreesEntr   0.804469       0.062692  0.443597                0.062692           0.443597            1       True          9\n10       ExtraTreesGini   0.804469       0.065061  0.448723                0.065061           0.448723            1       True          8\n11     RandomForestGini   0.798883       0.064575  0.510397                0.064575           0.510397            1       True          5\n12       KNeighborsUnif   0.648045       0.007998  0.008998                0.007998           0.008998            1       True          1\n13       KNeighborsDist   0.642458       0.007999  0.011001                0.007999           0.011001            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBM\n0.849162\n0.010999\n0.554687\n0.010999\n0.554687\n1\nTrue\n4\n\n\n1\nWeightedEnsemble_L2\n0.849162\n0.012000\n1.201853\n0.001000\n0.647166\n2\nTrue\n14\n\n\n2\nLightGBMLarge\n0.837989\n0.005001\n0.827996\n0.005001\n0.827996\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.024005\n2.761039\n0.024005\n2.761039\n1\nTrue\n10\n\n\n4\nNeuralNetTorch\n0.832402\n0.034000\n9.577353\n0.034000\n9.577353\n1\nTrue\n12\n\n\n5\nLightGBMXT\n0.826816\n0.004981\n0.426716\n0.004981\n0.426716\n1\nTrue\n3\n\n\n6\nCatBoost\n0.826816\n0.005996\n6.798872\n0.005996\n6.798872\n1\nTrue\n7\n\n\n7\nXGBoost\n0.821229\n0.008010\n0.680577\n0.008010\n0.680577\n1\nTrue\n11\n\n\n8\nRandomForestEntr\n0.815642\n0.063459\n0.504724\n0.063459\n0.504724\n1\nTrue\n6\n\n\n9\nExtraTreesEntr\n0.804469\n0.062692\n0.443597\n0.062692\n0.443597\n1\nTrue\n9\n\n\n10\nExtraTreesGini\n0.804469\n0.065061\n0.448723\n0.065061\n0.448723\n1\nTrue\n8\n\n\n11\nRandomForestGini\n0.798883\n0.064575\n0.510397\n0.064575\n0.510397\n1\nTrue\n5\n\n\n12\nKNeighborsUnif\n0.648045\n0.007998\n0.008998\n0.007998\n0.008998\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.642458\n0.007999\n0.011001\n0.007999\n0.011001\n1\nTrue\n2\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(tr.Survived == predictr.predict(tr.assign(Fsize = tr.SibSp + tr.Parch))).mean()\n\n0.9696969696969697\n\n\n\ntst.assign(Survived = predictr.predict(tst.assign(Fsize = tst.SibSp + tst.Parch))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)\n\n\n제출 결과 : 점수가 오히려 더 낮아졌음\n\n더 개선해보자"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#b.-fsize-drop",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#b.-fsize-drop",
    "title": "Kaggle | Autogluon",
    "section": "### B. Fsize + drop",
    "text": "### B. Fsize + drop\n1) data\n-피처 엔지니어링 (데이터 불러오는 건 위에서 했으니 일단 생략\n\n_tr = tr.assign(Fsize = lambda _df : _df.SibSp + _df.Parch).drop(['SibSp','Parch'],axis=1)\n_tst = tst.assign(Fsize = tst.SibSp + tst.Parch).drop(['SibSp','Parch'],axis=1)\n\n_tr.head()\n## df.drop(columns = [])\n## df.drop([], axis = 1) columns라고 지정해주지 않으면 디폴트로 행을 삭제하기 때문에\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\nA/5 21171\n7.2500\nNaN\nS\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n2) Predictor 생성\n\npredictr = TabularPredictor('Survived')\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132627\"\n\n\n3) 적합(fit)\n\npredictr.fit(_tr)\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132627\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.56 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1899.15 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.3s = Fit runtime\n    10 features in original data used to generate 23 features in processed data.\n    Train Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.36s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8212   = Validation score   (accuracy)\n    0.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.838    = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.8045   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8324   = Validation score   (accuracy)\n    7.6s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.7989   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8045   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.95s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8436   = Validation score   (accuracy)\n    10.87s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.82s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8492   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 26.66s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132627\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d858cd060&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.849162       0.043977  12.163954                0.000984           0.653803            2       True         14\n1        NeuralNetTorch   0.843575       0.032010  10.867509                0.032010          10.867509            1       True         12\n2              LightGBM   0.837989       0.010982   0.642641                0.010982           0.642641            1       True          4\n3         LightGBMLarge   0.832402       0.006009   0.821788                0.006009           0.821788            1       True         13\n4              CatBoost   0.832402       0.006051   7.597862                0.006051           7.597862            1       True          7\n5               XGBoost   0.826816       0.013022   0.450137                0.013022           0.450137            1       True         11\n6       NeuralNetFastAI   0.826816       0.017003   1.949074                0.017003           1.949074            1       True         10\n7            LightGBMXT   0.821229       0.006997   0.471555                0.006997           0.471555            1       True          3\n8      RandomForestEntr   0.810056       0.063482   0.526611                0.063482           0.526611            1       True          6\n9      RandomForestGini   0.804469       0.061717   0.544051                0.061717           0.544051            1       True          5\n10       ExtraTreesEntr   0.804469       0.064033   0.519959                0.064033           0.519959            1       True          9\n11       ExtraTreesGini   0.798883       0.064803   0.533057                0.064803           0.533057            1       True          8\n12       KNeighborsUnif   0.653631       0.006997   0.011003                0.006997           0.011003            1       True          1\n13       KNeighborsDist   0.648045       0.031997   0.016009                0.031997           0.016009            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.849162\n0.043977\n12.163954\n0.000984\n0.653803\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.843575\n0.032010\n10.867509\n0.032010\n10.867509\n1\nTrue\n12\n\n\n2\nLightGBM\n0.837989\n0.010982\n0.642641\n0.010982\n0.642641\n1\nTrue\n4\n\n\n3\nLightGBMLarge\n0.832402\n0.006009\n0.821788\n0.006009\n0.821788\n1\nTrue\n13\n\n\n4\nCatBoost\n0.832402\n0.006051\n7.597862\n0.006051\n7.597862\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.013022\n0.450137\n0.013022\n0.450137\n1\nTrue\n11\n\n\n6\nNeuralNetFastAI\n0.826816\n0.017003\n1.949074\n0.017003\n1.949074\n1\nTrue\n10\n\n\n7\nLightGBMXT\n0.821229\n0.006997\n0.471555\n0.006997\n0.471555\n1\nTrue\n3\n\n\n8\nRandomForestEntr\n0.810056\n0.063482\n0.526611\n0.063482\n0.526611\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.804469\n0.061717\n0.544051\n0.061717\n0.544051\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.804469\n0.064033\n0.519959\n0.064033\n0.519959\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.798883\n0.064803\n0.533057\n0.064803\n0.533057\n1\nTrue\n8\n\n\n12\nKNeighborsUnif\n0.653631\n0.006997\n0.011003\n0.006997\n0.011003\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.648045\n0.031997\n0.016009\n0.031997\n0.016009\n1\nTrue\n2\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(_tr.Survived == predictr.predict(_tr)).mean()\n\n0.9472502805836139\n\n\n\npredictr.predict(_tr)\n\n0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n886    0\n887    1\n888    0\n889    1\n890    0\nName: Survived, Length: 891, dtype: int64\n\n\n\n_tst.assign(Survived = predictr.predict(_tst)).loc[:, ['PassengerId', 'Survived']]\\\n.to_csv('autogluon(Fsize,Drop)_submission.csv', index = False)\n\n\n지금껏 가장 높은 결과가 나왔다!\n\n\n다중 공선성 문제를 개선한 결과라고 볼 수 있지… 음음.\n\n아니, 모자라. 더 개선해!!!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#c.-best_quality",
    "href": "posts/Machine Learning in Practice/Titanic/3. autogluon.html#c.-best_quality",
    "title": "Kaggle | Autogluon",
    "section": "### C. best_quality",
    "text": "### C. best_quality\n1) data\n\ntr = TabularDataset(\"./data/train.csv\")\ntst = TabularDataset(\"./data/test.csv\")\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./data/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n2) predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132948\"\n\n\n3) 적합(fit)\n\n어떤 자원이 들어가든, 전부 지원해줄 테니 가장 좋은 퀄리티로 산출해!!\n\n\npredictr.fit(tr, presets = 'best_quality') \n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132948\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.53 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1996.57 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.4s = Fit runtime\n    11 features in original data used to generate 24 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.39s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6296   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6352   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    3.82s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8373   = Validation score   (accuracy)\n    5.36s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8339   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8305   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8552   = Validation score   (accuracy)\n    72.17s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8238   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8316   = Validation score   (accuracy)\n    0.49s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\nNo improvement since epoch 7: early stopping\nNo improvement since epoch 6: early stopping\nNo improvement since epoch 7: early stopping\n    0.853    = Validation score   (accuracy)\n    20.42s   = Training   runtime\n    0.13s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8373   = Validation score   (accuracy)\n    3.6s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8462   = Validation score   (accuracy)\n    68.5s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8429   = Validation score   (accuracy)\n    8.68s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8552   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 188.35s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132948\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d90435000&gt;\n\n\n\n대신 시간이 상당히 오래 걸린다…\n\n- 리더보드 확인\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.855219       0.036927  72.167391                0.036927          72.167391            1       True          7\n1       WeightedEnsemble_L2   0.855219       0.038929  73.009209                0.002002           0.841818            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.130997  20.415231                0.130997          20.415231            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.194014  68.497755                0.194014          68.497755            1       True         12\n4      LightGBMLarge_BAG_L1   0.842873       0.056998   8.680638                0.056998           8.680638            1       True         13\n5            XGBoost_BAG_L1   0.837262       0.055978   3.598592                0.055978           3.598592            1       True         11\n6           LightGBM_BAG_L1   0.837262       0.061885   5.357185                0.061885           5.357185            1       True          4\n7         LightGBMXT_BAG_L1   0.835017       0.049997   3.816595                0.049997           3.816595            1       True          3\n8   RandomForestGini_BAG_L1   0.833895       0.096996   0.553528                0.096996           0.553528            1       True          5\n9     ExtraTreesEntr_BAG_L1   0.831650       0.095051   0.494969                0.095051           0.494969            1       True          9\n10  RandomForestEntr_BAG_L1   0.830527       0.101071   0.535026                0.101071           0.535026            1       True          6\n11    ExtraTreesGini_BAG_L1   0.823793       0.111044   0.513969                0.111044           0.513969            1       True          8\n12    KNeighborsDist_BAG_L1   0.635241       0.004996   0.006006                0.004996           0.006006            1       True          2\n13    KNeighborsUnif_BAG_L1   0.629630       0.015998   0.005992                0.015998           0.005992            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.855219\n0.036927\n72.167391\n0.036927\n72.167391\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.855219\n0.038929\n73.009209\n0.002002\n0.841818\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.130997\n20.415231\n0.130997\n20.415231\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.194014\n68.497755\n0.194014\n68.497755\n1\nTrue\n12\n\n\n4\nLightGBMLarge_BAG_L1\n0.842873\n0.056998\n8.680638\n0.056998\n8.680638\n1\nTrue\n13\n\n\n5\nXGBoost_BAG_L1\n0.837262\n0.055978\n3.598592\n0.055978\n3.598592\n1\nTrue\n11\n\n\n6\nLightGBM_BAG_L1\n0.837262\n0.061885\n5.357185\n0.061885\n5.357185\n1\nTrue\n4\n\n\n7\nLightGBMXT_BAG_L1\n0.835017\n0.049997\n3.816595\n0.049997\n3.816595\n1\nTrue\n3\n\n\n8\nRandomForestGini_BAG_L1\n0.833895\n0.096996\n0.553528\n0.096996\n0.553528\n1\nTrue\n5\n\n\n9\nExtraTreesEntr_BAG_L1\n0.831650\n0.095051\n0.494969\n0.095051\n0.494969\n1\nTrue\n9\n\n\n10\nRandomForestEntr_BAG_L1\n0.830527\n0.101071\n0.535026\n0.101071\n0.535026\n1\nTrue\n6\n\n\n11\nExtraTreesGini_BAG_L1\n0.823793\n0.111044\n0.513969\n0.111044\n0.513969\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.635241\n0.004996\n0.006006\n0.004996\n0.006006\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.629630\n0.015998\n0.005992\n0.015998\n0.005992\n1\nTrue\n1\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9158249158249159\n\n\n\ntst[['PassengerId']].assign(Survived = predictr.predict(tst))\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)\n\n\n하지만 결과는 확실하다. 무려 0.813…"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html",
    "title": "Kaggle | 1st practice",
    "section": "",
    "text": "Kaggle의 competition에 대해 차근차근히 알아보고 첫 제출까지 해보도록 하자."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html#라이브러리-imports",
    "title": "Kaggle | 1st practice",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\n\n\n# 캐글에 있는 노트북을 이용하면 가상 컴퓨터에 세 개의 파일들이 직접 들어온다.\n\ntr = pd.read_csv(\"./data/train.csv\")\ntr.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntst = pd.read_csv('./data/test.csv')\ntst.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html#kaggle-competition",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html#kaggle-competition",
    "title": "Kaggle | 1st practice",
    "section": "2. Kaggle Competition",
    "text": "2. Kaggle Competition\n\nA. 데이터 구경\n\n- 데이터의 설명을 빠르게 파악하는 방법\n1. 변수 위주로 kaggle 홈페이지에서 파악\n1. 구글 번역기 사용\n1. ChatGPT 이용\n\nChatGPT가 옳지 않은 소리를 할 때도 있지만, 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수 이름이 약어로 된 경우가 많은데, 이럴 경우 GPT가 유용하다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html#b.-메뉴",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html#b.-메뉴",
    "title": "Kaggle | 1st practice",
    "section": "### B. 메뉴",
    "text": "### B. 메뉴\n\nOverview(개요)\n\n\n경진대회 주최자가 경진 대회의 배경, 목표, 데이터셋 설명 등을 기술.\n\n\nData(데이터)\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 포함됨.\n\n\nCode(코드)\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 포함됨.\n\n\nDiscussion(토론)\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이뤄짐.\n\n\nLeaderboard(리더보드)\n\n\n경진대회 참가자들의 모델에 대한 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRule(규칙)\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 작업, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들 * Overview : martic, prize, timeline * Rules : matric, 외부데이터 사용 가능 여부, 하루 최대 제출 수, 최종 선택 가능한 솔루션 수(limit)\n- 대회의 유형 * Getting Started : 상을 제공하지 않음. 튜토리얼 전용. * Featured : 가장 일반적인 유형, 스폰서 회사의 비즈니스 관련 문제이므로 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야 하고 발표할 것을 요구받을 수 있음. * Analytics : 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html#데이터-분석",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html#데이터-분석",
    "title": "Kaggle | 1st practice",
    "section": "3. 데이터 분석",
    "text": "3. 데이터 분석\n\nA.test\n\n- 제출 결과는 리더보드에서 확인 가능\n- 답을 알 수 없고 제출해야 스코어만 확인할 수 있음"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html#b.-train---스스로-풀어보고-채점할-수-있음",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html#b.-train---스스로-풀어보고-채점할-수-있음",
    "title": "Kaggle | 1st practice",
    "section": "### B. train - 스스로 풀어보고 채점할 수 있음",
    "text": "### B. train - 스스로 풀어보고 채점할 수 있음\n- train 데이터를 채점해보자.\n# accuracy의 계산\n\ndf = pd.DataFrame({'surv' : [1,0,1,1,0], 'sex' : ['f','m','f','m','m']})\n\n- surv+열과 sex열에서 sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\ndf.surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: surv, dtype: int64\n\n\n\ndf.sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: sex, dtype: object\n\n\n\n(df.sex == 'f')*1  ## bool이 원소인 list에 1을 곱해준다. f이면 1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: sex, dtype: int32\n\n\n- 결과를 정리하면 아래와 같다.\n\npd.DataFrame({'real' : df.surv, 'estimate' : (df.sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n\nprint((df.surv == (df.sex == 'f')*1).sum()/5)\n##print((df.surv == (df.sex == 'f')*1).mean()) ## 동일한 코드\n\n0.8\n\n\n- 실제 train 자료에 접목해서 여성만 생존한다고 하여 accuracy를 구해보자.\n\n(tr.Survived == (tr.Sex == 'female')*1).mean()\n\n0.7867564534231201\n\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()  ## True or False는 0, 1로도 구분되나보다.\n\n0.7867564534231201\n\n\n- 그러면 예측한 데이터프레임을 파일로 만들어서 보내보자.\n\ntst[['PassengerId']].assign(Survived = (tst.Sex == 'female')*1)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\ntst[['PassengerId']].assign(Survived = (tst.Sex == 'female')*1).to_csv(\"gender_submission.csv\", index = False)\n\n\n해당 파일을 캐글에 업로드하면 submission이 완료된다.\n\n\nindex를 날려줘야 원하는 형식이 된다. (index = False를 하지 않으면 csv파일에 index의 숫자가 같이 저장된다…)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/1. practice.html#개념",
    "href": "posts/Machine Learning in Practice/Titanic/1. practice.html#개념",
    "title": "Kaggle | 1st practice",
    "section": "4. 개념",
    "text": "4. 개념\n- 캐글 대회는 시험과 비슷하다. * 캐글대회를 여는 사람은 보통 (1) 모의고사문제+답 (training set) (2) 실제시험문제 (test set)를 준다. * (1)의 자료에서는 문제(X,독립변수,설명변수)와 답(y,종속변수,반응변수)이 함께 주어진다. * (2)의 자료에서는 문제(X,독립변수,설명변수)만 주어진다. * 우리는 (1)을 이용하여 문제(X,독립변수,설명변수)와 답(y,종속변수,반응변수)사이의 관계를 찾아내는 훈련을 한다. * 그리고 그 훈련이 잘 되었는지를 평가하기 위해서 (2)를 풀어보고 그 결과를 제출한다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html",
    "title": "선형모형의 적",
    "section": "",
    "text": "우리의 주적은 북한인데, 선형모형의 적은?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#라이브러리-imports",
    "title": "선형모형의 적",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing\nimport sklearn.impute\nimport seaborn as sns\nimport sklearn.tree"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#선형모형의-적",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#선형모형의-적",
    "title": "선형모형의 적",
    "section": "2. 선형모형의 적",
    "text": "2. 선형모형의 적\n\nA. 결측치의 존재\n\n문제 : 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않는다.\n- 해결방안\n1 : 결측치를 제거\n\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n둘을 혼합\n\n\n\n결측치를 impute\n\n\n\ntrain에서는 fit_transform, test에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값으로 일괄 impute\ninterploation(이미지 또는 시계열, 근처의 값과 자연스럽게 연동되도록 만들 수 있음)\n~train, test data를 합쳐서 fit_transform~ 이건 정보누수로 실격사유가 된다\n\n\n- 사용 가능한 코드나 모듈\n\nisna(), dropna(), sklearn.inpute의 하위 모듈 등."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#b.-다중공선성의-존재",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#b.-다중공선성의-존재",
    "title": "선형모형의 적",
    "section": "### B. 다중공선성의 존재",
    "text": "### B. 다중공선성의 존재\n문제 : 데이터의 설명변수가 역할이 겹칠 경우, 선형모형의 일반화 성능이 좋지 않음.\n- 해결방안\n\n\n변수 제거 &gt; 설명변수 간 corr을 파악하고, 느낌적으로 제거 &gt; &gt; PCA 등 차원축소기법을 이용한 제거\n공선성을 가지는 변수를 모아 새로운 변수로 변환 &gt; 느낌적으로 변환 &gt; &gt; PCA를 이용한 변환\nLasso, Ridge 등 패널티 계열을 사용 &gt; Lasso : l1 / liblinear &gt; &gt; Ridge : l2 &gt; &gt; Elastic net\n\n\n- corr파악 후 느낌적으로 제거의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\n\npandas의 데이터프레임에는 자체적으로 해당 메소드를 지원한다.\n\n\nsns.heatmap(X.corr(), annot = True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n- toeic과 유사 toeic끼리 상관성이 짙네?\n\n제거한다.\n\n\nC. 관련이 없는 변수의 존재\n\n문제 : 데이터에서 불필요한 설명변수가 너무 많을 경우, 선형모형의 일반화 성능이 좋지 않음.(overfitting)\n\n예시 : 고객이름, ID, Index 관련 변수(물론 얘네들도 어딘가 쓸모가 있을 수도 있다…)\n\n- 해결방법\n\n\n변수 제거 &gt; (y, X)의 corr을 파악하고 느낌적으로 제거(위에서와 달리 관련이 있어야 한다.) &gt; &gt; PCA를 이용한 제거 &gt; &gt; Lasso를 이용한 제거(여기서 Ridge는 사용하면 안된다. 해당 모듈은 유사한 것들의 계수 합이 일정하도록 조정하는 거니까…)\n더 많은 데이터를 확보 &gt; 하지만 이는 어렵다… 어떤 변수가 관련이 없다는 것을 파악하기 위해선 데이터를 많이 가져와야 하는데, Feature의 수가 많아질 때 필요한 데이터의 수는 지수적으로 증가한다.\n\n\n- 느낌적으로 제거 예시\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(), annot = True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nbalance0 ~ 2는 employment와의 상관성이 낮다. 따라서 제거하고 분석한다.\n\n\n## 1\nX = df_train.loc[:, :'gpa']\ny = df_train.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.882\n\n\n- Lasso를 이용한 제거 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n로지스틱 선형 회귀가 필요한 경우이다. 로지스틱 또한 penalty 계열 분석을 할 수 있다.\n\n\n## 1\nX = df_train.drop('employment', axis = 1)\ny = df_train.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(Cs = [0.1, 1, 10, 100], penalty = 'l1', solver = 'liblinear', random_state = 42)\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.876\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))  ## 시리즈의 경우 1차원의 입력값만 받는다.\ns.index = X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64\n\n\n\n위에서 쓸모없는 것을 제거하고 분석한 것에 비해 점수가 낮지만, 쓸모없는 것이라는 사실을 모르는 상황에서는 Lasso가 상당히 괜찮다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#d.-이상치의-존재",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#d.-이상치의-존재",
    "title": "선형모형의 적",
    "section": "### D. 이상치의 존재",
    "text": "### D. 이상치의 존재\n문제 : 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 해결방법\n\n\n이상치를 제거하고 분석 &gt; 느낌적으로 제거 &gt; &gt; 이상치를 감지하는 지표를 사용하여 제거 &gt; &gt; 이상치를 자동으로 감지하는 모형 사용하여 이상치 제거 후 분석\n로버스트 선형회귀 계열을 이용 &gt; 이상치에 큰 영향을 받지 않음 &gt; sklearn.linear_model.HuberRegressor 등\n이상치를 완화시키는 변환을 사용 &gt; sklearn.preprocessing.PowerTransformer를 이용\n\n\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\n\ntransformr.fit_transform(df_train)\n\narray([[-1.40729341,  2.42405408],\n       [-1.31406689, -0.18677452],\n       [-1.13030154,  0.16485704],\n       [-0.50278108,  0.17667635],\n       [-0.02130412,  0.41617603],\n       [ 0.13926015,  0.55696978],\n       [ 0.81742569, -1.03040835],\n       [ 0.96759638, -0.62032873],\n       [ 0.96759638, -1.33362249],\n       [ 1.48386844, -0.56759919]])\n\n\n\nx, y = transformr.fit_transform(df_train).T\nx, y\n\n(array([-1.40729341, -1.31406689, -1.13030154, -0.50278108, -0.02130412,\n         0.13926015,  0.81742569,  0.96759638,  0.96759638,  1.48386844]),\n array([ 2.42405408, -0.18677452,  0.16485704,  0.17667635,  0.41617603,\n         0.55696978, -1.03040835, -0.62032873, -1.33362249, -0.56759919]))\n\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o', label = 'before')\nplt.plot(x, y, 'o', label = 'after')\nplt.xlabel('temp')\nplt.ylabel('ice sales')\nplt.legend()\nplt.show()\n\n\n\n\n\n강제로 정규화한 모습이다.\n\n\ntransformr.inverse_transform(transformr.fit_transform(df_train))\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n  warnings.warn(\n\n\narray([[-4.1       , 50.        ],\n       [-3.7       ,  9.2341745 ],\n       [-3.        ,  9.64277825],\n       [-1.3       ,  9.65789368],\n       [-0.5       ,  9.98778744],\n       [-0.3       , 10.20595116],\n       [ 0.3       ,  8.48692458],\n       [ 0.4       ,  8.81722682],\n       [ 0.4       ,  8.27315516],\n       [ 0.7       ,  8.8637837 ]])\n\n\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\n어차피 역변환 할 수 있는 것은 상관이 없다.\n\n\nE. 교호작용의 존재\n\n문제 : 설명 변수 간의 상호작용이 있는 경우, 이를 고려하지 않으면 데이터를 잘 설명하지 못할 수 있음.\n- 해결방안\n\n\n교호작용이 있는 열들의 값끼리 곱함\n교호작용에 영향을 받지 않는 모델 사용 &gt; sklearn.tree.DecisionTreeRegressor()\n\n\n- 교호작용이 있는 열을 곱함\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n둘 다 했을 때 가장 평균이 높고, 각각 하는 것만으로는 그렇게 큰 영향은 없는 것 같다.\n\n교호작용을 고려하지 않은 분석\n\n## 1\nX = df_train.drop('Weight_Loss', axis = 1)\ny = df_train.Weight_Loss\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.8208414124769222\n\n\n\npredictr.coef_\n\narray([5.21904037, 9.74766346])\n\n\n\n보충제는 5kg, 운동은 10kg의 감량효과가 있다고 추정하고 있다.\n\n\ndf_train.assign(Weight_Loss_hat = predictr.predict(X)).drop('Weight_Loss', axis = 1)\\\n.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss_hat', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n예측값과 실제 값의 차이가 크다.\n\n교호작용을 고려한 분석\n\ndf_train.assign(Interaction = df_train.Supplement * df_train.Exercise)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nInteraction\n\n\n\n\n0\nFalse\nFalse\n-0.877103\nFalse\n\n\n1\nTrue\nFalse\n1.604542\nFalse\n\n\n2\nTrue\nTrue\n13.824148\nTrue\n\n\n3\nTrue\nTrue\n13.004505\nTrue\n\n\n4\nTrue\nTrue\n13.701128\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\nFalse\n\n\n9996\nFalse\nFalse\n-0.217816\nFalse\n\n\n9997\nFalse\nTrue\n4.072701\nFalse\n\n\n9998\nTrue\nFalse\n-0.253796\nFalse\n\n\n9999\nFalse\nFalse\n-1.399092\nFalse\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\n## 1\n_df = df_train.assign(Interaction = df_train.Supplement * df_train.Exercise)\nX = _df.drop('Weight_Loss', axis = 1)\ny = _df.Weight_Loss\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9727754257714795\n\n\n\n정확도가 개선되었다.\n\n\ndf_train.assign(Weight_Loss_hat = predictr.predict(X)).drop('Weight_Loss', axis = 1)\\\n.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss_hat', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n평균을 보면(오차를 제거함) 표본과 동일한 것을 볼 수 있다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#교호작용",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#교호작용",
    "title": "선형모형의 적",
    "section": "3. 교호작용",
    "text": "3. 교호작용"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#a.-아이스크림-타입-별-판매량",
    "href": "posts/Machine Learning in Practice/practice/8. 선형모형의 적.html#a.-아이스크림-타입-별-판매량",
    "title": "선형모형의 적",
    "section": "### A. 아이스크림 타입 별 판매량",
    "text": "### A. 아이스크림 타입 별 판매량\n- 왠지 익숙한 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nset(df_train.type)\n\n{'choco', 'vanilla'}\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n- 아이스크림의 종류에 따라 온도가 판매량에 미치는 정도가 다를 것으로 예상된다.\n\n아이스크림 종류와 온도간에 교호작용이 있다.\n\n교호작용을 고려하지 않은 경우\n\n## 1\nX = pd.get_dummies(df_train.drop('sales', axis = 1))\ny = df_train.sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9249530603100549\n\n\n\n이것만으로도 나름 높은 점수가 나오긴 했지만… 언제나 개선할 수 있는 건 개선해야 한다.\n\n\n_df = df_train.assign(sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, _df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, _df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n마음속의 언더라잉과 맞지 않는다 : 언더피팅된 상황이다.\n\n교호작용을 고려\n\nvan = pd.get_dummies(df_train.type, drop_first = True)*1\nvan\n\n\n\n\n\n\n\n\nvanilla\n\n\n\n\n0\n0\n\n\n1\n0\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n...\n...\n\n\n95\n1\n\n\n96\n1\n\n\n97\n1\n\n\n98\n1\n\n\n99\n1\n\n\n\n\n200 rows × 1 columns\n\n\n\n\n_df = df_train.assign(Interaction = df_train.temp * van.vanilla)\n_df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nInteraction\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n-0.0\n\n\n1\n-3.7\n35.852524\nchoco\n-0.0\n\n\n2\n-3.0\n37.428335\nchoco\n-0.0\n\n\n3\n-1.3\n38.323681\nchoco\n-0.0\n\n\n4\n-0.5\n39.713362\nchoco\n-0.0\n\n\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n12.4\n\n\n96\n13.4\n129.300464\nvanilla\n13.4\n\n\n97\n14.7\n136.596568\nvanilla\n14.7\n\n\n98\n15.0\n136.213140\nvanilla\n15.0\n\n\n99\n15.2\n135.595252\nvanilla\n15.2\n\n\n\n\n200 rows × 4 columns\n\n\n\n\n## 1\nX = pd.get_dummies(_df.drop('sales', axis = 1), drop_first = True)\ny = _df.sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9865793819066231\n\n\n\n점수가 훨씬 높게 나왔다.\n\n\n__df = _df.assign(sales_hat = predictr.predict(X))\n__df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nInteraction\nsales_hat\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n-0.0\n31.403121\n\n\n1\n-3.7\n35.852524\nchoco\n-0.0\n32.209366\n\n\n2\n-3.0\n37.428335\nchoco\n-0.0\n33.620295\n\n\n3\n-1.3\n38.323681\nchoco\n-0.0\n37.046835\n\n\n4\n-0.5\n39.713362\nchoco\n-0.0\n38.659325\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n12.4\n122.492017\n\n\n96\n13.4\n129.300464\nvanilla\n13.4\n127.521196\n\n\n97\n14.7\n136.596568\nvanilla\n14.7\n134.059129\n\n\n98\n15.0\n136.213140\nvanilla\n15.0\n135.567883\n\n\n99\n15.2\n135.595252\nvanilla\n15.2\n136.573719\n\n\n\n\n200 rows × 5 columns\n\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, __df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, __df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\npredictr.coef_\n\narray([ 2.01561216,  3.01356716, 20.46306209])\n\n\n\n모델이 언더라잉을 잘 따라가는 것을 볼 수 있다.(애초에 계수가 세개가 됨…)\n\n\nB. 교호작용, tree\n\nsklearn.tree.DecisionTreeRegressor()를 사용하면 교호작용을 손쉽게 적합할 수 있다.\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n아까와 동일한 자료를 tree로 분석해보자…\n\n\n## 1\nX = pd.get_dummies(df_train.drop('sales', axis = 1), drop_first = True)\ny = df_train.sales\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9963887702553287\n\n\n\n높은 스코어가 나온다.(오버피팅된 것은 아닐까?)\n\n\n_df = df_train.assign(sales_hat = predictr.predict(X))\n_df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nsales_hat\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n32.950261\n\n\n1\n-3.7\n35.852524\nchoco\n35.852524\n\n\n2\n-3.0\n37.428335\nchoco\n37.428335\n\n\n3\n-1.3\n38.323681\nchoco\n38.323681\n\n\n4\n-0.5\n39.713362\nchoco\n39.713362\n\n\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n119.708075\n\n\n96\n13.4\n129.300464\nvanilla\n129.300464\n\n\n97\n14.7\n136.596568\nvanilla\n136.596568\n\n\n98\n15.0\n136.213140\nvanilla\n136.213140\n\n\n99\n15.2\n135.595252\nvanilla\n135.595252\n\n\n\n\n200 rows × 4 columns\n\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco', alpha = 0.5)\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla', alpha = 0.5)\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, _df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, _df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n오차항까지 적합하고는 있으나… 처음 교호작용을 고려하지 않은 모델보다 성능은 좋은 것 같다. 따라서 이는 상당히 유용하다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html",
    "title": "오버피팅, 다중공선성",
    "section": "",
    "text": "오버피팅은 뭐고, 다중공선성은 왜 발생할까? 그리고 해결은 어떻게 할까?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#라이브러리-imports",
    "title": "오버피팅, 다중공선성",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport sklearn"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#언더라잉과-오차항",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#언더라잉과-오차항",
    "title": "오버피팅, 다중공선성",
    "section": "2. 언더라잉과 오차항",
    "text": "2. 언더라잉과 오차항\n- 만약 내가 원한다면, 관련이 있든 없든 무수히 많은 데이터를 모을 수 있다고 가정하자…\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\ndf_balance = pd.DataFrame((np.random.randn(500,5000)&gt;0.5).reshape(500,5000)*1,columns = ['X'+str(i) for i in range(5000)])\ndf_merged = pd.concat([df,df_balance],axis=1)\ndf_merged\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nX0\nX1\nX2\nX3\nX4\nX5\nX6\n...\nX4990\nX4991\nX4992\nX4993\nX4994\nX4995\nX4996\nX4997\nX4998\nX4999\n\n\n\n\n0\n135\n0.051535\n0\n1\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n1\n1\n1\n1\n1\n\n\n1\n935\n0.355496\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n485\n2.228435\n0\n1\n0\n1\n0\n0\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n1\n0\n1\n\n\n3\n65\n1.179701\n0\n1\n1\n0\n0\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n4\n445\n3.962356\n1\n0\n0\n0\n0\n0\n1\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0\n0\n0\n1\n1\n1\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n496\n310\n2.601212\n1\n0\n1\n1\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n497\n225\n0.042323\n0\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n498\n320\n1.041416\n0\n1\n0\n0\n0\n1\n0\n1\n...\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n499\n375\n3.626883\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n\n\n500 rows × 5003 columns\n\n\n\n\nemployment의 예측과 상관이 없을 개인의 선호, balance_game을 가져왔다. (5000종류)\n\n\n## df_train, df_test =  sklearn.model_selection.train_test_split(test_size = 0.2) 이걸로 해도 된다.\n## step 1\nX = df_merged.drop(['employment'], axis = 1)[:400]\nXX = df_merged.drop(['employment'], axis = 1)[400:]\ny = df_merged.employment[:400]\nyy = df_merged.employment[400:]\n\n## step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n(1.0, 0.79)\n\n\n\n쓸모없는 변수(y와 상관관계가 낮은 변수)를 사용해서 오버피팅되었다. train score가 상당히 높게 나왔다.(오차항까지 예측한 상황)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n\n# 1\nX = df.drop(['employment'], axis = 1)[:400]\nXX = df.drop(['employment'], axis = 1)[400:]\ny = df[['employment']][:400]\nyy = df[['employment']][400:]\n\n# 2\nprdtr = sklearn.linear_model.LogisticRegression()\n\n# 3\nprdtr.fit(X,y)\n\n# 4\nprdtr.score(X,y), prdtr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n(0.8925, 0.83)\n\n\n\ntest 데이터에서 스코어가 더 높았다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#다중공선성",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#다중공선성",
    "title": "오버피팅, 다중공선성",
    "section": "3. 다중공선성",
    "text": "3. 다중공선성\n- 아래와 같은 가짜뉴스를 읽어보자.(ChatGPT를 이용하여 생성한 가짜뉴스)\n헤드라인: “텝스와 다른 영어 인증 시험들, 결국은 토익과 비슷한 결과를 보여준다?”\n본문:\n최근 몇 년 동안, 토익의 신뢰성에 대한 논란이 계속되어 왔습니다. 이러한 배경 속에서 텝스(TEPS), 토플(TOEFL) 등 여러 새로운 영어 능력 평가 시험이 등장하였습니다. 많은 학생들과 직장인들은 이러한 새로운 시험들이 토익보다 더 신뢰성 있고 현실적인 능력을 평가할 것이라는 기대감을 가지고 있었습니다.\n그러나 최근에 발표된 연구결과에 따르면, 텝스와 다른 영어 인증 시험들도 결국에는 토익과 매우 비슷한 성적 분포와 결과를 보여주었다고 합니다. 연구 팀은 여러 시험들간의 점수 분포와 성적의 상관관계를 분석한 결과, 대부분의 시험들이 실제 영어 능력에 대해 유사한 평가를 제공한다는 결론을 내렸습니다.\n“많은 사람들이 새로운 시험들이 더 현실적이거나 다양한 영어 능력을 평가할 것이라 기대했지만, 실제로는 모든 시험들이 비슷한 결과를 보여주었습니다.” 라며 연구 팀의 대표는 이렇게 언급하였습니다.\n이러한 연구결과는 영어 능력 평가 시험의 표준화와 신뢰성에 대한 논의를 새롭게 불러일으킬 것으로 보입니다.\n- 뉴스에 근거하여 아래의 가짜 자료를 생성했다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\nNaN\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\nNaN\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\nNaN\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\nNaN\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\nNaN\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\nNaN\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\nNaN\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\nNaN\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\nNaN\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\nNaN\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ntoeic0 ~ toeic499는 유사토익을 의미\n\n- 모르는 정보 : 사내 고용 법칙\n\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic * 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n학점 1 증가는 토익 100점 증가와 비슷하다고 고려하고 있다.\n\n\nA. 이대로 분석 | 잘못됨\n\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop(['employment_score'], axis = 1)\ny = df_train.employment_score\nXX = df_test.drop(['employment_score'], axis = 1)\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.11705078212495712)\n\n\n\n두 점수가 큰 차이가 난다.(오차항까지 적합해버린 오버피팅의 상황)\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제로는 gpa는 1, toeic은 0.01, 나머지는 0이 되어야 하지만, 많이 다르다…"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#b.-제대로-분석했다면",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#b.-제대로-분석했다면",
    "title": "오버피팅, 다중공선성",
    "section": "### B. 제대로 분석했다면?",
    "text": "### B. 제대로 분석했다면?\n- toeic과 gpa만이 유의미한 변수라는 걸 눈치챔. (아다리, 현실세계에선 일어날 수 없음)\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.loc[:, ['toeic', 'gpa']]\ny = df_train.employment_score\nXX = df_test.loc[:, ['toeic', 'gpa']]\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9133033622085311, 0.9127346436925985)\n\n\n\n스코어도 높음\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ntoeic    0.010063\ngpa      0.972163\ndtype: float64\n\n\n\n실제 계수값과 유사하도록 잘 추정됨\n\n\nC. 하다못해 toeic0와 gpa로 적합했다면???\n\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.loc[:, ['toeic0', 'gpa']]\ny = df_train.employment_score\nXX = df_test.loc[:, ['toeic0', 'gpa']]\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9120540945251211, 0.9115427614193155)\n\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ntoeic0    0.010101\ngpa       0.981302\ndtype: float64\n\n\n\n굉장히 합리적이다!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#d.-고찰",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#d.-고찰",
    "title": "오버피팅, 다중공선성",
    "section": "### D. 고찰",
    "text": "### D. 고찰\n- 의문 : 왜 변수를 더 많이 넣었는데, 정보를 더 많이 제공해줬는데, 이상한 결과가 나올까???\n\n규칙을 찾으면 안될 것 (반응변수와의 상관관계가 없는 것) 에서 규칙을 찾고 있으니까 (오차항을 적합) 잘 될리가 없지…\n\n- 쓸모없는 변수?\n\n\n진짜 쓰레기, 쓰잘데기 없는 것(X1 = 부먹/찍먹, X2 = 민초/반민초…) -&gt; 애초에 이딴걸 가지고 y를 맞출 생각도 들지 않음…\n\n\n실제론 쓸모 있는데, 대체제가 있는 경우 -&gt; 대체제를 보고 y를 맞출 것 같기도 한데, 둘은 너무 비슷함…\n\n\n- 1과 2 모두 과대적합(overfitting)을 야기하고, 2와 같은 상황에서 발생하는 문제를 다중공선성(multiple linearity)이라고 한다.\n\n1은 corr(x_1, y), corr(x_2, y)가 낮게 나온다 -&gt; y와의 관계가 없다. 2는 corr(x_1, y), corr(x_2, y)는 높게 나오는데, corr(x_1, x_2)도 높게 나온다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#다중공선성의-특징",
    "href": "posts/Machine Learning in Practice/practice/6. 오버피팅, 다중공선성.html#다중공선성의-특징",
    "title": "오버피팅, 다중공선성",
    "section": "4. 다중공선성의 특징",
    "text": "4. 다중공선성의 특징\n- 잘못된 분석을 재현하고, 계수를 해석해보자.\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n- 특이사항\n\ns.loc['toeic':].sum()\n\n0.010302732920633051\n\n\n\n비슷한 설명변수들의 회귀계수를 합하니까 0.01과 유사한 값이 나왔음…\n\n\nfig, ax = plt.subplots(3)\n\nfor i in range(3):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = i)\n    X = df_train.drop(['employment_score'], axis = 1)\n    XX = df_test.drop(['employment_score'], axis = 1)\n    y = df_train.employment_score\n    yy = df_test.employment_score\n\n    predictr = sklearn.linear_model.LinearRegression()\n\n    predictr.fit(X, y)\n\n    s = pd.Series(predictr.coef_)\n    ax[i].plot(s[1:], '-')\n    ax[i].set_title('sum of toeic coef = {}'.format(round(s[1:].sum(), 4)))\n\nfig.tight_layout()\n\n\n\n\n\n계수는 상당히 불안정하나, 그 합은 합리적인 값이 나온다.\n계수값의 해석이 용이하지 않다. 음의 계수값이 있다는 것은, 토익 유사한 시험의 점수를 올리면 취업이 오히려 안된다(…)라는 것과도 같다.\n\n\n이것의 해결은 직접 몇 개만 지우거나, 다중공선성을 해결하기 위해 패널티를 부여하는 모듈을 써서 해소 가능하다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html",
    "href": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html",
    "title": "sklearn.linear_model의 작동원리",
    "section": "",
    "text": "LogisticRegression의 작동원리와 sklearn.linear_model에 대해서 자세히 알아보자."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#라이브러리-imports",
    "title": "sklearn.linear_model의 작동원리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport itertools"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#로지스틱-회귀분석의-원리",
    "href": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#로지스틱-회귀분석의-원리",
    "title": "sklearn.linear_model의 작동원리",
    "section": "2. 로지스틱 회귀분석의 원리",
    "text": "2. 로지스틱 회귀분석의 원리\n\n저번에 봤었던 취업 자료를 가져와보자.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\nemployment를 예측하려면…\n\n## 1\nX = df.drop(['employment'], axis = 1)\ny = df.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## 3\npredictr.fit(X, y)\n\npredictr.predict(X)  ## yhat\n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], dtype=int64)\n\n\n\n\\(\\hat{y}\\)은 어떻게 나왔는가?\n- 아래 수식에 의하여 나왔음…\n\npredictr.coef_, predictr.intercept_  ## 로지스틱임에도 기울기와 절편이 있다.\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 - 8.45433334  ## yhat\nv = 1/(1+np.exp(-u))  # v : 확률같은 거\n\nv\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()  ## v가 0.5보다 클 경우 전부 True였음을 알 수 있음\n\n1.0\n\n\n해당 개체에 처리가 취해질 확률을 구하고, 그것이 0.5보다 크면 처리를 취한다.\n- 만약 적합된 v값을 알고 싶다면…\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3)\n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])\n\n\n\n우측의 값과 일치하는 것을 알 수 있다.(0번째 : 0일 확률, 1번째 : 1일 확률)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#predictor-파고들기",
    "href": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#predictor-파고들기",
    "title": "sklearn.linear_model의 작동원리",
    "section": "3. predictor 파고들기",
    "text": "3. predictor 파고들기\n\n아래와 같은 데이터를 가공해서 0~7까지는 train, 8~9까지는 test 셋으로 쓰도록 하자.\n\n\ndf = pd.DataFrame({'X':np.arange(20,30),'y':-np.arange(10)+1+np.random.randn(10)*0.1})\ndf\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n20\n0.992487\n\n\n1\n21\n-0.040013\n\n\n2\n22\n-0.984351\n\n\n3\n23\n-2.085536\n\n\n4\n24\n-3.023587\n\n\n5\n25\n-4.287162\n\n\n6\n26\n-5.085849\n\n\n7\n27\n-6.110568\n\n\n8\n28\n-6.798420\n\n\n9\n29\n-8.028488\n\n\n\n\n\n\n\n\ndf_train = df[:8]\ndf_test = df[8:]\n\ndf_train_X = df_train[['X']]\ndf_train_y = df_train[['y']]\ndf_test_X = df_test[['X']]\ndf_test_y = df_test[['y']]\n\n\n## predictor 두 개를 만들도록 리스트 컴프리헨션\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\npredictors[0].fit(df_train_X, df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n첫 번째 predictr에 적합하면 .score, .intercept_, .coef_가 해금된다. 두 번째 predictr에는 적용되지 않는다.~(당연한 거 아님?)~\n\n\nX, y에 들어갈 수 있는 형식\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr = sklearn.linear_model.LinearRegression()\n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'  ## 예외사항(error) 발생 시의 output\n\n\n가능한 형식들을 모두 모아놨다. 그럼 이것들을 가지고 어떤 녀석이 되는 지 딕셔너리 컴프리헨션을 해보자.\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n## itertools.product() : 원소들의 데카르트 곱을 리스트로 반환.\n## itertools.product('ABCD', repeat = 2)의 경우 크기가 2인 앞의 string 조합을 모두 반환\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론 | X에는 2차원 데이터만 들어올 수 있지만, y에는 1ㆍ2차원 데이터 모두가 들어올 수 있다.\n- 그리고 일반적으로, X에는 2차원 데이터 배열이 imput되기를 기대하고, y에는 1차원 데이터 배열이 imput되기를 기대한다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#첨언-데이터셋-이름-설정에-대하여",
    "href": "posts/Machine Learning in Practice/practice/4. predictor의 이해.html#첨언-데이터셋-이름-설정에-대하여",
    "title": "sklearn.linear_model의 작동원리",
    "section": "4. 첨언 | 데이터셋 이름 설정에 대하여",
    "text": "4. 첨언 | 데이터셋 이름 설정에 대하여\n\n설명변수와 반응변수, 테스트 셋과 트레인 셋을 부르는 변수 명을 어떻게 설정해야 할 지 나타내겠다.\n\nX : 설명변수 & Train\ny : 반응변수 & Train\nXX : 설명변수 & Test\nyy : 반응변수 & Test\nyhat : predictr.predict(X)\nyyhat : predictr.predict(XX)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "",
    "text": "예측해야 할 유형이 범주형일 때 사용할 수 있는 분석 기법 중 하나인 로지스틱 회귀분석을 해보자!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#라이브러리-imports",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "2. 로지스틱 회귀분석",
    "text": "2. 로지스틱 회귀분석\n- 연속형 설명변수와 범주형 반응변수와의 관계\n\nA. 성적과 취업 여부 데이터\n\n\n학점과 토익 성적, 그리고 취업 여부를 나타낸 데이터가 있다.(교수님이 만드신 페이크 데이터이다.)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf  ## 페이크 데이터입니다.\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nplt.plot(df.toeic[df.employment == 0], df.gpa[df.employment == 0], 'o')\nplt.plot(df.toeic[df.employment == 1], df.gpa[df.employment == 1], 'o')\nplt.show()\n\n\n\n\n- 뭔가 관련성을 찾을 수 있을 것 같지 않은가?~(당연하지 그렇게 만드셨으니까)~\n\n그래서 토익 성적ㆍ학점과 취업여부의 관계를 구하고 싶다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#b.-분석",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#b.-분석",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### B. 분석",
    "text": "### B. 분석\n\n# step 1\nX = pd.get_dummies(df[['toeic', 'gpa']])\ny = df.employment\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\ndf = df.assign(employment_hat = predictr.predict(X))\n\n\nsklearn.linear_model.LogisticRegression()\n\n\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemployment_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n로지스틱 회귀분석으로 적합 및 예측이 완료되었다.\n\n\nC. 평가\n\n\npredictr.score(X, y)\n\n0.882\n\n\n\n이건 y와 y_hat이 동일한 정도를 나타낸다, 나름 잘 맞춘 것 같지 않은가?\n\n- 시각화를 해야 정확히 알 수 있겠지? 현재 예측치와 기존 예측치를 비교해보자.\n\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemployment_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\ndf_filtered = df[predictr.predict(X) == 1]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,5))\n\nfig.suptitle('Constrat Pradiction and Real')\n\nax1.plot(df.toeic, df.gpa, 'o', color = 'C0', label = 'employed')\nax1.plot(df.loc[df.employment == 1].toeic, df.loc[df.employment == 1].gpa, 'o', color = 'C1', label = 'not yet employed')\nax1.set_title('Real Data')\n\nax2.plot(df.toeic, df.gpa, 'o', color = 'C0', label = 'employed')\nax2.plot(df_filtered.toeic, df_filtered.gpa, 'o', color = 'C1', label = 'not yet employed')\nax2.set_title('Estimated Data')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n어때요, 나름 합리적이지 않나요?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석의-실적용",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석의-실적용",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "3. 로지스틱 회귀분석의 실적용",
    "text": "3. 로지스틱 회귀분석의 실적용\n\n그럼 타이타닉 데이터에서 로지스틱 회귀분석을 통해 결과를 잘 예측할 수 있지 않을까요?\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/HollyRiver/Machine_learning_in_practice/main/kaggle/titanic/data/train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/HollyRiver/Machine_learning_in_practice/main/kaggle/titanic/data/test.csv')\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nkaggle 입문하기에서 보았던 타이타닉 데이터이다.\n- 여기서 반응변수를 쉽게 구하려면…\n\nset(df_train.columns) - set(df_test.columns)\n\n{'Survived'}\n\n\n- 하나만 남는 것을 볼 수 있다.\n\n아! 테스트 셋에 없는 열이니까 저게 y겠구나!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#a.-늘-해왔던-것처럼-분석하면-안된다",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#a.-늘-해왔던-것처럼-분석하면-안된다",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### A. 늘 해왔던 것처럼 분석…~(하면 안된다)~",
    "text": "### A. 늘 해왔던 것처럼 분석…~(하면 안된다)~\n\n# step 1\nX = pd.get_dummies(df_train.drop(['Survived'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(df_test)\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n\n\n오류가 나온다.\n\nInput X contains NaN.\n선형 회귀에서 설명변수의 input값에는 결측치가 있으면 안된다!!!\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\n결측치가 있는 열을 제거, 행을 제거, 둘 다. 또는 결측치를 impute해야 하는데…\n\n- 일단 Cabin 열은 결측치가 너무 많으니까 빼자!\n- Name이나 Ticket과 같은 변수는 이성적으로 봤을 때 바로 one-hot 인코딩 하기에는 어색하니 빼자!\n\nlen(set(df_train.Name)), len(set(df_train.Ticket))  ## 다 다름, 거의 다 다름\n\n(891, 681)\n\n\n- Age, Embarked에 포함된 약간의 결측치가 마음에 걸리니까 빼자!!\n\ndropna() 메소드나 preprocessing을 써도 되지만… 그건 나중에 해보자.\n\n\ndf_test.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n\n\n- Fare에 포함된 결측치도 걸린다 -&gt; 빼자! (평균으로 해주는 방법도 있는ㄷ ~나중에 하자고 좀~)\n\nB. 데이터 정리\n\n- 위에서 말한 조건들을 적용해서 데이터를 재가공한 뒤, 로지스틱 회귀를 해보자\n\ndf_train.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\n# step 1\nX = pd.get_dummies(df_train.drop(['Survived', 'Cabin', 'Name', 'Ticket', 'Age', 'Embarked', 'Fare'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(df_test.drop(['Cabin', 'Name', 'Ticket', 'Age', 'Embarked', 'Fare'], axis = 1))\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\nXX.assign(Survived = predictr.predict(XX))\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nSibSp\nParch\nSex_female\nSex_male\nSurvived\n\n\n\n\n0\n892\n3\n0\n0\nFalse\nTrue\n0\n\n\n1\n893\n3\n1\n0\nTrue\nFalse\n1\n\n\n2\n894\n2\n0\n0\nFalse\nTrue\n0\n\n\n3\n895\n3\n0\n0\nFalse\nTrue\n0\n\n\n4\n896\n3\n1\n1\nTrue\nFalse\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\n0\n0\nFalse\nTrue\n0\n\n\n414\n1306\n1\n0\n0\nTrue\nFalse\n1\n\n\n415\n1307\n3\n0\n0\nFalse\nTrue\n0\n\n\n416\n1308\n3\n0\n0\nFalse\nTrue\n0\n\n\n417\n1309\n3\n1\n1\nFalse\nTrue\n0\n\n\n\n\n418 rows × 7 columns\n\n\n\n\n정상적으로 잘 수행한 것 같다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#c.-평가-1",
    "href": "posts/Machine Learning in Practice/practice/2. 로지스틱 회귀분석.html#c.-평가-1",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### C. 평가",
    "text": "### C. 평가\n\npredictr.score(X, y)\n\n0.8002244668911336\n\n\n\n생각보단 잘 한 것 같다.\n\n\nD. 제출\n\n\nyy = pd.DataFrame({'Survived' : predictr.predict(XX)})\nsubmit_df = pd.concat([df_test.PassengerId, yy], axis = 1)\nsubmit_df\n\n#submit_df.to_csv(directory, index = False)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\nkaggle에 제출하고 두근대는 결과는… 0.77511, 그리 높진 않다."
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html",
    "href": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "",
    "text": "주어진 자료에서 입학년도를 추가하고 싶다면 어떻게 해야 할까?"
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html#사전작업",
    "href": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html#사전작업",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 설치\n\n\nimport numpy as np\nimport pandas as pd\n\n\n자료 받아오기 및 확인\n\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = [ '2023-12362', '2022-12471', '2023-12333', '2022-12400', '2022-12377',\n               '2022-12469', '2023-12314', '2022-12363', '2023-12445', '2023-12336',\n               '2023-12426', '2022-12380', '2023-12422', '2022-12488', '2022-12370',\n               '2023-12443', '2022-12463', '2023-12491', '2023-12340', '2022-12312' ]\ndf = pd.DataFrame({'student_id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf.head()\n\n\n\n\n\n\n\n\nstudent_id\natt\nrep\nmid\nfin\n\n\n\n\n0\n2023-12362\n65\n55\n50\n40\n\n\n1\n2022-12471\n95\n100\n50\n80\n\n\n2\n2023-12333\n65\n90\n60\n30\n\n\n3\n2022-12400\n55\n80\n75\n80\n\n\n4\n2022-12377\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n학번(student_id)에서 앞 네자리에 해당하는 숫자를 빼내어 새로운 열로 저장하면 좋을 것 같다."
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html#가공",
    "href": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html#가공",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "2. 가공",
    "text": "2. 가공\n\n아래의 코드는 student_id 열을 '-'를 기준으로 앞뒤로 나누고 첫번째 것을 취한다. 숫자형으로 바꾼 뒤, 리스트로 산출한다.\n\n\n[int(i.split('-')[0]) for i in df.student_id]\n\n[2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2022]\n\n\n\nlambda를 이용해 가공할 수도 있다.\n\n\nlist(map((lambda x : int(x.split('-')[0])), df.student_id))\n\n[2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2022]\n\n\n\n첫번째 코드와 똑같은 결과를 산출한다."
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html#출력",
    "href": "posts/Data Visualization/Solution Assemble/특정 열의 재가공.html#출력",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "3. 출력",
    "text": "3. 출력\n\n상기의 코드를 df에 새로운 열 year에 삽입한다.\n\n\ndf.assign(year = [int(i.split('-')[0]) for i in df.student_id])\n\n\n\n\n\n\n\n\nstudent_id\natt\nrep\nmid\nfin\nyear\n\n\n\n\n0\n2023-12362\n65\n55\n50\n40\n2023\n\n\n1\n2022-12471\n95\n100\n50\n80\n2022\n\n\n2\n2023-12333\n65\n90\n60\n30\n2023\n\n\n3\n2022-12400\n55\n80\n75\n80\n2022\n\n\n4\n2022-12377\n80\n30\n30\n100\n2022\n\n\n5\n2022-12469\n75\n40\n100\n15\n2022\n\n\n6\n2023-12314\n65\n45\n45\n90\n2023\n\n\n7\n2022-12363\n60\n60\n25\n0\n2022\n\n\n8\n2023-12445\n95\n65\n20\n10\n2023\n\n\n9\n2023-12336\n90\n80\n80\n20\n2023\n\n\n10\n2023-12426\n55\n75\n35\n25\n2023\n\n\n11\n2022-12380\n95\n95\n45\n0\n2022\n\n\n12\n2023-12422\n95\n55\n15\n35\n2023\n\n\n13\n2022-12488\n50\n80\n40\n30\n2022\n\n\n14\n2022-12370\n50\n55\n15\n85\n2022\n\n\n15\n2023-12443\n95\n30\n30\n95\n2023\n\n\n16\n2022-12463\n50\n50\n45\n10\n2022\n\n\n17\n2023-12491\n65\n55\n15\n45\n2023\n\n\n18\n2023-12340\n70\n70\n40\n35\n2023\n\n\n19\n2022-12312\n90\n90\n80\n90\n2022\n\n\n\n\n\n\n\n완료\n-감사합니다-"
  },
  {
    "objectID": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html",
    "href": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "",
    "text": "심슨의 역설"
  },
  {
    "objectID": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#라이브러리-imports",
    "href": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#라이브러리-imports",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *"
  },
  {
    "objectID": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#필요한-코드-비교를-위한-시각화",
    "href": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#필요한-코드-비교를-위한-시각화",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "2. 필요한 코드 | 비교를 위한 시각화",
    "text": "2. 필요한 코드 | 비교를 위한 시각화\n\nA. geom_col()\n\n- 예시1 : geom_col() 기본적인 막대 그래프\n\ndf = pd.DataFrame({'x':[0,1],'y':[40,60]})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n40\n\n\n1\n1\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'x', y = 'y'))   ## geom_bar()는 그냥 없다고 생각하자.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시2 : \\(x\\)축이 범주형인 경우\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score'))   ## 설명변수가 문자열이어도 산출해준다.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시3 : fill = 'index'예시2에서 범주별 색깔로 구분하고 싶은 경우\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score', fill = 'sex'))   ## color 옵션도 있으나, 이것은 바깥의 테두리 색상만 바꾼다.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시4 : 예시3에서 scale_fill_manual()을 이용하여 색상 변경하기\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n  \n    \n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score', fill = 'sex'))\n\nfig + bar + scale_fill_manual(['red','blue'])\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n색상 입력에는 이름이 정해진 색상들 뿐만 아니라 plt.plot(color = option)에서의 옵션과 같이 이미 설정된 C0, C1… 또는 hex code도 입력이 가능하다."
  },
  {
    "objectID": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#b.-facet_warp-한-면을-감싸다.",
    "href": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#b.-facet_warp-한-면을-감싸다.",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "### B. facet_warp() | 한 면을 감싸다.",
    "text": "### B. facet_warp() | 한 면을 감싸다.\n- 예시1 : facet_warp()를 이용한 면분할 – 반별로 면분할\n\ndf = pd.DataFrame({'sex':['male','female','male','female'],'score':[40,60,50,20],'class':['A','A','B','B']})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\nclass\n\n\n\n\n0\nmale\n40\nA\n\n\n1\nfemale\n60\nA\n\n\n2\nmale\n50\nB\n\n\n3\nfemale\n20\nB\n\n\n\n\n\n\n\n\nggplot(df) + geom_col(aes(x='sex',y='score',fill='sex')) + scale_fill_manual(['red','blue']) + facet_wrap('class')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nclass별로 그래프의 면을 분할해서 표시\n\n\nggplot(df) + geom_col(aes(x = 'sex', y = 'score', fill = 'sex')) + scale_fill_manual(['red','blue'])  ## 지정해주지 않을 경우 기본적으로 합산하여 지정된다.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시2 : 성별로 면분할\n\ndf = pd.DataFrame({'sex':['male','female','male','female'],'score':[40,60,50,20],'class':['A','A','B','B']})\ndf\n\n\n  \n    \n\n\n\n\n\n\nsex\nscore\nclass\n\n\n\n\n0\nmale\n40\nA\n\n\n1\nfemale\n60\nA\n\n\n2\nmale\n50\nB\n\n\n3\nfemale\n20\nB\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nggplot(df) + geom_col(aes(x='class',y='score',fill='sex')) + facet_wrap('sex')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n이 경우 'sex'로 color를 구분했으므로 면마다 다른 색의 그래프만이 나온다."
  },
  {
    "objectID": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#심슨의-역설",
    "href": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#심슨의-역설",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "4. 심슨의 역설",
    "text": "4. 심슨의 역설\n- 버클리 대학교의 입학 데이터에서 gender bias가 존재한다는 주장이 있었다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\nA. 시각화 1 : 전체 합격률 시각화 – pandas 초보\n\n- 단순무식하게 query만 이용해서 그룹화\n\ndf.query('gender == \"female\" and result == \"pass\"')['count'].sum()\n\n772\n\n\n\n여성 지원자 중 합격한 사람의 수\n\n\ndf.query('gender == \"female\"')['count'].sum()\n\n1835\n\n\n\n총 여성 지원자 수\n\n\n(df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum(),\n df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum())\n\n(0.420708446866485, 0.5202526941657376)\n\n\n\ntidydata_ = pd.DataFrame({'female' : [df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum()],\n                          'male' : [df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum()]})\n\ntidydata_\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\n0\n0.420708\n0.520253\n\n\n\n\n\n\n\n\n이렇게 하면 시각화 못해요…\n\n\ntidydata = pd.DataFrame({'sex' : ['male', 'female'],\n                         'rate' : [df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum(),\n                                   df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum()]})\n\ntidydata\n\n\n\n\n\n\n\n\nsex\nrate\n\n\n\n\n0\nmale\n0.420708\n\n\n1\nfemale\n0.520253"
  },
  {
    "objectID": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#b.-시각화-1-전체-합격률-시각화-pandas-고수",
    "href": "posts/Data Visualization/Review/9. 집단 간 비교_심슨의 역설.html#b.-시각화-1-전체-합격률-시각화-pandas-고수",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "### B. 시각화 1 : 전체 합격률 시각화 – pandas 고수",
    "text": "### B. 시각화 1 : 전체 합격률 시각화 – pandas 고수\ndf.pivot_table(index = row, columns = col, valuse = value_col, aggfunc = func)\n- 피벗 테이블을 만든다. (두 범주형 자료들을 나누는 것)\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n1063\n772\n\n\nmale\n1291\n1400\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count')    ## 집계함수의 디폴트 값이 mean이다\n\n\n  \n    \n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n177.166667\n128.666667\n\n\nmale\n215.166667\n233.333333\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n이 상태에서 reset_index()를 통해 자료를 쉽게 정리할 수 있다.\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\\\n.assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass']))\n## 비율까지 추가한 모습, lambda _df : _df를 통해 현재 데이터프레임까지 호출한 모습\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ngender\n\n\n\n\n\n\n\nfemale\n1063\n772\n0.420708\n\n\nmale\n1291\n1400\n0.520253\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\\\n.assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass'])).reset_index()\n\n\n\n\n\n\n\nresult\ngender\nfail\npass\nrate\n\n\n\n\n0\nfemale\n1063\n772\n0.420708\n\n\n1\nmale\n1291\n1400\n0.520253\n\n\n\n\n\n\n\n- 이제 가공된 데이터를 tidydata라고 하여 그래프를 그려보자.\n\ntidydata = df.pivot_table(index='gender',columns='result',values='count',aggfunc=sum).assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass'])).reset_index()\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x='gender',fill='gender',y='rate'))\nfig+bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n이 그래프를 보고 나온 주장 : 여성이 남성에 비해 합격률이 낮으니까 차별이 있다!!\n\n- 반박\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\n##df.pivot_table(index = ['department', 'gender'], columns = 'result', values = 'count', aggfunc = sum)\n## 위의 두 코드는 완전히 똑같은 동작을 한다. 아마도...\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ndepartment\ngender\n\n\n\n\n\n\nA\nfemale\n19\n89\n\n\nmale\n314\n511\n\n\nB\nfemale\n7\n18\n\n\nmale\n208\n352\n\n\nC\nfemale\n391\n202\n\n\nmale\n204\n121\n\n\nD\nfemale\n244\n131\n\n\nmale\n279\n138\n\n\nE\nfemale\n299\n94\n\n\nmale\n137\n54\n\n\nF\nfemale\n103\n238\n\n\nmale\n149\n224\n\n\n\n\n\n\n\n\ntemp.pass  ## pass 자체가 파이썬에서 특수하게 사용되는 조건어같은 거여서 안된다.\n\nSyntaxError: invalid syntax (2928908933.py, line 1)\n\n\n\ntemp = df.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\ntemp['fail'] + temp['pass']\n\ndepartment  gender\nA           female    108\n            male      825\nB           female     25\n            male      560\nC           female    593\n            male      325\nD           female    375\n            male      417\nE           female    393\n            male      191\nF           female    341\n            male      373\ndtype: int64\n\n\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\\\n.assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass']))\n##df.pivot_table(index = ['gender', 'department'], columns = 'result', values = 'count', aggfunc = sum).reset_index().assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass']))\n\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ndepartment\ngender\n\n\n\n\n\n\n\nA\nfemale\n19\n89\n0.824074\n\n\nmale\n314\n511\n0.619394\n\n\nB\nfemale\n7\n18\n0.720000\n\n\nmale\n208\n352\n0.628571\n\n\nC\nfemale\n391\n202\n0.340641\n\n\nmale\n204\n121\n0.372308\n\n\nD\nfemale\n244\n131\n0.349333\n\n\nmale\n279\n138\n0.330935\n\n\nE\nfemale\n299\n94\n0.239186\n\n\nmale\n137\n54\n0.282723\n\n\nF\nfemale\n103\n238\n0.697947\n\n\nmale\n149\n224\n0.600536\n\n\n\n\n\n\n\n- tidydata 완성\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\\\n.assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass'])).reset_index().drop(['fail', 'pass'], axis = 1)\n\ntidydata = _\ntidydata\n\n\n\n\n\n\n\nresult\ndepartment\ngender\nrate\n\n\n\n\n0\nA\nfemale\n0.824074\n\n\n1\nA\nmale\n0.619394\n\n\n2\nB\nfemale\n0.720000\n\n\n3\nB\nmale\n0.628571\n\n\n4\nC\nfemale\n0.340641\n\n\n5\nC\nmale\n0.372308\n\n\n6\nD\nfemale\n0.349333\n\n\n7\nD\nmale\n0.330935\n\n\n8\nE\nfemale\n0.239186\n\n\n9\nE\nmale\n0.282723\n\n\n10\nF\nfemale\n0.697947\n\n\n11\nF\nmale\n0.600536\n\n\n\n\n\n\n\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x = 'gender', y = 'rate', fill = 'gender'))\n\nfig + bar + scale_fill_manual(['red', 'blue']) + facet_wrap(['department'])\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nC와 E를 제외하고는 오히려 여성의 합격 비율이 더 높은 것을 알 수 있다.\n\n\nD. 해석\n\n- 시각화 1 : 남자의 합격률이 더 높다 -&gt; 성차별이 있어보인다(?)\n- 시각화 2 : 학과별로 살펴보니 오히려 A, B, F, D의 경우 여성의 합격률이 높다.\n- 교재에서 설명한 이유 : 여성이 합격률이 낮은 학과에만 많이 지원하였기 때문.\n\ndf.pivot_table(index='department', columns='gender', values='count',aggfunc='sum')\\\n.stack().reset_index().rename({0:'count'},axis=1)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n\ntidydata = df.pivot_table(index='department', columns='gender', values='count',aggfunc='sum')\\\n.stack().reset_index().rename({0:'count'},axis=1)\n\n \nfig = ggplot(tidydata) \ncol = geom_col(aes(x='department',y='count',fill='gender'),position='dodge')\nfig+col\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nA, B학과가 신체적 역량을 많이 요구로 하거나 하는 학과일 경우 아무래도 기피하겠지…\n은닉변수에 의해 왜곡이 될 수 있다"
  },
  {
    "objectID": "posts/Data Visualization/Review/7. Pandas 팁.html",
    "href": "posts/Data Visualization/Review/7. Pandas 팁.html",
    "title": "Pandas 사용 팁",
    "section": "",
    "text": "Pandas에서 유용하게 사용할 수 있는 여러가지 메소드들을 알아보자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/7. Pandas 팁.html#라이브러리-imports",
    "href": "posts/Data Visualization/Review/7. Pandas 팁.html#라이브러리-imports",
    "title": "Pandas 사용 팁",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *"
  },
  {
    "objectID": "posts/Data Visualization/Review/7. Pandas 팁.html#pabdas-transform-column",
    "href": "posts/Data Visualization/Review/7. Pandas 팁.html#pabdas-transform-column",
    "title": "Pandas 사용 팁",
    "section": "2. pabdas : transform column",
    "text": "2. pabdas : transform column\nA. lambda\nB. map\n\nC. s.apply(변환함수) | 원소들을 각각 변환\n\n\n변환함수 : 원래 형식을 보존하면서 원소들을 바꾸는 함수\n집계함수 : 벡터 -&gt; 스칼라 (평균을 불러주는 함수 : [1,2,3,4,5] -&gt; 3)\n\n\n라고 하자.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')   ## DataFrame\ns = df.Height   ## Series\n\n\ns\n\n0        189cm\n1        179cm\n2        172cm\n3        181cm\n4        172cm\n         ...  \n17655    190cm\n17656    195cm\n17657    190cm\n17658    187cm\n17659    186cm\nName: Height, Length: 17660, dtype: object\n\n\n\n뒤에 cm가 붙어있는 범주형 자료로 저장되어있음.\n\n\ns.apply(lambda x : int(x[:3])) ## 집계함수가 아닌 변환함수만 적용할 수 있음. 각 원소에 함수 적용.\n##s.apply(lambda x : x[:3]).apply(int)    ## 연쇄적으로\n##s.apply(lambda x : x[:3]).astype('int64')   ## astype() 이용\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64\n\n\n\ncm를 제거하고 포맷을 정수형으로 변경하였다."
  },
  {
    "objectID": "posts/Data Visualization/Review/7. Pandas 팁.html#d.-s.str-idx.str-string-오브젝트에만-사용할-수-있는-함수를-사용",
    "href": "posts/Data Visualization/Review/7. Pandas 팁.html#d.-s.str-idx.str-string-오브젝트에만-사용할-수-있는-함수를-사용",
    "title": "Pandas 사용 팁",
    "section": "### D. s.str, idx.str | string 오브젝트에만 사용할 수 있는 함수를 사용",
    "text": "### D. s.str, idx.str | string 오브젝트에만 사용할 수 있는 함수를 사용\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\n\"180cm\"[:3]\n\n'180'\n\n\n\n'180cm'.replace('cm','')\n\n'180'\n\n\n\n위와 같은 연산을 시리즈에 적용시키고 싶다.\n\n\ns.str[:3]\n##s.str.replace('cm', '')   ## 개별 문자열과 동일하게 메소드를 적용시켜도 된다.\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n\n문자열의 메소드를 그대로 적용 가능\n\n- 예시2 : 원소별로 isupper를 수행(대문자인지 판별)\n\n_s = pd.Series(['A','B','C','d','e','F'])\n_s\n\n0    A\n1    B\n2    C\n3    d\n4    e\n5    F\ndtype: object\n\n\n\n_s.str.isupper()\n\n0     True\n1     True\n2     True\n3    False\n4    False\n5     True\ndtype: bool\n\n\n- 예시3 : 원소별로 공백 제거(pd.Serise 뿐만 아니라 pd.index 자료형에도 사용가능)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\nidx = df.columns\n\n\nidx.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'Joined', 'LoanedFrom',\n       'ContractValidUntil', 'Height', 'Weight', 'ReleaseClause', 'KitNumber',\n       'BestOverallRating'],\n      dtype='object')\n\n\n- 쉽게 말해서 string데이터를 지닌 개체에 string에 사용할 수 있는 메소드를 사용할 수 있도록 하는 게 pandas의 str이라고 보면 된다.\n\nE. s.astype() | 조건을 충족한 시리즈의 타입을 변경\n\n- 예시1 : 원소의 타입을 변경\n\ns = pd.Series(list('12345'))\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: object\n\n\n\ns.astype(int)\n##s.apply(int)\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n- 예시2 : 원소의 타입을 변환한 이후 브로드캐스팅\n\ns1 = pd.Series(list('12345'))\ns2 = pd.Series([-1,-2,-3,-4,-5])\n\n\ns1+s2\n\nTypeError: ignored\n\n\n\nError : 형식이 달라 불가능\n\n\ns1.astype(int) + s2\n\n0    0\n1    0\n2    0\n3    0\n4    0\ndtype: int64\n\n\n\ns2.astype(str) + s1\n\n0    -11\n1    -22\n2    -33\n3    -44\n4    -55\ndtype: object\n\n\n- 예시3 : 원소의 타입을 변환한 이후 브로드캐스팅(str)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Embarked열과 Pclass열을 사용하여 아래와 같은 New Feature를 만들어라.\n\n\n\nEmbarked\nPclass\nNew Feature\n\n\n\n\n‘S’\n3\n‘S3’\n\n\n‘C’\n1\n‘C1’\n\n\n‘S’\n3\n‘S3’\n\n\n‘S’\n1\n‘S1’\n\n\n‘S’\n3\n‘S3’\n\n\n\n둘다 문자열이면 단순히 +를 이용해 브로드캐스팅하면 되지만, 타입이 달라 불가하다.\n\ndf.Embarked + df.Pclass.apply(str)\n##df.Embarked + df.Pclass.astype(str)\n##df.Embarked + pd.Series(list(map(lambda x : str(x)), df.Pclass))\n\n0    S3\n1    C1\n2    S3\n3    S1\n4    S3\ndtype: object"
  },
  {
    "objectID": "posts/Data Visualization/Review/7. Pandas 팁.html#f.-컴프리헨션-lambda-map을-무시하지-말-것",
    "href": "posts/Data Visualization/Review/7. Pandas 팁.html#f.-컴프리헨션-lambda-map을-무시하지-말-것",
    "title": "Pandas 사용 팁",
    "section": "### F. 컴프리헨션, lambda + map을 무시하지 말 것",
    "text": "### F. 컴프리헨션, lambda + map을 무시하지 말 것\n- 예시1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n위 자료에서 아래와 같은 변환을 하고 싶다면 apply만으로 사용하기에 부담이 된다.\n\\[\nf(\\text{sex}, \\text{sibsp}) =\n\\begin{cases}\n0.7 + 0.25 \\times \\text{sibsp} & \\text{if } \\text{sex} = \\text{'female'} \\\\\n0.2 + 0.15 \\times \\text{sibsp} & \\text{otherwise}\n\\end{cases}\n\\]\n\nlist(map(lambda sex, sibsp : 0.7+0.25*sibsp if sex == 'female' else 0.2+0.15*sibsp, df.Sex, df.SibSp))\n\n[0.35, 0.95, 0.7, 0.95, 0.2]\n\n\n\ndf.assign(Probablity = list(map(lambda sex, sibsp : 0.7+0.25*sibsp if sex == 'female' else 0.2+0.15*sibsp, df.Sex, df.SibSp)))\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\nProbablity\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n0.35\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n0.95\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n0.70\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n0.95\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n0.20\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 예시2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Name열을 아래와 같이 분리하는 작업을 수행하라.\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen\n\n\n\n- 풀이 1\n\ndf.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')\n\n0                            [Braund, Mr, Owen Harris]\n1    [Cumings, Mrs, John Bradley (Florence Briggs T...\n2                             [Heikkinen, Miss, Laina]\n3       [Futrelle, Mrs, Jacques Heath (Lily May Peel)]\n4                           [Allen, Mr, William Henry]\nName: Name, dtype: object\n\n\n\n[[title, name + ' ' + f_name] for f_name, title, name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n- 풀이 2 : 이중 컴프리헨션이 될까 해서 해봤는데… 되네?~(솔직히 안될 이유가 없긴 함, 리스트를 반환하는 거니까…)~\n\n[[names[0], names[1] + ' ' + f_name] for f_name, names in [[f_name, names.split('. ')] for f_name, names in df.Name.str.split(', ')]]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n\nlists = [[names[0], names[1] + ' ' + f_name] for f_name, names in [[f_name, names.split('. ')] for f_name, names in df.Name.str.split(', ')]]\npd.DataFrame({'title' : np.array(lists)[:,0], 'Name' : np.array(lists)[:,1]})\n\n\n\n\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen"
  },
  {
    "objectID": "posts/Data Visualization/Review/5. Pandas의 기본.html",
    "href": "posts/Data Visualization/Review/5. Pandas의 기본.html",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "",
    "text": "pandas에서 행과 열을 선택하는 기술에 대해서 알아보도록 하자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/5. Pandas의 기본.html#라이브러리-import",
    "href": "posts/Data Visualization/Review/5. Pandas의 기본.html#라이브러리-import",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/Data Visualization/Review/5. Pandas의 기본.html#pandas-행과-열의-선택",
    "href": "posts/Data Visualization/Review/5. Pandas의 기본.html#pandas-행과-열의-선택",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "2. pandas : 행과 열의 선택",
    "text": "2. pandas : 행과 열의 선택\n- 같은 자료, 다른 두 형태의 데이터프레임\n\ndf = pd.DataFrame({'date': ['12/30','12/31','01/01','01/02','01/03'], 'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]})\ndf\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n얘는 인덱스는 그저 숫자의 의미이고\n\n\nts = pd.DataFrame({'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]}, index=['12/30','12/31','01/01','01/02','01/03'])\nts  ## 중요한 코드는 아님, 근데 그냥 index 지정해주는 거잖아\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n12/31\n95\n100\n50\n80\n\n\n01/01\n65\n90\n60\n30\n\n\n01/02\n55\n80\n75\n80\n\n\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n얘는 인덱스에 시계열적 표현이 있다.\n\n\nts.reset_index()  ## 결국 이렇게 하는 게 다루기 편하다.\n\n\n  \n    \n\n\n\n\n\n\nindex\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n안바꾸고 냅두는 경우, index가 time seat를 의미하는 경우에는 안바꾸기도 한다. ~근데 위는 왜 다시 바꾼거~\n\n\nA. 열의 선택\n\n1번째 방법 : df.ㆍ\n! 치명적인 단점 : 변수 이름에 공백 등이 있으면 불러올 수 없음.\n\n\ndf.X1   ## df 또한 하나의 object이므로\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\n2번째 방법 : df['ㆍ'], df[['ㆍ']]\n\n\ndf['X1']  ## df를 일종의 딕셔너리처럼 취급하는 방법\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\nSeries로 불러온다.\n\ndictionary?\n\ndct = dict({'date': ['12/30','12/31','01/01','01/02','01/03'], 'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]})\ndct['X1']\n\n[65, 95, 65, 55, 80]\n\n\n\ndf.keys()\n\nIndex(['date', 'X1', 'X2', 'X3', 'X4'], dtype='object')\n\n\n\ndct.keys()\n\ndict_keys(['date', 'X1', 'X2', 'X3', 'X4'])\n\n\n\nkey와 value가 있는 것처럼 column의 한 값(key)에 대한 데이터(value)가 있는 모습이다.\n\n\ndf[['X1']]  ## 프레임으로 산출\n\n\n  \n    \n\n\n\n\n\n\nX1\n\n\n\n\n0\n65\n\n\n1\n95\n\n\n2\n65\n\n\n3\n55\n\n\n4\n80\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf[['X1', 'X2']]  ## 2개 이상 산출 가능\n\n\n  \n    \n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n65\n55\n\n\n1\n95\n100\n\n\n2\n65\n90\n\n\n3\n55\n80\n\n\n4\n80\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n3번째 방법 : df.iloc[:, ㆍ] &gt; 통째로 np.array와 같다고 보면 된다.\n\n\ndf.iloc[:, 0] ## numpy에서 행렬을 다루는 것과 완전히 같게 사용 가능.\n\n0    12/30\n1    12/31\n2    01/01\n3    01/02\n4    01/03\nName: date, dtype: object\n\n\n\n#df.iloc[:,0] ## int - 0번째 행 | [0]이면 데이터프레임으로\n#df.iloc[:,-2:] # int:int, -2번째 행부터 -1번째 행까지(뒤에서 두 개)\n#df.iloc[:,1::2] # int:int:int - 스트라이딩, 1번째(두번째) 행부터 2개 단위로 추출\n#df.iloc[:,[0,2]] # [int,int] - 특정 행(0, 2번째)을 데이터프레임의 형태로 반환\n#df.iloc[:,[True,True,False,False,False]] # bool의 list\n#df.iloc[:,range(2)] # range, 앞에서 2개\n\n\n4번째 방법 : df.loc[:, ㆍ] &gt; 완전히 새로운 방법\n\n\n#df.loc[:,'X1'] # str - 시리즈 | ['X1']이면 데이터프레임으로\n#df.loc[:,'X1':'X3'] # 'str':'str' -- 칼럼이름으로 슬라이싱 **\n#df.loc[:,'X1'::2] # 'str'::int -- 칼럼이름으로 스트라이딩 **\n#df.loc[:,['X1','X4']] # [str,str] - 특정 행만 데이터프레임으로\n#df.loc[:,[True,False,False,True,False]] # bool의 list\n\n\"\"\"\n그냥 왠만해선 다 됨\n\"\"\"\n\n- 'date'행 부터\n\ndf.loc[:, 'date':]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- True가 입력된 행만\n\ndf.loc[:, [True, False, True, False, True]]   ## columns의 이름에 어떤 조건을 걸어서 True에 해당하는 열만 산출 가능\n\n\n  \n    \n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 2칸씩 띄며 스트라이딩\n\ndf.loc[:, ::2]  ## 2 간격으로 스트라이딩\n\n\n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n\n\n\nB. 행의 선택\n\n방법1 : df[]\n\n\ndf[:2]    ## int:int -- 슬라이싱 // df.iloc[:2, :], df.iloc[:2] 와 같음\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf[::2]  ## 스트라이딩, df.iloc[::2]와 같음\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n# df[:2] # int:int -- 슬라이싱 // df.iloc[:2,:], df.iloc[:2] 와 같음\n# df[::2] # int:int -- 스트라이딩\n# ts['12/30':'01/02'] # str:str -- 슬라이싱 &gt; 인덱스가 문자열 등일 경우\n# ts['12/31'::2] # str:str -- 스트라이딩\n# df[['12' in date for date in df.date]] # [bool,bool] `12`가 데이터에 포함되어 있을 경우\n# df[df.X1 &lt; 70] # pd.Series([bool,bool])\n\n\n방법2 : df.iloc[]\n\n\n# df.iloc[0] # int  df.iloc[0, :]에서 생략된 표현\n# df.iloc[-2:] # int:int -- 슬라이싱\n# df.iloc[1::2] # int:int -- 스트라이딩\n# df.iloc[[0]] # [int]\n# df.iloc[[0,1]] # [int,int]\n# df.iloc[['12' in date for date in df.date]] # [bool,bool] 이 경우는 []와 동일하다.\n# df.iloc[range(2)] # range\n\n- 해당 방법은 리스트나 어레이의 원소를 다루는 것과 완전히 동일해서… 아래를 참고하면 된다.\n\nlst = [[1,2,3], [3,4,5]]\nlst[0]\n\n[1, 2, 3]\n\n\n\nary = np.array(lst)\nary[0]\n\narray([1, 2, 3])\n\n\n\n방법3 : df.loc[]\n\n\n# df.loc[0] # int \n# ts.loc['12/30'] # str \n# df.loc[:2] # int:int \n# ts.loc[:'01/02'] # str:str \n# df.loc[[0,1]] # [int,int]\n# ts.loc[['12/30','01/01']] # [str,str]\n# df.loc[['12' in date for date in df.date]] # [bool,bool] 이 경우는 []와 동일하다.\n# df.loc[df.X1&gt;70] # pd.Series([bool,bool]) \n\n\ndf.loc[:2]  ## character와 비슷한 형식이기 때문에 2까지 포함이 된다.\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[df.X1 &gt; 70]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[df.X1 &gt; 70]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n위처럼 튜플로 넣을 수도 있다. 근데 iloc의 경우 위와 같은 코드로 입력하면 오류가 난다.\n\n\n## df.iloc[df.X1 &gt; 70] &gt; 오류, bool을 받을 수 있으나, 튜플의 형태로 들어가면 반환하지 않는다.\ndf.iloc[list(df.X1 &gt; 70)]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nD. 교수님 방식\n-가장 안전한 코드\n\ndf.loc[:,:] ## 해당 코드를 써놓고 시작, generally\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n하나의 col을 뽑으려 할 때\n\n\n# df.X1       ## 제일 간단함. 게다가 눌러보면 변수 이름들이 나옴\n# df['X1']\n# df[['X1']]\n\n\nrow를 슬라이싱\n\n\n# df[:5]\n# ts[:'01/02']  # 시계열일 경우\n\n\n조건에 맞는 row를 뽑을 때 좋은 코드\n\n\n# df[df.X1&lt;60]  ## 이게 좋기는 한데, True, False를 직접 만들어야 하는 경우도 많음.\n# df.loc[['12' in date for date in df.date]]\n\n\n['12' in date for date in df.date]\n\n[True, True, False, False, False]\n\n\n\n하나의 row를 뽑으려 할 때 좋은 코드\n\n\n# df.iloc[0]\n# df.loc[0]\n\n\nts.iloc[[0]]\n# ts.iloc[0]의 경우 오류가 남(인덱스 이름이 숫자열이 아님\n\n\n  \n    \n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\n(row,col)을 뽑으려 할 때 좋은 코드\n\n\n# df.X1[0]    ## &lt;- pd.Series를 뽑고 인덱스로 접근\n# df['X1'][0]\n\n\n# df.iloc[0,0]\n# df.loc[0,'X1']\n\n*위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n\n\n요약\n\n알아두면 좋은 규칙\n.iloc[] 와 .iloc[,:]는 완전히 동등하다.\n.loc[] 와 .loc[,:]는 완전히 동등하다.\n결과를 pd.Series 형태가 아닌 pd.DataFrame 형태로 얻고 싶다면 [[?]]를 사용하면 된다.\n\n\n\nROW\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[3,:]\n\n\nint:int\nX\nO\nO\n\\(\\Delta\\)\ndf[3:5]\n\n\n[int,int]\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[idx,:]\n\n\nstr\nX\nX\nX\nO\nts.loc['time1',:]\n\n\nstr:str\nX\nO\nX\nO\nts.loc['time1':'time2',:]\n\n\n[str,str]\nX\nX\nX\nO\n안할 듯\n\n\n[bool,bool]\nX\nO\nO\nO\ndf[filtered_idx]\n\n\npd.Series([bool,bool])\nX\nO\nX\nO\ndf[df.X1&gt;20]\n\n\n\n\n\nCOL\n\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\ntarget\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\ncol\nX\nX\nO\nX\ndf.iloc[:,0]\n\n\nint:int\ncol\nX\nX\nO\nX\ndf.iloc[:,0:2]\n\n\n[int,int]\ncol\nX\nX\nO\nX\ndf.iloc[:,idx]\n\n\nstr\ncol\nO\nO\nX\nO\ndf.loc[:,'X1']\n\n\nstr:str\ncol\nX\nX\nX\nO\ndf.loc[:,'X1':'X4']\n\n\n[str,str]\ncol\nX\nO\nX\nO\ndf.loc[:,colname_list]\n\n\n[bool,bool]\ncol\nX\nX\nO\nO\ndf.loc[:,bool_list]"
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html",
    "href": "posts/Data Visualization/Review/3. Seaborn.html",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "",
    "text": "seaborn을 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html#라이브러리-import",
    "href": "posts/Data Visualization/Review/3. Seaborn.html#라이브러리-import",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html#seaborn과-matplotlib",
    "href": "posts/Data Visualization/Review/3. Seaborn.html#seaborn과-matplotlib",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "2. Seaborn과 Matplotlib",
    "text": "2. Seaborn과 Matplotlib\n\nmatplotlib : 벡터 친화적\nseaborn : 데이터프레임 친화적\n\n\n분석할 데이터가 태뷸러데이터 형식인 경우가 많다.\nmatplotlib는 여전히 강력하지만, seaborn등 데이터프레임 친화적인 패키지가 우수한 경우가 많다.\n\n\nA. scatter plot\n\n\n## titanic data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x = 'logFare',  ## 요금에 로그를 취한 값(너무 변동이 크니까)\n    y = 'Age',\n    hue = 'Sex',    ## 색상, 색조. 변수 별 색상을 나눠 표기한다.\n    style = 'Survived', style_order = [1,0],  ## Survived 여부로 마커 표시, style_order의 디폴트 값이 [0 =&gt; O,1 =&gt; X]이므로 그 순서를 변경\n    alpha = 0.6     ## 투명도 조절\n)\n\n&lt;Axes: xlabel='logFare', ylabel='Age'&gt;\n\n\n\n\n\n\nplt.hist(df.Age)\nplt.hist(df.Age[df.Survived == 1])  ## 그냥 그리면 겹쳐짐\n\nplt.show()\n\n\n\n\n- seaborn은 데이터과학에서 거의 표준적인 패키지. * 안하는 이유 * 간단한 시각화는 matplotlib가 유리 * seaborn에 대한 고급기능은 matplotlib에 대한 통찰이 있어야 가능 * plotline이 더 우수함(ggplot2) * plotly가 모든 면에서 seaborn을 압도하는 추세임\n\n\nB. seaborn의 고급기능 이해\n\nsns.scatterplot(\n    df,\n    x = 'logFare',\n    y = 'Age',\n    hue = 'Sex',\n    style = 'Survived', style_order = [1,0],\n    alpha = 0.8\n)\n\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scattor Plot')\n\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\n## ax_mini = fig.add_axes([0.6,0.2,0.25,0.25])과 동일\n\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived == 1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nplt.show()\n\n\n\n\n\ntype(fig) ## seaborn으로 제작하였음에도 Figure의 형식을 지닌다.\n\nmatplotlib.figure.Figure"
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html#훌륭한-시각화",
    "href": "posts/Data Visualization/Review/3. Seaborn.html#훌륭한-시각화",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "3. 훌륭한 시각화",
    "text": "3. 훌륭한 시각화"
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html#애드워드-터프티",
    "href": "posts/Data Visualization/Review/3. Seaborn.html#애드워드-터프티",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "### 애드워드 터프티",
    "text": "### 애드워드 터프티\n- 데이터 시각화계의 거장\n\n엄격한 미니멀리즘\n최소한의 잉크로 많은 정보를 전달할 수 있다면 그것이 바로 좋은 그래프이다.\n\n\n너무 구시대적인 사고일 수도 있음. 적합할 수도 있고."
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html#찰스미나드의-도표",
    "href": "posts/Data Visualization/Review/3. Seaborn.html#찰스미나드의-도표",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "### 찰스미나드의 도표",
    "text": "### 찰스미나드의 도표\n\n\n터프티도 극찬하고 ~중국이 놀라고, 일본이 경악하고…~\n\n\n군대의 크기, 2차원 평면상의 위치, 군대의 이동방향, 모스크바에서 퇴각하는 동안의 여러 날짜와 그 시점에서의 온도 -&gt; 6차원의 변수를 한 평면상에 표현\n\n미나드는 여러 그림을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함."
  },
  {
    "objectID": "posts/Data Visualization/Review/3. Seaborn.html#미나드처럼-그리는-게-왜-어려운가",
    "href": "posts/Data Visualization/Review/3. Seaborn.html#미나드처럼-그리는-게-왜-어려운가",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "4. 미나드처럼 그리는 게 왜 어려운가?",
    "text": "4. 미나드처럼 그리는 게 왜 어려운가?\n- 몸무게, 키, 성별, 국적을 나타내는 자료\n\ndf1=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male1.csv')   ## 남성의 키\ndf2=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male2.csv')   ## 남성의 몸무게\ndf3=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/female.csv')  ## 여성의 키와 몸무게\ndf4=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/foreign.csv') ## 외국인의 키와 몸무게, 성별, 국적\n\n\n_df = pd.concat([pd.concat([df1,df2],axis=1)\\\n                 .assign(g='m'),df3.assign(g='f')])\ndf = pd.concat([_df.assign(g2='korea'),df4.assign(g2='foreign')])\\\n.reset_index(drop=True)\ndf\n\n\n  \n    \n\n\n\n\n\n\nw\nh\ng\ng2\n\n\n\n\n0\n72.788217\n183.486773\nm\nkorea\n\n\n1\n66.606430\n173.599877\nm\nkorea\n\n\n2\n69.806324\n173.237903\nm\nkorea\n\n\n3\n67.449439\n173.223805\nm\nkorea\n\n\n4\n70.463183\n174.931946\nm\nkorea\n\n\n...\n...\n...\n...\n...\n\n\n1525\n78.154632\n188.324350\nm\nforeign\n\n\n1526\n74.754308\n183.017979\nf\nforeign\n\n\n1527\n91.196208\n190.100456\nm\nforeign\n\n\n1528\n87.770394\n187.987255\nm\nforeign\n\n\n1529\n88.021995\n193.456798\nm\nforeign\n\n\n\n\n\n1530 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nsns.scatterplot(\n    data=df,\n    x='w',\n    y='h',\n    hue='g',    ## group 1 : gender\n    style='g2',  ## group 2 : region\n    alpha=0.6\n)\n\n&lt;Axes: xlabel='w', ylabel='h'&gt;\n\n\n\n\n\n\n그래프를 이해하기 어려운 것은 아니지만, 아무래도 난잡한 것은 사실이다.\n\n-어려운 점 :\n\n센스 부족 : 센스가 없어서 그룹 구분할 생각을 못함\n개념 부족 : 타이디데이터( =tidy dataframe, long form dataframe) 형태로 데이터를 정리할 생각을 못함.\n코딩 못함 : 타이디테이터로 데이터를 변형하는 코드를 모름."
  },
  {
    "objectID": "posts/Data Visualization/Review/12. merge와 concat.html",
    "href": "posts/Data Visualization/Review/12. merge와 concat.html",
    "title": "1. 라이브러리 imports",
    "section": "",
    "text": "groupby() 메소드의 심화\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/Data Visualization/Review/12. merge와 concat.html#groupby",
    "href": "posts/Data Visualization/Review/12. merge와 concat.html#groupby",
    "title": "1. 라이브러리 imports",
    "section": "2. groupby",
    "text": "2. groupby\n\nA. df.groupby\n\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\ng = df.groupby(by = 'department')\ng\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000029243C3B250&gt;\n\n\n\nset(dir(g)) & {'__iter__'} # g는 반복문을 돌리기 유리하게 설계되어 있음\n\n{'__iter__'}\n\n\n\ng에 적용할 수 있는 메소드(dir(g))중에 '__iter__'라는 게 있다. 즉, g는 for문을 돌리려고 만든 오브젝트이다.\n\n\n[i for i in g]  ## list(g)\n\n[('A',\n    department  gender  count\n  0          A    male      1\n  1          A  female      2),\n ('B',\n    department  gender  count\n  2          B    male      3\n  3          B  female      1)]\n\n\n\n두 개의 튜플이 나온다. 튜플의 원소는 그룹화하는 열과 sub_dataframe과 같은 형식이다.\n\n\ndct = {k:df for k, df in g}  ## 딕셔너리 컴프리헨션\ndct\n\n{'A':   department  gender  count\n 0          A    male      1\n 1          A  female      2,\n 'B':   department  gender  count\n 2          B    male      3\n 3          B  female      1}\n\n\n\ndisplay(dct['A'])\ndisplay(dct['B'])\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\n딕셔너리의 원소가 sub-dataframe"
  },
  {
    "objectID": "posts/Data Visualization/Review/12. merge와 concat.html#b.-g의-이용법",
    "href": "posts/Data Visualization/Review/12. merge와 concat.html#b.-g의-이용법",
    "title": "1. 라이브러리 imports",
    "section": "### B. g의 이용법",
    "text": "### B. g의 이용법\n- g를 이용하여 원래의 df를 복원하라.\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ng = df.groupby('department')\n\n\npd.concat([df for _,df in g])\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\n단순히 묶어주기만 해도 이렇게 나온다.\n\n- g를 이용하여 아래와 동일한 기능을 하는 코드를 작성하라. (agg함수 사용 금지)\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ndf.groupby('department').agg({'count':'sum'})\n\n\n\n\n\n\n\n\ncount\n\n\ndepartment\n\n\n\n\n\nA\n3\n\n\nB\n4\n\n\n\n\n\n\n\n\nlist(g)\n\n[('A',\n    department  gender  count\n  0          A    male      1\n  1          A  female      2),\n ('B',\n    department  gender  count\n  2          B    male      3\n  3          B  female      1)]\n\n\n\npd.DataFrame(pd.Series({i:sum(j['count']) for i, j in g}))  ## 리스트로 먼저 묶어줘야 함.\n##pd.DataFrame(pd.Series({k:df['count'].sum() for k,df in g})) 이게 더 직관적\n\n\n\n\n\n\n\n\n0\n\n\n\n\nA\n3\n\n\nB\n4\n\n\n\n\n\n\n\n- 이 데이터프레임을 class를 기준으로 그룹핑하여 sub-dataframe을 만들고, score가 높은 순서로 정렬하는 코드를 작성하라.\n\ndf = pd.DataFrame({'class':['A']*5+['B']*5, 'id':[0,1,2,3,4]*2, 'score':[60,20,40,60,90,20,30,90,95,95]})\ndf\n\n\n\n\n\n\n\n\nclass\nid\nscore\n\n\n\n\n0\nA\n0\n60\n\n\n1\nA\n1\n20\n\n\n2\nA\n2\n40\n\n\n3\nA\n3\n60\n\n\n4\nA\n4\n90\n\n\n5\nB\n0\n20\n\n\n6\nB\n1\n30\n\n\n7\nB\n2\n90\n\n\n8\nB\n3\n95\n\n\n9\nB\n4\n95\n\n\n\n\n\n\n\n\ng = df.groupby('class')\n\npd.concat([df.sort_values('score', ascending = False) for i, df in g], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nclass\nid\nscore\n\n\n\n\n0\nA\n4\n90\n\n\n1\nA\n0\n60\n\n\n2\nA\n3\n60\n\n\n3\nA\n2\n40\n\n\n4\nA\n1\n20\n\n\n5\nB\n3\n95\n\n\n6\nB\n4\n95\n\n\n7\nB\n2\n90\n\n\n8\nB\n1\n30\n\n\n9\nB\n0\n20"
  },
  {
    "objectID": "posts/Data Visualization/Review/12. merge와 concat.html#merge",
    "href": "posts/Data Visualization/Review/12. merge와 concat.html#merge",
    "title": "1. 라이브러리 imports",
    "section": "3. merge",
    "text": "3. merge\n\nA. 가장 빈번하게 사용되는 상황\n\n- 예시 : big 데이터프레임에 groupby+agg를 사용하여 small 데이터프레임이 생긴 경우\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'department':['A','B'], 'count_sum':[3,4]})\n## big.groupby('department').aggregate({'count':'sum'}).rename({'count' : 'count_sum'}, axis = 1)\n\n\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ncount_sum\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\ndepartment | gender | count –&gt; big\ndepartment | count_sum –&gt; small\n\ndepartment | gender | count | count_sum\n\n이러한 작업을 하고 싶을 때, pd.merge()를 사용하면 된다.\n\n\npd.merge(big, small)\n## big.merge(small)\n## small.merge(big)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n사실 아래가 정확한 코드이다.\n\npd.merge(big, small, on = 'department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n공통부분인 department에 따라 데이터프레임이 병합됨\n\n\n두 데이터프레임은 IndexLabel에 대하여 서로 다른 정보를 각각 정리한 상황\n두 데티어프레임에서 공통인 열(IndexLabel(을 찾고, 이것을 기준으로 데이터의 정보를 병합한다.\n\n\n\nB. 여러가지 파라메터\n\n# on\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = big.groupby('department').agg({'count':'sum'}).reset_index()\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ncount\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n- 잘못된 코드\n\npd.merge(big, small)  ## department와 count가 겹친다. 이름은 겹치지만 count는 의미가 다름\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n\n\n\n\n\n\n연결고리로 이해\n\n‘A’, 1\n‘A’, 2\n‘B’, 3\n‘B’, 1\n\n‘A’, 3\n‘B’, 4\n두 데이터프레임의 연결고리가 없다. 따라서 아무것도 산출할 수 없다…\n- 제대로 쓴 코드\n\npd.merge(big, small, on = 'department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ncount_y\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n- 열의 이름을 살리면서…\n\npd.merge(big, small.rename({'count' : 'count_sum'}, axis = 1))  ## 공통부분을 없애줌\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n어차피 이름을 바꿔야 하니, 처음부터 양식에 맞게 해주는 게 더 좋을 수 있음…\n\n사실 아래 둘은 같은 코드이다.\n\npd.merge(big,small,on='department')\npd.merge(big,small,left_on='department', right_on='department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ncount_y\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n# left_on, right_on\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'dept':['A','B'], 'count':[3,4]})\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndept\ncount\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n\n공통부분은 count이고, 오히려 다른 부분은 모두 공통되지 않은 상황\n\n\npd.merge(big, small)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ndept\n\n\n\n\n0\nB\nmale\n3\nA\n\n\n\n\n\n\n\n\ncount로 합치면서 지랄이 난다.\n\n- department, dept를 기준으로 병합…\n\npd.merge(big, small, left_on = 'department', right_on = 'dept')\n## 왼쪽 데이터프레임에선 department, 오른쪽 데이터프레임에선 dept를 기준으로 삼음...\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ndept\ncount_y\n\n\n\n\n0\nA\nmale\n1\nA\n3\n\n\n1\nA\nfemale\n2\nA\n3\n\n\n2\nB\nmale\n3\nB\n4\n\n\n3\nB\nfemale\n1\nB\n4\n\n\n\n\n\n\n\n- 더 직관적이고 편하게…\n\npd.merge(big, small.rename({'dept' : 'department', 'count' : 'count_sum'}, axis = 1))\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n강제로 한 열만 이름이 같도록 해서 공통부분을 지정해줬다.(훨씬 편하지요옹?)\n\n# how\n\ndf1 = pd.DataFrame({\n    'dept':['통계','수학','과학','IAB'], \n    'count':[20,30,25,50]\n})\ndf2 = pd.DataFrame({\n    'dept':['통계','수학','과학','신설학과'], \n    'desc':['통계학과는...','수학과는...','과학학과는...','이 학과는 내년에 신설될 예정이고...']\n})\ndisplay(\"df1\",df1)\ndisplay(\"df2\",df2)\n\n'df1'\n\n\n\n\n\n\n\n\n\ndept\ncount\n\n\n\n\n0\n통계\n20\n\n\n1\n수학\n30\n\n\n2\n과학\n25\n\n\n3\nIAB\n50\n\n\n\n\n\n\n\n'df2'\n\n\n\n\n\n\n\n\n\ndept\ndesc\n\n\n\n\n0\n통계\n통계학과는...\n\n\n1\n수학\n수학과는...\n\n\n2\n과학\n과학학과는...\n\n\n3\n신설학과\n이 학과는 내년에 신설될 예정이고...\n\n\n\n\n\n\n\n공통의 열인 dept, 서로 다른 정보인 count와 desc\n\non, left_on, right_on을 사용할 필요가 없다.\n\n\npd.merge(df1, df2)  ## IAB, 신설학과가 미아됨\n\n\n\n\n\n\n\n\ndept\ncount\ndesc\n\n\n\n\n0\n통계\n20\n통계학과는...\n\n\n1\n수학\n30\n수학과는...\n\n\n2\n과학\n25\n과학학과는...\n\n\n\n\n\n\n\n\n근데 겹치지 않는 것이 없어졌음…\n\n겹치지 않는 자료를 처리하는 방식은 4가지 경우로 나누어진다.\n\n#pd.merge(df1, df2, how = 'inner')  ## 보수적으로, 자료가 없으면 없앰, default\n#pd.merge(df1, df2, how = 'left')  ## 왼쪽 거 기준으로(첫 번째)\n#pd.merge(df1, df2, how = 'right')  ## 오른쪽 거 기준으로(두 번째)\npd.merge(df1, df2, how = 'outer')  ## 개방적으로, 자료를 전부 보전\n\n\n\n\n\n\n\n\ndept\ncount\ndesc\n\n\n\n\n0\n통계\n20.0\n통계학과는...\n\n\n1\n수학\n30.0\n수학과는...\n\n\n2\n과학\n25.0\n과학학과는...\n\n\n3\nIAB\n50.0\nNaN\n\n\n4\n신설학과\nNaN\n이 학과는 내년에 신설될 예정이고..."
  },
  {
    "objectID": "posts/Data Visualization/Review/12. merge와 concat.html#concat-merge를-이용한-데이터-병합",
    "href": "posts/Data Visualization/Review/12. merge와 concat.html#concat-merge를-이용한-데이터-병합",
    "title": "1. 라이브러리 imports",
    "section": "4. concat, merge를 이용한 데이터 병합",
    "text": "4. concat, merge를 이용한 데이터 병합\n\ndf_course2023 = pd.DataFrame({\n    'name':['최규빈']*3+['최혜미']*2+['이영미']+['양성준'],\n    'year':[2023]*7,\n    'course':['파이썬프로그래밍', '데이터시각화', '기계학습활용','수리통계1', '수리통계2','회귀분석1','통계수학']})\ndf_course2023\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n\n\n1\n최규빈\n2023\n데이터시각화\n\n\n2\n최규빈\n2023\n기계학습활용\n\n\n3\n최혜미\n2023\n수리통계1\n\n\n4\n최혜미\n2023\n수리통계2\n\n\n5\n이영미\n2023\n회귀분석1\n\n\n6\n양성준\n2023\n통계수학\n\n\n\n\n\n\n\n\ndf_course2024 = pd.DataFrame({\n    'name':['최규빈','이영미','이영미','양성준','최혜미'],\n    'year':[2024]*5,\n    'course':['기계학습활용','수리통계1', '수리통계2','회귀분석1','통계수학']})\ndf_course2024\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2024\n기계학습활용\n\n\n1\n이영미\n2024\n수리통계1\n\n\n2\n이영미\n2024\n수리통계2\n\n\n3\n양성준\n2024\n회귀분석1\n\n\n4\n최혜미\n2024\n통계수학\n\n\n\n\n\n\n\n\ndf_score = pd.DataFrame({\n    'name':['최규빈','최규빈','이영미','이영미','양성준','양성준','최혜미','최혜미'],\n    'year':[2023,2024]*4,\n    'score':[1, 1.2, 5,5,5,5,5,5]})\ndf_score\n\n\n\n\n\n\n\n\nname\nyear\nscore\n\n\n\n\n0\n최규빈\n2023\n1.0\n\n\n1\n최규빈\n2024\n1.2\n\n\n2\n이영미\n2023\n5.0\n\n\n3\n이영미\n2024\n5.0\n\n\n4\n양성준\n2023\n5.0\n\n\n5\n양성준\n2024\n5.0\n\n\n6\n최혜미\n2023\n5.0\n\n\n7\n최혜미\n2024\n5.0\n\n\n\n\n\n\n\n\ndf_sex = pd.DataFrame({'name':['최규빈','이영미','양성준','최혜미'],\n                        'sex':['male','female','male','female']})\ndf_sex\n\n\n\n\n\n\n\n\nname\nsex\n\n\n\n\n0\n최규빈\nmale\n\n\n1\n이영미\nfemale\n\n\n2\n양성준\nmale\n\n\n3\n최혜미\nfemale\n\n\n\n\n\n\n\n주어진 정보를 바탕으로, 4개의 데이터프레임을 결합하라.\n- 풀이\ndf_course2023  ## 7개 강의목록\ndf_course2024  ## 5개 강의목록\ndf_score  ## 점수\ndf_sex  ## 교수님 성별\n\n위 두개는 열이 똑같다. (concat)\n\n\n아래 두 개는 다른 정보이다. (merge)\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n\n\n1\n최규빈\n2023\n데이터시각화\n\n\n2\n최규빈\n2023\n기계학습활용\n\n\n3\n최혜미\n2023\n수리통계1\n\n\n4\n최혜미\n2023\n수리통계2\n\n\n5\n이영미\n2023\n회귀분석1\n\n\n6\n양성준\n2023\n통계수학\n\n\n7\n최규빈\n2024\n기계학습활용\n\n\n8\n이영미\n2024\n수리통계1\n\n\n9\n이영미\n2024\n수리통계2\n\n\n10\n양성준\n2024\n회귀분석1\n\n\n11\n최혜미\n2024\n통계수학\n\n\n\n\n\n\n\n\n단순히 두 개의 데이터프레임을 합쳤다.\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True).merge(df_score)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\nscore\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n1.0\n\n\n1\n최규빈\n2023\n데이터시각화\n1.0\n\n\n2\n최규빈\n2023\n기계학습활용\n1.0\n\n\n3\n최혜미\n2023\n수리통계1\n5.0\n\n\n4\n최혜미\n2023\n수리통계2\n5.0\n\n\n5\n이영미\n2023\n회귀분석1\n5.0\n\n\n6\n양성준\n2023\n통계수학\n5.0\n\n\n7\n최규빈\n2024\n기계학습활용\n1.2\n\n\n8\n이영미\n2024\n수리통계1\n5.0\n\n\n9\n이영미\n2024\n수리통계2\n5.0\n\n\n10\n양성준\n2024\n회귀분석1\n5.0\n\n\n11\n최혜미\n2024\n통계수학\n5.0\n\n\n\n\n\n\n\n\n연도별로 점수를 넣어줬다. name과 year만 겹치므로 알아서 합쳐진다.\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True).merge(df_score).merge(df_sex)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\nscore\nsex\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n1.0\nmale\n\n\n1\n최규빈\n2023\n데이터시각화\n1.0\nmale\n\n\n2\n최규빈\n2023\n기계학습활용\n1.0\nmale\n\n\n3\n최규빈\n2024\n기계학습활용\n1.2\nmale\n\n\n4\n최혜미\n2023\n수리통계1\n5.0\nfemale\n\n\n5\n최혜미\n2023\n수리통계2\n5.0\nfemale\n\n\n6\n최혜미\n2024\n통계수학\n5.0\nfemale\n\n\n7\n이영미\n2023\n회귀분석1\n5.0\nfemale\n\n\n8\n이영미\n2024\n수리통계1\n5.0\nfemale\n\n\n9\n이영미\n2024\n수리통계2\n5.0\nfemale\n\n\n10\n양성준\n2023\n통계수학\n5.0\nmale\n\n\n11\n양성준\n2024\n회귀분석1\n5.0\nmale\n\n\n\n\n\n\n\n\nname이 겹치므로 알아서 합쳐진다."
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html",
    "title": "Tidydata 만들기",
    "section": "",
    "text": "여러가지 방법들을 사용해서 tidydata를 만들어보자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#라이브러리-imports",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#라이브러리-imports",
    "title": "Tidydata 만들기",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot\nfrom plotnine import *"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#pandas---lambda-_df-의-활용",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#pandas---lambda-_df-의-활용",
    "title": "Tidydata 만들기",
    "section": "2. Pandas - lambda _df :의 활용",
    "text": "2. Pandas - lambda _df :의 활용\n\nA. lambda _df : with indexer\n\n- 예시 1 : 아래와 같은 데이터프레임이 있다고 할 때, 표현 1, 2, 3은 모두 같은 문법이다.\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n표현 1\n\ndf[df.A.isna()]  ## 행 슬라이싱, [False, True, False, False]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n표현 2\n\ndf[(lambda _df : _df.A.isna())(df)]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n표현 3\n\ndf[lambda _df : _df.A.isna()]  ## 괄호로 함수를 묶어줘도 되고(그럼 구분이 쉬워진다), 안해도 됨\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n- 예시 2 : .loc, .iloc\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.loc[lambda _df : _df.A.isna()]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\ndf.iloc[lambda _df : list(_df.A.isna())]  ## iloc은 튜플로 입력이 안된다.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\niloc의 경우 범용성이 떨어지긴 한다… 그러니까 왠만해선 .loc을 사용하거나 시리즈를 list로 묶어주자…\n\n근데 왜 이런 문법이 있을까? 연속적으로 DataFrame을 변화시켜야 할 경우 유용하기 때문이다.\n- 예시 3\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.assign(D = df.A + df.B + df.C)  ## 결측치가 있을 경우 합은 NaN이 됨.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\n해당 데이터프레임에서 결측치의 수가 50%가 넘는 열만 고르고 싶다면?\n\n\ndf.assign(D = df.A + df.B + df.C).loc[:, lambda _df : _df.isna().mean() &gt; 0.5]\n\n\n\n\n\n\n\n\nD\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n3\n11.0"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-lambda-df-with-assign",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-lambda-df-with-assign",
    "title": "Tidydata 만들기",
    "section": "### B. lambda df: with assign",
    "text": "### B. lambda df: with assign\n예시 1\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.assign(D = df.A + df.B + df.C)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\n여기에서 결측치의 값을 count하여 새로운 열 E에 할당하고 싶다면?\n\n\ndf.assign(D = df.A + df.B + df.C).assign(E = lambda _df : _df.isna().sum(axis = 1))\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n2\n\n\n1\nNaN\n3.0\n4.0\nNaN\n2\n\n\n2\n1.0\nNaN\n5.0\nNaN\n2\n\n\n3\n1.0\n4.0\n6.0\n11.0\n0\n\n\n\n\n\n\n\n예시 2 : 원본 데이터를 손상시키지 않으며 데이터를 변형하고 싶을 때\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':[12,234,3456,12345,654222]})\ndf\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n\ndf2 = df\ndf2['B'] = np.log(df2.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n???\n\n~이성적으로 이해하기 어려운 결과~ 왜 이렇게 될까? 왜냐면 df2는 df와 같은 녀석을 의미하기 때문이다.(id가 같음)\n\ndf2 = df.copy()\ndf2['B'] = np.log(df2.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n이러면 원본 데이터를 손상시키지 않는다. 또는…\n\n\ndf.assign(B = lambda _df : np.log(df.A)).assign(C = lambda _df : (_df.B - _df.B.mean())/_df.B.std())\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#pandas---multi_index의-이해",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#pandas---multi_index의-이해",
    "title": "Tidydata 만들기",
    "section": "3. Pandas - Multi_index의 이해",
    "text": "3. Pandas - Multi_index의 이해\n\nA. 원래 df, s는 딕셔너리 계열임\n\n- 예시 1 : df는 dict에서 만들 수 있음\n\ndct = {'A': [1,2,3],'B': [2,3,4]}  ## 애초에 데이터프레임에 입력되는 값과 동일...\ndf = pd.DataFrame(dct)\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 예시 2 : s도 dict에서 만들 수 있음.\n\ndct = {'43052': 80, '43053': 90, '43054': 50}   ## key가 index가 된다.\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n- 예시 3 : dict의 키로 올 수 있는 것들\n\n튜플로 dict를 만든다면?\n\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns\n\n43052  4    80\n43053  1    90\n43054  2    50\ndtype: int64\n\n\n\ns.index\n\nMultiIndex([('43052', 4),\n            ('43053', 1),\n            ('43054', 2)],\n           )\n\n\n\n멀쩡하게 잘 작동한다. 그리고 MultiIndex가 나온다."
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-.index혹은-.columns에-name이-있는-경우",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-.index혹은-.columns에-name이-있는-경우",
    "title": "Tidydata 만들기",
    "section": "### B. .index혹은 .columns에 name이 있는 경우",
    "text": "### B. .index혹은 .columns에 name이 있는 경우\n예시 1 : index에 이름이 있는 경우\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns.rename_axis(['id'])  ## set_axis()가 인덱스와 컬럼의 이름을 조정하는 것이라면 이건 그것의 이름을 조정한다.\n\nid\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns.index, s.rename_axis(['id']).index\n\n(Index(['43052', '43053', '43054'], dtype='object'),\n Index(['43052', '43053', '43054'], dtype='object', name='id'))\n\n\n예시 2 : index에 이름이 있는 경우(멀티 인덱스)\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns.rename_axis(['id','year'])\n\nid     year\n43052  4       80\n43053  1       90\n43054  2       50\ndtype: int64\n\n\n\nMultiIndex에서 인덱스의 이름을 각각 지정해준 경우이다.\n\n\n예시 2가 데이터프레임이라면 이렇게 보인다.\n\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ndf = pd.DataFrame(s.rename_axis(['id','year']))  ## index의 이름을 지정해줌\ndf\n\n\n\n\n\n\n\n\n\n0\n\n\nid\nyear\n\n\n\n\n\n43052\n4\n80\n\n\n43053\n1\n90\n\n\n43054\n2\n50\n\n\n\n\n\n\n\n\n만약 여기서 각 원소를 호출하고 싶다면…\n\n\ndf.loc[('43052', 4)]\n\n0    80\nName: (43052, 4), dtype: int64\n\n\n\n그냥 멀티인덱스, 튜플을 입력하여 호출하면 된다."
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#tidydata",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#tidydata",
    "title": "Tidydata 만들기",
    "section": "4. Tidydata",
    "text": "4. Tidydata\n\nA. tidydata의 개념\n\n- 아래의 자료는 불리하다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\n만약 A학과에 해당하는 결과만 뽑고 싶다면? -&gt; department가 column으로 있어야 함…\npass인 사람만 bar plot을 그리고 싶다면? result가 column으로 있어야 함…\n\n원하는 정보를 쉽게 뽑아낼 수 있는 데이터를 tidydata라고 한다.\n\ntidydata = df['pass'].reset_index()\n#---#\nfig = ggplot(tidydata)\ncol = geom_col(aes(x='department',y='pass',fill='gender'),position='dodge')   ## dodge 설정으로 누적으로 표기하지 않고 옆에 늘여서 표시\nfig + col\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-tidydata가-아닌-예시",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-tidydata가-아닌-예시",
    "title": "Tidydata 만들기",
    "section": "### B. tidydata가 아닌 예시",
    "text": "### B. tidydata가 아닌 예시\n\nMultiIndex 구조를 가지면 무조건 tidydata가 아님\n열의 값이 여러 개의 정보를 가지고 있다면 tidydata가 아님\nwide data는 tidydata가 아님 -&gt; melt나 pivot_table등을 활용하여 조정해줘야…\n\n- wide df 예시\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n- 여러 방법을 통해 tidydata로 변환\n\ndf.set_index(['Date']).stack().reset_index().rename({'level_1' : 'Brand', 0 : 'Sales'}, axis = 1)\n\n\n\n\n\n\n\n\nDate\nBrand\nSales\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-10\nApple\n324\n\n\n2\n2019-10\nHuawei\n136\n\n\n3\n2019-10\nXiaomi\n109\n\n\n4\n2019-10\nOppo\n76\n\n\n...\n...\n...\n...\n\n\n203\n2020-10\nNokia\n20\n\n\n204\n2020-10\nLenovo\n22\n\n\n205\n2020-10\nOnePlus\n9\n\n\n206\n2020-10\nSony\n22\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n- melt를 통해 변환\n\ndf.melt(id_vars = ['Date'])  ## 이럼 한번에 되서 편하긴 하다. wide data 한정\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-11\nSamsung\n461\n\n\n2\n2019-12\nSamsung\n426\n\n\n3\n2020-01\nSamsung\n677\n\n\n4\n2020-02\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06\nAsus\n16\n\n\n204\n2020-07\nAsus\n12\n\n\n205\n2020-08\nAsus\n20\n\n\n206\n2020-09\nAsus\n15\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\ndf.melt(id_vars = [])는 index로 지정한 열외의 모든 열들을 한 행에 엮어버린다.\ndf.stack()의 경우 set_index()나 reset_index()등 사용해야 할 게 많을 수 있다. 하지만 직관적이고 사용에 용이하다.\ndf.stack()의 반대로 df.unstack()을 사용하여 인덱스를 컬럼으로 올릴수도 있다."
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#pivot_table-groupby-aggregate",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#pivot_table-groupby-aggregate",
    "title": "Tidydata 만들기",
    "section": "5. pivot_table, groupby + aggregate",
    "text": "5. pivot_table, groupby + aggregate\n\n대부분은 pivot_table로 해결이 된다.\n\n\nA. intro\n\n- tidydata 만드는 개념 : 그룹화 -&gt; 집계\n예제 1 : 아래의 데이터프레임에서 학과, 성별로 count의 합계를 구하라.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\ndf.pivot_table(index = ['department', 'gender'], values = 'count', aggfunc = sum).reset_index()\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n\n인덱스에만 몰아주든 둘다 넣어주든 똑같이 tidydata를 만들어준다.\n\n- 예시에서 본 작업은 아래의 작업들로 세분화할 수 있다.\n\n그룹화(쿼리) : 하나의 DataFrame을 sub-dataframe으로 나누는 과정, 전체 자료를 (학과, 성별)로 묶어 총 12개의 sub-dataframe을 만든다.\n각각집계 : 나눠진 sub-dataframe에서 어떠한 계산을 각각 수행함, 나눠진 sub-dataframe에서 지원자 수의 합계를 각각 구함\n\n- 위와 같은 작업을 하려면 아래와 같은 요소들이 필요하다.\n\n그룹변수~(없는 용어임)~ : 그룹화를 위해 필요한 변수 : DataFrame을 sub-dataframe으로 나누는 역할. &gt;&gt; index and columns &gt; 범주형이거나 범주형으로 바꿀 수 있는 데이터\n집계변수~(이것도 없는 용어임)~ : 집계함수의 대상이 되는 변수 &gt;&gt; values\n집계함수 : 그룹화된 데이터프레임에 수행하는 계산을 정의하는 함수 &gt;&gt; aggfunc"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-pivot_table의-문법",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#b.-pivot_table의-문법",
    "title": "Tidydata 만들기",
    "section": "### B. pivot_table의 문법",
    "text": "### B. pivot_table의 문법\n- pivot_table의 문법\ndf.pivot_table(\n    index = 그룹변수\n    columns = 그룹변수\n    values = 집계변수\n    aggfunc = 집계함수\n)\nindex & columns에 그룹변수를 적절히 나누어 입력한다.\n예시 : 집계함수 전달방법\n\ndf = pd.DataFrame({'category':['A']*5+['B']*5, 'value':np.concatenate([np.random.randn(5), np.random.randn(5)+10])})\ndf\n\n\n\n\n\n\n\n\ncategory\nvalue\n\n\n\n\n0\nA\n0.383420\n\n\n1\nA\n1.084175\n\n\n2\nA\n1.142778\n\n\n3\nA\n0.307894\n\n\n4\nA\n0.237787\n\n\n5\nB\n10.355951\n\n\n6\nB\n8.336925\n\n\n7\nB\n8.617227\n\n\n8\nB\n8.073155\n\n\n9\nB\n8.513784\n\n\n\n\n\n\n\n\ndf.pivot_table(index = 'category', values = 'value', aggfunc = np.sum)\ndf.pivot_table(index = 'category', values = 'value', aggfunc = 'sum')\n## 동일한 코드\n\n\n\n\n\n\n\n\nvalue\n\n\ncategory\n\n\n\n\n\nA\n3.156054\n\n\nB\n43.897041\n\n\n\n\n\n\n\n\ndf.pivot_table(index = 'category', values = 'value', aggfunc = ['sum', 'count'])\n\n\n\n\n\n\n\n\nsum\ncount\n\n\n\nvalue\nvalue\n\n\ncategory\n\n\n\n\n\n\nA\n3.156054\n5\n\n\nB\n43.897041\n5\n\n\n\n\n\n\n\n\n집계함수들의 리스트를 넣는다면 각각의 집계치를 따로 알아서 구해준다.\n\n\nC. groupby + aggregate의 문법\n\n- groupby + aggregate\n\ndf.groupby(그룹변수).aggregate({집계변수:집계함수})\n\n그룹화를 한 후(index, columns), 무엇을 집계할 것인지 dictionary로 지정해준다.(values, aggfunc)\n\n딕셔너리로 지정해주는 것은 pivot_table도 가능하긴 하다…"
  },
  {
    "objectID": "posts/Data Visualization/Review/10. tidydata, 시각화.html#연습---airline-data",
    "href": "posts/Data Visualization/Review/10. tidydata, 시각화.html#연습---airline-data",
    "title": "Tidydata 만들기",
    "section": "6. 연습 - Airline Data",
    "text": "6. 연습 - Airline Data\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58492 entries, 0 to 58491\nData columns (total 14 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   MONTH      58492 non-null  int64  \n 1   DAY        58492 non-null  int64  \n 2   WEEKDAY    58492 non-null  int64  \n 3   AIRLINE    58492 non-null  object \n 4   ORG_AIR    58492 non-null  object \n 5   DEST_AIR   58492 non-null  object \n 6   SCHED_DEP  58492 non-null  int64  \n 7   DEP_DELAY  57659 non-null  float64\n 8   AIR_TIME   57474 non-null  float64\n 9   DIST       58492 non-null  int64  \n 10  SCHED_ARR  58492 non-null  int64  \n 11  ARR_DELAY  57474 non-null  float64\n 12  DIVERTED   58492 non-null  int64  \n 13  CANCELLED  58492 non-null  int64  \ndtypes: float64(3), int64(8), object(3)\nmemory usage: 6.2+ MB\n\n\n- ChatGPT의 도움을 받아 해당 인포를 정리\n\nMONTH: 비행이 이루어진 월을 나타냄. 1에서 12 사이의 값을 갖음.\nDAY: 비행이 이루어진 일자를 나타냄. 월에 따라 1~28/29/30/31 사이의 값을 1. 가질 수 있음.\nWEEKDAY: 비행이 이루어진 요일을 나타냄. 일반적으로 1(일요일)부터 7(토요일1)까지의 값을 갖음.\nAIRLINE: 해당 항공편을 운영하는 항공사의 약어나 코드를 나타냄.\nORG_AIR: 비행기가 출발하는 공항의 약어나 코드를 나타냄.\nDEST_AIR: 비행기가 도착하는 공항의 약어나 코드를 나타냄.\nSCHED_DEP: 원래의 예정된 출발 시간을 나타냄. 시간은 일반적으로 HHMM 형식으로 표시될 수 있음.\nDEP_DELAY: 출발 지연 시간을 나타냄. 음수 값은 조기 출발, 양수 값은 지연을 의미함.\nAIR_TIME: 실제 공중에서 비행한 시간을 분 단위로 나타냄.\nDIST: 비행 거리를 나타냄. 일반적으로 마일 또는 킬로미터로 표시됨.\nSCHED_ARR: 원래의 예정된 도착 시간을 나타냄. SCHED_DEP와 같은 형식으로 표시될 수 있음.\nARR_DELAY: 도착 지연 시간을 나타냄. 음수는 조기 도착, 양수는 지연을 의미함.\nDIVERTED: 항공편이 다른 곳으로 우회되었는지를 나타냄. 1은 우회, 0은 정상 경로를 의미함.\nCANCELLED: 항공편이 취소되었는지 여부를 나타냄. 1은 취소, 0은 취소되지 않음을 의미함.\n\n# 예제1 : 항공사별로 도착지연시간의 평균을 구하라.\n\ndf.pivot_table(index = 'AIRLINE', values = 'ARR_DELAY', aggfunc = 'mean')\ndf.groupby(by = 'AIRLINE').aggregate({'ARR_DELAY' : 'mean'})\n## 동일한 코드\n\n\n\n\n\n\n\n\nARR_DELAY\n\n\nAIRLINE\n\n\n\n\n\nAA\n5.542661\n\n\nAS\n-0.833333\n\n\nB6\n8.692593\n\n\nDL\n0.339691\n\n\nEV\n7.034580\n\n\nF9\n13.630651\n\n\nHA\n4.972973\n\n\nMQ\n6.860591\n\n\nNK\n18.436070\n\n\nOO\n7.593463\n\n\nUA\n7.765755\n\n\nUS\n1.681105\n\n\nVX\n5.348884\n\n\nWN\n6.397353\n\n\n\n\n\n\n\n# 예제2 : 항공사 별로 비행취소건수의 합계를 구하라. 취소건수가 높은 항공사 순으로 정렬하라.\n\ndf.pivot_table(index = 'AIRLINE', values = 'CANCELLED', aggfunc = 'sum').sort_values('CANCELLED', ascending = False)\ndf.groupby(by = 'AIRLINE').aggregate({'CANCELLED' : 'sum'}).sort_values('CANCELLED', ascending = False)\n## 역시 동일한 코드\n\n\n\n\n\n\n\n\nCANCELLED\n\n\nAIRLINE\n\n\n\n\n\nAA\n154\n\n\nMQ\n152\n\n\nEV\n146\n\n\nOO\n142\n\n\nUA\n93\n\n\nWN\n93\n\n\nDL\n38\n\n\nNK\n25\n\n\nUS\n21\n\n\nF9\n10\n\n\nVX\n6\n\n\nB6\n1\n\n\nAS\n0\n\n\nHA\n0\n\n\n\n\n\n\n\n# 예제3 : 항공사별로 비행취소율을 구하라. 비행취소율이 가장 높은 항공사 순으로 정렬하라\n\ndf.pivot_table(index = 'AIRLINE', values = 'CANCELLED', aggfunc = 'mean').sort_values('CANCELLED', ascending = False)\ndf.groupby(by = 'AIRLINE').aggregate({'CANCELLED' : 'mean'}).sort_values('CANCELLED', ascending = False)\n## 동일한 코드\n\n\n\n\n\n\n\n\nCANCELLED\n\n\nAIRLINE\n\n\n\n\n\nMQ\n0.043791\n\n\nEV\n0.024923\n\n\nOO\n0.021554\n\n\nAA\n0.017303\n\n\nNK\n0.016491\n\n\nUS\n0.013003\n\n\nUA\n0.011935\n\n\nWN\n0.011048\n\n\nF9\n0.007593\n\n\nVX\n0.006042\n\n\nDL\n0.003585\n\n\nB6\n0.001842\n\n\nAS\n0.000000\n\n\nHA\n0.000000\n\n\n\n\n\n\n\n# 예제4 : (항공사, 요일)별 비행취소건수와 비행취소율을 조사하라.\n\ndf.pivot_table(index = ['AIRLINE', 'WEEKDAY'], values = 'CANCELLED', aggfunc = ['sum', 'mean'])\ndf.groupby(by = ['AIRLINE', 'WEEKDAY']).aggregate({'CANCELLED' : ['sum','mean']})\n## 동일\n\n\n\n\n\n\n\n\n\nCANCELLED\n\n\n\n\nsum\nmean\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\nAA\n1\n41\n0.032106\n\n\n2\n9\n0.007341\n\n\n3\n16\n0.011949\n\n\n4\n20\n0.015004\n\n\n5\n18\n0.014151\n\n\n...\n...\n...\n...\n\n\nWN\n3\n18\n0.014118\n\n\n4\n10\n0.007911\n\n\n5\n7\n0.005828\n\n\n6\n10\n0.010132\n\n\n7\n7\n0.006066\n\n\n\n\n98 rows × 2 columns\n\n\n\n# 예제4 : (항공사, 요일)별로 CANCELLED는 평균과 합계를 구하고, AIR_TIME은 평균과 표준편차를 구하여라.\n\ndf.pivot_table(index = ['AIRLINE', 'WEEKDAY'], values = ['CANCELLED', 'AIR_TIME'], aggfunc = {'CANCELLED' : ['sum', 'mean'], 'AIR_TIME' : ['mean', 'std']})\ndf.groupby(by = ['AIRLINE', 'WEEKDAY']).aggregate({'CANCELLED' : ['mean', 'sum'], 'AIR_TIME' : ['mean', 'std']})\n## 거의 유사한 코드임\n\n\n\n\n\n\n\n\n\nCANCELLED\nAIR_TIME\n\n\n\n\nmean\nsum\nmean\nstd\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n41\n147.610569\n73.442540\n\n\n2\n0.007341\n9\n143.851852\n73.211275\n\n\n3\n0.011949\n16\n144.514005\n73.340675\n\n\n4\n0.015004\n20\n141.124618\n69.220840\n\n\n5\n0.014151\n18\n145.430966\n76.711095\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n18\n104.219920\n53.869040\n\n\n4\n0.007911\n10\n107.200800\n54.466218\n\n\n5\n0.005828\n7\n107.893635\n57.172695\n\n\n6\n0.010132\n10\n109.247433\n56.149388\n\n\n7\n0.006066\n7\n107.602273\n56.419207\n\n\n\n\n98 rows × 4 columns\n\n\n\n# 예제5 : 운행구간(거리의 구간)을 그룹화하고, 운행구간 별 비행취소건수와 취소율을 구하여라.\n\npd.qcut(df.DIST, q = 4)  ## 어레이나 시리즈를 넣어주면 해당 시리즈를 분위수로 컷한다.\n\n0          (391.0, 690.0]\n1        (1199.0, 4502.0]\n2          (391.0, 690.0]\n3         (690.0, 1199.0]\n4        (1199.0, 4502.0]\n               ...       \n58487    (1199.0, 4502.0]\n58488      (391.0, 690.0]\n58489     (66.999, 391.0]\n58490     (690.0, 1199.0]\n58491      (391.0, 690.0]\nName: DIST, Length: 58492, dtype: category\nCategories (4, interval[float64, right]): [(66.999, 391.0] &lt; (391.0, 690.0] &lt; (690.0, 1199.0] &lt; (1199.0, 4502.0]]\n\n\n\ndf.assign(DIST_CUT = pd.qcut(df.DIST, q = 4))\\\n.pivot_table(index = 'DIST_CUT', values = 'CANCELLED', aggfunc = ['sum', 'mean'])\n\n\n\n\n\n\n\n\nsum\nmean\n\n\n\nCANCELLED\nCANCELLED\n\n\nDIST_CUT\n\n\n\n\n\n\n(66.999, 391.0]\n334\n0.022659\n\n\n(391.0, 690.0]\n196\n0.013503\n\n\n(690.0, 1199.0]\n203\n0.013637\n\n\n(1199.0, 4502.0]\n148\n0.010313\n\n\n\n\n\n\n\n\n긴 구간일 수록 취소율이 낮은 것 같다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RiverFlow",
    "section": "",
    "text": "Python을 이용한 문제 풀이, Kaggle competition, R 프로그래밍 연습 등 학습 내역과 작업물들을 기록합니다.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nR과 Python, 그 외 공부하는 데 필요했던 자료들이나 대회, 공모전, 연습했던 것들을 올립니다."
  },
  {
    "objectID": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html",
    "href": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "",
    "text": "파이썬을 이용하여 간단한 그래프를 그려보고, 이미지를 이퀼라이징하는 방법을 알아보도록 하자."
  },
  {
    "objectID": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#사전작업",
    "href": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#사전작업",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 import\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n##--------이퀼라이징을 위한 라이브러리--------\n#!pip install opencv-python\nimport cv2\n\n##--------parameter 설정--------\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (3,2)\nmatplotlib.rcParams['figure.dpi'] = 150 ## 450 : 300\n\n\n오늘 알아볼 함수들\n\nplt.boxplot()      ## 박스플롯 생성\nnp.random.randn()  ## 정규분포 하 확률변수 추출(default : 표준정규분포에서 1개 추출)\nnp.random.seed()   ## 시드 생성\nplt.hist()         ## 히스토그램 생성\ncv2.imread()       ## 이미지를 행렬로 읽어들임\nplt.imshow()       ## 행렬로 저장된 이미지를 시각화\ncv2.equalizeHist() ## 히스토그램 이퀼라이징\n\n!wget link         ## 파일 다운로드(리눅스)"
  },
  {
    "objectID": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#플롯",
    "href": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#플롯",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "2. 플롯",
    "text": "2. 플롯\n\nA. Boxplot\n전북고등학교에는 10명의 학생이 있는 두 개의 학급이 있고, 각 학생들이 받은 점수는 아래와 같다.\n\ny1 = [75,75,76,76,77,77,78,79,79,98]\ny2 = [76,76,77,77,78,78,79,80,80,81]\n\n\ny1_frame = pd.DataFrame(y1)\ny1_frame.describe()\n\n\n  \n    \n\n\n\n\n\n\n0\n\n\n\n\ncount\n10.000000\n\n\nmean\n79.000000\n\n\nstd\n6.831301\n\n\nmin\n75.000000\n\n\n25%\n76.000000\n\n\n50%\n77.000000\n\n\n75%\n78.750000\n\n\nmax\n98.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 1반의 평균은 \\(79\\)\n\ny2_frame = pd.DataFrame(y2)\ny2_frame.describe()\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n10.00000\n\n\nmean\n78.20000\n\n\nstd\n1.75119\n\n\nmin\n76.00000\n\n\n25%\n77.00000\n\n\n50%\n78.00000\n\n\n75%\n79.75000\n\n\nmax\n81.00000\n\n\n\n\n\n\n\n- 2반의 평균은 \\(78.2\\)이다.\n그렇다면 1반(y1)과 2반(y2), 두 반을 지도하는 선생님 중 어떤 선생님이 우수할까?\n\n아마도… : 평균을 중심으로 분석할 시 y1이 더 잘 지도했다고 판단할 수 있다.\n반론 : 평균은 A반이 더 높으나, 편차또한 더 크다. 고득점을 받는 한 학생(outlier)을 제외하면 전체적으로 B반 학생들이 시험을 더 잘 보았다고 해석할 수 있다.\n\n단순한 평균비교보다 학생들이 받은 점수의 분포를 비교하는 것이 중요.\n따라서 그 분포를 알아보기 위해 Boxplot을 그려보자.\n\n\nmatplotlib로 boxplot 그리기\n\nplt.boxplot(y1);  ## 세미콜론을 붙이면 결과값만 출력한다.\n\n\n\n\n\nplt.boxplot(y2);\n\n\n\n\n\nplt.boxplot([y1,y2]); ## 2차원의 리스트를 넣어 여러 개를 동시에 출력시킬 수도 있다.\n\n##np.array([y1, y2]).shape ## &gt; (2, 10)\n\n\n\n\n위처럼 하나의 outlier를 배제한다면, 나머지의 분포는 2반이 더 높게 위치함을 알 수 있다.\n\n박스플롯의 장점 : 단순히 평균만 제공하는 것보다 데이터를 파악하고 직관을 얻기에 유용하다.\n박스플롯이 이용되는 범위 : 초기 자료 분포를 파악하기 용이, 두 개 이상의 방법을 비교\n\n\n\nB. Histogram\n- 중심경향치(평균, 중앙값)만 가지고 집단을 비교할 순 없다.\n이전의 자료도 결과론적으로 중앙값이 더 타당해 보이나, 이것을 근거로 B반이 공부를 더 잘했다는 주장도 비합리적이다.\n\n단순 평균비교로 이러한 질문에 답을 하기 어려움.\n박스플롯으로 전체분포를 파악해도 어떤 반이 공부를 더 잘한다는 기준을 잡기 애매함.\n\nBut!\n특수한 경우에는 두 반 중에 누가 더 공부를 잘하냐는 질문에 명확히 대답할 수 있다.\n정규분포 전북고등학교 : 평균은 좋은 측정값인가?\n\nnp.random.seed(43052)\ny1 = np.random.randn(10000)   ## random.randn, standard normal distribution\ny2 = np.random.randn(10000) + 0.5\n\n- 두 반의 성적은 모두 표준정규분포를 따르는데, 2반의 성적이 일괄적으로 0.5가 높은 상황\n\nnp.mean(y1), np.mean(y2)\n\n(-0.011790879905079434, 0.4979147460611458)\n\n\n\nplt.boxplot([y1,y2]);\n\n\n\n\n\n분포의 모양이 거의 비슷한데, 중앙값(평균)이 2반이 더 높으므로 성적이 더 높다고 말할 수 있다. &gt; 게다가 평균적으로 0.5점 정도 더 공부를 잘한다고 대답할 수 있다!\n\n근데, 위와 같은 경우는 정규분포에서 뽑힌 랜덤샘플이라 분포의 모양이 같다고 하긴 했는데… 실제 데이터를 확인할 때는 박스플롯으로 하긴 어려워보인다.\n따라서!\n히스토그램을 그려 확인해보자\nplt.hist(array, bins = int, range = list)\n\nplt.hist([y1, y2], bins = 100);\n\n\n\n\n\n둘의 분포는 비슷하지만, 2반(주황색)이 조금 더 높은 수준에서 자리함을 알 수 있다."
  },
  {
    "objectID": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#equalization",
    "href": "posts/Data Visualization/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#equalization",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "3. Equalization",
    "text": "3. Equalization\n히스토그램이나 이미지를 눈으로 보기 쉽도록 이퀼라이징해보자!\n이미지 자료 다운로드\n\n#!pip install wget\nimport wget\n\nwget.download(\"https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\")\nimg = cv2.imread(\"Unequalized_Hawkes_Bay_NZ.jpg\")\n\n##--------리눅스 환경 충족 시--------\n##!wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\n##img = cv2.imread('https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg')\n##!rm Unequalized_Hawkes_Bay_NZ.jpg\n\n## 파일을 들여오고 인식한 뒤 삭제하는 코드이다.\n\nRequirement already satisfied: wget in c:\\users\\hollyriver\\anaconda3\\envs\\ssk\\lib\\site-packages (3.2)\n100% [............................................................................] 110895 / 110895\n\n\n\nplt.imshow(img) ## image show\n\n&lt;matplotlib.image.AxesImage at 0x184b83fab00&gt;\n\n\n\n\n\n\nplt.imshow()를 통해서 이미지를 가져왔다!\n\n근데, img는 어떤 값으로 저장된 걸까?\n\nA. 사실 이미지는 숫자열이었다!\n\n_img1 = np.array([0,30,90,120,150,180,210,240,255]).reshape(3,-1)  ## 3행 3열로 변경\n_img1\n\narray([[  0,  30,  90],\n       [120, 150, 180],\n       [210, 240, 255]])\n\n\n\nplt.imshow(_img1, cmap = 'gray')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n_img2 = np.array([0,20,40,60,80,100,120,140,160]).reshape(3,3)\n_img2\n\narray([[  0,  20,  40],\n       [ 60,  80, 100],\n       [120, 140, 160]])\n\n\n\nplt.imshow(_img2, cmap = 'gray', vmin = 0, vmax = 255)  ## vmin, vmax를 설정해주지 않으면 가장 큰 값이 max(white)가 된다\nplt.colorbar()\nplt.show()\n\n\n\n\n255에 가까울 수록 하얀색, 0에 가까울 수록 검정색인 이미지로 변환된 것을 볼 수 있다. 숫자만으로 이뤄진 행렬이 이미지가 된 것이다!\n크게, 더 크게 해보자!\n\n_img3 = np.concatenate([_img1,_img2], axis = 1)  ## 열로 병합, default는 행으로 병합\n_img3\n\narray([[  0,  30,  90,   0,  20,  40],\n       [120, 150, 180,  60,  80, 100],\n       [210, 240, 255, 120, 140, 160]])\n\n\n\nplt.imshow(_img3, cmap = 'gray')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\nB. RGB값을 더한 그림 그리기\n\n먼저, RGB값에 해당하는 수를 각각 array로 지정해주자.\n\n\nr = np.array(\n    [[255, 255, 255,  0,   0],\n     [255, 255, 255,  0,   0],\n     [255, 255, 255,  0,   0],\n     [  0,   0,   0,  0,   0],\n     [  0,   0,   0,  0,   0]]\n)\ng = np.array(\n    [[  0,   0, 255, 255, 255],\n     [  0,   0, 255, 255, 255],\n     [  0,   0, 255, 255, 255],\n     [  0,   0,   0,   0,   0],\n     [  0,   0,   0,   0,   0]]\n)\nb = np.array(\n    [[  0,   0,   0,   0,   0],\n     [  0,   0,   0,   0,   0],\n     [255, 255, 255, 255, 255],\n     [255, 255, 255, 255, 255],\n     [255, 255, 255, 255, 255]]\n)\nz = np.array(\n    [[ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0]]\n)\n\n\n그리고 합쳐서 RGB값을 할당해준다.\n\n\nred = np.stack([r,z,z], axis = -1)\ngreen = np.stack([z,g,z], axis = -1)\nblue = np.stack([z,z,b], axis = -1)\n\n\ntemp = np.stack([r,g,b], axis = -1);temp\n\narray([[[255,   0,   0],\n        [255,   0,   0],\n        [255, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[255,   0,   0],\n        [255,   0,   0],\n        [255, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[255,   0, 255],\n        [255,   0, 255],\n        [255, 255, 255],\n        [  0, 255, 255],\n        [  0, 255, 255]],\n\n       [[  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255]],\n\n       [[  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255]]])\n\n\n\n원소 하나 당 3개의 값이 할당된 것을 알 수 있다.\n\n\nnp.stack([], axis = -1) 크기가 동일한 행렬들의 각 원소들을 리스트화 하여 원소로 저장한다.\n\n\nplt.imshow(red+green+blue)\nplt.show()\n\n\n\n\nR, G, B를 같은 비율로 섞으면 다시 흑백이미지가 된다.\n\narr2 = np.array([[10, 40], [80, 60]])\narr2\n\narray([[10, 40],\n       [80, 60]])\n\n\n\narr3 = np.stack([arr2, arr2, arr2], axis = -1)  ## rgb값이 각각 동일\nplt.imshow(arr3)\nplt.show()\n\n\n\n\n\nimg.shape ## 원소의 리스트 수가 3이라는 것으로 rgb가 포함되어 있음을 추측할 수 있음.\n\n(683, 1024, 3)\n\n\n\n\nC. 히스토그램 이퀼라이징\n그래서 이퀼라이징은 뭐냐고!\n\nimg에서 추출해온 행렬로 아래와 같은 히스토그램을 만들어보자.\n\n\nr = img[:, :, 0]  ## 첫 번째 원소\ng = img[:, :, 1]  ## 두 번째 원소\nb = img[:, :, 2]  ## 세 번째 원소\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255])\nplt.show()\n\n\n\n\n\n120~200 사이에 값이 몰려있음\n120~200의 분포된 모양은 그대로 유지하면서 range를 0~255까지 늘린다면?\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\ncv2 라이브러리의 equalizeHist() 사용하면 행렬의 모든 원소들의 분포 정도를 고르게(0~255) 바꾼다.\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='befor');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');  ## cv2.equalizeHist() 사용\nplt.legend()\nplt.show()\n\n\n\n\n그렇다면 이것을 응용하여 위에서의 이미지를 이퀼라이징하면?\n- 이퀼라이징된 각 원소들을 다시 이어붙여 하나의 이미지로 만들어본다.\n\nimg2 = np.stack([rr,gg,bb], axis = -1)  ## axis = -1 &gt; z축(원소 내에서 확장)으로 추가\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(img2)\nplt.show()\n\n\n\n\n\nplt.imshow(np.concatenate([img,img2], axis = 1))\nplt.show()\n\n\n\n\n\n이렇게, 이미지를 조금 더 구별하기 쉽도록 바꿀 수 있다."
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html",
    "title": "Tidydata 심화 실습",
    "section": "",
    "text": "Tidydata를 만드는 방법의 모든(?) 것"
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html#라이브러리-imports",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html#라이브러리-imports",
    "title": "Tidydata 심화 실습",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotnine import *"
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html#사전학습",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html#사전학습",
    "title": "Tidydata 심화 실습",
    "section": "2. 사전학습",
    "text": "2. 사전학습\n\npd.concat()\n\n\ndf1 = pd.DataFrame({'A':[1,2,3],'B':[2,3,4]})\ndf2 = pd.DataFrame({'A':[-1,-2,-3],'B':[-2,-3,-4]})\n\n\ndisplay(\"df1\", df1)\ndisplay(\"df2\", df2)\n\n'df1'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n'df2'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n-1\n-2\n\n\n1\n-2\n-3\n\n\n2\n-3\n-4\n\n\n\n\n\n\n\n\n위 두 개의 데이터프레임을 합치고 싶다면?\n\n\ndisplay(pd.concat([df1, df2], axis = 1))\ndisplay(pd.concat([df1, df2], axis = 0).reset_index(drop = True))\n\n\n\n\n\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n1\n2\n-1\n-2\n\n\n1\n2\n3\n-2\n-3\n\n\n2\n3\n4\n-3\n-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n3\n-1\n-2\n\n\n4\n-2\n-3\n\n\n5\n-3\n-4"
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html#df.merge",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html#df.merge",
    "title": "Tidydata 심화 실습",
    "section": "### df.merge()",
    "text": "### df.merge()\n- 사이즈가 맞지 않는 두 데이터프레임의 정보를 결합\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'department':['A','B'], 'total':[3,4]})\n\ndisplay('big', big) ## title을 달아주고 아래 산출\ndisplay('small', small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ntotal\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n\ndisplay(big.merge(small))  ## 큰 거를 기준으로 작은거 병합\ndisplay(small.merge(big))  ## 작은거를 기준으로 큰거를 병합\n## 사실 둘 다 비슷하긴 함\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ntotal\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartment\ntotal\ngender\ncount\n\n\n\n\n0\nA\n3\nmale\n1\n\n\n1\nA\n3\nfemale\n2\n\n\n2\nB\n4\nmale\n3\n\n\n3\nB\n4\nfemale\n1\n\n\n\n\n\n\n\n\ndf.applymap()\n\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':np.random.rand(3), 'B':np.random.rand(3)})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.817682\n0.619777\n\n\n1\n0.049532\n0.122541\n\n\n2\n0.838686\n0.117128\n\n\n\n\n\n\n\n\n0.5보다 크면 yes, 0.5보다 작으면 no로 바꾸고 싶다면…\n\n\ndf.applymap(lambda x : 'yes' if x &gt; 0.5 else 'no')\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nyes\nyes\n\n\n1\nno\nno\n\n\n2\nyes\nno"
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html#d.-df.astype",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html#d.-df.astype",
    "title": "Tidydata 심화 실습",
    "section": "### D. df.astype()",
    "text": "### D. df.astype()\n- 데이터프레임이나 시리즈의 형식을 일괄적으로 변경\n\ndf = pd.DataFrame({'A':[0,1,2],'B':[4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0\n4\n\n\n1\n1\n5\n\n\n2\n2\n6\n\n\n\n\n\n\n\n\ndf.astype(float)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.0\n4.0\n\n\n1\n1.0\n5.0\n\n\n2\n2.0\n6.0\n\n\n\n\n\n\n\n\nE. 데이터프레임 열의 형식\n\n- info()에서의 형식, object는 일괄적으로 문자형이라는 것을 의미하는 게 아님.\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':['1','2','0','1',2], 'B':['2','3','0','0',0]})  ## integer가 포함되어 있다.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   A       5 non-null      object\n 1   B       5 non-null      object\ndtypes: object(2)\nmemory usage: 208.0+ bytes\n\n\n- column의 이름이 이상하게 들어가 있는 경우도 있음.\n\ndf = pd.DataFrame({('A',''):[0,0,0], ('B',''):[1,1,1]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n\n\n\n\n\n\ndf['A']\ndf[('A', '')]\n\n0    0\n1    0\n2    0\nName: (A, ), dtype: int64\n\n\n\n놀랍게도 둘은 같다. 인덱스를 다시 설정해주는 편이 정신건강에 이로움\n\n\ndf.columns ## 쓸모없는 멀티인덱스\n\nMultiIndex([('A', ''),\n            ('B', '')],\n           )"
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html#실습-에너지-사용량-시각화",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html#실습-에너지-사용량-시각화",
    "title": "Tidydata 심화 실습",
    "section": "3. 실습 : 에너지 사용량 시각화",
    "text": "3. 실습 : 에너지 사용량 시각화\n- 문제\n\npd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv')\n\n\n\n\n\n\n\n\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n종로구\n17,851\n9,204,140\n63,492\n76,653\n799\n\n\n1\n중구\n10,383\n10,078,848\n79,223\n68,210\n497\n\n\n2\n용산구\n17,138\n10,756,612\n51,229\n79,805\n11,128\n\n\n3\n성동구\n13,980\n11,804,313\n59,832\n99,986\n0\n\n\n4\n광진구\n21,556\n12,272,738\n68,756\n123,447\n0\n\n\n5\n동대문구\n21,794\n12,664,554\n65,913\n111,420\n0\n\n\n6\n중랑구\n23,950\n15,182,802\n59,370\n109,284\n7,442\n\n\n7\n성북구\n27,112\n15,938,807\n77,007\n148,376\n0\n\n\n8\n강북구\n23,334\n9,458,987\n47,731\n100,045\n0\n\n\n9\n도봉구\n13,168\n10,644,704\n44,985\n90,379\n5,268\n\n\n10\n노원구\n9,704\n17,197,086\n77,010\n94,340\n50,859\n\n\n11\n은평구\n25,200\n14,735,131\n75,914\n130,159\n14,370\n\n\n12\n서대문구\n17,651\n12,559,425\n65,164\n111,542\n6,330\n\n\n13\n마포구\n18,844\n15,024,186\n92,453\n114,931\n20,148\n\n\n14\n양천구\n14,690\n15,428,339\n70,721\n82,857\n49,258\n\n\n15\n강서구\n20,446\n20,641,866\n86,809\n128,786\n35,896\n\n\n16\n구로구\n17,204\n13,509,894\n59,916\n120,457\n2,963\n\n\n17\n금천구\n12,135\n7,420,441\n34,791\n69,814\n732\n\n\n18\n영등포구\n18,133\n14,914,027\n87,480\n114,238\n13,531\n\n\n19\n동작구\n20,102\n13,612,946\n66,811\n132,285\n899\n\n\n20\n관악구\n26,460\n14,997,859\n85,416\n158,543\n0\n\n\n21\n서초구\n12,856\n21,560,285\n135,491\n121,437\n38,866\n\n\n22\n강남구\n16,129\n29,961,585\n180,121\n149,045\n83,459\n\n\n23\n송파구\n19,331\n26,573,343\n139,117\n143,601\n71,954\n\n\n24\n강동구\n16,636\n15,048,315\n70,341\n121,931\n11,921\n\n\n\n\n\n\n\n에너지 사용량은 2018년부터 2021년까지의 기간 동안 서울, 부산 등 여러 지역에 대해 정리되어 있으며, 아래 주소 형식으로 저장되어 있다.\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2020.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2021.csv\n...\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2018.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2019.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2020.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2021.csv\n\n아래의 url, prov를 참고하여 모든 자료를 불러온 뒤 pd.concat()을 이용하여 하나의 df로 합쳐라.\n\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\n\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\n\n1. 풀이\n\nurl.format('Seoul2018')  ## 이런 식으로 하나하나 리스트로 지정해줘야 함\n\n'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv'\n\n\n\n[url.format(region + str(year)) for year in range(2018,2022) for region in prov]\n\n['https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2021.csv']\n\n\n\ndf = pd.concat([pd.read_csv(url.format(region + str(year))).assign(년도 = year, 시도 = region) for year in range(2018,2022) for region in prov], axis = 0)\n\n\ndf\n\n\n\n\n\n\n\n\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n년도\n시도\n\n\n\n\n0\n종로구\n17,929\n9,141,777\n64,818\n82,015\n111\n2018\nSeoul\n\n\n1\n중구\n10,598\n10,056,233\n81,672\n75,260\n563\n2018\nSeoul\n\n\n2\n용산구\n17,201\n10,639,652\n52,659\n85,220\n12,043\n2018\nSeoul\n\n\n3\n성동구\n14,180\n11,631,770\n60,559\n107,416\n0\n2018\nSeoul\n\n\n4\n광진구\n21,520\n12,054,796\n70,609\n130,308\n0\n2018\nSeoul\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19\n함양군\n12,505\n1,509,149\n6,328\n3,164\n0\n2021\nGyeongsangnam-do\n\n\n20\n거창군\n14,607\n2,322,093\n10,404\n8,850\n0\n2021\nGyeongsangnam-do\n\n\n21\n합천군\n16,039\n1,612,734\n7,587\n0\n0\n2021\nGyeongsangnam-do\n\n\n0\n제주시\n67,053\n20,275,738\n103,217\n25,689\n0\n2021\nJeju-do\n\n\n1\n서귀포시\n35,230\n7,512,206\n37,884\n2,641\n0\n2021\nJeju-do\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n문자열을 pd.read_csv()에 넣어준 후, 컴프리헨션 된 리스트를 행 방향으로 concat했다. 또한 연도와 시도의 정보를 유지시켰다.\n\n\n의미상 숫자형이지만, 문자형으로 입력이 된 자료를 모두 전처리하라.\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1000 entries, 0 to 1\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   지역                1000 non-null   object\n 1   건물동수              1000 non-null   object\n 2   연면적               1000 non-null   object\n 3   에너지사용량(TOE)/전기    1000 non-null   object\n 4   에너지사용량(TOE)/도시가스  1000 non-null   object\n 5   에너지사용량(TOE)/지역난방  1000 non-null   object\n 6   년도                1000 non-null   int64 \n 7   시도                1000 non-null   object\ndtypes: int64(1), object(7)\nmemory usage: 70.3+ KB\n\n\n\n지역, 시도의 경우 문자형으로 입력된 게 맞음. 하지만 나머지는 다 숫자형이 되어야 한다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : x.replace(',',''))\n\nAttributeError: 'int' object has no attribute 'replace'\n\n\n\n문자형이 아닌 숫자형인 녀석이 몇몇 있나보다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   지역                1000 non-null   object\n 1   년도                1000 non-null   int64 \n 2   시도                1000 non-null   object\n 3   건물동수              1000 non-null   int32 \n 4   연면적               1000 non-null   int32 \n 5   에너지사용량(TOE)/전기    1000 non-null   int32 \n 6   에너지사용량(TOE)/도시가스  1000 non-null   int32 \n 7   에너지사용량(TOE)/지역난방  1000 non-null   int32 \ndtypes: int32(5), int64(1), object(2)\nmemory usage: 43.1+ KB\n\n\n\n자료의 형식이 알맞게 설정되었다.\n\n\n년도에는 쉼표가 없으므로 integer로 바꿈(여기선 애초에 숫자형으로 들어가긴 함)\n년도와 시도, 지역을 배제(문자형)\n혹시라도 integer인 녀석들을 string으로 변경 후 문자열 바꾸는 메소드를 통해 ,를 제거, 인덱스 초기화\n열의 이름을 아래와 같이 바꿔라.\n\n\nname_dict = {\n    '년도': 'Year',\n    '시도': 'Prov',\n    '지역': 'Reg',\n    '건물동수': 'BldgCount',\n    '연면적': 'Area',\n    '에너지사용량(TOE)/전기': 'Elec',\n    '에너지사용량(TOE)/도시가스': 'Gas',\n    '에너지사용량(TOE)/지역난방': 'Heat'\n}\n\n\n딕셔너리가 주어졌으므로 그냥 바꾸면 된다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)  ## axis를 꼭 지정해주자.\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n아래와 같은 그림을 시각화하라. \n\n\n가로축이 Year, 세로축이 LogEnergyUse(에너지 사용량에 log를 취한 것)이고, Region으로 면분할했으며, Type으로 라인의 색상을 구분했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n사용해야 할 것은 Prov, Year, Elec Gas Heat.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nlevel_2\n0\n\n\n\n\n0\nSeoul\n2018\nElec\n64818\n\n\n1\nSeoul\n2018\nGas\n82015\n\n\n2\nSeoul\n2018\nHeat\n111\n\n\n3\nSeoul\n2018\nElec\n81672\n\n\n4\nSeoul\n2018\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n2021\nGas\n25689\n\n\n2996\nJeju-do\n2021\nHeat\n0\n\n\n2997\nJeju-do\n2021\nElec\n37884\n\n\n2998\nJeju-do\n2021\nGas\n2641\n\n\n2999\nJeju-do\n2021\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n사용할 두 개의 열을 골라주고, 에너지 관련 세 개 열을 long data로 변환했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nSeoul\n2018\nElec\n64818\n\n\n1\nSeoul\n2018\nGas\n82015\n\n\n2\nSeoul\n2018\nHeat\n111\n\n\n3\nSeoul\n2018\nElec\n81672\n\n\n4\nSeoul\n2018\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n2021\nGas\n25689\n\n\n2996\nJeju-do\n2021\nHeat\n0\n\n\n2997\nJeju-do\n2021\nElec\n37884\n\n\n2998\nJeju-do\n2021\nGas\n2641\n\n\n2999\nJeju-do\n2021\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n이름을 바꾸고…\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n\n\n1\nBusan\n2018\nGas\n708240\n\n\n2\nBusan\n2018\nHeat\n23694\n\n\n3\nBusan\n2019\nElec\n602980\n\n\n4\nBusan\n2019\nGas\n675882\n\n\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n\n\n200\nUlsan\n2020\nHeat\n0\n\n\n201\nUlsan\n2021\nElec\n196412\n\n\n202\nUlsan\n2021\nGas\n312276\n\n\n203\nUlsan\n2021\nHeat\n0\n\n\n\n\n204 rows × 4 columns\n\n\n\n\n지역별로 중복되는 것들을 더했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.assign(LogEnergyUse = lambda _df : _df.EnergyUse.apply(np.log))\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\nLogEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n13.326971\n\n\n1\nBusan\n2018\nGas\n708240\n13.470538\n\n\n2\nBusan\n2018\nHeat\n23694\n10.072977\n\n\n3\nBusan\n2019\nElec\n602980\n13.309639\n\n\n4\nBusan\n2019\nGas\n675882\n13.423774\n\n\n...\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n12.634264\n\n\n200\nUlsan\n2020\nHeat\n0\n-inf\n\n\n201\nUlsan\n2021\nElec\n196412\n12.187970\n\n\n202\nUlsan\n2021\nGas\n312276\n12.651643\n\n\n203\nUlsan\n2021\nHeat\n0\n-inf\n\n\n\n\n204 rows × 5 columns\n\n\n\n\n그리고 로그를 취해준 것을 새로운 열로 할당해줬다. 이정도면 타이디데이터라 할 만 하다.\n\n\ntidydata = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.assign(LogEnergyUse = lambda _df : _df.EnergyUse.apply(np.log))\n\n시각화\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = 'Year', y = 'LogEnergyUse', color = 'Type', linetype = 'Type'))\n\nfig + line + facet_wrap('Prov', scales = 'free')  ## 해당 옵션은 그래프마다 스케일을 따로 적용시킨다.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n뭔가 시각화는 되었는데, 마음에 들지 않는다.\n\n\n## matplotlib로 해당 개체를 이전\nfig = (fig + line + facet_wrap('Prov', scales = 'free')).draw()\nfig\n\n\n\n\n\nfig.set_size_inches(10, 6)\nfig.set_dpi(150)\nfig\n\n\n\n\n\nmatplotlib에서의 메소드를 쉽게 적용시킬 수 있다.\n\n\nProv별로 총 에너지사용량이 많은 상위5개의 Reg을 찾고 아래와 같이 시각화 하라. \n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nReg\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\nSeoul\n64818\n82015\n111\n\n\n1\n중구\nSeoul\n81672\n75260\n563\n\n\n2\n용산구\nSeoul\n52659\n85220\n12043\n\n\n3\n성동구\nSeoul\n60559\n107416\n0\n\n\n4\n광진구\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n거창군\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n합천군\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n제주시\nJeju-do\n103217\n25689\n0\n\n\n999\n서귀포시\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n필요없는 열을 없앤다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nProv\nReg\nType\nEnergyUse\n\n\n\n\n0\nSeoul\n종로구\nElec\n64818\n\n\n1\nSeoul\n종로구\nGas\n82015\n\n\n2\nSeoul\n종로구\nHeat\n111\n\n\n3\nSeoul\n중구\nElec\n81672\n\n\n4\nSeoul\n중구\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n제주시\nGas\n25689\n\n\n2996\nJeju-do\n제주시\nHeat\n0\n\n\n2997\nJeju-do\n서귀포시\nElec\n37884\n\n\n2998\nJeju-do\n서귀포시\nGas\n2641\n\n\n2999\nJeju-do\n서귀포시\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n지역과 구, 타입과 에너지를 표기했다. 이름도 적절히 설정해줬다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Reg'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nReg\nEnergyUse\n\n\n\n\n0\nBusan\n강서구\n200386\n\n\n1\nBusan\n금정구\n451212\n\n\n2\nBusan\n기장군\n287926\n\n\n3\nBusan\n남구\n491030\n\n\n4\nBusan\n동구\n156302\n\n\n...\n...\n...\n...\n\n\n245\nUlsan\n남구\n607820\n\n\n246\nUlsan\n동구\n281094\n\n\n247\nUlsan\n북구\n334844\n\n\n248\nUlsan\n울주군\n394217\n\n\n249\nUlsan\n중구\n395158\n\n\n\n\n250 rows × 3 columns\n\n\n\n\n구역별로 에너지 사용량을 합쳐버렸다.\n\n\ng = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Reg'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.groupby(by = 'Prov')\n\npd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\n\n\n\n\n\n\n\n\nProv\nReg\nEnergyUse\n\n\n\n\n0\nBusan\n부산진구\n690344\n\n\n1\nBusan\n해운대구\n689901\n\n\n2\nBusan\n사하구\n522150\n\n\n3\nBusan\n북구\n493913\n\n\n4\nBusan\n남구\n491030\n\n\n...\n...\n...\n...\n\n\n0\nUlsan\n남구\n607820\n\n\n1\nUlsan\n중구\n395158\n\n\n2\nUlsan\n울주군\n394217\n\n\n3\nUlsan\n북구\n334844\n\n\n4\nUlsan\n동구\n281094\n\n\n\n\n78 rows × 3 columns\n\n\n\n\n구간마다 순위를 정해주기 위해 groupby 함수를 사용, sub-dataframe으로 쪼갠 후에 각각 sort_values() 해주었다.\n\n\npd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\\\n.reset_index().rename({'index' : 'rank'}, axis = 1)\n\n\n\n\n\n\n\n\nrank\nProv\nReg\nEnergyUse\n\n\n\n\n0\n0\nBusan\n부산진구\n690344\n\n\n1\n1\nBusan\n해운대구\n689901\n\n\n2\n2\nBusan\n사하구\n522150\n\n\n3\n3\nBusan\n북구\n493913\n\n\n4\n4\nBusan\n남구\n491030\n\n\n...\n...\n...\n...\n...\n\n\n73\n0\nUlsan\n남구\n607820\n\n\n74\n1\nUlsan\n중구\n395158\n\n\n75\n2\nUlsan\n울주군\n394217\n\n\n76\n3\nUlsan\n북구\n334844\n\n\n77\n4\nUlsan\n동구\n281094\n\n\n\n\n78 rows × 4 columns\n\n\n\n\n인덱스는 랭크와 동일하므로 따로 남겨둔다.\n\n\ntidydata = pd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\\\n.reset_index().rename({'index' : 'rank'}, axis = 1)\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x = 'rank', y = 'EnergyUse', fill = 'Prov'))\n\nfig + bar + facet_wrap('Prov')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n정보는 모두 포함하나, 짜임새가 없으므로 matplotlib로 이전\n\n\nfig = (fig + bar + facet_wrap('Prov')).draw()\n\n\nfig.set_size_inches(12, 6)\nfig.set_dpi(150)\nfig\n\n\n\n\n\n완료\n\n\n(Prov,Year)별 전기에너지 사용량 비율을 구하고 아래와 같이 시각화 하라. + 제주를 제외한 지역으로 한정하고 시각화하라.\n\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n사용해야 할 것\n\nx = ‘Year’, y = ‘ElecRate’, facet_wrap(’Prov”)\nElec, Gas, Heat\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nYear\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n2018\nSeoul\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2021\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n2021\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n2021\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n2021\nJeju-do\n103217\n25689\n0\n\n\n999\n2021\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n필요없는 걸 없애고…\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nYear\nProv\nType\nEnergyUse\n\n\n\n\n0\n2018\nSeoul\nElec\n64818\n\n\n1\n2018\nSeoul\nGas\n82015\n\n\n2\n2018\nSeoul\nHeat\n111\n\n\n3\n2018\nSeoul\nElec\n81672\n\n\n4\n2018\nSeoul\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\n2021\nJeju-do\nGas\n25689\n\n\n2996\n2021\nJeju-do\nHeat\n0\n\n\n2997\n2021\nJeju-do\nElec\n37884\n\n\n2998\n2021\nJeju-do\nGas\n2641\n\n\n2999\n2021\nJeju-do\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\nlong data로 변환했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\n\n\n\n\n\n\n\n\nType\nElec\nGas\nHeat\n\n\nYear\nProv\n\n\n\n\n\n\n\n2018\nBusan\n613522\n708240\n23694\n\n\nChungcheongbuk-do\n361490\n288927\n55002\n\n\nChungcheongnam-do\n456260\n420315\n24286\n\n\nDaegu\n457556\n599115\n77399\n\n\nDaejeon\n309660\n379571\n51341\n\n\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n403399\n4321\n\n\nJeollanam-do\n338032\n281895\n9012\n\n\nSejongsi\n70915\n30533\n61404\n\n\nSeoul\n3486022\n3617731\n546491\n\n\nUlsan\n196412\n312276\n0\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n지역(Prov)과 연도(Year)가 중복되는 값들을 각각 더해줘서 정리했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\n\n\n\n\n\n\n\n\nType\nElec\nGas\nHeat\nEnergyUse\n\n\nYear\nProv\n\n\n\n\n\n\n\n\n2018\nBusan\n613522\n708240\n23694\n1345456\n\n\nChungcheongbuk-do\n361490\n288927\n55002\n705419\n\n\nChungcheongnam-do\n456260\n420315\n24286\n900861\n\n\nDaegu\n457556\n599115\n77399\n1134070\n\n\nDaejeon\n309660\n379571\n51341\n740572\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n403399\n4321\n764778\n\n\nJeollanam-do\n338032\n281895\n9012\n628939\n\n\nSejongsi\n70915\n30533\n61404\n162852\n\n\nSeoul\n3486022\n3617731\n546491\n7650244\n\n\nUlsan\n196412\n312276\n0\n508688\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n총 에너지 사용량 중 전기 에너지만을 구해야 하니 먼저 총 에너지를 구해준다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\n\n\n\n\n\n\n\n\nType\nElec\nEnergyUse\nElecRate\n\n\nYear\nProv\n\n\n\n\n\n\n\n2018\nBusan\n613522\n1345456\n0.455996\n\n\nChungcheongbuk-do\n361490\n705419\n0.512447\n\n\nChungcheongnam-do\n456260\n900861\n0.506471\n\n\nDaegu\n457556\n1134070\n0.403464\n\n\nDaejeon\n309660\n740572\n0.418136\n\n\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n764778\n0.466878\n\n\nJeollanam-do\n338032\n628939\n0.537464\n\n\nSejongsi\n70915\n162852\n0.435457\n\n\nSeoul\n3486022\n7650244\n0.455675\n\n\nUlsan\n196412\n508688\n0.386115\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n필요없는 것을 없애고 비율을 넣어줬다. 이제 필요한 것은 비율 뿐이다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\\\n.drop(['Elec', 'EnergyUse'], axis = 1).reset_index()\n\n\n\n\n\n\n\nType\nYear\nProv\nElecRate\n\n\n\n\n0\n2018\nBusan\n0.455996\n\n\n1\n2018\nChungcheongbuk-do\n0.512447\n\n\n2\n2018\nChungcheongnam-do\n0.506471\n\n\n3\n2018\nDaegu\n0.403464\n\n\n4\n2018\nDaejeon\n0.418136\n\n\n...\n...\n...\n...\n\n\n63\n2021\nJeollabuk-do\n0.466878\n\n\n64\n2021\nJeollanam-do\n0.537464\n\n\n65\n2021\nSejongsi\n0.435457\n\n\n66\n2021\nSeoul\n0.455675\n\n\n67\n2021\nUlsan\n0.386115\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n타이디데이터 같다.\n\n\ntidydata = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\\\n.drop(['Elec', 'EnergyUse'], axis = 1).reset_index()\n\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = 'Year', y = 'ElecRate', color = 'Prov'), linetype = 'dashed')\npoint = geom_point(aes(x = 'Year', y = 'ElecRate', color = 'Prov', shape = 'Prov'))\n\nfig + line + point\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 16 rows containing missing values.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\guides\\guides.py:259: PlotnineWarning: geom_point legend : Removed 4 rows containing missing values.\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n나름 괜찮지만 그래도 matplotlib에 데려와보자.\n\n\nfig_ = (fig + line + point).draw()\nfig_.set_dpi(150)\nfig_\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 16 rows containing missing values.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\guides\\guides.py:259: PlotnineWarning: geom_point legend : Removed 4 rows containing missing values.\n\n\n\n\n\n\n~와 나 너무 잘하는 거 아님?~"
  },
  {
    "objectID": "posts/Data Visualization/Review/11. tidydata 심화실습.html#pd.merge의-이용",
    "href": "posts/Data Visualization/Review/11. tidydata 심화실습.html#pd.merge의-이용",
    "title": "Tidydata 심화 실습",
    "section": "4. pd.merge()의 이용",
    "text": "4. pd.merge()의 이용\n\n그냥 뇌정지 올 것 같아도 일단 tidydata로 변환하고 시작하자!!\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 6 columns\n\n\n\n\n이런 데이터가 있으면… 일단 value 세 개인 Elec, Gas, Heat를 녹여야 함.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nEnergyUse\n\n\n\n\n0\nBusan\n2018\n1345456\n\n\n1\nBusan\n2019\n1301422\n\n\n2\nBusan\n2020\n1314100\n\n\n3\nBusan\n2021\n1951909\n\n\n4\nChungcheongbuk-do\n2018\n705419\n\n\n...\n...\n...\n...\n\n\n63\nSeoul\n2021\n7650244\n\n\n64\nUlsan\n2018\n512512\n\n\n65\nUlsan\n2019\n491191\n\n\n66\nUlsan\n2020\n500742\n\n\n67\nUlsan\n2021\n508688\n\n\n\n\n68 rows × 3 columns\n\n\n\n\nwidedata로 만들었음. 일단 해!\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n\n\n1\nBusan\n2018\nGas\n708240\n\n\n2\nBusan\n2018\nHeat\n23694\n\n\n3\nBusan\n2019\nElec\n602980\n\n\n4\nBusan\n2019\nGas\n675882\n\n\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n\n\n200\nUlsan\n2020\nHeat\n0\n\n\n201\nUlsan\n2021\nElec\n196412\n\n\n202\nUlsan\n2021\nGas\n312276\n\n\n203\nUlsan\n2021\nHeat\n0\n\n\n\n\n204 rows × 4 columns\n\n\n\n\n큰 데이터와 작은 데이터가 만들어졌다.\n\n\nbig = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\nsmall = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\npd.merge(big, small, on = ['Year', 'Prov']).loc[lambda _df : _df['Type'] == 'Elec'].reset_index(drop = True)\\\n.assign(ElecRate = lambda _df : _df.EnergyUse_x / _df.EnergyUse_y)\\\n.drop(['EnergyUse_x', 'EnergyUse_y'], axis = 1)\n\n\n\n\n\n\n\n\nProv\nYear\nType\nElecRate\n\n\n\n\n0\nBusan\n2018\nElec\n0.455996\n\n\n1\nBusan\n2019\nElec\n0.463324\n\n\n2\nBusan\n2020\nElec\n0.457401\n\n\n3\nBusan\n2021\nElec\n0.534566\n\n\n4\nChungcheongbuk-do\n2018\nElec\n0.512447\n\n\n...\n...\n...\n...\n...\n\n\n63\nSeoul\n2021\nElec\n0.455675\n\n\n64\nUlsan\n2018\nElec\n0.384385\n\n\n65\nUlsan\n2019\nElec\n0.392067\n\n\n66\nUlsan\n2020\nElec\n0.387118\n\n\n67\nUlsan\n2021\nElec\n0.386115\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n타이디데이터가 됐다."
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "",
    "text": "matplotlib를 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#사전작업",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#사전작업",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 import\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (3, 2)\nmatplotlib.rcParams['figure.dpi'] = 150"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#간단한-꺾은선-그래프",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#간단한-꺾은선-그래프",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "2. 간단한 꺾은선 그래프",
    "text": "2. 간단한 꺾은선 그래프\nplt.plot()을 사용하여 간단하게 그래프를 그릴 수 있다.\n\ny값만 지정한 경우\n\n\nplt.plot([1,2,4,3])\nplt.show()\n\n\n\n\n\nx값과 y값 같이 지정한 경우\n\n\nplt.plot([1,2,3,4],[1,2,4,3])\nplt.show()\n\n\n\n\n\nx값과 y값에 변수를 지정하여 넣어주는 경우\n\n\nx = [1,2,3,4]\ny = [1,2,4,3]\n\nplt.plot(x,y)\nplt.show()\n\n\n\n\n- 이외에도 다양한 옵션을 사용하여 그래프를 다채롭게 그릴 수 있는데, 지금부터 그것들을 알아보도록 하자.\n\nplt.plot의 옵션\nplt.plot()에서 괄호 안에 문자열을 넣음으로서 세 가지 옵션을 간단하게 적용할 수 있다.\nplt.plot(x,y,'--')  ## 파선 그래프\nplt.plot(x,y,':')   ## 점선 그래프\nplt.plot(x,y,'r')   ## 선의 색상이 빨간색\nplt.plot(x,y,'r--') ## 빨간색의 파선 그래프\n...\n- 게다가 세 옵션을 순서 상관없이 집어넣어 적용 가능하다!\n\nLine StylesColorsMarkers\n\n\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\n\n\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\n\n\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\n\n\n그 외에 다른 옵션을 보고 싶다면 아래를 참조하라.\n\nother options or colors\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\nhttps://matplotlib.org/2.0.2/examples/color/named_colors.html\nhex code\nhttps://htmlcolorcodes.com/\nother linestyles\nhttps://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html\n\n- preset에 있는 색상 외 다른 색상을 적용\n\nplt.plot(x,y,'--',color = 'lime')\n\n\n\n\n\nusing color name\n\n\nplt.plot(x,y,color = '#751F9B')\n\n\n\n\n\nusing hex code\n\n- 선의 형태를 다양하게 변경\n\nplt.plot(x,y,linestyle = 'dashed')\nplt.show()\n\n\n\n\n\n문자열로 직접 지정\n\n\nplt.plot(x,y,linestyle = (0, (1,1)))\n\n\n\n\n\n파선의 길이를 직접 지정\n\n\n\nplt.plot()에서 scatter plot을 생성\nmarker 옵션을 변경하여 scatter plot을 손쉽게 그릴 수도 있다.\n\nplt.plot(x,y,'db')  ## diamonds, blue\n\n\n\n\n\n\ndot connected plot\n\nplt.plot(x,y,':or')  ## dotline(:), circle(o), red\n\n\n\n\n\n\npile up\nplt.show()를 입력하기 전 계속해서 그래프를 그리면 중첩된다.\n\nplt.plot([1,2,3,2], '--o', color = 'orange')\nplt.plot([2,3,1,4], '--o', color = 'skyblue')\n\nplt.show()\n\nplt.plot([4,4,2,1], '--o', color = 'cyan')\n\nplt.show()\n\n\n\n\n\n\n\n\nplt.plot([1,2,3,2], '--o', color = 'C1')\nplt.plot([2,3,1,4], '--o', color = 'C0')\n\nplt.show()\n\n\n\n\n\n위와 같은 경우에는 color를 지정하지 않을 경우 먼저 입력한 그래프에 C0가 지정된다.\n\n\n\n응용 : scatter plot and line plot\n- 유사 단순선형회귀\n설명변수와 오차, 반응변수를 지정해주자.\n\nx = np.arange(-5,5,0.1)\neps = np.random.randn(100)\ny = 2*x + eps ## 벗어나도록 겹치게\n\n\nplt.plot(x,y,'.b')     ## 실제 데이터\nplt.plot(x,2*x,'--r')  ## 회귀선\nplt.show()\n\n\n\n\n\n\n적합한 그래프를 그릴 때\n- summary: boxplot, histogram, lineplot, scatterplot\n\n라인플랏: 추세\n☆★☆ 스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#객체지향적-시각화",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#객체지향적-시각화",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "3. 객체지향적 시각화",
    "text": "3. 객체지향적 시각화\n\nA. 배경지식\n- 그림을 저장해둔 뒤 나중에 꺼내보고 싶다면? | plt.gcf() : Get Current Figure.\n\nplt.plot([1, 2, 3, 2],'--o')\nfig = plt.gcf() ## plt.show()를 하기 전, 현재 표기되는 figure를 얻는다.\n\n\n\n\n\nfig\n\n\n\n\n\n위와 같이 변수에 저장된 것을 알 수 있다.\n\n\n\nB. fig의 해체\nfig\nfig.axes\n\nax = fig.axes[0]\nax.yaxis\nax.xaxis\n\nlines = ax.get_lines()[0]\nlines[0]\n\nfig &gt; 그래프 그 자체\naxes &gt; 그래프의 구역\naxis &gt; x축, y축\nline &gt; 직선형 그래프\n\n등등등…\n아무튼 여러 개체가 나뉘어있다.\n\n개념(비유) : * Figure(fig) : 도화지 * Axes(ax) : 도화지에 존재하는 그림틀 * Axis, Lines : 그림틀 위에 올려지는 물체(object)\n\n\n\nC. plt.plot()없이 그래프 그리기\n\nplt.plot([1,2,4,3], '--o')\nplt.show()\n\n\n\n\n위와 같은 그래프를 plt.plot()없이 만들어보자!\n- 아래의 코드를 하나하나 뜯어보자.\n\nfig = plt.Figure()\n\nax = fig.add_axes([0.125,0.11,0.775,0.77])\nax.set_xlim([-0.15, 3.15])  # setting x axis limit\nax.set_ylim([0.9, 3.1])     # setting y axis limit\nline = matplotlib.lines.Line2D(\n    xdata = [0,1,2,3],\n    ydata = [1,2,3,2],\n    linestyle = '--',\n    marker = 'o'\n)\nax.add_line(line)\n\nfig\n\n\n\n\n1. 최상위 하이라이트(figure) 생성\n\nfig = plt.figure(); fig   ## 최상위 하이라이트인 그림만 만들어냄.\n\n&lt;Figure size 450x300 with 0 Axes&gt;\n\n\n&lt;Figure size 450x300 with 0 Axes&gt;\n\n\n2. 그래프가 들어갈 공간(axes) 생성\n\nax = fig.add_axes([0.125,0.11,0.775,0.77]); fig  ## 가로시작, 세로시작, 종횡비\n\n\n\n\n3. 직선을 지정 후 추가\n\nline = matplotlib.lines.Line2D(\n    xdata = [0,1,2,3],\n    ydata = [1,2,3,2],\n    linestyle = '--',\n    marker = 'o'\n)\n\n\nmatplotlib에서 라인을 만드는 함수가 따로 있었다.\n\n\nax.add_line(line)\n\n&lt;matplotlib.lines.Line2D at 0x1c3c591e380&gt;\n\n\n\nfig\n\n\n\n\n4. 직선이 제대로 표기되지 않는 것 같으니 x축과 y축의 한계를 설정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\nfig\n\n\n\n\n\n\nD. 또 코드의 대체\n1. line2D 오브젝트를 쓰지 않는 방법\n\n## genarally\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.plot([1,2,3,2], '--o')\nfig\n\n\n\n\n\nax.plot()을 사용\n\n2. add_axes()를 쓰지 않는 방법(중요!)\n\nfig = plt.Figure()\nax = fig.subplots(1)\nax.plot([1,2,3,2], '--o')\nfig\n\n\n\n\n\nax = fig.subplots()을 사용\n\n3. fig와 ax들을 한번에 지정(중요!)\n\nfig, ax = plt.subplots(1) ## 중요함\nax.plot([1,2,3,2], '--o')\nplt.show()\n\n\n\n\n\n\nE. 정리 (\\(\\star\\star\\star\\))\n아래의 코드는 모두 같은 애들이었다.\n\nplt.plot([1,2,3,2], '--o')\n\n\nfig, ax = plt.subplots()\nax.plot([1,2,3,2], '--o')\n\n\nfig = plt.Figure()\nax = fig.subplots()\nax.plot([1,2,3,2], '--o')\nfig\n\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.plot([1,2,3,2], '--o')\nfig\n\nplt.subplots()과 ax.plot()의 경우 상당히 유용한 코드이니 꼭 숙지할 것!"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#미니맵과-서브플롯",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#미니맵과-서브플롯",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "4. 미니맵과 서브플롯",
    "text": "4. 미니맵과 서브플롯\n\nA. 미니맵\nfig.add_axes()를 사용한다.\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2]); fig\n\n\n\n\n\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])  ## 가로 세로 위치(중심위치), 종횡비\nax.plot([1,5,3,4], '--o')\nax_mini.plot([1,2,3,1], '--or')\n\nfig\n\n\n\n\n\n생성된 fig에 axes를 하나 더 추가하여 만들어냈다.\n\n\n\nB. 서브플롯\nplt.subplots(), fig.subplots()을 이용해보자.\n\nfig, axs = plt.subplots(2)  ## 2행\n\n\n\n\n\naxs\n\narray([&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;], dtype=object)\n\n\n\naxs에 ax들이 array형태로 저장되어 있다.\n\n\naxs[0].plot([1,2,3,2], '--r')\naxs[1].plot([1,2,4,3], '--o')\n\nfig\n\n\n\n\n\n뭔가 레이아웃이 가려져있고 이상하다.\n\n\nfig.tight_layout(); fig\n\n\n\n\n\n왠만해선 fig.tight_layout()을 해주도록 하자.\n\n\n차피 axs가 array 형태로 저장되므로 그것을 따로 지정해주고 싶다면 아래와 같이 사용하는 것을 권장한다.\n\n\nfig, (ax1, ax2) = plt.subplots(2)\nax1.plot([1,2,3,2], '--r')\nax2.plot([1,2,4,3], '--o')\nfig.tight_layout()\n\n\n\n\n\n\nC. 서브플롯 스케일 조정 및 다중화\n- 스케일 변경\n\nfig, (ax1, ax2) = plt.subplots(2, figsize = (3,3))  ## 종횡비\nax1.plot([1,2,3,2], '--r')\nax2.plot([1,2,4,3], '--o')\nfig.tight_layout()\n\n\n\n\n\n미리 설정해줬던 dpi에 의거하여 종횡비가 배수로 적용된다.\n\n- 더 많은 서브플롯 생성\n\nfig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize = (3,3))\nax1.plot([1,2,4,3], 'o', color = 'C0')\nax2.plot([1,2,4,3], 'o', color = 'C1')\nax3.plot([1,2,4,3], 'o', color = 'C2')\nax4.plot([1,2,4,3], 'o', color = 'C3')\nfig.tight_layout()\n\n\n\n\n- 사용자 정의 서브플롯 생성\nplt.subplot() ## s가 없는 subplot(), 즉, 하나만 만들어진다.\n\nplt.figure(figsize=(3,3))\nplt.subplot(2,2,1)  ## 2×2의 1\nplt.plot([1,2,4,3],'o', color='C0')\nplt.subplot(1,2,2)\nplt.plot([1,2,4,3],'o', color='C1')\nplt.subplot(2,2,3)\nplt.plot([1,2,4,3],'o', color='C2')\nplt.tight_layout()\n\nfig = plt.gcf()\n\n\n\n\n\n이미 생성된 figure의 크기를 조정\n\n\nfig.set_size_inches(2,2); fig"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#title",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#title",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "5. title",
    "text": "5. title\n\ntitle을 만드는 함수는 어떤 오브젝트에 소속되는 게 좋을까? 1. plt -&gt; subplot의 제목을 설정 가능 2. fig -&gt; 전체제목(super title)을 설정할 수 있음 3. ax -&gt; subplot들의 제목을 설정할 수 있음\n\n\nA. plt.title()\nfigure를 생성하지 않은 기본적인 환경에서 타이틀을 달아준다.\n\n## 가장 평범한 플롯\nplt.plot([1,2,3,2])\nplt.title('asdf')\nplt.show()\n\n\n\n\n\n\nB. ax.set_title()\nfigure와 axes를 생성했을 경우, 각 ax마다 타이틀을 달아줄 수 있다.\n\n## title이 axes에 존재\nfig, ax = plt.subplots()\nax.set_title('asdf')\nax.plot([1,2,3,2])\n\nplt.show()\n\n\n\n\n\n\nC. fig.suptitle() | 권장하지 않는 방법\n원래 figure 자체에 타이틀을 붙이는 것은 불가능하다.\n\n##--------fig : 원래는 불가능--------\nplt.plot([1,2,3,2])\nfig = plt.gcf()\nfig.suptitle('asdf')\n\nplt.show()\n\n\n\n\n\n\nD. 응용\n\nplt.subplots()과 set_title()을 이용\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (4,2))\nax1.set_title('asdf')\nax2.set_title('1234')\nax1.plot([1,2,3,2])\nax2.plot([1,2,3,2])\nfig.tight_layout()\n\n\n\n\n\nfigure를 생성하지 않고 plt.subplot()과 plt.title()을 이용하여 손수 지정\n\n\nplt.subplot(1,2,1)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(1,2,2)\nplt.plot([1,2,3])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\nfig.suptitle()을 이용한 방법\n\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\nE. plt.gca()\nplt.gca()를 통해 ax개체를 다룰 수도 있다.\n\nplt.plot([1,2,3,2])\nax = plt.gca()\nax.set_title('asdf')  ## 현재의 axis에 바로 타이틀을 설정해준다.\n\nText(0.5, 1.0, 'asdf')"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도의-응용-표본상관계수",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도의-응용-표본상관계수",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "6. 산점도의 응용 | 표본상관계수",
    "text": "6. 산점도의 응용 | 표본상관계수\n\nA. 산점도와 표본상관계수\n아래처럼 두 연속형 자료가 주어질 경우 산점도로 나타낼 수 있다.\n\nweight = [44,48,49,58,62,68,69,70,76,79]\nheight = [159,160,162,165,167,162,165,175,165,172]\n\nplt.plot(weight,height,'.')  ## option : '.' marker가 .인 산점도 산출\nplt.show()\n\n\n\n\n아래 표본상관계수의 정의에 따라 데이터에서의 표본상관계수를 구해보자.\n- (표본)상관계수의 정의\n\\[r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2 }}=\\sum_{i=1}^{n}\\tilde{x}_i\\tilde{y}_i \\]\n\\[단,~\\tilde{x}_i=\\frac{(x_i-\\bar{x})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}},~ \\tilde{y}_i=\\frac{(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}\\]\n\n위 식에서 \\(\\tilde{x}_i\\)와 \\(\\tilde{y}_i\\)는 \\(x_i\\)와 \\(y_i\\)를 표준화한 것이다.\n\n(데이터를 불러오자)\n\nx=[44,48,49,58,62,68,69,70,76,79]\ny=[159,160,162,165,167,162,165,175,165,172]\n\n(평균을 0으로)\n\nxx = x - np.mean(x); print(xx)\nyy = y - np.mean(y); print(yy)\n\n[-18.3 -14.3 -13.3  -4.3  -0.3   5.7   6.7   7.7  13.7  16.7]\n[-6.2 -5.2 -3.2 -0.2  1.8 -3.2 -0.2  9.8 -0.2  6.8]\n\n\n(퍼진 정도를 표준화)\n\nx_standard = xx/np.sqrt(np.sum(xx**2))\ny_standard = yy/np.sqrt(np.sum(yy**2))\n\n(표본상관계수 산출)\n\nnp.sum(x_standard*y_standard)\n\n0.7138620583559141\n\n\n\n이미 정의된 코드를 통해 해당 결과가 맞는지 확인해보자.\n\n\nnp.corrcoef(x,y)\n\narray([[1.        , 0.71386206],\n       [0.71386206, 1.        ]])"
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#b.-산점도를-보고-상관계수의-부호를-해석",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#b.-산점도를-보고-상관계수의-부호를-해석",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "### B. 산점도를 보고 상관계수의 부호를 해석",
    "text": "### B. 산점도를 보고 상관계수의 부호를 해석\n- 아래의 그림은 상관계수 r의 값이 양수인가 음수인가?\n\nx=[44,48,49,58,62,68,69,70,76,79]\ny=[159,160,162,165,167,162,165,175,165,172]\n\nplt.plot(x, y, 'o')\nplt.show()\n\n\n\n\n\nxx = x-np.mean(x)\nyy = y-np.mean(y) \nxxx = xx/np.sqrt(np.sum(xx**2))\nyyy = yy/np.sqrt(np.sum(yy**2))\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (10,3))\nax1.plot(x,y, 'o')\nax1.set_title(r'$(x_i,y_i)$')\nax2.plot(xx,yy,'o') ## mean to 0\nax2.set_title(r'$(x_i-\\bar{x}, y_i-\\bar{y})$')\nax3.plot(xxx,yyy,'o') ## standarized\nax3.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\n\nplt.show()\n\n\n\n\n\n마지막 \\(\\tilde{x}_i\\), \\(\\tilde{y}_i\\)를 곱한 값이 양수인 것과 음수인 것을 체크해보자.\n\n\n1,3사분면에 점들이 많으므로 상관계수의 부호는 양수일 것이다."
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#d.-산점도를-보고-상관계수의-절대값을-해석",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#d.-산점도를-보고-상관계수의-절대값을-해석",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "### D. 산점도를 보고 상관계수의 절대값을 해석",
    "text": "### D. 산점도를 보고 상관계수의 절대값을 해석\n- 기울기가 동일하지만 직선 근처의 퍼짐이 다른 두 개의 자료\n\nx=np.arange(0,10,0.1)\ny1=x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## N(0,1)\ny2=x+np.random.normal(loc=0,scale=7.0,size=len(x))  ## N(0,7)\n\nplt.plot(x,y1,'.')\nplt.plot(x,y2,'x')\nplt.show()\n\n\n\n\n\n표준화하는 함수 tilde() 정의\n\n\ndef tilde(x):\n    xx = x-np.mean(x)\n    xxx = xx / np.sqrt(np.sum(xx**2))\n    return xxx\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (4,2))\nax1.plot(x,y1,'.'); ax1.plot(x,y2,'x'); ax1.set_title(r'$(x_i,y_i)$')\nax2.plot(tilde(x), tilde(y1),'.'); ax2.plot(tilde(x), tilde(y2), 'x'); ax2.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\nfig.tight_layout()\n\n\n\n\n- 직선 근처의 퍼짐은 동일하지만, 직선의 기울기가 다른 경우\n\nx=np.arange(0,10,0.1)\ny1=x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## 기울기가 1\ny2=0.2*x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## 기울기가 0.2\n\nplt.plot(x,y1,'.')\nplt.plot(x,y2,'x')\n\nplt.show()\n\n\n\n\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(4,2))\nax1.plot(x,y1,'.'); ax1.plot(x,y2,'x'); ax1.set_title(r'$(x_i,y_i)$')\nax2.plot(tilde(x),tilde(y1),'.'); ax2.plot(tilde(x),tilde(y2),'x'); ax2.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\nfig.tight_layout()\n\n\n\n\n기울기가 클수록, 퍼짐 정도가 작을수록 상관계수의 절댓값이 높다."
  },
  {
    "objectID": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도-응용예제2---앤스콤의-4분할",
    "href": "posts/Data Visualization/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도-응용예제2---앤스콤의-4분할",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "7. 산점도 응용예제2 - 앤스콤의 4분할",
    "text": "7. 산점도 응용예제2 - 앤스콤의 4분할\n- 표본상관계수가 모두 동일한 네 자료를 보라.\n\nx1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n\nx2 = x1\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n\nx3 = x1\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\nnp.corrcoef(x1,y1),np.corrcoef(x2,y2),np.corrcoef(x3,y3),np.corrcoef(x4,y4)\n\n(array([[1.        , 0.81642052],\n        [0.81642052, 1.        ]]),\n array([[1.        , 0.81623651],\n        [0.81623651, 1.        ]]),\n array([[1.        , 0.81628674],\n        [0.81628674, 1.        ]]),\n array([[1.        , 0.81652144],\n        [0.81652144, 1.        ]]))\n\n\n\n음, 다 비슷한 자료겠구나… 양의 상관관계를 띄겠네?\n\n라고 속단하긴 이르다.\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(6,4))\nax1.plot(x1,y1,'o'); ax1.set_title(f'corrcoef = {np.corrcoef(x1,y1)[0,1] : .6f}')\nax2.plot(x2,y2,'o'); ax2.set_title(f'corrcoef = {np.corrcoef(x2,y2)[0,1] : .6f}')\nax3.plot(x3,y3,'o'); ax3.set_title(f'corrcoef = {np.corrcoef(x3,y3)[0,1] : .6f}')\nax4.plot(x4,y4,'o'); ax4.set_title(f'corrcoef = {np.corrcoef(x4,y4)[0,1] : .6f}')\nfig.tight_layout()\n\n\n\n\n4개의 그림은 모두 같은 상관계수를 가지나, 그 느낌이 전혀 다르다.\n- 앤스콤플랏의 4개의 그림은 모두 같은 상관계수를 가진다. 하지만, 4개의 그림은 느낌이 전혀 다르다.\n- 같은 표본상관계수를 가진다고 하여 같은 관계성을 가지는 것은 아니다. 표본상관계수는 x,y의 비례정도를 측정하는데 그 값이 1에 가깝다고 하여 꼭 정비례의 관계가 있음을 의미하는 건 아니다.\n\\((x_i,y_i)\\)의 산점도가 선형성을 보일 때만 “표본상관계수가 1에 가까우므로 정비례의 관계에 있다”라는 논리전개가 성립한다.\n\n앤스콤의 첫번째 플랏 : 산점도가 선형 -&gt; 표본상관계수가 0.816 = 정비례의 관계가 0.816 정도\n앤스콤의 두번째 플랏 : 산점도가 선형이 아님 -&gt; 표본상관계수가 크게 의미없음.\n앤스콤의 세번째 플랏 : 산점도가 선형인듯 보이나 하나의 이상치가 있음 -&gt; 하나의 이상치가 표본상관계수의 값을 무너뜨릴 수 있으므로 표본상관계수 값을 신뢰할 수 없음.\n앤스콤의 네번째 플랏 : 산점도를 그려보니 이상한 그림 -&gt; 표본상관계수를 계산할 수는 있으나, 그게 무슨 의미가 있을까?\n\n산점도가 선형성을 보일 때만 표본상관계수가 1에 가까우므로 정비례의 관계에 있다라는 논리전개가 성립한다.\n\n1번만 의미가 있음. 3번의 경우 이상치가 존재하여 신뢰할 수 없음.\n\n\n교훈\n상관계수를 해석하기에 앞서서 산점도가 선형성을 보이는 지 체크할 것! 항상 통계량은 적절한 가정하에서만 말이 된다는 사실을 기억할 것!"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html",
    "href": "posts/Data Visualization/Review/4. Plotnine.html",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "",
    "text": "plotnine : R에서의 문법을 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#라이브러리-import",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#라이브러리-import",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\n##!pip install plotnine   ## plotnine이 구축되지 않은 경우 설치해야 한다.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n\n\nimport plotnine\n\n\nplotnine.options.dpi= 150\nplotnine.options.figure_size = (6, 5)\n\n\n간단한 그래프 설정이다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#mpg-data",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#mpg-data",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "2. mpg data",
    "text": "2. mpg data\n\nA. read data\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/mpg.csv')\ndf\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#b.-descriptions",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#b.-descriptions",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. descriptions",
    "text": "### B. descriptions\n\ndf.columns\n\nIndex(['manufacturer', 'model', 'displ', 'year', 'cyl', 'trans', 'drv', 'cty',\n       'hwy', 'fl', 'class'],\n      dtype='object')\n\n\n- 각 행들이 어떤 의미를 가지는 지 Chat GPT에게 분석을 요청해봤다.\n\n- 그렇단다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#mpg의-시각화-2차원",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#mpg의-시각화-2차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "3. mpg의 시각화 : 2차원",
    "text": "3. mpg의 시각화 : 2차원\n\nA. x=displ, y=hwy\n\n- 예시 1 : 정직하게 메뉴얼대로…\n\n파라미터를 직접 지정해주는 경우\n\n\nggplot(data = df) + geom_point(mapping = aes(x = 'displ', y = 'hwy')) ## aes : dictionary와 유사하다고 생각하면 된다.\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n파라미터 생략\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#b.-rpy2-코랩-아닌-경우-실습-금지",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#b.-rpy2-코랩-아닌-경우-실습-금지",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. rpy2 : 코랩 아닌 경우 실습 금지",
    "text": "### B. rpy2 : 코랩 아닌 경우 실습 금지\n- R에서도 거의 똑같은 문법으로 그릴 수 있음\n\n#import rpy2\n#%load_ext rpy2.ipython\n\n\n#%%R\n#library(tidyverse)\n#df = mpg\n#ggplot(df)+geom_point(aes(x=displ,y=hwy))"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#mpg의-시각화-3차원",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#mpg의-시각화-3차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "4. mpg의 시각화 : 3차원",
    "text": "4. mpg의 시각화 : 3차원\n\nA. x=displ, y=hwy, shape=class\n\n\nset(df['class'])  ## 중복되지 않은 값이 어느 것이 있는 지 산출\n## df['class'].unique() : 이건 array로 산출된다. 동일한 코드\n\n{'2seater', 'compact', 'midsize', 'minivan', 'pickup', 'subcompact', 'suv'}\n\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', shape = 'class'))   ## class를 shape로 구분 &gt; 불편함\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#b.-xdispl-yhwy-colorclass",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#b.-xdispl-yhwy-colorclass",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. x=displ, y=hwy, color=class",
    "text": "### B. x=displ, y=hwy, color=class\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n모양까지 class별로 달랐으면 좋겠다.\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n전체적으로 포인트의 사이즈를 키우고 싶다.\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'), size = 5)  ## 외부 파라미터\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n너무 커서 겹치니까 투명도 조정\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'), size = 5, alpha = 0.5)\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\ngeom_point()에서 내부 aes()에 넣은 값들은 값들을 구분하도록 되며, 외부에 입력된 값은 전체 개체들을 바꿔버린다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#mpg의-시각화-4차원-5차원",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#mpg의-시각화-4차원-5차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "5. mpg의 시각화 : 4차원, 5차원",
    "text": "5. mpg의 시각화 : 4차원, 5차원\n\nset(df['drv'])  ## 4륜구동, 전륜구동(front), 후륜구동(r)\n\n{'4', 'f', 'r'}\n\n\n\nA. drive metiod에 더 중점\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'drv', shape = 'class'), size = 4, alpha = 0.5)\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n4륜구동이 연비가 낮은 걸 확인할 수 있다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#b.-5차원-시각화",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#b.-5차원-시각화",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 5차원 시각화",
    "text": "### B. 5차원 시각화\n\nset(df['cyl'])  ## 실린더 수, 4,5,6,8\n\n{4, 5, 6, 8}\n\n\n\nggplot(df) + geom_point(aes(x='displ',y='hwy',color='drv',shape='class', size = 'cyl'), alpha = 0.5)  ## 외부 파라미터에 size는 제거\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n여기까지가 기본적인 사용 방법이다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#객체지향적-시각화",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#객체지향적-시각화",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "6. 객체지향적 시각화",
    "text": "6. 객체지향적 시각화\n- ggplot의 정체는 뭐지?\n\ntype(ggplot)\n\ntype\n\n\n\nclass. 어떤 물체를 만들어내는 함수와 비슷. matplotlib에서의 plt.figure()와 유사하다고 보면 된다.\n\n- 그럼 geom_point는 정체가 뭐지?\n\ntype(geom_point)  ## class, 생성함수.\n\nplotnine.utils.Registry\n\n\n\ngeom은 그림, 그래프라고 보면 된다. ’fig.add_axes()후 추가된ax`에 그래프를 그리는 것과 유사"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#a.-fig-geom_point-geom_smooth",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#a.-fig-geom_point-geom_smooth",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### A. fig + geom_point + geom_smooth",
    "text": "### A. fig + geom_point + geom_smooth\n\nfig  = ggplot(df)\npoint = geom_point(aes(x = 'displ', y = 'hwy'))\n\n\npoint ## 아무것도 나오지 않음\n\n&lt;plotnine.geoms.geom_point.geom_point at 0x121c8015390&gt;\n\n\n\nfig + point\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n두 개체를 합치니 피규어에 그래프가 들어가버린 형태가 되었다.\n\n\ngeom_smooth() | 산점도가 아닌 직선 그래프를 그려준다.\n\n\nsmooth = geom_smooth(aes(x = 'displ', y = 'hwy'))\n\n\nfig + smooth  ## ggplot(df) + geom_smooth(aes(x = 'displ', y = 'hwy')), 추세선 산출\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n그럼 셋을 합쳐보면…\n\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy')) + geom_smooth(aes(x = 'displ', y = 'hwy'))과 동일하다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#b.-시각화-개선",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#b.-시각화-개선",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 시각화 개선",
    "text": "### B. 시각화 개선\n\ngeom_point()를 개선\n\n\npoint_better = geom_point(aes(x='displ',y='hwy',color='drv',size='cyl'),alpha=0.5)  ## 색상과 크기로 구분\n\n\nfig + point_better\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\ngeom_smooth() 개선\n\n\nsmooth_better = geom_smooth(aes(x = 'displ',  y = 'hwy', color = 'drv'), linetype = 'dashed')  ## 차종별로 추세선\n\n\nfig + smooth_better\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\nassemble\n\n\nfig + smooth_better + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#c.-다양한-조합",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#c.-다양한-조합",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### C. 다양한 조합",
    "text": "### C. 다양한 조합\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\nfig + smooth_better + point_better\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n전체 추세선 추가\n\n\nfig + smooth_better + point_better + geom_smooth(aes(x = 'displ', y = 'hwy'), color = 'white', linetype = 'dashed', size = 3)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#아이스크림을-많이-먹으면-걸리는-병---인과관계와-상관관계",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#아이스크림을-많이-먹으면-걸리는-병---인과관계와-상관관계",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "7. 아이스크림을 많이 먹으면 걸리는 병 - 인과관계와 상관관계",
    "text": "7. 아이스크림을 많이 먹으면 걸리는 병 - 인과관계와 상관관계"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#a.-교회의-수와-범죄-아이스크림과-소아마비",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#a.-교회의-수와-범죄-아이스크림과-소아마비",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### A. 교회의 수와 범죄, 아이스크림과 소아마비",
    "text": "### A. 교회의 수와 범죄, 아이스크림과 소아마비\n\n\n\n\n교회의 개수\n범죄건수\n\n\n\n\n전주\n100\n20\n\n\n부산\n1000\n200\n\n\n서울\n5000\n1000\n\n\n\n\n결론(?) : 교회가 많을 수록 범죄도 많아진다???\n\n\n배경없이 숫자만 비교할 경우, 상관관계를 인과관계로 착각할 수도 있다.\n인구에 대한 인과를 착각\n\n- 내용요약\n\n여름 → 수영장 → 소아마비\n여름 → 아이스크림\n아이스크림과 소아마비는 상관관계가 높다. 따라서 아이스크림 성분 중에서 소아마비를 유발하는 유해물질이 있을 것이다(?)\n\n\n다른 변인을 통제하고(인구가 동일한 지역), 비교하려는 대상만 차이를 부여해야 한다."
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#b.-기상자료",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#b.-기상자료",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 기상자료",
    "text": "### B. 기상자료\n- 기상자료 다운로드\n\ntemp=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()\n## 판다스 데이터의 4번째 열만 가져와 numpy.array로 만든다.\n\n\nplt.plot(temp)    ## 이럴 때는 ggplot보다 matplotlib가 훨씬 편하다.\nplt.show()"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#c.-숨은-진짜-상황-1-온도---아이스크림-판매량",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#c.-숨은-진짜-상황-1-온도---아이스크림-판매량",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### C. 숨은 진짜 상황 1 : 온도 -> 아이스크림 판매량",
    "text": "### C. 숨은 진짜 상황 1 : 온도 -&gt; 아이스크림 판매량\n-아래와 같은 관계를 가정하자. \\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\text{오차}\\]\n\nnp.random.seed(1)   ## 결과가 같도록 시드 설정\nicecream = 20 + 2 * temp + np.random.randn(len(temp))*10  ## N(0, 10^2)\nplt.plot(temp, icecream, 'o', alpha = 0.5)\nplt.show()"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#d.-숨은-진짜-상황-2-온도---소아마비-반응수치",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#d.-숨은-진짜-상황-2-온도---소아마비-반응수치",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### D. 숨은 진짜 상황 2 : 온도 -> 소아마비 반응수치",
    "text": "### D. 숨은 진짜 상황 2 : 온도 -&gt; 소아마비 반응수치\n- 아래와 같은 관계를 가정하자. \\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\text{오차}\\]\n\nnp.random.seed(2)\n\ndisease = 30 + 0.5 * temp + np.random.randn(len(temp))*1  ## N(0,1)\nplt.plot(temp, disease, 'o', alpha = 0.5)\nplt.show()"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#e.-우리가-관측한-상황온도는-은닉되어-있음",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#e.-우리가-관측한-상황온도는-은닉되어-있음",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### E. 우리가 관측한 상황(온도는 은닉되어 있음)",
    "text": "### E. 우리가 관측한 상황(온도는 은닉되어 있음)\n\nplt.plot(icecream, disease, 'o', alpha=0.3)\nplt.show()\n\n\n\n\n\nnp.corrcoef(icecream,disease)\n\narray([[1.        , 0.86298975],\n       [0.86298975, 1.        ]])\n\n\n여름만 뽑아서 플랏한다면?\n\nplt.plot(icecream,disease,'o',alpha=0.3)\nplt.plot(icecream[temp&gt;25], disease[temp&gt;25],'o') ## 기온이 25도 이상, 즉, 여름(아마도)\nplt.show()"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#f.-ggplot으로-온도구간을-세분화하여-시각화하자.",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#f.-ggplot으로-온도구간을-세분화하여-시각화하자.",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### F. ggplot으로 온도구간을 세분화하여 시각화하자.",
    "text": "### F. ggplot으로 온도구간을 세분화하여 시각화하자.\n- 데이터를 데이터프레임으로\n\ndf = pd.DataFrame({'temp' : temp, 'ice' : icecream, 'dis' : disease})\ndf\n\n\n\n\n\n\n\n\ntemp\nice\ndis\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n\n\n1\n1.4\n16.682436\n30.643733\n\n\n2\n2.6\n19.918282\n29.163804\n\n\n3\n2.0\n13.270314\n32.640271\n\n\n4\n2.5\n33.654076\n29.456564\n\n\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n\n\n652\n20.4\n76.554679\n38.920443\n\n\n653\n18.3\n68.666079\n39.882650\n\n\n654\n12.8\n42.771364\n36.613159\n\n\n655\n6.7\n30.736731\n34.902513\n\n\n\n\n656 rows × 3 columns\n\n\n\n- 구간별로 나눈 변수를 추가 : pd.cut(df, bins = int)\n\ndf.assign(temp_cut = pd.cut(df.temp, bins = 5))   ## 온도를 4구간으로 분할한다\n\n\n\n\n\n\n\n\ntemp\nice\ndis\ntemp_cut\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n(-3.92, 4.56]\n\n\n1\n1.4\n16.682436\n30.643733\n(-3.92, 4.56]\n\n\n2\n2.6\n19.918282\n29.163804\n(-3.92, 4.56]\n\n\n3\n2.0\n13.270314\n32.640271\n(-3.92, 4.56]\n\n\n4\n2.5\n33.654076\n29.456564\n(-3.92, 4.56]\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n(13.04, 21.52]\n\n\n652\n20.4\n76.554679\n38.920443\n(13.04, 21.52]\n\n\n653\n18.3\n68.666079\n39.882650\n(13.04, 21.52]\n\n\n654\n12.8\n42.771364\n36.613159\n(4.56, 13.04]\n\n\n655\n6.7\n30.736731\n34.902513\n(4.56, 13.04]\n\n\n\n\n656 rows × 4 columns\n\n\n\n\ncut_df = df.assign(temp_cut = pd.cut(df.temp, bins = 7))\n\nfig = ggplot(cut_df)\npoint = geom_point(aes(x = 'ice', y = 'dis', color = 'temp_cut'), alpha = 0.2)\nsmooth = geom_smooth(aes(x = 'ice', y = 'dis', color = 'temp_cut'), linetype = 'dashed')\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n실제로 보니 상관관계가 없어보인다.\n\n\n진짜 아이스크림을 먹고 배탈이 났다면?\n\nnp.random.seed(1)\nicecream_sales = 30 + 2 * temp + np.random.randn(len(temp))*10\n\n\nnp.random.seed(2)\ndisease = 30 + 0 * temp + 0.15 * icecream + np.random.randn(len(temp))*1  ## temp, 온도가 미치는 영향을 제로로\n\n\ndf2 = pd.DataFrame({'temp' : temp, 'ice' : icecream_sales, 'dis' : disease})\ndf2.assign(temp_cut = pd.cut(df2.temp, bins = 7))\n\n\n\n\n\n\n\n\ntemp\nice\ndis\ntemp_cut\n\n\n\n\n0\n-0.5\n45.243454\n34.869760\n(-6.343, -0.286]\n\n\n1\n1.4\n26.682436\n32.446099\n(-0.286, 5.771]\n\n\n2\n2.6\n29.918282\n30.851546\n(-0.286, 5.771]\n\n\n3\n2.0\n23.270314\n33.630818\n(-0.286, 5.771]\n\n\n4\n2.5\n43.654076\n33.254676\n(-0.286, 5.771]\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n78.839992\n40.009905\n(17.886, 23.943]\n\n\n652\n20.4\n86.554679\n40.203645\n(17.886, 23.943]\n\n\n653\n18.3\n78.666079\n41.032562\n(17.886, 23.943]\n\n\n654\n12.8\n52.771364\n36.628863\n(11.829, 17.886]\n\n\n655\n6.7\n40.736731\n36.163023\n(5.771, 11.829]\n\n\n\n\n656 rows × 4 columns\n\n\n\n\nfig = ggplot(df2.assign(temp_cut = pd.cut(df2.temp,bins=7)))\npoint = geom_point(aes(x='ice',y='dis',color='temp_cut'),alpha=0.2)\nsmooth = geom_smooth(aes(x='ice',y='dis',color='temp_cut'),linetype='dashed')\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n무친 인과관계"
  },
  {
    "objectID": "posts/Data Visualization/Review/4. Plotnine.html#결론",
    "href": "posts/Data Visualization/Review/4. Plotnine.html#결론",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "8. 결론",
    "text": "8. 결론\n\n아이스크림 먹어도 소아마비 안걸려!\n\n\n온도라는 흑막(은닉변수)을 잘 찾았고, 결과적으로 온도 -&gt; 아이스크림 판매량 & 소아마비라는 합리적인 진리를 얻을 수 있었다.\n\n\n고려할 흑막이 온도뿐이라는 보장이 있나?\n\n\n이론적으로는 모든 은닉변수들을 통제하였을 경우에도 corr(X,Y)의 절댓값이 1에 가깝다면 그때는 인과성이 있음이라고 주장할 수 있다.(이 경우에도 둘 중 어느것이 원인인지 파악하는 것은 불가)\n즉, 모든 은닉변수를 제거하면 상관성 = 인과성이다.\n\n\n모든 흑막을 제거하는 건 사실상 불가능하지 않나?\n\n\n실험계획을 잘 하면 흑막을 제거한 효과가 있음(무작위 추출 등)\n인과추론 : 실험계획이 사실상 불가능한 경우가 있음 -&gt; 모인 데이터에서 최대한 흑막2ㆍ3ㆍ4ㆍㆍㆍ등이 비슷한 그룹끼리 “매칭”을 시킨 뒤, 그룹간 corr을 구하여 규명한다!\n\n\n데이터의 수가 방대해지면서 가능해졌다."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "",
    "text": "열 이름 변경, 열 추가, 리스트 컴프리헨션, 결측치 파악, query, 매핑 등등… 해당 내용은 왠만해선 다 알아두는 게 좋다."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#import",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#import",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "1. import",
    "text": "1. import\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-기본기능",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-기본기능",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "2. Pandas 기본기능",
    "text": "2. Pandas 기본기능\n\nA. 열의 이름 변경\n\n\ndf = pd.DataFrame(np.random.randn(3, 2))\ndf\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n-0.000655\n0.686701\n\n\n1\n0.591774\n0.842045\n\n\n2\n-0.027722\n-0.703161\n\n\n\n\n\n\n\n- 방법1 : df.columns에 대입\n\ndf.columns = ['A', 'B']\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n-0.000655\n0.686701\n\n\n1\n0.591774\n0.842045\n\n\n2\n-0.027722\n-0.703161\n\n\n\n\n\n\n\n- 방법2 : df.set_axis() \\(\\star\\star\\star\\)\n\ndf2 = pd.DataFrame(np.random.randn(5,3))\ndf2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.200618\n-0.567175\n-0.249051\n\n\n1\n0.805185\n-0.479624\n0.797904\n\n\n2\n-1.278647\n-0.061503\n1.048704\n\n\n3\n0.308626\n-3.294418\n0.326681\n\n\n4\n1.585979\n-1.200001\n0.386765\n\n\n\n\n\n\n\n\ndf2 = df2.set_axis(['A','B','C'], axis = 1)\ndf2\n\n#df2.set_axis(['a','b','c','d,',e'], axis = 0)으로 하면 인덱스가 바뀐다.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n0.200618\n-0.567175\n-0.249051\n\n\n1\n0.805185\n-0.479624\n0.797904\n\n\n2\n-1.278647\n-0.061503\n1.048704\n\n\n3\n0.308626\n-3.294418\n0.326681\n\n\n4\n1.585979\n-1.200001\n0.386765\n\n\n\n\n\n\n\n- 방법3 : df.rename()\n\ndf3 = pd.DataFrame(np.random.randn(5,3))\ndf3\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.202540\n0.265273\n1.855420\n\n\n1\n-0.422516\n-0.954117\n-0.050532\n\n\n2\n-0.010961\n-1.681503\n-1.613766\n\n\n3\n0.855199\n0.773191\n1.149413\n\n\n4\n0.310184\n-0.063591\n-0.572836\n\n\n\n\n\n\n\n\ndf3.rename({0 : 'A'}, axis = 1) ## dictionary 형태로 지정, 특정 열만 바꿈\n##df3.rename(columns = {0 : 'A'})와 동일\n\n\n\n\n\n\n\n\nA\n1\n2\n\n\n\n\n0\n0.202540\n0.265273\n1.855420\n\n\n1\n-0.422516\n-0.954117\n-0.050532\n\n\n2\n-0.010961\n-1.681503\n-1.613766\n\n\n3\n0.855199\n0.773191\n1.149413\n\n\n4\n0.310184\n-0.063591\n-0.572836"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-행의-이름-변경",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-행의-이름-변경",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. 행의 이름 변경",
    "text": "### B. 행의 이름 변경\n- 방법1 : df.index에 대입\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.108275\n-0.802206\n-3.011323\n\n\n1\n-1.437775\n-1.868590\n-0.079212\n\n\n\n\n\n\n\n\ndf.index = ['a', 'b']\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n0.108275\n-0.802206\n-3.011323\n\n\nb\n-1.437775\n-1.868590\n-0.079212\n\n\n\n\n\n\n\n- 방법2 : df.set_axis() \\(\\star\\star\\star\\)\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-0.179379\n0.684650\n1.678079\n\n\n1\n0.487614\n-1.358992\n-0.661587\n\n\n\n\n\n\n\n\ndf.set_axis(['1','2'], axis = 0)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n1\n-0.179379\n0.684650\n1.678079\n\n\n2\n0.487614\n-1.358992\n-0.661587\n\n\n\n\n\n\n\n- 방법3 : df.rename()\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-0.051285\n1.185885\n0.841335\n\n\n1\n0.118555\n1.527457\n0.544870\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({0 : 'A'}, axis = 0)    ## default = 0\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nA\n-0.051285\n1.185885\n0.841335\n\n\n1\n0.118555\n1.527457\n0.544870\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 방법 4 : df.set_index() &gt; 임의의 열을 행이름으로 지정, 이미 있던 열 하나를 인덱스로 잡고 싶을 시 사용\n\ndf = pd.DataFrame({'id':['2020-43052','2021-43053'], 'X1':[1,2],'X2':[2,3]})\ndf\n\n\n\n\n\n\n\n\nid\nX1\nX2\n\n\n\n\n0\n2020-43052\n1\n2\n\n\n1\n2021-43053\n2\n3\n\n\n\n\n\n\n\n\ndf.set_index('id')\n\n\n\n\n\n\n\n\nX1\nX2\n\n\nid\n\n\n\n\n\n\n2020-43052\n1\n2\n\n\n2021-43053\n2\n3\n\n\n\n\n\n\n\n\n# A~B에 대한 연습문제\n\n- 데이터 load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n# 예제1 : 열의 이름을 출력하고, 열의 이름중 공백()이 있을 경우 언더바(_) 로 바꾸자.\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n- 방법1 : df.columns에 직접 대입\n\ndf_ = df\ndf_.columns = [i.replace(' ', '_') for i in df_.columns]\n\n\ndf_.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n- 방법2 : set_axis() 이용 \\(\\star\\star\\star\\)\n\ndf_ = df\ndf_.set_axis([col.replace(' ', '_') for col in df_.columns], axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n- 방법 3 : rename() 이용(안중요함)\n\ntemp3 = df\n\ndic = {i:i.replace(' ','_') for i in df.columns}\ntemp3.rename(dic, axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n# 예제2: ID를 row-index로 지정하라.\n\ndf.ID\n\n0        209658\n1        212198\n2        224334\n3        192985\n4        224232\n          ...  \n17655    269526\n17656    267946\n17657    270567\n17658    256624\n17659    256376\nName: ID, Length: 17660, dtype: int64\n\n\n- 방법1 : 직접지정\n\ndf_ = df\ndf_.index = df.ID\ndf_.index\n\nIndex([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961, 208333,\n       210514,\n       ...\n       256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567, 256624,\n       256376],\n      dtype='int64', name='ID', length=17660)\n\n\n- 방법2 : set_axis() \\(\\star\\star\\star\\)\n\ndf_ = df\ndf_ = df_.set_axis(df.ID)   ## default : axis = 0, df_.set_axis(df.ID, axis = 0)과 동일\ndf_.index\n\nInt64Index([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961,\n            208333, 210514,\n            ...\n            256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567,\n            256624, 256376],\n           dtype='int64', name='ID', length=17660)\n\n\n- 방법3 : set_index()\n\n이 경우 해당 열을 나중에 따로 드랍하지 않아도 됨\n\n\ndf_ = df\ndf_ = df_.set_index('ID')\ndf_.index\n\nIndex([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961, 208333,\n       210514,\n       ...\n       256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567, 256624,\n       256376],\n      dtype='int64', name='ID', length=17660)"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#c.-df.t-데이터프레임을-전치",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#c.-df.t-데이터프레임을-전치",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. df.T | 데이터프레임을 전치",
    "text": "### C. df.T | 데이터프레임을 전치\ndf.T를 이용하여 데이터를 살피면 편리함\n- data load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n- df.T : 데이터프레임을 전치(transition)한다.\n\ndf.T.loc[:,:3]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nID\n209658\n212198\n224334\n192985\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\nK. De Bruyne\n\n\nAge\n27\n27\n30\n31\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nhttps://cdn.sofifa.net/players/192/985/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\nBelgium\n\n\nFlag\nhttps://cdn.sofifa.net/flags/de.png\nhttps://cdn.sofifa.net/flags/pt.png\nhttps://cdn.sofifa.net/flags/ar.png\nhttps://cdn.sofifa.net/flags/be.png\n\n\nOverall\n87\n86\n85\n91\n\n\nPotential\n88\n87\n85\n91\n\n\nClub\nFC Bayern München\nManchester United\nSevilla FC\nManchester City\n\n\nClub Logo\nhttps://cdn.sofifa.net/teams/21/30.png\nhttps://cdn.sofifa.net/teams/11/30.png\nhttps://cdn.sofifa.net/teams/481/30.png\nhttps://cdn.sofifa.net/teams/10/30.png\n\n\nValue\n€91M\n€78.5M\n€46.5M\n€107.5M\n\n\nWage\n€115K\n€190K\n€46K\n€350K\n\n\nSpecial\n2312\n2305\n2303\n2303\n\n\nPreferred Foot\nRight\nRight\nLeft\nRight\n\n\nInternational Reputation\n4.0\n3.0\n2.0\n4.0\n\n\nWeak Foot\n4.0\n3.0\n3.0\n5.0\n\n\nSkill Moves\n3.0\n4.0\n3.0\n4.0\n\n\nWork Rate\nHigh/ Medium\nHigh/ High\nHigh/ High\nHigh/ High\n\n\nBody Type\nUnique\nUnique\nStocky (170-185)\nUnique\n\n\nReal Face\nYes\nYes\nNo\nYes\n\n\nPosition\n&lt;span class=\"pos pos28\"&gt;SUB\n&lt;span class=\"pos pos15\"&gt;LCM\n&lt;span class=\"pos pos7\"&gt;LB\n&lt;span class=\"pos pos13\"&gt;RCM\n\n\nJoined\nJul 1, 2018\nJan 30, 2020\nSep 14, 2020\nAug 30, 2015\n\n\nLoaned From\nNaN\nNaN\nNaN\nNaN\n\n\nContract Valid Until\n2026\n2026\n2024\n2025\n\n\nHeight\n189cm\n179cm\n172cm\n181cm\n\n\nWeight\n82kg\n69kg\n69kg\n70kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n€198.9M\n\n\nKit Number\n8.0\n8.0\n19.0\n17.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n- 출력옵션 조정\n\npd.options.display.max_rows = 10\ndisplay(df.T.iloc[:, :3])\npd.reset_option('display.max_rows')   ## 디폴트 옵션으로 변경\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\n209658\n212198\n224334\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\n\n\nAge\n27\n27\n30\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\n\n\n...\n...\n...\n...\n\n\nHeight\n189cm\n179cm\n172cm\n\n\nWeight\n82kg\n69kg\n69kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n\n\nKit Number\n8.0\n8.0\n19.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\n\n\n\n\n29 rows × 3 columns\n\n\n\n\n여기선 설명을 위해 줄이는 옵션을 사용했지만, 보통은 늘려서 사용함."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#d.-df.dtypes-sdtype-데이터의-타입-산출",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#d.-df.dtypes-sdtype-데이터의-타입-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### D. df.dtypes, s,dtype | 데이터의 타입 산출",
    "text": "### D. df.dtypes, s,dtype | 데이터의 타입 산출\n- df.dtypes\n\n데이터프레임 각 열에 저장된 데이터들의 타입을 알려준다.\n\n\ndf.dtypes\n\nID                            int64\nName                         object\nAge                           int64\nPhoto                        object\nNationality                  object\nFlag                         object\nOverall                       int64\nPotential                     int64\nClub                         object\nClub Logo                    object\nValue                        object\nWage                         object\nSpecial                       int64\nPreferred Foot               object\nInternational Reputation    float64\nWeak Foot                   float64\nSkill Moves                 float64\nWork Rate                    object\nBody Type                    object\nReal Face                    object\nPosition                     object\nJoined                       object\nLoaned From                  object\nContract Valid Until         object\nHeight                       object\nWeight                       object\nRelease Clause               object\nKit Number                  float64\nBest Overall Rating          object\ndtype: object\n\n\n\nobject : string이라고 생각해도 무방. 범주형 자료.\n\n- s.dtype Series에 붙여 사용\n\ndf.Name.dtype   ## 한 행의 데이터 타입만을 산출\n\ndtype('O')\n\n\n\n다양한 활용이 가능\n\n\ndf.Name.dtype == np.object_\n\nTrue\n\n\n\ndf.Age.dtype == np.int64\n\nTrue\n\n\n\ndf['International Reputation'].dtype == np.float64\n\nTrue\n\n\n\nbool을 산출하니까 컴프리헨션에 조건문 걸어서 해도 되고… 활용의 여지가 넓다.\n\n# 예제: df에서 int64 자료형만 출력\n- 풀이 1 : 표를 보고 직접 뽑음\n\npd.Series(list(df.dtypes))\n\n0       int64\n1      object\n2       int64\n3      object\n4      object\n5      object\n6       int64\n7       int64\n8      object\n9      object\n10     object\n11     object\n12      int64\n13     object\n14    float64\n15    float64\n16    float64\n17     object\n18     object\n19     object\n20     object\n21     object\n22     object\n23     object\n24     object\n25     object\n26     object\n27    float64\n28     object\ndtype: object\n\n\n\ndf.iloc[:, [0,2,6,7,12]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns\n\n\n\n- 풀이 2 : 리스트 컴프리핸션 이용\n\ndf.loc[:, [o == np.int64 for o in df.dtypes]]\n##df.loc[:, [o == 'int64' for o in df.dtypes]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#e.-df.sort_values-데이터들을-정렬",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#e.-df.sort_values-데이터들을-정렬",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### E. df.sort_values() | 데이터들을 정렬",
    "text": "### E. df.sort_values() | 데이터들을 정렬\n- 예시1 : 순서대로 정렬\n\ndf.sort_values('Age')   ## 나이가 어린 순서대로 오름차순 정렬 / 인덱스 개판\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n13712\n271072\nE. Topcu\n16\nhttps://cdn.sofifa.net/players/271/072/23_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.net/flags/ie.png\n48\n58\nDrogheda United\nhttps://cdn.sofifa.net/teams/1572/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 8, 2022\nNaN\n2022\n183cm\n65kg\n€175K\n20.0\nNaN\n\n\n13078\n259442\n22 R. van den Berg\n16\nhttps://cdn.sofifa.net/players/259/442/22_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n60\n81\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 24, 2020\nNaN\n2024\n190cm\n73kg\n€1.8M\n33.0\nNaN\n\n\n11257\n266205\n22 Y. Koré\n16\nhttps://cdn.sofifa.net/players/266/205/22_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n59\n74\nParis FC\nhttps://cdn.sofifa.net/teams/111817/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nAug 11, 2022\nNaN\n2025\n187cm\n75kg\n€1.1M\n34.0\nNaN\n\n\n11278\n261873\n21 H. Kumagai\n16\nhttps://cdn.sofifa.net/players/261/873/21_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n52\n70\nVegalta Sendai\nhttps://cdn.sofifa.net/teams/112836/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 16, 2021\nNaN\n2023\n174cm\n64kg\n€375K\n48.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시 2 : 내림차순으로 정렬\n\ndf.sort_values('Age', ascending = False)  ## default : ascending = True\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17360\n261023\n21 H. Broun\n16\nhttps://cdn.sofifa.net/players/261/023/21_60.png\nScotland\nhttps://cdn.sofifa.net/flags/gb-sct.png\n52\n72\nKilmarnock\nhttps://cdn.sofifa.net/teams/82/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 17, 2020\nNaN\n2022\n182cm\n70kg\n€523K\n40.0\nNaN\n\n\n15536\n263639\n22 M. Pavel\n16\nhttps://cdn.sofifa.net/players/263/639/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n51\n69\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2021\nNaN\n2023\n178cm\n66kg\n€277K\n77.0\nNaN\n\n\n11398\n256405\n21 W. Essanoussi\n16\nhttps://cdn.sofifa.net/players/256/405/21_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n59\n75\nVVV-Venlo\nhttps://cdn.sofifa.net/teams/100651/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2019\nNaN\n2022\n178cm\n70kg\n€1.1M\n24.0\nNaN\n\n\n15030\n270594\nT. Walczak\n16\nhttps://cdn.sofifa.net/players/270/594/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n54\n68\nWisła Płock\nhttps://cdn.sofifa.net/teams/1569/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 7, 2021\nNaN\n2023\n191cm\n88kg\n€494K\n99.0\nNaN\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시 3 : 능력치가 좋은 순서대로 정렬\n\ndf.sort_values(by = 'Overall', ascending = False)    ## 수가 높을수록 위로 가니까, by 생략해도 됨.\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n41\n188545\nR. Lewandowski\n33\nhttps://cdn.sofifa.net/players/188/545/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n91\n91\nFC Barcelona\nhttps://cdn.sofifa.net/teams/241/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 18, 2022\nNaN\n2025\n185cm\n81kg\n€172.2M\n9.0\nNaN\n\n\n124\n165153\nK. Benzema\n34\nhttps://cdn.sofifa.net/players/165/153/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n91\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos21\"&gt;CF\nJul 9, 2009\nNaN\n2023\n185cm\n81kg\n€131.2M\n9.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n56\n158023\nL. Messi\n35\nhttps://cdn.sofifa.net/players/158/023/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n91\n91\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos23\"&gt;RW\nAug 10, 2021\nNaN\n2023\n169cm\n67kg\n€99.9M\n30.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15513\n266751\n22 Jung Ho Yeon\n20\nhttps://cdn.sofifa.net/players/266/751/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n45\n53\nGwangJu FC\nhttps://cdn.sofifa.net/teams/112258/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 20, 2022\nNaN\n2026\n180cm\n73kg\n€145K\n23.0\nNaN\n\n\n16215\n268279\n22 J. Looschen\n24\nhttps://cdn.sofifa.net/players/268/279/22_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n44\n47\nSV Meppen\nhttps://cdn.sofifa.net/teams/110597/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMar 19, 2022\nNaN\n2026\n178cm\n78kg\n€92K\n42.0\nNaN\n\n\n16042\n255283\n20 Kim Yeong Geun\n22\nhttps://cdn.sofifa.net/players/255/283/20_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n44\n49\nGyeongnam FC\nhttps://cdn.sofifa.net/teams/111588/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 9, 2020\nNaN\n2020\n174cm\n71kg\n€53K\n43.0\nNaN\n\n\n14634\n269038\n22 Zhang Wenxuan\n16\nhttps://cdn.sofifa.net/players/269/038/22_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n44\n59\nGuangzhou FC\nhttps://cdn.sofifa.net/teams/111839/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 1, 2022\nNaN\n2022\n175cm\n70kg\n€239K\n29.0\nNaN\n\n\n17618\n168933\n07 I. Paskov\n33\nhttps://cdn.sofifa.net/players/168/933/07_60.png\nBulgaria\nhttps://cdn.sofifa.net/flags/bg.png\n43\n42\nNaN\nhttps://cdn.sofifa.net/flags/bg.png\n...\nNaN\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n184cm\n79kg\nNaN\n24.0\nNaN\n\n\n\n\n\n17660 rows × 29 columns"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#f.-df.info-행별-information-산출",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#f.-df.info-행별-information-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### F. df.info() | 행별 information 산출",
    "text": "### F. df.info() | 행별 information 산출\n시험에는 절대 안 낼 거지만 매우 중요한 것\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17660 entries, 0 to 17659\nData columns (total 29 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   ID                        17660 non-null  int64  \n 1   Name                      17660 non-null  object \n 2   Age                       17660 non-null  int64  \n 3   Photo                     17660 non-null  object \n 4   Nationality               17660 non-null  object \n 5   Flag                      17660 non-null  object \n 6   Overall                   17660 non-null  int64  \n 7   Potential                 17660 non-null  int64  \n 8   Club                      17449 non-null  object \n 9   Club Logo                 17660 non-null  object \n 10  Value                     17660 non-null  object \n 11  Wage                      17660 non-null  object \n 12  Special                   17660 non-null  int64  \n 13  Preferred Foot            17660 non-null  object \n 14  International Reputation  17660 non-null  float64\n 15  Weak Foot                 17660 non-null  float64\n 16  Skill Moves               17660 non-null  float64\n 17  Work Rate                 17660 non-null  object \n 18  Body Type                 17622 non-null  object \n 19  Real Face                 17622 non-null  object \n 20  Position                  17625 non-null  object \n 21  Joined                    16562 non-null  object \n 22  Loaned From               694 non-null    object \n 23  Contract Valid Until      17299 non-null  object \n 24  Height                    17660 non-null  object \n 25  Weight                    17660 non-null  object \n 26  Release Clause            16509 non-null  object \n 27  Kit Number                17625 non-null  float64\n 28  Best Overall Rating       21 non-null     object \ndtypes: float64(4), int64(5), object(20)\nmemory usage: 3.9+ MB\n\n\n\ndata들의 현황을 한눈에 파악하기 좋다.\n\n\ndf.iloc[:, [28]].sort_values('Best Overall Rating')\n\n\n  \n    \n\n\n\n\n\n\nBest Overall Rating\n\n\n\n\n13299\n&lt;span class=\"bp3-tag p p-54\"&gt;54&lt;/span&gt;\n\n\n14366\n&lt;span class=\"bp3-tag p p-56\"&gt;56&lt;/span&gt;\n\n\n16779\n&lt;span class=\"bp3-tag p p-58\"&gt;58&lt;/span&gt;\n\n\n16968\n&lt;span class=\"bp3-tag p p-58\"&gt;58&lt;/span&gt;\n\n\n16835\n&lt;span class=\"bp3-tag p p-59\"&gt;59&lt;/span&gt;\n\n\n...\n...\n\n\n17655\nNaN\n\n\n17656\nNaN\n\n\n17657\nNaN\n\n\n17658\nNaN\n\n\n17659\nNaN\n\n\n\n\n\n17660 rows × 1 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[:, ['Best Overall Rating']].isna().sum()\n\nBest Overall Rating    17639\ndtype: int64\n\n\n\n결측치가 매우 많은 것을 볼 수 있다."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#g.-df.isna-각-원소가-결측치인지-아닌지-산출",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#g.-df.isna-각-원소가-결측치인지-아닌지-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### G. df.isna()| 각 원소가 결측치인지 아닌지 산출",
    "text": "### G. df.isna()| 각 원소가 결측치인지 아닌지 산출\n- 예시 1 : 열별로 결측치 카운트\n\ndf.isna()   ## NaN 값이 있다면 True 산출\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17656\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17657\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17658\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17659\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\ndf.isna().sum(axis = 0)   ## default : axis = 0\n\nID                              0\nName                            0\nAge                             0\nPhoto                           0\nNationality                     0\nFlag                            0\nOverall                         0\nPotential                       0\nClub                          211\nClub Logo                       0\nValue                           0\nWage                            0\nSpecial                         0\nPreferred Foot                  0\nInternational Reputation        0\nWeak Foot                       0\nSkill Moves                     0\nWork Rate                       0\nBody Type                      38\nReal Face                      38\nPosition                       35\nJoined                       1098\nLoaned From                 16966\nContract Valid Until          361\nHeight                          0\nWeight                          0\nRelease Clause               1151\nKit Number                     35\nBest Overall Rating         17639\ndtype: int64\n\n\n\narr = np.array([(True, False), (True, False), (False, True)])\narr\n\narray([[ True, False],\n       [ True, False],\n       [False,  True]])\n\n\n\narr.shape\n\n(3, 2)\n\n\n\narr.sum(axis = 0) ## 열별로 합\n\narray([2, 1])\n\n\n\narr.sum(axis = 1)   ## 행별로 합\n\narray([1, 1, 1])\n\n\n- 예시2 : 결측치가 50% 이상인 열 출력\n\ntype(df.isna().mean() &gt; 0.5)  ## 이 값 자체가 시리즈이므로 리스트로 넣으면 안된다.\n\npandas.core.series.Series\n\n\n\ndf.loc[:, df.isna().mean() &gt; 0.5]  ## [df.isna().mean() &gt; 0.5]는 에러뜸\n\n\n\n\n\n\n\n\nLoaned From\nBest Overall Rating\n\n\n\n\n0\nNaN\nNaN\n\n\n1\nNaN\nNaN\n\n\n2\nNaN\nNaN\n\n\n3\nNaN\nNaN\n\n\n4\nNaN\nNaN\n\n\n...\n...\n...\n\n\n17655\nNaN\nNaN\n\n\n17656\nNaN\nNaN\n\n\n17657\nNaN\nNaN\n\n\n17658\nNaN\nNaN\n\n\n17659\nNaN\nNaN\n\n\n\n\n17660 rows × 2 columns"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#h.-df.drop-특정-행이나-열을-drop",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#h.-df.drop-특정-행이나-열을-drop",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### H. df.drop() | 특정 행이나 열을 drop",
    "text": "### H. df.drop() | 특정 행이나 열을 drop\n- 예시 1 : [0,1,2,3] 행을 drop\n\ndf.drop([0,1,2,3])\n## df.drop([0,1,2,3], axis = 0)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n5\n212622\nJ. Kimmich\n27\nhttps://cdn.sofifa.net/players/212/622/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n89\n90\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos9\"&gt;RDM\nJul 1, 2015\nNaN\n2025\n177cm\n75kg\n€182M\n6.0\nNaN\n\n\n6\n197445\nD. Alaba\n30\nhttps://cdn.sofifa.net/players/197/445/23_60.png\nAustria\nhttps://cdn.sofifa.net/flags/at.png\n86\n86\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos6\"&gt;LCB\nJul 1, 2021\nNaN\n2026\n180cm\n78kg\n€113.8M\n4.0\nNaN\n\n\n7\n187961\n22 Paulinho\n32\nhttps://cdn.sofifa.net/players/187/961/22_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n83\n83\nAl Ahli\nhttps://cdn.sofifa.net/teams/112387/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJul 22, 2021\nNaN\n2024\n183cm\n80kg\n€48.5M\n15.0\nNaN\n\n\n8\n208333\nE. Can\n28\nhttps://cdn.sofifa.net/players/208/333/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n82\n82\nBorussia Dortmund\nhttps://cdn.sofifa.net/teams/22/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nFeb 18, 2020\nNaN\n2024\n186cm\n86kg\n€51.9M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17656 rows × 29 columns\n\n\n\n- 예시 2 : ['Name', 'Age']열을 drop\n\ndf.drop(columns = ['Name', 'Age'])\n## df.drop(['Name', 'Age'], axis = 1)\n\n\n  \n    \n\n\n\n\n\n\nID\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\nValue\nWage\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n€91M\n€115K\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n€78.5M\n€190K\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n€46.5M\n€46K\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n€107.5M\n€350K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n€89.5M\n€110K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n€70K\n€2K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n\n17660 rows × 27 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n결국에 axis 옵션만 기억하면 다른 parameter를 기억하지 않아도 된다.\n\n\n# G~H 에 대한 연습문제\n# 예제: 결측치가 50퍼 이상인 열을 제외하라.\n- 풀이 1 : 무지성 직접 제외\n\ndf.isna().mean()  ## := df.isna().sum() / len(df). Series\n\nID                          0.000000\nName                        0.000000\nAge                         0.000000\nPhoto                       0.000000\nNationality                 0.000000\nFlag                        0.000000\nOverall                     0.000000\nPotential                   0.000000\nClub                        0.011948\nClub Logo                   0.000000\nValue                       0.000000\nWage                        0.000000\nSpecial                     0.000000\nPreferred Foot              0.000000\nInternational Reputation    0.000000\nWeak Foot                   0.000000\nSkill Moves                 0.000000\nWork Rate                   0.000000\nBody Type                   0.002152\nReal Face                   0.002152\nPosition                    0.001982\nJoined                      0.062174\nLoaned From                 0.960702\nContract Valid Until        0.020442\nHeight                      0.000000\nWeight                      0.000000\nRelease Clause              0.065176\nKit Number                  0.001982\nBest Overall Rating         0.998811\ndtype: float64\n\n\n\ndf.drop(columns=['Loaned From','Best Overall Rating'])\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns\n\n\n\n- 풀이 2 : 이성적인 풀이\n\ndf.loc[:, df.isna().mean() &lt; 0.5]  ## 시리즈니까 리스트로 묶지 말것!!\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-missing",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-missing",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "3. Pandas : missing",
    "text": "3. Pandas : missing"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-numpy",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-numpy",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. Numpy",
    "text": "### A. Numpy\n- 발생 : np.nan\n\nnp.nan\n\nnan\n\n\n\narr = np.array([1,2,3,np.nan])\narr\n\narray([ 1.,  2.,  3., nan])\n\n\n\narr.mean()\n\nnan\n\n\n\nprint(type(np.nan))  ## np.nan 자체는 일종의 float로 취급된다.\nprint(type(arr[0]))\n\n&lt;class 'float'&gt;\n&lt;class 'numpy.float64'&gt;"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-pandas",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-pandas",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. Pandas",
    "text": "### B. Pandas\n- 발생 : np.nan, pd.NA\n\npd.Series([np.nan,1,2,3])\n\n0    NaN\n1    1.0\n2    2.0\n3    3.0\ndtype: float64\n\n\n\npd.Series([pd.NA, 1, 2, 3])\n\n0    &lt;NA&gt;\n1       1\n2       2\n3       3\ndtype: object\n\n\n\n위 두 개의 코드는 동일하다고 봐도 무방하다.\n\n- pd.Serise에 NaN 또는 &lt;NA&gt;가 있다면 연산할 때 제외함\n\nprint(f'np.nan을 넣은 시리즈의 평균 : {pd.Series([np.nan,1,2,3]).mean()} = pd.NA를 넣은 시리즈의 평균 : {pd.Series([pd.NA,1,2,3]).mean()}')\n\nnp.nan을 넣은 시리즈의 평균 : 2.0 = pd.NA를 넣은 시리즈의 평균 : 2.0\n\n\n\ns1 = pd.Series([np.nan,1,2,3])\ntype(s1[0])\n\nnumpy.float64\n\n\n\ns2 = pd.Series([pd.NA, 1,2,3])\ntype(s2[0])\n\npandas._libs.missing.NAType\n\n\n\nmissing은 그냥 NaN이라고 보자.\n\n- 검출(\\(\\star\\))(중요한가?)\n\ns1.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\ns2.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\npd.isna(s1[0]), pd.isnull(s1[0])  ## 결측치가 있느냐?\n\n(True, True)\n\n\n\npd.isna(s2[0]), pd.isnull(s2[0])  ## 결측치가 있느냐?\n\n(True, True)\n\n\n\nid(pd.isna), id(pd.isnull) # 같은함수\n\n(135146401797248, 135146401797248)\n\n\n\nid를 찍었을 때 같다면 같은 함수이다."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-query",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "5. Pandas : query",
    "text": "5. Pandas : query\n\n개 간단하고 쉽지만 고점은 낮은 데이터 처리방식\n\n\nts = pd.DataFrame(np.random.normal(size=(20,4)),columns=list('ABCD'),index=pd.date_range('20221226',periods=20)).assign(E=['A']*10+['B']*10)\nts\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-02\n-1.136712\n0.595607\n-1.938775\n0.201931\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-06\n-0.095612\n-0.692796\n0.456627\n-0.395918\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-08\n-0.939328\n0.745621\n0.632724\n0.032088\nB\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n2023-01-10\n-0.339565\n-0.460976\n0.320097\n0.482333\nB\n\n\n2023-01-11\n-0.117493\n-1.964531\n-1.867120\n2.325713\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB\n\n\n2023-01-14\n-1.069801\n-0.659982\n-0.368828\n1.286645\nB"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-기본-query",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-기본-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. 기본 query",
    "text": "### A. 기본 query\n- 예시1: A&gt;0 and B&lt;0\n\nts.query('A&gt;0 and B&lt;0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n\n\n\n\n\n- 예시2: A&lt;B&lt;C\n\nts.query('A&lt;B&lt;C')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-14\n-1.069801\n-0.659982\n-0.368828\n1.286645\nB\n\n\n\n\n\n\n\n- 예시3: (A+B/2) &gt; 0\n\nts.query('(A+B)/2 &gt; 0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB\n\n\n\n\n\n\n\n- 예시4: (A+B/2) &gt; 0 and E=='A'\n\nts.query('(A+B)/2 &gt; 0 and E == \"A\"')   ## string 조건을 넣어주고 싶으면 다른 따옴표로 구분하면 된다.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n\n\n\n\n\n\n그냥 스트링으로 된 것들 중에는 생각헀던 건 왠만해선 다 된다."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-외부변수를-이용",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-외부변수를-이용",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. 외부변수를 이용",
    "text": "### B. 외부변수를 이용\n- 예시1: A &gt; mean(A)\n\nmean = ts.A.mean()\nmean\n\n0.14414224086779243\n\n\n\n#ts.query('A &gt; np.mean(A)')   ## 이건 안됨\n#ts.query('A &gt; A.mean()')      ## 이건 되긴 함\n\n#ts.query('A &gt; mean')    ## column 중 하나인지 뭔지 헷갈림, 그래서 안됨\n\nts.query('A &gt; @mean')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n\n\n\n\n\n\nA.mean()보다 작은 값들을 산출했다.\n\n\nvalue = np.percentile(ts.B, 77)  ## ts.B에서 77백분위수에 해당하는 숫자\nts.query('B &gt; @value')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-01-02\n-1.136712\n0.595607\n-1.938775\n0.201931\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-08\n-0.939328\n0.745621\n0.632724\n0.032088\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#c.-index로-query",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#c.-index로-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. Index로 query",
    "text": "### C. Index로 query\n- 예시: (2022년 12월30일 보다 이전 날짜) \\(\\cup\\) (2023년 1월10일)\n\nts.query('index &lt; \"2022-12-30\" or index == \"2023-01-10\"')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2023-01-10\n-0.339565\n-0.460976\n0.320097\n0.482333\nB"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#d.-열의-이름에-공백이-있을-경우",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#d.-열의-이름에-공백이-있을-경우",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### D. 열의 이름에 공백이 있을 경우",
    "text": "### D. 열의 이름에 공백이 있을 경우\n열의 이름에 공백이 있으면 백틱을 이용하면 된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n\ndf.query('`Skill Moves` &gt; 4')  ## `를 사용\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n19\n193082\nJ. Cuadrado\n34\nhttps://cdn.sofifa.net/players/193/082/23_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n83\n83\nJuventus\nhttps://cdn.sofifa.net/teams/45/30.png\n...\nYes\n&lt;span class=\"pos pos3\"&gt;RB\nJul 1, 2017\nNaN\n2023\n179cm\n72kg\n€23M\n11.0\nNaN\n\n\n27\n189509\nThiago\n31\nhttps://cdn.sofifa.net/players/189/509/23_60.png\nSpain\nhttps://cdn.sofifa.net/flags/es.png\n86\n86\nLiverpool\nhttps://cdn.sofifa.net/teams/9/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nSep 18, 2020\nNaN\n2024\n174cm\n70kg\n€102.7M\n6.0\nNaN\n\n\n44\n232411\nC. Nkunku\n24\nhttps://cdn.sofifa.net/players/232/411/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n86\n89\nRB Leipzig\nhttps://cdn.sofifa.net/teams/112172/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n175cm\n73kg\n€166.9M\n12.0\nNaN\n\n\n62\n233927\nLucas Paquetá\n24\nhttps://cdn.sofifa.net/players/233/927/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n82\n87\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nOct 1, 2020\nNaN\n2025\n180cm\n72kg\n€90.9M\n10.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4516\n253755\nTalles Magno\n20\nhttps://cdn.sofifa.net/players/253/755/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n71\n83\nNew York City FC\nhttps://cdn.sofifa.net/teams/112828/30.png\n...\nNo\n&lt;span class=\"pos pos16\"&gt;LM\nMay 18, 2021\nNaN\n2026\n186cm\n70kg\n€7.7M\n43.0\nNaN\n\n\n4643\n246548\nO. Sahraoui\n21\nhttps://cdn.sofifa.net/players/246/548/23_60.png\nNorway\nhttps://cdn.sofifa.net/flags/no.png\n67\n78\nVålerenga Fotball\nhttps://cdn.sofifa.net/teams/920/30.png\n...\nNo\n&lt;span class=\"pos pos27\"&gt;LW\nMay 15, 2019\nNaN\n2023\n170cm\n65kg\n€3.3M\n10.0\nNaN\n\n\n4872\n251570\nR. Cherki\n18\nhttps://cdn.sofifa.net/players/251/570/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n73\n88\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 7, 2019\nNaN\n2023\n176cm\n71kg\n€17.7M\n18.0\nNaN\n\n\n5361\n225712\nD. Bahamboula\n27\nhttps://cdn.sofifa.net/players/225/712/23_60.png\nCongo\nhttps://cdn.sofifa.net/flags/cg.png\n63\n63\nLivingston FC\nhttps://cdn.sofifa.net/teams/621/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 9, 2022\nNaN\n2024\n185cm\n70kg\n€875K\n7.0\nNaN\n\n\n10452\n212455\n17 H. Mastour\n18\nhttps://cdn.sofifa.net/players/212/455/17_60.png\nMorocco\nhttps://cdn.sofifa.net/flags/ma.png\n65\n76\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\n&lt;a href=\"/team/47/ac-milan/\"&gt;AC Milan&lt;/a&gt;\nJun 30, 2017\n175cm\n63kg\nNaN\n98.0\nNaN\n\n\n\n\n65 rows × 29 columns"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-할당",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-할당",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "5. Pandas : 할당",
    "text": "5. Pandas : 할당\n아래와 같은 자료를 조건에 맞게 가공해서 새로운 열을 추가해보자.\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\ndf = pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-df.assign",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-df.assign",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. df.assign()",
    "text": "### A. df.assign()\n- 예시: total = att*0.1 + rep*0.2 + mid*0.35 + fin*0.35 를 계산하여 할당\n\ndf.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35)\n## 원래 데이터 손상시키지 않음\n\ndf_total = df.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35)\ndf_total\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n49.00\n\n\n1\n95\n100\n50\n80\n75.00\n\n\n2\n65\n90\n60\n30\n56.00\n\n\n3\n55\n80\n75\n80\n75.75\n\n\n4\n80\n30\n30\n100\n59.50\n\n\n5\n75\n40\n100\n15\n55.75\n\n\n6\n65\n45\n45\n90\n62.75\n\n\n7\n60\n60\n25\n0\n26.75\n\n\n8\n95\n65\n20\n10\n33.00\n\n\n9\n90\n80\n80\n20\n60.00\n\n\n10\n55\n75\n35\n25\n41.50\n\n\n11\n95\n95\n45\n0\n44.25\n\n\n12\n95\n55\n15\n35\n38.00\n\n\n13\n50\n80\n40\n30\n45.50\n\n\n14\n50\n55\n15\n85\n51.00\n\n\n15\n95\n30\n30\n95\n59.25\n\n\n16\n50\n50\n45\n10\n34.25\n\n\n17\n65\n55\n15\n45\n38.50\n\n\n18\n70\n70\n40\n35\n47.25\n\n\n19\n90\n90\n80\n90\n86.50"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-df.eval",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-df.eval",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. df.eval()",
    "text": "### B. df.eval()\n- A에서와 동일한 할당\n\ndf.eval('total = att*0.1 + rep*0.2 + mid*0.3 + fin*0.4')    ## query를 쓰는 것처럼, 원본 데이터를 변화시키지 않음\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0\n\n\n\n\n\n\n\n\nbut, 사칙연산과 같은 기초연산 수준에서만 잘 작동한다."
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#c.-dfcolname-xxx",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#c.-dfcolname-xxx",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. df[colname] = xxx",
    "text": "### C. df[colname] = xxx\n\n별로 안쓰는 방법\n\n\ndf['total'] = df.att*0.1 + df.rep*0.2 + df.mid*0.3 + df.fin*0.4   ## 원래의 데이터프레임을 손상시킨다.\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-transform-columnstarstarstar",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-transform-columnstarstarstar",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "6. Pandas : transform column(\\(\\star\\star\\star\\))",
    "text": "6. Pandas : transform column(\\(\\star\\star\\star\\))"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-lambda",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#a.-lambda",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. lambda",
    "text": "### A. lambda\n- 예시1: \\(x \\to x+2\\)\n\n\"\"\"\ndef f(x) :\n  return x + 2\n\n해당 코드와 동일하다.\n\"\"\"\n\nf = lambda x: x+2\nprint(f(3))\n\nprint((lambda x : x+2)(3))    ## (lambda x : x+2) 자체가 함수이므로, 뒤에 변수만 지정해주면 된다.\n\n5\n5\n\n\n- 예시2: \\(x,y \\to x+y\\)\n\n(lambda x,y : x+y)(1,3)\n\n4\n\n\n- 예시3: ‘2023-09’ \\(\\to\\) 9 (format : int)\n\n(lambda x : int(x[-2:]))('2023-09')   ## -1번째(뒤에서 두번째 원소까지 추출)\n\n9\n\n\n- 예시4: ‘2023-09’ \\(\\to\\) (2023, 9) (format : tuple)\n\n(lambda x : (int(x[:4]), int(x[-2:])))('2023-09')\n\n(2023, 9)\n\n\n- 예시5: 문자열이 ‘cat’이면 1 ’dog’ 이면 0 // ’cat이면 1 ’cat’이 아니면 0\n\n(lambda x : 1 if x == 'cat' else 0)('cat')\n## (lambda x : pd.Series(x == 'cat').sum())('cat') ## 이것도 된다.\n\n1"
  },
  {
    "objectID": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-map",
    "href": "posts/Data Visualization/Review/6. Pandas, 데이터프레임 핸들링.html#b.-map",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. map",
    "text": "### B. map\n\n함수의 output들을 엮는다. 매핑하는 거\n\n:- 개념: map(f,[x1,x2,...xn])=[f(x1),f(x2),...,f(xn)]\n- 예시1: x-&gt;x+1을 [1,2,3]에 적용\n\nf = lambda x: x+1\nlist(map(f,[1,2,3]))\n\n[2, 3, 4]\n\n\n\nlist(map(lambda x : x + 1, [1,2,3]))\n\n[2, 3, 4]\n\n\n\n매핑하는 것 자체는 수나 리스트가 아니기 때문에 리스트로 엮어줘야 값을 알 수 있다.\n\n- 예시2 df.Height열 변환하기 (xxxcm 라고 적혀있는 것을 cm 없애고 키만 뽑기)\ns.str.replace('cm', '')\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height[:5]\ns\n\n0    189cm\n1    179cm\n2    172cm\n3    181cm\n4    172cm\nName: Height, dtype: object\n\n\n\nlist(map(lambda x : int(x[:-2]), s))\n\n[189, 179, 172, 181, 172]\n\n\n- 풀이 1 : map 이용\n\ndf.assign(Height = list(map(lambda x: int(x.replace('cm','')), df.Height)))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 풀이 2 : 사실 수틀리면 컴프리헨션 쓰면 된다.\n\ndf.assign(Height = [int(s.replace('cm', '')) for s in df.Height])\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n\n17660 rows × 29 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n# 예시4 – df.Position 열에 아래와 같은 변환을 수행하고, 변환된 열을 할당하라.\n\n\n\nbefore\nafter\n\n\n\n\n&lt;span class=\"pos pos28\"&gt;SUB\nSUB\n\n\n&lt;span class=\"pos pos15\"&gt;LCM\nLCM\n\n\n&lt;span class=\"pos pos7\"&gt;LB\nLB\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n\n- 풀이 1\n\n데이터를 불러와서…\n\n\nlist(map(lambda x : x.str.split('&gt;')[-1] if x.isna() == False else pd.NA, df.Position))\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\n\n\n저장된 꼬라지를 보면…\n\n\nx = df.Position[0]\nx\n\n'&lt;span class=\"pos pos28\"&gt;SUB'\n\n\n\n게다가 결측치까지 있네???\n\n\ndf.Position.isna().sum()\n\n35\n\n\n\n결측치 처리 + 데이터 변환\n\n\ndf.assign(Position = list(map(lambda x : x.split('&gt;')[-1] if not pd.isna(x) else pd.NA, df.Position))).Position\n\n0        SUB\n1        LCM\n2         LB\n3        RCM\n4        RCM\n        ... \n17655    RES\n17656    RES\n17657    RES\n17658    RES\n17659    RES\nName: Position, Length: 17660, dtype: object\n\n\n- (풀이2) – 수틀리면 리스트컴프리헨션\n\nf = lambda x: x.split(\"&gt;\")[-1] if not pd.isna(x) else pd.NA\n\n\ndf.assign(Position = [f(s) for s in df.Position]).Position\n\n0        SUB\n1        LCM\n2         LB\n3        RCM\n4        RCM\n        ... \n17655    RES\n17656    RES\n17657    RES\n17658    RES\n17659    RES\nName: Position, Length: 17660, dtype: object\n\n\n\nmapping하는 게 조금 더 자연스럽고 한번에 쓸 수 있다. ~(어차피 이미 람다로 함수 만들었잖아?)~"
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "",
    "text": "FIFA23 선수 데이터에서 결측치를 처리하고 여러 열을 가공하여 시각화해보자!"
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#라이브러리-import",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#라이브러리-import",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *"
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#학습할-코드",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#학습할-코드",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "2. 학습할 코드",
    "text": "2. 학습할 코드\n\nA. dropna()\n\n- 행에서 결측치가 하나라도 있으면 제거한다.\n\ndf = pd.DataFrame({\n    'A': [1,2,3,np.nan,5,6,7],\n    'B': [11,np.nan,33,np.nan,55,66,77], \n    'C': [111,222,333,np.nan,555,666,np.nan]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n1\n2.0\nNaN\n222.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n6\n7.0\n77.0\nNaN\n\n\n\n\n\n\n\n\n이러한 데이터가 있다고 할 때…\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n\n\n\n\n\n\n결측치가 하나라도 있는 행은 모두 드롭된다. (원본 데이터 손상 X)"
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#b.-_",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#b.-_",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### B. _",
    "text": "### B. _\n- 파이썬에서 가장 최근 콘솔에 띄워진 결과는 _로 불러올 수 있다.\n\na = [1,2,3]\na + [4] \n\n[1, 2, 3, 4]\n\n\n\n_\n\n[1, 2, 3, 4]\n\n\n\n_ + [5]\n\n[1, 2, 3, 4, 5]\n\n\n\n_.pop()  ## 마지막 요소를 리턴하고 그 요소는 삭제\n\n5\n\n\n\n_ + 1\n\n6\n\n\n\n리스트에서 숫자까지… _의 다사다난한 모험"
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#fifa23-시각화-문제",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#fifa23-시각화-문제",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "3. FIFA23 시각화 문제",
    "text": "3. FIFA23 시각화 문제\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n포지션별로 선수들의 능력치(ShotPower,SlidingTackle)와 급여(Wage)를 시각화하고 싶다. 아래의 세부지침에 맞추어 포지션별 ShotPower와 SlidingTackle의 산점도를 그려라.\n\ndf.Position\n\n0        &lt;span class=\"pos pos18\"&gt;CAM\n1        &lt;span class=\"pos pos11\"&gt;LDM\n2         &lt;span class=\"pos pos24\"&gt;RS\n3        &lt;span class=\"pos pos13\"&gt;RCM\n4          &lt;span class=\"pos pos7\"&gt;LB\n                    ...             \n16705    &lt;span class=\"pos pos29\"&gt;RES\n16706    &lt;span class=\"pos pos29\"&gt;RES\n16707    &lt;span class=\"pos pos29\"&gt;RES\n16708    &lt;span class=\"pos pos28\"&gt;SUB\n16709    &lt;span class=\"pos pos28\"&gt;SUB\nName: Position, Length: 16710, dtype: object\n\n\n세부지침\nA. Column의 이름에서 공백을 제거하라.\nB. 결측치가 50%이상인 컬럼을 찾고 이를 제거하라. 그 뒤에 .dropna()를 사용하여 결측치가 포함된 행을 제거하라.\nC. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\n\nD. df.Wage를 적절하게 변환하라.\nE. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.\n시각화 예시"
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#fifa23-시각화---풀이",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#fifa23-시각화---풀이",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "4. FIFA23 시각화 - 풀이",
    "text": "4. FIFA23 시각화 - 풀이\n\nA. Column의 이름에서 공백을 제거하라.\n\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Jersey Number', 'Joined', 'Loaned From', 'Contract Valid Until',\n       'Height', 'Weight', 'Crossing', 'Finishing', 'HeadingAccuracy',\n       'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy',\n       'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility',\n       'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength',\n       'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision',\n       'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle',\n       'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'Best Position', 'Best Overall Rating', 'Release Clause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.columns.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'JerseyNumber',\n       'Joined', 'LoanedFrom', 'ContractValidUntil', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'BestPosition', 'BestOverallRating', 'ReleaseClause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.set_axis(df.columns.str.replace(' ', ''), axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'JerseyNumber',\n       'Joined', 'LoanedFrom', 'ContractValidUntil', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'BestPosition', 'BestOverallRating', 'ReleaseClause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\n공백이 없어진 것을 확인할 수 있다.\n\n\n\nB. 결측치가 50%이상인 컬럼을 찾고 이를 제거하라. 그 뒤에 .dropna()를 사용하여 결측치가 포함된 행을 제거하라.\n\n\ndf_b = df.set_axis(df.columns.str.replace(' ', ''), axis = 1)\n\n\ndf_b.loc[:, df_b.isna().mean() &gt;= 0.5].columns\n\nIndex(['LoanedFrom', 'Marking'], dtype='object')\n\n\n\n위 두 개의 컬럼이 결측치가 50% 이상이다.\n\n\ndf_b.loc[:, df_b.isna().mean() &lt; 0.5].dropna()\n##df_b.loc[:, [s &lt; 0.5 for s in df_b.isna().mean()]].dropna()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n따라서 해당 조건에 반대를 슬라이싱하는 방식으로 해당 컬럼을 제거하였다."
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#c.-position_dict를-이용하여-df.position을-적절하게-변환하라.-변환된-값을-df.position에-저장하라.",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#c.-position_dict를-이용하여-df.position을-적절하게-변환하라.-변환된-값을-df.position에-저장하라.",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### C. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.",
    "text": "### C. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.\n\nposition_dict\n\n{'GOALKEEPER': {'GK'},\n 'DEFENDER': {'CB', 'LB', 'LCB', 'LWB', 'RB', 'RCB', 'RWB'},\n 'MIDFIELDER': {'CAM',\n  'CDM',\n  'CM',\n  'LAM',\n  'LCM',\n  'LDM',\n  'LM',\n  'RAM',\n  'RCM',\n  'RDM',\n  'RM'},\n 'FORWARD': {'CF', 'LF', 'LS', 'LW', 'RF', 'RS', 'RW', 'ST'},\n 'SUB': {'SUB'},\n 'RES': {'RES'}}\n\n\n\ndf_c = df_b.loc[:, df_b.isna().mean() &lt; 0.5].dropna()\ndf_c.Position\n\n0        &lt;span class=\"pos pos18\"&gt;CAM\n1        &lt;span class=\"pos pos11\"&gt;LDM\n2         &lt;span class=\"pos pos24\"&gt;RS\n3        &lt;span class=\"pos pos13\"&gt;RCM\n4          &lt;span class=\"pos pos7\"&gt;LB\n                    ...             \n16703    &lt;span class=\"pos pos29\"&gt;RES\n16704    &lt;span class=\"pos pos29\"&gt;RES\n16706    &lt;span class=\"pos pos29\"&gt;RES\n16707    &lt;span class=\"pos pos29\"&gt;RES\n16708    &lt;span class=\"pos pos28\"&gt;SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n뒤의 &gt;를 제외한 문자열을 추출해서 바꿔줘야 할 것 같다.\n\n\ndf_c.assign(Position = df_c.Position.str.split('&gt;').str[-1]).Position\n\n0        CAM\n1        LDM\n2         RS\n3        RCM\n4         LB\n        ... \n16703    RES\n16704    RES\n16706    RES\n16707    RES\n16708    SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n뒤의 문자열만 추출\n\n- 무지성으로 쥐어짜내본 아이디어\n\nx = df_c.Position.str.split('&gt;').str[-1][2]\nlst = [i != 0 for i in [key if x in value else 0 for key, value in position_dict.items()]];lst\n\n[False, False, False, True, False, False]\n\n\n\n[i != 0 for i in [key if x in value else 0 for key, value in position_dict.items()]].index(True)\n\n3\n\n\n\nlst = [[key if i in value else np.nan if i == np.nan else 1 for key, value in position_dict.items()] for i in df_c.Position.str.split('&gt;').str[-1]]\n\n\nvalue와 같은 값이면 key, 아니면 1, 결측치면 np.nan을 넣어줘봤음.\n\n\ndef cutting_1(lst):\n    for i in lst:\n        if i != 1:\n            return i\n\nPosition_s = pd.Series(lst).apply(cutting_1); Position_s\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n14393           RES\n14394           RES\n14395           RES\n14396           RES\n14397           SUB\nLength: 14398, dtype: object\n\n\n\n잘 된듯~(근데 앞에서 .dropna()를 안해서 쓸데없는 코드까지 작성해버렸다.)~\n\n\nlst_2 = list(map(lambda x : [key for key, value in position_dict.items() if x in value], df_c.Position.str.split('&gt;').str[-1]))\n## [i[0] for i in lst_2] : 왜인지 안됨\n## [i.pop() for i in lst_2] : 이건 뭐 객체 저장인지 뭔지 문제라는데, 다른 변수에 copy()해서 넣어봐도 안됨\n\n\ndf_c.reset_index(drop = True).assign(Position = Position_s).Position\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n14393           RES\n14394           RES\n14395           RES\n14396           RES\n14397           SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n잘못된 코드\n\n\ndf_c.assign(Position = Position_s).Position  ## 문제가 있는 코드, reset_index(drop = True)를 안해줘서 인덱스가 꼬임\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n16703           NaN\n16704           NaN\n16706           NaN\n16707           NaN\n16708           NaN\nName: Position, Length: 14398, dtype: object\n\n\n\n뭘 잘못했는 지 알겠지? index가 달라서 값이 엮이지가 않잖아…\n\n- 매우 간단한 교수님의 해법\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda _df: _df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n아마도 모든 문제의 원흉은 dropna()를 하지 않은 너에게 있었다. (결측치가 있으면 작동이 힘든가봄)\n\n\nD. df.Wage를 적절하게 변환하라.\n\n\ndf.Wage\n\n0        €250K\n1        €140K\n2        €135K\n3        €350K\n4         €45K\n         ...  \n16705      €1K\n16706     €550\n16707     €700\n16708     €500\n16709       €0\nName: Wage, Length: 16710, dtype: object\n\n\n\n시각화 해주려면 숫자형 자료여야 하는데, 범주형으로 들어가있다.\n\n- 앞의 유로를 없애고, K는 1000을 곱해주자.\n\ndf_d = df_c.reset_index(drop = True).assign(Position = Position_s)\n\n\ndf_d.Wage.str.replace('€','').str.replace('K','000')\n##[int(i) if i[-1] != 'K' else int(i.replace('K',''))*1000 for i in df_d.Wage.str.replace('€','')]와 동일\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n14393       650\n14394       950\n14395       550\n14396       700\n14397       500\nName: Wage, Length: 14398, dtype: object\n\n\n\ndf_d.assign(Wage = df_d.Wage.str.replace('€','').str.replace('K','000').astype(int)).Wage\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n14393       650\n14394       950\n14395       550\n14396       700\n14397       500\nName: Wage, Length: 14398, dtype: int32\n\n\n\n잘 된 것을 볼 수 있다."
  },
  {
    "objectID": "posts/Data Visualization/Review/8. 실습_FIFA23.html#e.-positiondefender-or-positionforward에-해당하는-관측치를-고른-뒤-x축에-shotpower-y축에-slidingtackle을-시각화하라.-이때-position은-color로-구분하고-wage는-size와-alpha로-구분하라.",
    "href": "posts/Data Visualization/Review/8. 실습_FIFA23.html#e.-positiondefender-or-positionforward에-해당하는-관측치를-고른-뒤-x축에-shotpower-y축에-slidingtackle을-시각화하라.-이때-position은-color로-구분하고-wage는-size와-alpha로-구분하라.",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### E. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.",
    "text": "### E. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.\n\ndf_e = df_d.assign(Wage = df_d.Wage.str.replace('€','').str.replace('K','000').astype(int))\n\n\ndf_e.loc[(df_e.Position == \"DEFENDER\") | (df_e.Position == \"FORWARD\")]\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n11\n155862\nSergio Ramos\n35\nhttps://cdn.sofifa.com/players/155/862/22_60.png\nSpain\nhttps://cdn.sofifa.com/flags/es.png\n88\n88\nParis Saint-Germain\nhttps://cdn.sofifa.com/teams/73/30.png\n...\n91.0\n11.0\n8.0\n9.0\n7.0\n11.0\nCB\n88.0\n€44.4M\n84.0\n\n\n12\n197445\nD. Alaba\n29\nhttps://cdn.sofifa.com/players/197/445/22_60.png\nAustria\nhttps://cdn.sofifa.com/flags/at.png\n84\n84\nReal Madrid CF\nhttps://cdn.sofifa.com/teams/243/30.png\n...\n82.0\n5.0\n7.0\n14.0\n15.0\n9.0\nCB\n84.0\n€72.8M\n86.0\n\n\n20\n210514\nJoão Cancelo\n27\nhttps://cdn.sofifa.com/players/210/514/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n86\n87\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n80.0\n6.0\n9.0\n15.0\n14.0\n14.0\nRB\n86.0\n€137.6M\n79.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13109\n203430\nG. Ray\n27\nhttps://cdn.sofifa.com/players/203/430/22_60.png\nWales\nhttps://cdn.sofifa.com/flags/gb-wls.png\n59\n60\nExeter City\nhttps://cdn.sofifa.com/teams/143/30.png\n...\n59.0\n13.0\n10.0\n12.0\n9.0\n8.0\nCB\n60.0\n€420K\n58.0\n\n\n13124\n187154\nN. Canavan\n30\nhttps://cdn.sofifa.com/players/187/154/22_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.com/flags/ie.png\n63\n63\nBradford City\nhttps://cdn.sofifa.com/teams/1804/30.png\n...\n62.0\n6.0\n10.0\n11.0\n14.0\n6.0\nCB\n63.0\n€700K\n63.0\n\n\n13183\n263968\nK. Sow\n18\nhttps://cdn.sofifa.com/players/263/968/22_60.png\nSwitzerland\nhttps://cdn.sofifa.com/flags/ch.png\n54\n76\nFC Lausanne-Sport\nhttps://cdn.sofifa.com/teams/1862/30.png\n...\n55.0\n6.0\n9.0\n13.0\n7.0\n14.0\nCB\n56.0\n€796K\n54.0\n\n\n13238\n263022\nM. Rosenfelder\n18\nhttps://cdn.sofifa.com/players/263/022/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n57\n71\nSC Freiburg II\nhttps://cdn.sofifa.com/teams/110691/30.png\n...\n60.0\n10.0\n10.0\n8.0\n10.0\n11.0\nCB\n59.0\n€726K\n58.0\n\n\n13405\n261062\nLee Han Beom\n19\nhttps://cdn.sofifa.com/players/261/062/22_60.png\nKorea Republic\nhttps://cdn.sofifa.com/flags/kr.png\n53\n72\nFC Seoul\nhttps://cdn.sofifa.com/teams/982/30.png\n...\n53.0\n7.0\n14.0\n5.0\n6.0\n15.0\nCB\n55.0\n€431K\n52.0\n\n\n\n\n3298 rows × 63 columns\n\n\n\n\ntidydata = df_e.loc[(df_e.Position == \"DEFENDER\") | (df_e.Position == \"FORWARD\")]\n\n\nfig = ggplot(tidydata)\npoint = geom_point(aes(x = 'ShotPower', y = 'SlidingTackle', color = 'Position', size = 'Wage', alpha = 'Wage'), position = 'jitter')\n\nfig + point\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n해치웠나…?\n\n결론\n\n- 데이터의 결측치를 반드시 먼저 처리하고 하자!(dropna()의 필요성)\n- pop()을 사용하기 전에는 결측치를 반드시 모두 없애자!\n- 데이터를 가공하여 순서가 바뀐 경우 왠만해선 인덱스를 초기화해주자!(reset_index()의 필요성)"
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html",
    "href": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "",
    "text": "데이터프레임 df의 열이름에 actor라는 단어가 포함된 column만을 선택하는 코드를 작성하라"
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html#사전작업",
    "href": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html#사전작업",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 설치\n\n\nimport pandas as pd\nimport numpy as np\n\n\n데이터 불러오기 및 확인\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\n...\nnum_user_for_reviews\nlanguage\ncountry\ncontent_rating\nbudget\ntitle_year\nactor_2_facebook_likes\nimdb_score\naspect_ratio\nmovie_facebook_likes\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n...\n3054.0\nEnglish\nUSA\nPG-13\n237000000.0\n2009.0\n936.0\n7.9\n1.78\n33000\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n...\n1238.0\nEnglish\nUSA\nPG-13\n300000000.0\n2007.0\n5000.0\n7.1\n2.35\n0\n\n\n2\nColor\nSam Mendes\n602.0\n148.0\n0.0\n161.0\nRory Kinnear\n11000.0\n200074175.0\nAction|Adventure|Thriller\n...\n994.0\nEnglish\nUK\nPG-13\n245000000.0\n2015.0\n393.0\n6.8\n2.35\n85000\n\n\n3\nColor\nChristopher Nolan\n813.0\n164.0\n22000.0\n23000.0\nChristian Bale\n27000.0\n448130642.0\nAction|Thriller\n...\n2701.0\nEnglish\nUSA\nPG-13\n250000000.0\n2012.0\n23000.0\n8.5\n2.35\n164000\n\n\n4\nNaN\nDoug Walker\nNaN\nNaN\n131.0\nNaN\nRob Walker\n131.0\nNaN\nDocumentary\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n7.1\nNaN\n0\n\n\n\n\n5 rows × 28 columns\n\n\n\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n열 이름에서 단어의 구분이 모두 '_'로 되어있으므로, 열이름에 split()함수를 적용시킬 수 있을 것 같다."
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html#풀이",
    "href": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html#풀이",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "2. 풀이",
    "text": "2. 풀이\n\nfor 문을 이용하여 풀이해보자.\n\n\n['actor' in i.split('_') for i in df.columns]\n\n[False,\n False,\n False,\n False,\n False,\n True,\n True,\n True,\n False,\n False,\n True,\n False,\n False,\n False,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n False,\n False,\n False]"
  },
  {
    "objectID": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html#결과",
    "href": "posts/Data Visualization/Solution Assemble/특정 단어를 포함하는 열 선택.html#결과",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "3. 결과",
    "text": "3. 결과\n\ndf.loc[:, ['actor' in i.split('_') for i in df.columns]]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n완료"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "",
    "text": "sklearn의 linear_mode.LinearRegression()을 사용하여 선형회귀분석을 해보자!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#라이브러리-imports",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#data",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#data",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "2. Data",
    "text": "2. Data\n\n전주시의 기온 자료\n\n\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()   ## 자료를 크기 순서대로 정렬, sort_values()와 비슷하달까...\n\n\ntemp\n\narray([-4.1, -3.7, -3. , -1.3, -0.5, -0.3,  0.3,  0.4,  0.4,  0.7,  0.7,\n        0.9,  0.9,  1. ,  1.2,  1.4,  1.4,  1.5,  1.5,  2. ,  2. ,  2. ,\n        2.3,  2.5,  2.5,  2.5,  2.6,  2.6,  2.9,  3.2,  3.5,  3.5,  3.6,\n        3.7,  3.8,  4.2,  4.4,  4.5,  4.5,  4.6,  4.9,  4.9,  4.9,  5. ,\n        5. ,  5.1,  5.6,  5.9,  5.9,  6. ,  6. ,  6.1,  6.1,  6.3,  6.3,\n        6.4,  6.4,  6.5,  6.7,  6.8,  6.8,  7. ,  7. ,  7.1,  7.2,  7.4,\n        7.7,  8. ,  8.1,  8.1,  8.3,  8.4,  8.4,  8.4,  8.5,  8.8,  8.9,\n        9.1,  9.2,  9.3,  9.4,  9.4,  9.5,  9.6,  9.6,  9.7,  9.8,  9.9,\n       10.2, 10.3, 10.6, 10.6, 10.8, 11.2, 12.1, 12.4, 13.4, 14.7, 15. ,\n       15.2])\n\n\n- 아래와 같은 모형을 가정하자. \\[\\textup{아이스크림 판매량}= 20 ＋ \\textup{온도} × 2.5 × \\textup{오차(운)}\\]\n\n더미 모형 생성\n\n\nnp.random.seed(43052)\neps = np.random.randn(100)*3  ## 오차\nicecream_sales = 20 + temp * 2.5 + eps\n\n\nplt.plot(temp, icecream_sales, 'o')\nplt.show()\n\n\n\n\n\n상기 결과를 관측했다고 생각합시다.\n\n\ndf = pd.DataFrame({'temp' : temp, 'sales' : icecream_sales})\ndf\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n0\n-4.1\n10.900261\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n...\n...\n...\n\n\n95\n12.4\n54.926065\n\n\n96\n13.4\n54.716129\n\n\n97\n14.7\n56.194791\n\n\n98\n15.0\n60.666163\n\n\n99\n15.2\n61.561043\n\n\n\n\n100 rows × 2 columns"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#게임세팅",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#게임세팅",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "3. 게임세팅",
    "text": "3. 게임세팅\n- 편의상 아래와 같은 기호를 도입하자.\n\n(df.temp[0], df.temp[1], … , df.temp[99]) = \\((x_1,x_2,\\dots,x_{100})=(-4.1,-3.7,\\dots,15.2)\\)\n(df.sales[0], df.sales[1], … , df.sales[99]) = \\((y_1,y_2,\\dots,y_{100})=(10.90,14.00, \\dots,61.56)\\)\n\n\n이 자료 \\(\\big\\{(x_i,y_i)\\big\\}_{i=1}^{100}\\)를 바탕으로 어떠한 패턴을 발견하여 새로운 \\(x\\)에 대한 예측값을 알고 싶다 : \\(\\hat{y}\\)\n\nA. 질문\n- 기온이 \\(x = -2.0\\)일 때, 아이스크림을 얼마정도 판다고 보는 게 타당할까?\nB. 답 1\n- \\(x = -2.0\\) 근처의 데이터를 살펴보자.\n\ndf[(-4.0 &lt; df.temp) & (0.0 &gt; df.temp)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n5\n-0.3\n20.317853\n\n\n\n\n\n\n\n\n\\(-1.3\\)이 제일 가까운데, 대충 \\(17.67\\) 언저리 아닐까…?\n\n\nA. 산점도와 추세선\n\n- 자료를 바탕으로 그림을 그려보자\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot([-2.0],[17.67],'x')     # 이미 들어가있는 플롯에 점을 하나 찍는다. 마커는 X\n\nplt.show()\n\n\n\n\n\n예상한 것(17.67)보다 못팔 것 같은데…?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-아이디어",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-아이디어",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 아이디어",
    "text": "### B. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자!~(사실 모형을 우리가 만들었으니 이미 추세선을 알고 있긴 함)~\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.temp, 20+df.temp*2.5, '--')  ## 위에서 직접 설정했던 자료의 관계, 절편이 20이고 기울기가 2.5\n\nplt.show()\n\n\n\n\n- 사실 \\(y = 20 + 2.5x\\)라는 추세선을 이미 알고 있었음.\n- 그래서 \\(x = -2\\)라면 \\(y = 20 - 2.5 × 2 = 15\\)라고 보는 게 합리적임(오차를 고려 안하면)\n\n허나, 실제 상황에서 우리는 \\(20, 2.5\\)라는 숫자를 모른다.\n\n- 게임셋팅 * 원래 게임 : 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임 * 변형된 게임 : \\(20, 2.5\\)라는 숫자를 잘 찾는 게임. 즉, 데이처를 보고 최대한 \\(y_i \\approx ax_i+b\\)가 되도록 \\(a, b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#분석",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "4. 분석",
    "text": "4. 분석\n\n그렇다면 늘 했던 것처럼 네 단계로 분석을 해보자.\n\n\nA. 데이터\n\n\n# step 1 -- data\ntrain = pd.DataFrame({'temp' : temp, 'sales' : icecream_sales})\n\nX = train[['temp']]\ny = train['sales']\n\n\n데이터를 학습해서 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄 수 있는 아이(predictor)를 만들자.~(근데 이정도면 학생이 아니라 노예…)~"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-predictor",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-predictor",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. predictor",
    "text": "### B. predictor\n\n# step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n\nsklearn의 linear_model.LinearRegression()을 사용했다. 이러면 가장 기본적인 선형회귀를 진행한다.(LSE를 쓰는 그거 있잖아…)\n\n\nC. 학습\n\n\n# step 3\npredictr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n학생이 train을 완료했다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#d.-예측predict",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#d.-예측predict",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### D. 예측(predict)",
    "text": "### D. 예측(predict)\n- 학생(predictr) : 데이터를 살펴보니 True는 이럴 것 같아요.\n\ny_hat = predictr.predict(X)  ## X값에 해당하는 y_hat값을 예측하여 산출.\n\n\nplt.plot(X, y, 'o', alpha = 0.5)\nplt.plot(X, y_hat, 'o--', alpha = 0.5)\n\nplt.show()\n\n\n\n\n- 그럼 기울기와 절편은 어디에 저장된 걸까?\n- predictr : 여깄음.\n\n(predictr.coef_, predictr.intercept_)\n\n(array([2.51561216]), 19.66713126947925)\n\n\n- 새로운 데이터 \\(x = -2\\)에 대한 예측\n\nfloat(predictr.coef_)*(-2) + float(predictr.intercept_)\n\n14.63590694951262\n\n\n\n해당 결과값을 그래프에 나타내면…\n\n\nX_input = pd.DataFrame({'temp' : [-2.0]})\n\n\nplt.plot(X, y, 'o', alpha = 0.5)\nplt.plot(X, y_hat, '--', alpha = 0.5)\nplt.plot(X_input, predictr.predict(X_input), 'xr')  ## 원래는 리스트나 어레이로 넣어주는 게 정배긴 함\n\nplt.show()\n\n\n\n\n\n예측값이 직선상에 위치함을 알 수 있다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#두-타입의-아이스크림초코-바닐라에-대한-회귀분석",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#두-타입의-아이스크림초코-바닐라에-대한-회귀분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "5. 두 타입의 아이스크림(초코 / 바닐라)에 대한 회귀분석",
    "text": "5. 두 타입의 아이스크림(초코 / 바닐라)에 대한 회귀분석\n\n이전의 기온 자료를 바꿔 아래와 같은 모형을 가정해보자.\n\n\nA. Data\n\n\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()   ## 자료를 크기 순서대로 정렬\ntemp  ## 전주시의 기온 100개 자료\n\narray([-4.1, -3.7, -3. , -1.3, -0.5, -0.3,  0.3,  0.4,  0.4,  0.7,  0.7,\n        0.9,  0.9,  1. ,  1.2,  1.4,  1.4,  1.5,  1.5,  2. ,  2. ,  2. ,\n        2.3,  2.5,  2.5,  2.5,  2.6,  2.6,  2.9,  3.2,  3.5,  3.5,  3.6,\n        3.7,  3.8,  4.2,  4.4,  4.5,  4.5,  4.6,  4.9,  4.9,  4.9,  5. ,\n        5. ,  5.1,  5.6,  5.9,  5.9,  6. ,  6. ,  6.1,  6.1,  6.3,  6.3,\n        6.4,  6.4,  6.5,  6.7,  6.8,  6.8,  7. ,  7. ,  7.1,  7.2,  7.4,\n        7.7,  8. ,  8.1,  8.1,  8.3,  8.4,  8.4,  8.4,  8.5,  8.8,  8.9,\n        9.1,  9.2,  9.3,  9.4,  9.4,  9.5,  9.6,  9.6,  9.7,  9.8,  9.9,\n       10.2, 10.3, 10.6, 10.6, 10.8, 11.2, 12.1, 12.4, 13.4, 14.7, 15. ,\n       15.2])\n\n\n- 아래와 같은 모형을 가정하자.\n\\[\\textup{초코 아이스크림 판매량} = 20 + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\\[\\textup{바닐라 아이스크림 판매량} = 40 + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\nnp.random.seed(43052)\nchoco = 20 + temp*2.5 + np.random.randn(100)*3  ## random normal distribution\nvanilla = 40 + temp*2.5 + np.random.randn(100)*3\n\n\nplt.plot(temp, choco, 'o', label = 'choco')\nplt.plot(temp, vanilla, 'o', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n\n우리는 위와 같은 정보를 관측했다고 가정하자.\n\n\ndf1 = pd.DataFrame({'temp' : temp, 'type' : ['choco' for i in range(100)], 'sales' : choco})\ndf2 = pd.DataFrame({'temp' : temp, 'type' : ['vanilla' for i in range(100)], 'sales' : vanilla})\n\ndf = pd.concat([df1, df2], axis = 0).reset_index(drop = True)\ndf\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n-4.1\nchoco\n10.900261\n\n\n1\n-3.7\nchoco\n14.002524\n\n\n2\n-3.0\nchoco\n15.928335\n\n\n3\n-1.3\nchoco\n17.673681\n\n\n4\n-0.5\nchoco\n19.463362\n\n\n...\n...\n...\n...\n\n\n195\n12.4\nvanilla\n68.708075\n\n\n196\n13.4\nvanilla\n75.800464\n\n\n197\n14.7\nvanilla\n79.846568\n\n\n198\n15.0\nvanilla\n78.713140\n\n\n199\n15.2\nvanilla\n77.595252\n\n\n\n\n200 rows × 3 columns"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-분석",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 분석",
    "text": "### B. 분석\n- 언제처럼 늘 그랬던 것처럼…\n\n# step 1\n## X = pd.get_dummies(df).drop(['sales'], axis = 1) ## 이게 제일 범용적이긴 함\nX = df.loc[:, ['temp', 'type']].assign(type = (df.type == 'choco')) ## 직관적으로 쓴 코드, 범주형은 인식을 못한다.\ny = df['sales']\n\n# step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\ndf = df.assign(sales_hat = predictr.predict(X));df\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\nsales_hat\n\n\n\n\n0\n-4.1\nchoco\n10.900261\n9.286731\n\n\n1\n-3.7\nchoco\n14.002524\n10.295689\n\n\n2\n-3.0\nchoco\n15.928335\n12.061366\n\n\n3\n-1.3\nchoco\n17.673681\n16.349439\n\n\n4\n-0.5\nchoco\n19.463362\n18.367355\n\n\n...\n...\n...\n...\n...\n\n\n195\n12.4\nvanilla\n68.708075\n71.446479\n\n\n196\n13.4\nvanilla\n75.800464\n73.968875\n\n\n197\n14.7\nvanilla\n79.846568\n77.247989\n\n\n198\n15.0\nvanilla\n78.713140\n78.004708\n\n\n199\n15.2\nvanilla\n77.595252\n78.509187\n\n\n\n\n200 rows × 4 columns\n\n\n\n- 가장 중요한 시각화까지…\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.loc[df.type == 'choco'].temp, df.loc[df.type == 'choco'].sales_hat, '--', color = 'brown', label = 'choco')\nplt.plot(df.loc[df.type == 'vanilla'].temp, df.loc[df.type == 'vanilla'].sales_hat, '--', color = 'yellow', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n\n별다른 뜻 없이 (초코, 바닐라)에 (1, 0)을 넣었는데, 어떻게 뭐가 나오긴 했다.\n\n\n어케했음???\n\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([  2.52239574, -20.54021854]), 40.16877158069265)\n\n\n\ncoef_(기울기)가 2개지요.\n\n온도와 범주형 자료인 아이스크림 종류에 따라 기울기가 다르다. 온도 1도가 변할때마다 판매량은 2.52239574가 변하고, 아이스크림 종류가 1 변할때마다(0에서 1이니까 바닐라에서 초코로 바뀜) -20.54를 곱한 수를 더하여 수식을 설명하였다.\n예측\n- 온도가 \\(-2\\)이고, type이 vanilla(0)라면 예측값은?\n\nXnew = pd.DataFrame({'temp' : [-2], 'type' : [0]})\n\npredictr.predict(Xnew)\n\narray([35.1239801])\n\n\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.loc[df.type == 'choco'].temp, df.loc[df.type == 'choco'].sales_hat, '--', color = 'brown', label = 'choco')\nplt.plot(df.loc[df.type == 'vanilla'].temp, df.loc[df.type == 'vanilla'].sales_hat, '--', color = 'yellow', label = 'vanilla')\nplt.plot(Xnew.temp, predictr.predict(Xnew), 'or', label = 'prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nC. 데이터 전처리\n\n- 아까 pd.get_dummies()를 잠시 본 것 같은데, 이걸 어떻게, 왜 써야 하는 지 알아보자.\n\nX = df[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = pd.get_dummies(X);X\n\n\n\n\n\n\n\n\ntemp\ntype_choco\ntype_vanilla\n\n\n\n\n0\n-4.1\nTrue\nFalse\n\n\n1\n-3.7\nTrue\nFalse\n\n\n2\n-3.0\nTrue\nFalse\n\n\n3\n-1.3\nTrue\nFalse\n\n\n4\n-0.5\nTrue\nFalse\n\n\n...\n...\n...\n...\n\n\n195\n12.4\nFalse\nTrue\n\n\n196\n13.4\nFalse\nTrue\n\n\n197\n14.7\nFalse\nTrue\n\n\n198\n15.0\nFalse\nTrue\n\n\n199\n15.2\nFalse\nTrue\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n원-핫 인코딩 : 표현하고 싶은 단어에는 1을, 그것이 아닌 것에는 0을 부여\n\n- LinearRegression 모델의 경우 범주형 자료를 자동으로 인식하지 못한다. 따라서 구분할 범주형 변수가 많다면, pd.get_dummies()를 통해 범주를 나눠주어야 한다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#d.-모형의-평가",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#d.-모형의-평가",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### D. 모형의 평가",
    "text": "### D. 모형의 평가\n- 단순선형회귀분석의 경우 모형을 \\(R^2\\)(결정계수)로 평가한다.\n\n다만 이것이 높다고 해서 무조건적으로 좋은 건 아니고, 명확한 기준도 없다. 모형 간 상대적인 좋음을 비교하는 것 뿐이다.\n\n- LogisticRegression에서는 적중률로 딱 떨어지게 점수를 내줄 수 있겠지만, 이건 그렇게 해버리면 0점이 나와버리겠지…"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#설명변수가-많을-때",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#설명변수가-많을-때",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "6. 설명변수가 많을 때",
    "text": "6. 설명변수가 많을 때\n- kaggle에서 “Medical Cose Personal Datasets”을 다운로드\n\nhttps://www.kaggle.com/datasets/mirichoi0218/insurance\n\n\ndf = pd.read_csv(\".\\data\\insurance.csv\")\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\nA. 분석\n\n\n열 이름을 먼저 알아보자.\n\n\nset(df.columns)\n\n{'age', 'bmi', 'charges', 'children', 'region', 'sex', 'smoker'}\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\n- 대충 여러가지 범주형ㆍ연속형 설명변수들과 보험료의 관계를 요약하고 싶다고 하자.\n\n먼저 범주형 자료(sex, smoker, region)들을 원-핫 인코딩 해주자.\n\n\nX = pd.get_dummies(df.drop(['charges'], axis = 1))\ny = df.charges\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n- 그럼 뭐 늘 하던대로…\n\n# 2\npredictr = sklearn.linear_model.LinearRegression()\n\n# 3\npredictr.fit(X, y)\n\n# 4\ndf = df.assign(y_hat = predictr.predict(X))\n\n\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\ny_hat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns\n\n\n\n- charge와 y_hat이 잘 안맞는 것 같은데…?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-평가",
    "href": "posts/Machine Learning in Practice/practice/1. 회귀분석_아이스크림.html#b.-평가",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 평가",
    "text": "### B. 평가\n\npredictr.score(X, y)\n\n0.7509130345985205\n\n\n\n\\(R^2 = \\frac{SSR}{SST} = 0.7509130345985205\\)\n0.7 이상이면 망한 모형까진 아니지만…\n\n계수 해석\n- 상수항\n\npredictr.intercept_\n\n-666.9377199366372\n\n\n\n기본적인 보험료(다른 모든 것이 0일 때)는 -666이다.~(딱봐도 이상하죠? 그래서 별로 의미는 없다.)~\n\n- 계수\n\npd.DataFrame({'columns' : X.columns, 'coef' : predictr.coef_})\n\n\n\n\n\n\n\n\ncolumns\ncoef\n\n\n\n\n0\nage\n256.856353\n\n\n1\nbmi\n339.193454\n\n\n2\nchildren\n475.500545\n\n\n3\nsex_female\n65.657180\n\n\n4\nsex_male\n-65.657180\n\n\n5\nsmoker_no\n-11924.267271\n\n\n6\nsmoker_yes\n11924.267271\n\n\n7\nregion_northeast\n587.009235\n\n\n8\nregion_northwest\n234.045336\n\n\n9\nregion_southeast\n-448.012814\n\n\n10\nregion_southwest\n-373.041756\n\n\n\n\n\n\n\n\n연속형 : 나이, bmi, 자녀의 수가 많을수록 보험료는 올라갔다.\n범주형 : 여성, 흡연자의 경우 보험료가 더 비쌌다.\n지역은 잘 모르겠으나, 나머지는 꽤 그럴듯해 보인다.(지역에 대한 정보는 알기 어려움…)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/3. 결측치.html",
    "href": "posts/Machine Learning in Practice/practice/3. 결측치.html",
    "title": "결측치의 처리",
    "section": "",
    "text": "결측치를 시각화해보고, 계산해서 대치(impute)해보기도 하자!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/3. 결측치.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/3. 결측치.html#라이브러리-imports",
    "title": "결측치의 처리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#!pip install missingno # missingno 라이브러리가 설치되어있지 않을 경우\n\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport sklearn.impute"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/3. 결측치.html#missingno의-활용",
    "href": "posts/Machine Learning in Practice/practice/3. 결측치.html#missingno의-활용",
    "title": "결측치의 처리",
    "section": "2. missingno의 활용",
    "text": "2. missingno의 활용\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/msno.csv\")\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n0.383420\n1.385096\nNaN\n-0.545132\n-0.732395\n\n\n1\n1.084175\n0.080613\n-0.770527\n-0.272143\n-0.749881\n\n\n2\n1.142778\n1.258419\nNaN\n-0.072007\n-0.440757\n\n\n3\n0.307894\n0.521400\n0.446974\n0.329530\n-1.457388\n\n\n4\n0.237787\n0.132401\n-0.516630\n0.177995\n0.416182\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.041092\n-1.308165\n1.085820\n1.136210\nNaN\n\n\n996\n-1.286358\n1.547987\nNaN\n-0.174334\n-0.579486\n\n\n997\n0.710257\n1.764058\nNaN\n-0.353928\nNaN\n\n\n998\n-1.908729\n-0.804691\nNaN\nNaN\n-0.066739\n\n\n999\n0.650026\n2.206549\nNaN\n-0.919945\nNaN\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n결측치가 딱봐도 엄청 많아보인다. missingno는 그것을 시각화해준다.\n\n\nmsno.matrix(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n우측 노이즈와 같은 그래프에서 0에 있는 것은 해당 행에 데이터가 하나도 없다는 뜻이고, 5에 있는 것은 다섯개의 데이터가 해당 행에 존재한다는 것이다. 데이터셋이 다섯개니까 그 합이 그래프로 표기된다.\n\n\nmsno.heatmap(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nmsno.dendrogram(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n구조가 비슷한 자료들을 엮어놓는다.\n\n그럼 시각화를 했으니까, 이제 결측치를 처리해야겠지?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/3. 결측치.html#숫자형-자료의-impute결측치를-대체하는-것",
    "href": "posts/Machine Learning in Practice/practice/3. 결측치.html#숫자형-자료의-impute결측치를-대체하는-것",
    "title": "결측치의 처리",
    "section": "3. 숫자형 자료의 impute(결측치를 대체하는 것)",
    "text": "3. 숫자형 자료의 impute(결측치를 대체하는 것)\n- 주어진 자료\n\nA = [2.1, 1.9, 2.2, np.nan, 1.9]\nB = [0, 0, np.nan, 0, 0]\n\n\ndf = pd.DataFrame({'A' : A, 'B' : B})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\n결측치를 무엇으로 채워주면 좋을까?\n\n\n일단 평균으로 해보면 얼추 맞을 것 같다.\n\n\ndf2 = df\ndf2.loc[3, 'A'] = df2.A.mean()  ## mean과 같은 메소드는 결측치를 반영하지 않는다.\ndf2.loc[2, 'B'] = df2.B.mean()\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 근데 이게 엄청 많으면 언제 다 일일히 하고 있어? &gt; 자동으로 하려면?\n(방법1) | 평균으로 impute\n\nimputr = sklearn.impute.SimpleImputer()  ## SimpleImputer(strategy = 'mean')\nimputr\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\npredictr.fit하는 것처럼 결측치가 있는 열에 적합해야 한다.\n\n\nimputr.fit(df)\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\npredictr.predict하는 것처럼 인풋시켜야 한다.\n\n\nimputr.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n\n위에서와 똑같은 결과를 산출했다.\n\n해당 과정은 imputr.fit_transform(df)로 한번에 시행할 수 있다.\n- 만약 평균이 아닌 다른 방식으로 결측치를 대체하고 싶다면…\n(방법 2) | median으로 impute\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'median')\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법 3) | 최빈값으로 대체\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법 4) | 정해진 상수값으로 대체\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 999)\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/3. 결측치.html#범주형-자료의-impute",
    "href": "posts/Machine Learning in Practice/practice/3. 결측치.html#범주형-자료의-impute",
    "title": "결측치의 처리",
    "section": "4. 범주형 자료의 impute",
    "text": "4. 범주형 자료의 impute\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n(방법 1) | 최빈값을 이용\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\nimputr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법 2) | 상수(지정값)로 대체함\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'G')\nA_ = pd.Series(imputr.fit_transform(df[['A']]).reshape(-1))\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'economy')\nB_ = pd.Series(imputr.fit_transform(df[['B']]).reshape(-1))\n\n\npd.concat([A_, B_], axis = 1).set_axis(['A','B'], axis = 1)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\neconomy\n\n\n3\nY\nstat\n\n\n4\nG\nbio\n\n\n\n\n\n\n\n\n## 또는\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'G')\nA_ = imputr.fit_transform(df[['A']])\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'economy')\nB_ = imputr.fit_transform(df[['B']])\n\npd.DataFrame(np.concatenate([A_,B_], axis = 1), columns = ['A','B'])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\neconomy\n\n\n3\nY\nstat\n\n\n4\nG\nbio\n\n\n\n\n\n\n\n\n일반적으로 연속형ㆍ숫자형 자료에는 평균, 범주형 자료에는 최빈값으로 대체한다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "",
    "text": "sklearn.preprocessing을 이용하여 자료의 범위를 전처리해보자."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#라이브러리-import",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#라이브러리-import",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.preprocessing"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#minmaxscaler",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#minmaxscaler",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "2. MinMaxScaler",
    "text": "2. MinMaxScaler\n\nA. 모티브\n\n- 예제자료 : 학점, 토익 등이 취업에 미치는 정도\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과… (sklearn.linear_model.Linear_Regression())\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n그래서… * 토익이 중요해? 아니면 학점이 중요해? * 무엇이 얼만큼 중요해?\n- 모티브 : 토익과 gpa 모두 0~1 사이의 척도로 바꾸면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#b.-사용방법",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#b.-사용방법",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "### B. 사용방법",
    "text": "### B. 사용방법\n\nclass를 이용, object를 생성하는 방법(이전과 유사한 방법)\n\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit(df)\n\nscalr.transform(df)  ## 전처리의 경우에는 transform을 사용한다. .impute.SimpleImputer()에서도 그랬잖아?\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n역시 한번에 할 수도 있다.\n\n\nscalr.fit_transform(df)  ## 당연히 원래 자료를 훼손하진 않는다.\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\nsklearn.preprocessing.minmax_scale(df)  ## 한 번에 할 수도 있다.\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n위처럼 할 수도 있는데, 이 경우는 scalr를 test셋에 적용시킬 수 없기 때문에 사용하지 않는다.\n\n\nC. 옳고 그른 방법론\n\n# 1 비효율적인 전환\n- 주어진 자료가 아래와 같이 train/test로 나뉘어있다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 5.0]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 3.5]).reshape(-1,1)\n\nX, XX\n\n(array([[1.],\n        [2.],\n        [3.],\n        [4.],\n        [5.]]),\n array([[1.5],\n        [2.5],\n        [3.5]]))\n\n\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit_transform(X), scalr.fit_transform(XX)\n\n(array([[0.  ],\n        [0.25],\n        [0.5 ],\n        [0.75],\n        [1.  ]]),\n array([[0. ],\n        [0.5],\n        [1. ]]))\n\n\n\n같은 값임에도 다르게 스케일을 변환시키는 것을 볼 수 있다.(X에선 5가 1인데, XX에선 3.5가 1이 됨.\n\n# 2 권장하는 스케일링 방법\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit(X)\n\nscalr.transform(X), scalr.transform(XX)\n\n(array([[0.  ],\n        [0.25],\n        [0.5 ],\n        [0.75],\n        [1.  ]]),\n array([[0.125],\n        [0.375],\n        [0.625]]))\n\n\n\n더 합리적이다.\n\n# 3 변환값의 범위\n- 변환한 값이 무조건 0과 1 사이가 되는 것은 아니다.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n## XX의 5.0은 X에서의 최대값인 4.0을 초과한다.\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X)\n\nsclr.transform(X), sclr.transform(XX)\n\n(array([[0.        ],\n        [0.33333333],\n        [0.66666667],\n        [1.        ],\n        [0.83333333]]),\n array([[0.16666667],\n        [0.5       ],\n        [1.33333333]]))\n\n\n\n스케일링한 값이 1보다 커질 수 있다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#d.-아주아주-잘못된-스케일링-방법---정보누수",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#d.-아주아주-잘못된-스케일링-방법---정보누수",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "### D. 아주아주 잘못된 스케일링 방법 - 정보누수",
    "text": "### D. 아주아주 잘못된 스케일링 방법 - 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- train data와 test data를 합친다….????!!!??!?!??\n\nnp.concatenate([X, XX], axis = 0)\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친 데이터에서 스케일링….\n\nsklearn.preprocessing.MinMaxScaler().fit_transform(np.concatenate([X, XX], axis = 0))\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625],\n       [0.125],\n       [0.375],\n       [1.   ]])\n\n\n\n이렇게 전저리하는 것은 정보누수에 해당한다. 본래 test dataset은 알지 못한 상태인데 그것을 합칠 순 없다!\n대회에서 이런 일이 발생하면 cheating으로 간주되어 탈락된다.\n\n\n위에서 minmax_scale()로 처리하는 것은 전략적으로 비효율적인 문제이지 치팅과 관련된 치명적인 문제가 아니다. (만약 어떠한 경우에 minmax_scale 전처리 방식이 유리하다는 생각이 들면 사용해도 무방함)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#standardscaler",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#standardscaler",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "3. StandardScaler",
    "text": "3. StandardScaler\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n\n여기서 토익과 gpa가 미치는 영향을 비교하기 위해 각 값들을 표준화해보자.\n\n\nA. 사용법\n\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n\nMinMaxScaler도 마찬가지로 여러 열을 한번에 할 수 있다.\n\n- 원리\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식, 자유도는 0(모분산으로 취급)\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\n그냥 표준화하는 것"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#비교",
    "href": "posts/Machine Learning in Practice/practice/5. 연속형 자료의 범위 조정.html#비교",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "4. 비교",
    "text": "4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxSclaer:\n\n장점 : 원하는 범위 내로 데이터를 조정할 때 유용, 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력값을 조정하는 데 유용.\n단점 : 이상치에 매우 민감하다.\n\nStandardScaler:\n\n장점 : 이상치에 덜 민감함, 많은 통계적 기법들 - 선형 알고리즘에서 잘 작동함\n단점 : 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.\n\n\n\n단순히 MinMaxScaler는 데이터가 0~1 또는 -1~1사이의 범위에 있다고 가정한다.\n\n그래서 둘 중 어느 것을 선택해야 하는데???\n\n둘 중 이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 신경망의 경우 MinMaxScaler가 적합할 수 있다.\n\n결론적으로 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n\n결론"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html",
    "title": "다중공선성의 해소",
    "section": "",
    "text": "Ridge와 Lasso를 통해 다중공선성을 극복해보자."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#라이브러리-imports",
    "title": "다중공선성의 해소",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#ridge-l2-penalty",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#ridge-l2-penalty",
    "title": "다중공선성의 해소",
    "section": "2. Ridge : L2-penalty",
    "text": "2. Ridge : L2-penalty\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n위와 같은 데이터에서 toeic0~toeic499는 설명변수 간 상관관계가 높은 녀석들이다.\n\n\nA. True World\n\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic']\nyy = df_test[['employment_score']]\n## step2 \npredictr = sklearn.linear_model.LinearRegression()\n## step3\npredictr.fit(X,y)\n## step4 : pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(f'train_score:\\t{predictr.score(X,y):.4f}')\nprint(f'test_score:\\t{predictr.score(XX,yy):.4f}')\n\ntrain_score:    0.9133\ntest_score: 0.9127\n\n\n- 언더라잉만 잘 적합한 결과, 오차항 때문에 1.0은 나오기 힘듦\n\n이 점수는 현실적으로 달성하기 어려워…"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#b.-무지성",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#b.-무지성",
    "title": "다중공선성의 해소",
    "section": "### B. 무지성…",
    "text": "### B. 무지성…\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.drop(['employment_score'], axis = 1)\ny = df_train[['employment_score']]\nXX = df_test.drop(['employment_score'], axis = 1)\nyy = df_test[['employment_score']]\n## step2 \npredictr = sklearn.linear_model.LinearRegression()\n## step3\npredictr.fit(X,y)\n## step4 : pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\n명백한 오버피팅…\n\n\nC. Ridge\n\n- 통계학자 : 이럴경우 Ridge를 사용하면 됩니다…\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge()  ## 로지스틱의 경우 LogisticRegressionCV(penalty = 'l2')를 사용 가능\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1173\n\n\n\n??? 안되는데요?\n\n- 하이퍼 파라미터를 튜닝하면 됩니다…\n\n## step1 --- 다넣음\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)  ## alpha = 500000000.\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\n오라클에 비할 바는 아니긴 한데, 공선성이 있는 경우라도 적절한 alpha를 고른다면 망하지는 않음."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#d.-ridge의-작동원리",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#d.-ridge의-작동원리",
    "title": "다중공선성의 해소",
    "section": "### D. Ridge의 작동원리",
    "text": "### D. Ridge의 작동원리\n- 정확한 설명…\nSVD를 이용하여 이론적으로 계산하면, sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 가 항상 존재함을 증명할 수 있음…\n\n그렇다네요.\n\n- 직관적 설명(엄밀하지 않은 설명)\n\nLinearRegression은 왜 망했지???\n\n\n취업 자료의 예제를 보면 토익 성적의 계수는 실제로 0.01이다. 적당히… * toeic_coef+toeic0_coef+…+toeic499_coef \\(\\approx\\) 0.01이라면 대충 맞는 답이다.\n\n\n근데 사실 이 0.01이라는 값은 몇 개의 계수만 있어도 만들 순 있을거임… -&gt; 나머지 설명변수가 모두 불필요한 특징이 됨.\n\n\n그래가지고 불필요한 특징은 다중공선성의 문제 때문에 오버피팅을 유발한다.\n\n그래서 Ridge는 몇 개의 계수만 빼고 나머지들이 쓸모없는 게 되지 않도록, 다 유의미하도록 계수에 패널티를 부여한다.\n\nE. \\(\\alpha\\)에 따른 계수값 변화\n\n- 여러 개의 predictor를 alpha의 값을 달리하며 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## 아래에서 배울 RidgeCV에서 이 값들 중 어느 값이 가장 좋을 지 결정하게 할 수 있음\n\n## step3 \nfor predictr in predictrs:  ## 이건 리스트로 만드는 게 아니니까...\n    predictr.fit(X,y)\n## step4 -- pass \n\n\nplt.plot(predictrs[0].coef_[1:], label = r'$\\alpha$ = {}'.format(predictrs[0].alpha))\nplt.plot(predictrs[2].coef_[1:], label = r'$\\alpha$ = {}'.format(predictrs[2].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nalpha의 값이 작을수록, 그 변동 폭이 줄어듦을 알 수 있다.\n- 마지막 predictor의 계수값을 살펴보면…\n\n\ns = pd.Series(predictrs[-1].coef_)\ns.set_axis(X.columns, axis = 0)\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음(한두개로 계수 0.01을 만들 수 없음)\n모든 변수는 대량 2e-5(\\(\\approx\\frac{1}{100}\\frac{1}{501}\\))정도 똑같이 중요하다고 생각된다.\n살짝 (\\(\\frac{1}{100}\\frac{1}{501}\\))보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아니다.\n\n\n[predictr.coef_[1:].sum() for predictr in predictrs]\n\n[0.010274546089787007,\n 0.010157633994689774,\n 0.009948779293105905,\n 0.009866050921714562,\n 0.009854882844936588,\n 0.009820059959693872,\n 0.00949099901512329]\n\n\n\n갈수록 합의 크기가 작아짐…\n\n\n1/100*1/501\n\n1.9960079840319362e-05\n\n\n\n게대가 본래 기대될 회귀계수의 값보다 전체적으로 조금씩 낮은 편"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#f.-alpha-정리",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#f.-alpha-정리",
    "title": "다중공선성의 해소",
    "section": "### F. \\(\\alpha\\) 정리",
    "text": "### F. \\(\\alpha\\) 정리\n- L2-penalty는 대충 분산같은 것…\n\nx = np.random.randn(5)\nL2_penalty = (x**2).sum()  ## 제곱합, 평균에서 멀어진...\n(L2_penalty, 5*(x.var() + (x.mean()**2)))  ## 2차 적률인듯. E(X**2)\n\n(10.591975556137934, 10.591975556137934)\n\n\n\nfor predictr in predictrs :\n    print(\n        f'alpha={predictr.alpha:.0e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}')\n\nalpha=5e+02 l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5e+03 l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5e+04 l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5e+05 l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5e+06 l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5e+07 l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5e+08 l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n\nalpha의 값이 늘어날수록, penalty의 값이 규모가 작아진다. 그에따라 계수들의총합도 점점 낮아진다…\n게다가 test_score도 어느순간부터 낮아지기 시작한다…"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#ridgecv",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#ridgecv",
    "title": "다중공선성의 해소",
    "section": "3. RidgeCV",
    "text": "3. RidgeCV\n- 입력한 alpha값들 중에서 가장 적절한 alpha값을 제시해준다.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()  ## 일단 alpha를 지정해주지 않는 모습...\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\nprint(f'df_train score : {predictr.score(X, y):.5f}')\nprint(f'df_test score : {predictr.score(XX, yy):.5f}')\n\ndf_train score : 1.00000\ndf_test score : 0.11915\n\n\n\n아직 overfitting된 모습…\n\n왜냐! alphas의 후보는 0.1, 1.0, 10.0이 디폴트니까…\n- 따라서 이 후보를 직접 넣어주자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 -- 여기서 alpha의 후보들을 alphas에 리스트로 지정해준다.\npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8])\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\n(predictr.score(X, y), predictr.score(XX, yy))\n\n(0.7521268560159359, 0.7450309251010893)\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n\nalpha를 5,000,000로 설정했더니 가장 좋은 결과가 나왔다는 것을 알 수 있다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#lasso",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#lasso",
    "title": "다중공선성의 해소",
    "section": "4. Lasso",
    "text": "4. Lasso\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\nA. Lasso를 이용한 분석\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\npredictr = sklearn.linear_model.Lasso()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n(0.8600312387900632, 0.8306176063318933)\n\n\n\nprint(f'train_score:\\t {predictr.score(X,y):.4f}')\nprint(f'test_score:\\t {predictr.score(XX,yy):.4f}')\n\ntrain_score:     0.8600\ntest_score:  0.8306\n\n\n\nalpha를 default로 두었음에도 굉장히 우수한 결과가 나왔다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#b.-lasso의-원리",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#b.-lasso의-원리",
    "title": "다중공선성의 해소",
    "section": "### B. Lasso의 원리",
    "text": "### B. Lasso의 원리\n- 정확한 설명\n\n지금 이해하기엔 어려움…\n\n- 상관성이 짙은 설명변수 몇개로만 그 합의 계수를 만들게 해서는 안된다.\n\n아주 적은 숫자의 coef만 살려두고, 나머지는 0으로 강제한다.\n계수가 0이라는 것은 해당 변수를 제거한 것과 같은 효과를 가진다.\n\n\nplt.plot(predictr.coef_[1:])\n\n\n\n\n\n실제로 계수값이 0인 녀석이 많음을 알 수 있다.\n\n\nC. \\(\\alpha\\)의 값에 따른 변화\n\n- 여러 개의 predictor를 학습시켜 계수값들의 변화를 관찰해보자.\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\nalphas = np.linspace(0.1, 2, 20)\npredictrs = [sklearn.linear_model.Lasso(alpha = alpha) for alpha in alphas]\n\n## 3\nfor predictr in predictrs:\n    predictr.fit(X, y)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.115e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.991e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.375e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.588e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.730e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.671e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.117e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.875e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.192e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n\nplt.plot(predictrs[0].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[9].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[9].alpha.round(5)))\nplt.plot(predictrs[-1].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\n계수값들의 분산이 갈수록 작아지는 것을 느낄 수 있다.\n\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[9].alpha:.4f}\\tsum(toeic_coef)={predictrs[9].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.1000    sum(toeic_coef)=0.010169320378140704\nalpha=1.0000    sum(toeic_coef)=0.009987870459109604\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n\npredictor들의 toeic 계수 합은 여전히 0.01 근처….\n\n\nplt.plot([(predictr.coef_ != 0).sum() for predictr in predictrs])\n\n\n\n\n\nalpha값이 커질수록 0이 아닌 계수의 갯수가 줄어드는 것을 볼 수 있다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#d.-lassocvcross-validation",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#d.-lassocvcross-validation",
    "title": "다중공선성의 해소",
    "section": "### D. LassoCV(Cross Validation)",
    "text": "### D. LassoCV(Cross Validation)\n- 가장 적합한 \\(\\alpha\\)값을 자동으로 찾아준다.\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\npredictr = sklearn.linear_model.LassoCV(alphas = np.linspace(0.1, 2, 20))\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.989e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.633e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.252e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.352e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.306e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.798e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.872e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.992e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.627e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.897e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.514e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.461e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.234e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.637e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.876e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.340e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.293e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.578e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.930e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.021e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.646e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.779e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.310e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.075e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.837e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.093e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.277e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.617e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.724e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.746e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.480e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.328e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.987e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.132e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.659e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.356e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.074e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.031e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.921e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.669e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.082e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.384e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.782e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.134e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.001e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.107e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.057e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.464e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.704e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.481e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.384e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.173e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.835e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.883e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.713e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n0.9555099850022306\n\n\n\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9555099850022306, 0.8756348559919926)\n\n\n\n살짝 과적합된 면이 있으나, 그래도 상당히 높은 수치이다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#coef를-0으로-만드는-수학적-장치",
    "href": "posts/Machine Learning in Practice/practice/7. 다중공선성의 해소.html#coef를-0으로-만드는-수학적-장치",
    "title": "다중공선성의 해소",
    "section": "5. coef를 0으로 만드는 수학적 장치",
    "text": "5. coef를 0으로 만드는 수학적 장치\n\nRidge : L2-penalty\n\ncoef의 값들을 가중치에 따라 분할하는 수학적 장치.\n\n\n\n패널티 : 상관성이 짙은 설명변수들의 계수값을 제곱한 뒤 합치고(L2-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여.\n\n\nLasso : L1-penalty\n\n다수의 coef 값들을 0으로 만드는 수학적 장치\n\n\n\n패널티 : 상관성이 짙은 설명변수들의 계수값의 절대값을 구한 뒤에 합치고(L1-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "",
    "text": "의사결정나무를 이용하여 모형을 간단하게 적합해보자!"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#라이브러리-imports",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport sklearn.tree\nimport seaborn as sns\nimport sklearn.model_selection"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#다중공선성",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#다중공선성",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "2. 다중공선성",
    "text": "2. 다중공선성\n\nA. Data\n\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n유사 토익들이 매우 많은, 설명변수에 다중공선성이 존재하는 자료이다.\n\n- 근데 Lasso를 쓰지 않고도, 여러 설명변수를 배제하지 않고도, 자료를 쉽게 적합할 수 있는 방법이 있다면 믿겠는가???\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.8203054095383844)\n\n\n\n본래 모델은 트리 모형의 작동방식에 따라 정확도가 1이 나오지만, 예측 모델의 성능도 나름 나쁘지 않다.\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.linear_model.LassoCV(alphas = np.linspace(0.1, 2, 20))\n\n## 3\npredictr.fit(X, y)\n\n## 4\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.066e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.424e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.596e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.086e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.995e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.758e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.269e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.408e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.055e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.412e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.745e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.516e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.655e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.318e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.655e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.360e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.781e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.542e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.733e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.424e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.040e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.248e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.404e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.849e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.468e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.599e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.833e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.540e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.155e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.046e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.674e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.826e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.194e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.973e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.597e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.071e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.339e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.054e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.383e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.403e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.355e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.533e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.173e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.488e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.664e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.037e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.565e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.378e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.431e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.947e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.213e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.636e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.980e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.452e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.598e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.233e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.944e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.593e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.450e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.626e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.864e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.382e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.903e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.244e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.406e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.041e+02, tolerance: 3.346e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ]))\n\n\n\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9554916123291357, 0.8733838672032972)\n\n\n\nLasso로 적합한 결과가 더 스코어가 높긴 하지만, 트리 모형은 직관적이고 단순하다. 그리고 하이퍼파라미터도 바꿀 수 있잖아?\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 5)\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9435995918442013, 0.853062350997775)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#b.-평가",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#b.-평가",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### B. 평가",
    "text": "### B. 평가\n\nLasso가 좀 더 좋긴 한데, 의사결정나무도 공선성이 있는 상황에서 간단하게 사용가능하다.\n\n\nLasso는 엄청 발전된 모델이고, 의사결정나무는 아주 초기모델이라… 개선의 여지가 많다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#오버피팅",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#오버피팅",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "3. 오버피팅",
    "text": "3. 오버피팅\n\nA. 사전작업\n\n\n종속변수와 관련이 없는 변수들을 무작위로 생성하는 함수\n\n\ndef generating_df(n_balance):\n    global df\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\n## 물론 해당 함수에 df를 따로 입력하게 하여 length를 조절한 후 아무 df에 사용가능하도록 만들수도 있다.\ndef random_generation_df(df, n) :\n    df_random = pd.DataFrame(np.random.randn(len(df), n), columns = ['random'+str(i) for i in range(n)])\n    return pd.concat([df, df_random], axis = 1)\n\n\ngenerating_df(1)\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\n\n\n\n\n0\n135\n0.051535\n0\n0.039391\n\n\n1\n935\n0.355496\n0\n0.616426\n\n\n2\n485\n2.228435\n0\n0.985876\n\n\n3\n65\n1.179701\n0\n1.677899\n\n\n4\n445\n3.962356\n1\n-0.167288\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.936697\n\n\n496\n310\n2.601212\n1\n-0.184044\n\n\n497\n225\n0.042323\n0\n-0.805794\n\n\n498\n320\n1.041416\n0\n-0.483178\n\n\n499\n375\n3.626883\n1\n0.717244\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\nX = df_train.drop('employment', axis = 1)\ny = df_train.employment\nXX = df_test.drop('employment', axis = 1)\nyy = df_test.employment  ## 실제론 모름"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#b.-분석",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#b.-분석",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### B. 분석",
    "text": "### B. 분석\n# 1 : 의사결정나무\n\n## 2\npredictr = sklearn.tree.DecisionTreeClassifier()  ## 범주형일 때 더 유용\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.82)\n\n\n\n오버피팅된 상황이다.\n\n# 2 : Lasso(L1 penalty)\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(penalty = 'l1', solver = 'liblinear')\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(employment_hat = predictr.predict(X))\ndf_test = df_test.assign(employment_hat = predictr.predict(XX))\n\n##-##\nprint('train score = ' + str(round(predictr.score(X, y), 4)))\nprint('test score = ' + str(round(predictr.score(XX, yy), 4)))\n\ntrain score = 0.8829\ntest score = 0.8733\n\n\n\n~역시 Lasso가 최고다~\n\n# 3 : Ridge(L2 penalty)\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(penalty = 'l2')\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(employment_hat = predictr.predict(X))\ndf_test = df_test.assign(employment_hat = predictr.predict(XX))\n\n##-##\nprint('train score = ' + str(round(predictr.score(X, y), 4)))\nprint('test score  = ' + str(round(predictr.score(XX, yy), 4)))\n\ntrain score = 0.8886\ntest score  = 0.88\n\n\n\n~Ridge도 최고다~\n\n\nC. 설명변수들의 증가\n\n- 관련없는 변수들의 수가 커짐에 따라서 각 방법들의 train/test score는 어떻게 변화할까?\n\n데이터프레임과 predictor, 반응변수 열 이름을 넣어주면 fitting하고, 스코어를 배출하는 함수\n\n\ndef fitting_df(df, predictor, response) :\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n    X = df_train.drop(response, axis = 1)\n    y = df_train[response]\n    XX = df_test.drop(response, axis = 1)\n    yy = df_test[response]\n\n    predictor.fit(X, y)\n\n    return predictor.score(X, y), predictor.score(XX, yy)\n\n\npredictor의 리스트와 필요없는 설명변수들이 가득찬 열의 개수를 정의\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(),\n             sklearn.linear_model.LogisticRegressionCV(penalty = 'l1', solver = 'liblinear'),\n             sklearn.linear_model.LogisticRegressionCV(penalty = 'l2')]\n\nn_balance_list = range(0, 5000, 50)\n\n\n그것들을 기반으로 세 변수들에게서 피팅하면서 나온 점수들을 원소로 하는 리스트를 컴프리헨션\n\n\nlst = [fitting_df(generating_df(n_balance), predictr, 'employment') for predictr, n_balance in [(predictr, n_balance) for n_balance in n_balance_list for predictr in predictrs]]\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nKeyboardInterrupt: \n\n\n\n좀 오래 걸릴 수밖에 없음…(이중으로 컴프리헨션 하는 게 훨씬 적합한 것 같긴 하다. 나중에 처리를 또 해야되니…)\n\n- 실험결과의 정리\n\narr = np.array(lst)\ntr = arr[:, :, 0]\ntst = arr[:, :, 1]\narr.shape\n\n\nar = np.array(lst)\narr = arr.reshape(100, 3, 2)\ntr = arr[:, :, 0]\ntst = arr[:, :, 1]\n\n\n전반적으로 관련없는 변수가 많아질수록 스코어가 떨어지기는 하는데, 뒤로 갈수록 tree의 점수는 다른 것들보다 감소폭이 적어 역전되는 상황이다.\n\n- 이를 시각화해보자.\n\npd.DataFrame(tr, columns = ['tree', 'lasso', 'ridge'])\n_.assign(dataset = 'train')\n\n\ntr_score = pd.DataFrame(tr, columns = ['tree', 'lasso', 'ridge'])\ntst_score = pd.DataFrame(tst, colunms = ['tree', 'lasso', 'ridge'])\n\nresult_df = pd.concat([tr_score, tst_score], axis = 0)"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#이상치",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#이상치",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "4. 이상치",
    "text": "4. 이상치"
  },
  {
    "objectID": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#a.-데이터",
    "href": "posts/Machine Learning in Practice/practice/9 의사결정나무의 활용.html#a.-데이터",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### A. 데이터",
    "text": "### A. 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.show()\n\n\n\n\n\n나머지 자료들은 모두 선형을 띄고 있는데, (-4.1, 200)이라는 이상치 하나가 모형의 설명을 어렵도록 만들고 있다.\n\n\nB. 분석\n\n# 1 일반적인 선형 회귀\n\n## 1\nX = df_train[['temp']]\ny = df_train.ice_sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(ice_sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.plot(df_train.temp, df_train.ice_sales_hat, '--', color = 'red')\nplt.show()\n\n\n\n\n\npredictr.coef_\n\narray([-0.64479089])\n\n\n\n기울기가 음수이다.\n- 늘 하던 것처럼 선형회귀로 적합했을 때, 해당 모형은 완전히 언더피팅된 것으로 보여진다…\n\n# 2 의사결정나무를 사용\n(주의!) Lasso와 Ridge는 공선성이 있는 모델에서만 잘 작동할 뿐, 그렇지 않은 경우 선형회귀와 유사하게 적합되니 사용하지 말것!(Ridge는 설명변수가 적을 때 특히 똑같게 적합하는 것 같다.)\n\n## 1\nX = df_train[['temp']]\ny = df_train.ice_sales\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(ice_sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.plot(df_train.temp, df_train.ice_sales_hat, '--', color = 'red')\nplt.show()\n\n\n\n\n\npredictr.score(X, y)\n\n0.9992029367488545\n\n\n\n_df = df_train.drop(0, axis = 0)\nplt.plot(_df.temp, _df.ice_sales, 'o', label = 'observations')\nplt.plot(_df.temp, _df.ice_sales_hat, '--', color = 'red', label = 'prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nDecisionTreeRegressor의 경우 언더라잉만 적합하는 것을 넘어 오버피팅이 되게 하나, 결과는 언더피팅이 되는 경우보단 나름 합리적이다.\n적합에 관여한 구간 외의 값이 인풋으로 들어오면 해당 모형은 예측하지 못한다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html",
    "href": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "",
    "text": "Discussion 탭에서 가장 상위에 있는 안내 자료를 살펴보자."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#라이브러리-import",
    "href": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#라이브러리-import",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#data-불러오기",
    "href": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#data-불러오기",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "2. Data 불러오기",
    "text": "2. Data 불러오기\n\ntrain_data = pd.read_csv(\"./data/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntest_data = pd.read_csv(\"./data/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#alexis의-코드-forecast",
    "href": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#alexis의-코드-forecast",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "3. Alexis의 코드 | forecast",
    "text": "3. Alexis의 코드 | forecast\n\nA. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]  ## 결측치가 많은 것들과 이상한 녀석들을 배제했다.\nX = pd.get_dummies(train_data[features])  ## 변수들 중 범주형 자료를 더미변수로 만든다.\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)  ## 예측값을 담아둔다. predictr.predict(XX)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n##output.to_csv('submission_AlexisCook.csv', index=False)  ##파일을 자꾸 만들어서 주석처리했다.\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nRandomForestClassifier 모듈을 사용하여 fitting 하였다.\n\n- 간단한 수정\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)  ## predict를 train data에 실시\n\n\n(predictions == y).mean()\n\n0.8159371492704826\n\n\n\nmodel.score(X, y)\n\n0.8159371492704826\n\n\n\nscore는 모델이 데이터와 맞는 정도를 내준다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#alexis-cook의-코드를-수정해보자",
    "href": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#alexis-cook의-코드를-수정해보자",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "### Alexis Cook의 코드를 수정해보자!",
    "text": "### Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\n## hyper parameter 조정\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nmodel.score(X, y)\n\n0.8170594837261503\n\n\n\n바꾼 게 더 좋은 것 같은데???\n\n- 이것도 제출 결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n\npd.read_csv(\"./data/test.csv\")[['PassengerId']].assign(Survived = predictions)#\\\n#.to_csv(\"AlexisCook수정_submission.csv\", index = False)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\nindex를 꼭 누락시켜야 한다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#제출결과의-비교",
    "href": "posts/Machine Learning in Practice/Titanic/2. code-by-alexis-cook.html#제출결과의-비교",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "4. 제출결과의 비교",
    "text": "4. 제출결과의 비교\n\nhyper parameter를 조정해서 train score는 더 높아졌지만, 실제 test score는 더 낮아졌다.\n\n- overfitting된 경우 둘의 차이가 극명하게 난다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html",
    "href": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html",
    "title": "Kaggle | 결측치의 처리",
    "section": "",
    "text": "Titanic 데이터에는 결측치가 상당히 많았는데, 그것을 처리해서 분석해보자."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#라이브러리-imports",
    "href": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#라이브러리-imports",
    "title": "Kaggle | 결측치의 처리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#!pip install missingno\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.impute\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\nimport missingno as msno"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#데이터-불러오기",
    "href": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#데이터-불러오기",
    "title": "Kaggle | 결측치의 처리",
    "section": "2. 데이터 불러오기",
    "text": "2. 데이터 불러오기\n\n#!kaggle competitions download -c titanic\n#!unzip titanic.zip -d ./titanic\n#df_train = pd.read_csv('titanic/train.csv')\n#df_test = pd.read_csv('titanic/test.csv')\n#!rm titanic.zip\n#!rm -rf titanic/\n\n## 리눅스 서버가 구축되어 있다면 데이터를 바로 불러오기가 편리하다.\n\n\ndf_test = pd.read_csv('./data/test.csv')\ndf_train = pd.read_csv('./data/train.csv')"
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#결측치-확인-및-처리",
    "href": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#결측치-확인-및-처리",
    "title": "Kaggle | 결측치의 처리",
    "section": "3. 결측치 확인 및 처리",
    "text": "3. 결측치 확인 및 처리\n결측치 확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n시각화\n\nmsno.matrix(df_train)\n\n&lt;Axes: &gt;\n\n\n\n\n\n결측치 처리\n\n수치형은 수치형끼리, 범주형은 범주형끼리 처리하자.\n\n\ndf_imputed = df_train.copy()\n\ntrain_num = df_train.select_dtypes(include = 'number')\ntrain_obj = df_train.select_dtypes(exclude = 'number')\n\ndf_imputed[train_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(train_num)\ndf_imputed[train_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\n\ndf_imputed.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    float64\n 1   Survived     891 non-null    float64\n 2   Pclass       891 non-null    float64\n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          891 non-null    float64\n 6   SibSp        891 non-null    float64\n 7   Parch        891 non-null    float64\n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        891 non-null    object \n 11  Embarked     891 non-null    object \ndtypes: float64(7), object(5)\nmemory usage: 83.7+ KB\n\n\n\n결측치가 완전히 메꿔진 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#분석",
    "href": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#분석",
    "title": "Kaggle | 결측치의 처리",
    "section": "4. 분석(?)",
    "text": "4. 분석(?)\n늘 해왔던 것처럼…\n\nset(df_train.columns) - set(df_test.columns)\n\n{'Survived'}\n\n\n\nSurvived : 반응변수\n\n- 근데 몇 번 결측치 처리를 반복해야 하므로 위에서의 과정을 함수로 만들어버리자.\n\ndef impute_missing(df):\n    \"\"\"\n    imputing missing and output whole dataframe\n    \n    df : DataFrame include NaN value\n    \"\"\"\n    df_ = df.copy()  ## 데이터를 복사\n    \n    df_num = df_.select_dtypes(include = 'number')  ## 해당하는 데이터 타입만 선택\n    df_obj = df_.select_dtypes(exclude = 'number')\n    \n    df_[df_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(df_num)\n    df_[df_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent').fit_transform(df_obj)\n    \n    return df_\n\n\npd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1)))\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbing, Mr. Anthony\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n3.0\n22.000000\n1.0\n0.0\n7.2500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\n2.0\n1.0\n38.000000\n1.0\n0.0\n71.2833\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3.0\n3.0\n26.000000\n0.0\n0.0\n7.9250\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n4.0\n1.0\n35.000000\n1.0\n0.0\n53.1000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\n5.0\n3.0\n35.000000\n0.0\n0.0\n8.0500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887.0\n2.0\n27.000000\n0.0\n0.0\n13.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n887\n888.0\n1.0\n19.000000\n0.0\n0.0\n30.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n888\n889.0\n3.0\n29.699118\n1.0\n2.0\n23.4500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n889\n890.0\n1.0\n26.000000\n0.0\n0.0\n30.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n890\n891.0\n3.0\n32.000000\n0.0\n0.0\n7.7500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n891 rows × 1730 columns\n\n\n\n\n늘 그랬던 것처럼 get_dummies를 해줬는데… 뭔가 이상하다.\n\n\npd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1))).shape\n\n(891, 1730)\n\n\n행이 1730개??? &gt; 이러한 상황에서는 선형 모델이 제대로 작동하지 않는다…!\n\n# step 1\nX = pd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1)))\ny = df_train.Survived\nXX = pd.get_dummies(impute_missing(df_test))\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\npredictr.predict(XX)\n\nC:\\Users\\hollyriver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nValueError: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cabin_A11\n- Cabin_A18\n- Cabin_A21\n- Cabin_A29\n- Cabin_A9\n- ...\nFeature names seen at fit time, yet now missing:\n- Cabin_A10\n- Cabin_A14\n- Cabin_A16\n- Cabin_A19\n- Cabin_A20\n- ...\n\n\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\n형식이 object인 것들이 가지고 있는 유니크한 값들이 몇개인지를 딕셔너리 컴프리헨션 해봤다.\n\n\n사실상 해당 수가 엄청나게 많은 Name, Ticket, Cabin의 경우 없애는 편이 더 좋아보인다."
  },
  {
    "objectID": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#진짜-분석",
    "href": "posts/Machine Learning in Practice/Titanic/4. 로지스틱_결측치 처리.html#진짜-분석",
    "title": "Kaggle | 결측치의 처리",
    "section": "5. 진짜 분석",
    "text": "5. 진짜 분석\n\n## step 1\nX = pd.get_dummies(impute_missing(df_train).drop(['Name', 'Ticket', 'Cabin', 'Survived'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(impute_missing(df_test).drop(['Name', 'Ticket', 'Cabin'], axis = 1))\n\n## step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\ndf_test[['PassengerId']].assign(Survived = predictr.predict(XX))\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\ndf_test[['PassengerId']].assign(Survived = predictr.predict(XX)).to_csv(\"submission\", index = False)\n\n\n이렇게 하면 된다."
  },
  {
    "objectID": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html",
    "href": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html",
    "title": "Kaggle | 결측치의 처리",
    "section": "",
    "text": "Titanic 데이터에는 결측치가 상당히 많았는데, 그것을 처리해서 분석해보자."
  },
  {
    "objectID": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#라이브러리-imports",
    "href": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#라이브러리-imports",
    "title": "Kaggle | 결측치의 처리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#!pip install missingno\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.impute\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\nimport missingno as msno"
  },
  {
    "objectID": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#데이터-불러오기",
    "href": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#데이터-불러오기",
    "title": "Kaggle | 결측치의 처리",
    "section": "2. 데이터 불러오기",
    "text": "2. 데이터 불러오기\n\n#!kaggle competitions download -c titanic\n#!unzip titanic.zip -d ./titanic\n#df_train = pd.read_csv('titanic/train.csv')\n#df_test = pd.read_csv('titanic/test.csv')\n#!rm titanic.zip\n#!rm -rf titanic/\n\n## 리눅스 서버가 구축되어 있다면 데이터를 바로 불러오기가 편리하다.\n\n\ndf_test = pd.read_csv('./data/test.csv')\ndf_train = pd.read_csv('./data/train.csv')"
  },
  {
    "objectID": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#결측치-확인-및-처리",
    "href": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#결측치-확인-및-처리",
    "title": "Kaggle | 결측치의 처리",
    "section": "3. 결측치 확인 및 처리",
    "text": "3. 결측치 확인 및 처리\n결측치 확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n시각화\n\nmsno.matrix(df_train)\n\n&lt;Axes: &gt;\n\n\n\n\n\n결측치 처리\n\n수치형은 수치형끼리, 범주형은 범주형끼리 처리하자.\n\n\ndf_imputed = df_train.copy()\n\ntrain_num = df_train.select_dtypes(include = 'number')\ntrain_obj = df_train.select_dtypes(exclude = 'number')\n\ndf_imputed[train_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(train_num)\ndf_imputed[train_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\n\ndf_imputed.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    float64\n 1   Survived     891 non-null    float64\n 2   Pclass       891 non-null    float64\n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          891 non-null    float64\n 6   SibSp        891 non-null    float64\n 7   Parch        891 non-null    float64\n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        891 non-null    object \n 11  Embarked     891 non-null    object \ndtypes: float64(7), object(5)\nmemory usage: 83.7+ KB\n\n\n\n결측치가 완전히 메꿔진 것을 확인할 수 있다."
  },
  {
    "objectID": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#분석",
    "href": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#분석",
    "title": "Kaggle | 결측치의 처리",
    "section": "4. 분석(?)",
    "text": "4. 분석(?)\n늘 해왔던 것처럼…\n\nset(df_train.columns) - set(df_test.columns)\n\n{'Survived'}\n\n\n\nSurvived : 반응변수\n\n- 근데 몇 번 결측치 처리를 반복해야 하므로 위에서의 과정을 함수로 만들어버리자.\n\ndef impute_missing(df):\n    \"\"\"\n    imputing missing and output whole dataframe\n    \n    df : DataFrame include NaN value\n    \"\"\"\n    df_ = df.copy()  ## 데이터를 복사\n    \n    df_num = df_.select_dtypes(include = 'number')  ## 해당하는 데이터 타입만 선택\n    df_obj = df_.select_dtypes(exclude = 'number')\n    \n    df_[df_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(df_num)\n    df_[df_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent').fit_transform(df_obj)\n    \n    return df_\n\n\npd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1)))\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbing, Mr. Anthony\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n3.0\n22.000000\n1.0\n0.0\n7.2500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\n2.0\n1.0\n38.000000\n1.0\n0.0\n71.2833\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3.0\n3.0\n26.000000\n0.0\n0.0\n7.9250\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n4.0\n1.0\n35.000000\n1.0\n0.0\n53.1000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\n5.0\n3.0\n35.000000\n0.0\n0.0\n8.0500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887.0\n2.0\n27.000000\n0.0\n0.0\n13.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n887\n888.0\n1.0\n19.000000\n0.0\n0.0\n30.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n888\n889.0\n3.0\n29.699118\n1.0\n2.0\n23.4500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n889\n890.0\n1.0\n26.000000\n0.0\n0.0\n30.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n890\n891.0\n3.0\n32.000000\n0.0\n0.0\n7.7500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n891 rows × 1730 columns\n\n\n\n\n늘 그랬던 것처럼 get_dummies를 해줬는데… 뭔가 이상하다.\n\n\npd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1))).shape\n\n(891, 1730)\n\n\n행이 1730개??? &gt; 이러한 상황에서는 선형 모델이 제대로 작동하지 않는다…!\n\n# step 1\nX = pd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1)))\ny = df_train.Survived\nXX = pd.get_dummies(impute_missing(df_test))\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\npredictr.predict(XX)\n\nC:\\Users\\hollyriver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nValueError: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cabin_A11\n- Cabin_A18\n- Cabin_A21\n- Cabin_A29\n- Cabin_A9\n- ...\nFeature names seen at fit time, yet now missing:\n- Cabin_A10\n- Cabin_A14\n- Cabin_A16\n- Cabin_A19\n- Cabin_A20\n- ...\n\n\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\n형식이 object인 것들이 가지고 있는 유니크한 값들이 몇개인지를 딕셔너리 컴프리헨션 해봤다.\n\n\n사실상 해당 수가 엄청나게 많은 Name, Ticket, Cabin의 경우 없애는 편이 더 좋아보인다."
  },
  {
    "objectID": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#진짜-분석",
    "href": "2023_MP/Titanic/4. 로지스틱_결측치 처리.html#진짜-분석",
    "title": "Kaggle | 결측치의 처리",
    "section": "5. 진짜 분석",
    "text": "5. 진짜 분석\n\n## step 1\nX = pd.get_dummies(impute_missing(df_train).drop(['Name', 'Ticket', 'Cabin', 'Survived'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(impute_missing(df_test).drop(['Name', 'Ticket', 'Cabin'], axis = 1))\n\n## step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\ndf_test[['PassengerId']].assign(Survived = predictr.predict(XX))\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\ndf_test[['PassengerId']].assign(Survived = predictr.predict(XX)).to_csv(\"submission\", index = False)\n\n\n이렇게 하면 된다."
  },
  {
    "objectID": "2023_MP/Titanic/2. code-by-alexis-cook.html",
    "href": "2023_MP/Titanic/2. code-by-alexis-cook.html",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "",
    "text": "Discussion 탭에서 가장 상위에 있는 안내 자료를 살펴보자."
  },
  {
    "objectID": "2023_MP/Titanic/2. code-by-alexis-cook.html#라이브러리-import",
    "href": "2023_MP/Titanic/2. code-by-alexis-cook.html#라이브러리-import",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "2023_MP/Titanic/2. code-by-alexis-cook.html#data-불러오기",
    "href": "2023_MP/Titanic/2. code-by-alexis-cook.html#data-불러오기",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "2. Data 불러오기",
    "text": "2. Data 불러오기\n\ntrain_data = pd.read_csv(\"./data/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntest_data = pd.read_csv(\"./data/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS"
  },
  {
    "objectID": "2023_MP/Titanic/2. code-by-alexis-cook.html#alexis의-코드-forecast",
    "href": "2023_MP/Titanic/2. code-by-alexis-cook.html#alexis의-코드-forecast",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "3. Alexis의 코드 | forecast",
    "text": "3. Alexis의 코드 | forecast\n\nA. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]  ## 결측치가 많은 것들과 이상한 녀석들을 배제했다.\nX = pd.get_dummies(train_data[features])  ## 변수들 중 범주형 자료를 더미변수로 만든다.\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)  ## 예측값을 담아둔다. predictr.predict(XX)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n##output.to_csv('submission_AlexisCook.csv', index=False)  ##파일을 자꾸 만들어서 주석처리했다.\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nRandomForestClassifier 모듈을 사용하여 fitting 하였다.\n\n- 간단한 수정\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)  ## predict를 train data에 실시\n\n\n(predictions == y).mean()\n\n0.8159371492704826\n\n\n\nmodel.score(X, y)\n\n0.8159371492704826\n\n\n\nscore는 모델이 데이터와 맞는 정도를 내준다."
  },
  {
    "objectID": "2023_MP/Titanic/2. code-by-alexis-cook.html#alexis-cook의-코드를-수정해보자",
    "href": "2023_MP/Titanic/2. code-by-alexis-cook.html#alexis-cook의-코드를-수정해보자",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "### Alexis Cook의 코드를 수정해보자!",
    "text": "### Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\n## hyper parameter 조정\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nmodel.score(X, y)\n\n0.8170594837261503\n\n\n\n바꾼 게 더 좋은 것 같은데???\n\n- 이것도 제출 결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n\npd.read_csv(\"./data/test.csv\")[['PassengerId']].assign(Survived = predictions)#\\\n#.to_csv(\"AlexisCook수정_submission.csv\", index = False)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\nindex를 꼭 누락시켜야 한다."
  },
  {
    "objectID": "2023_MP/Titanic/2. code-by-alexis-cook.html#제출결과의-비교",
    "href": "2023_MP/Titanic/2. code-by-alexis-cook.html#제출결과의-비교",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "4. 제출결과의 비교",
    "text": "4. 제출결과의 비교\n\nhyper parameter를 조정해서 train score는 더 높아졌지만, 실제 test score는 더 낮아졌다.\n\n- overfitting된 경우 둘의 차이가 극명하게 난다."
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "",
    "text": "의사결정나무를 이용하여 모형을 간단하게 적합해보자!"
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#라이브러리-imports",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#라이브러리-imports",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport sklearn.tree\nimport seaborn as sns\nimport sklearn.model_selection"
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#다중공선성",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#다중공선성",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "2. 다중공선성",
    "text": "2. 다중공선성\n\nA. Data\n\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n유사 토익들이 매우 많은, 설명변수에 다중공선성이 존재하는 자료이다.\n\n- 근데 Lasso를 쓰지 않고도, 여러 설명변수를 배제하지 않고도, 자료를 쉽게 적합할 수 있는 방법이 있다면 믿겠는가???\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.8203054095383844)\n\n\n\n본래 모델은 트리 모형의 작동방식에 따라 정확도가 1이 나오지만, 예측 모델의 성능도 나름 나쁘지 않다.\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.linear_model.LassoCV(alphas = np.linspace(0.1, 2, 20))\n\n## 3\npredictr.fit(X, y)\n\n## 4\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.066e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.424e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.596e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.086e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.995e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.758e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.269e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.408e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.055e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.412e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.745e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.516e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.655e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.318e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.655e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.360e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.781e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.542e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.733e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.424e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.040e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.248e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.404e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.849e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.468e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.599e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.833e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.540e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.155e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.046e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.674e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.826e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.194e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.973e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.597e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.071e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.339e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.054e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.383e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.403e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.355e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.533e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.173e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.488e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.664e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.037e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.565e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.378e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.431e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.947e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.213e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.636e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.980e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.452e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.598e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.233e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.944e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.593e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.450e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.626e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.864e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.382e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.903e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.244e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.406e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.041e+02, tolerance: 3.346e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ]))\n\n\n\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9554916123291357, 0.8733838672032972)\n\n\n\nLasso로 적합한 결과가 더 스코어가 높긴 하지만, 트리 모형은 직관적이고 단순하다. 그리고 하이퍼파라미터도 바꿀 수 있잖아?\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 5)\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9435995918442013, 0.853062350997775)"
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#b.-평가",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#b.-평가",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### B. 평가",
    "text": "### B. 평가\n\nLasso가 좀 더 좋긴 한데, 의사결정나무도 공선성이 있는 상황에서 간단하게 사용가능하다.\n\n\nLasso는 엄청 발전된 모델이고, 의사결정나무는 아주 초기모델이라… 개선의 여지가 많다."
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#오버피팅",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#오버피팅",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "3. 오버피팅",
    "text": "3. 오버피팅\n\nA. 사전작업\n\n\n종속변수와 관련이 없는 변수들을 무작위로 생성하는 함수\n\n\ndef generating_df(n_balance):\n    global df\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\n## 물론 해당 함수에 df를 따로 입력하게 하여 length를 조절한 후 아무 df에 사용가능하도록 만들수도 있다.\ndef random_generation_df(df, n) :\n    df_random = pd.DataFrame(np.random.randn(len(df), n), columns = ['random'+str(i) for i in range(n)])\n    return pd.concat([df, df_random], axis = 1)\n\n\ngenerating_df(1)\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\n\n\n\n\n0\n135\n0.051535\n0\n0.039391\n\n\n1\n935\n0.355496\n0\n0.616426\n\n\n2\n485\n2.228435\n0\n0.985876\n\n\n3\n65\n1.179701\n0\n1.677899\n\n\n4\n445\n3.962356\n1\n-0.167288\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.936697\n\n\n496\n310\n2.601212\n1\n-0.184044\n\n\n497\n225\n0.042323\n0\n-0.805794\n\n\n498\n320\n1.041416\n0\n-0.483178\n\n\n499\n375\n3.626883\n1\n0.717244\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\nX = df_train.drop('employment', axis = 1)\ny = df_train.employment\nXX = df_test.drop('employment', axis = 1)\nyy = df_test.employment  ## 실제론 모름"
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#b.-분석",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#b.-분석",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### B. 분석",
    "text": "### B. 분석\n# 1 : 의사결정나무\n\n## 2\npredictr = sklearn.tree.DecisionTreeClassifier()  ## 범주형일 때 더 유용\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.82)\n\n\n\n오버피팅된 상황이다.\n\n# 2 : Lasso(L1 penalty)\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(penalty = 'l1', solver = 'liblinear')\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(employment_hat = predictr.predict(X))\ndf_test = df_test.assign(employment_hat = predictr.predict(XX))\n\n##-##\nprint('train score = ' + str(round(predictr.score(X, y), 4)))\nprint('test score = ' + str(round(predictr.score(XX, yy), 4)))\n\ntrain score = 0.8829\ntest score = 0.8733\n\n\n\n~역시 Lasso가 최고다~\n\n# 3 : Ridge(L2 penalty)\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(penalty = 'l2')\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(employment_hat = predictr.predict(X))\ndf_test = df_test.assign(employment_hat = predictr.predict(XX))\n\n##-##\nprint('train score = ' + str(round(predictr.score(X, y), 4)))\nprint('test score  = ' + str(round(predictr.score(XX, yy), 4)))\n\ntrain score = 0.8886\ntest score  = 0.88\n\n\n\n~Ridge도 최고다~\n\n\nC. 설명변수들의 증가\n\n- 관련없는 변수들의 수가 커짐에 따라서 각 방법들의 train/test score는 어떻게 변화할까?\n\n데이터프레임과 predictor, 반응변수 열 이름을 넣어주면 fitting하고, 스코어를 배출하는 함수\n\n\ndef fitting_df(df, predictor, response) :\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n    X = df_train.drop(response, axis = 1)\n    y = df_train[response]\n    XX = df_test.drop(response, axis = 1)\n    yy = df_test[response]\n\n    predictor.fit(X, y)\n\n    return predictor.score(X, y), predictor.score(XX, yy)\n\n\npredictor의 리스트와 필요없는 설명변수들이 가득찬 열의 개수를 정의\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(),\n             sklearn.linear_model.LogisticRegressionCV(penalty = 'l1', solver = 'liblinear'),\n             sklearn.linear_model.LogisticRegressionCV(penalty = 'l2')]\n\nn_balance_list = range(0, 5000, 50)\n\n\n그것들을 기반으로 세 변수들에게서 피팅하면서 나온 점수들을 원소로 하는 리스트를 컴프리헨션\n\n\nlst = [fitting_df(generating_df(n_balance), predictr, 'employment') for predictr, n_balance in [(predictr, n_balance) for n_balance in n_balance_list for predictr in predictrs]]\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nKeyboardInterrupt: \n\n\n\n좀 오래 걸릴 수밖에 없음…(이중으로 컴프리헨션 하는 게 훨씬 적합한 것 같긴 하다. 나중에 처리를 또 해야되니…)\n\n- 실험결과의 정리\n\narr = np.array(lst)\ntr = arr[:, :, 0]\ntst = arr[:, :, 1]\narr.shape\n\n\nar = np.array(lst)\narr = arr.reshape(100, 3, 2)\ntr = arr[:, :, 0]\ntst = arr[:, :, 1]\n\n\n전반적으로 관련없는 변수가 많아질수록 스코어가 떨어지기는 하는데, 뒤로 갈수록 tree의 점수는 다른 것들보다 감소폭이 적어 역전되는 상황이다.\n\n- 이를 시각화해보자.\n\npd.DataFrame(tr, columns = ['tree', 'lasso', 'ridge'])\n_.assign(dataset = 'train')\n\n\ntr_score = pd.DataFrame(tr, columns = ['tree', 'lasso', 'ridge'])\ntst_score = pd.DataFrame(tst, colunms = ['tree', 'lasso', 'ridge'])\n\nresult_df = pd.concat([tr_score, tst_score], axis = 0)"
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#이상치",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#이상치",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "4. 이상치",
    "text": "4. 이상치"
  },
  {
    "objectID": "2023_MP/practice/9 의사결정나무의 활용.html#a.-데이터",
    "href": "2023_MP/practice/9 의사결정나무의 활용.html#a.-데이터",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### A. 데이터",
    "text": "### A. 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.show()\n\n\n\n\n\n나머지 자료들은 모두 선형을 띄고 있는데, (-4.1, 200)이라는 이상치 하나가 모형의 설명을 어렵도록 만들고 있다.\n\n\nB. 분석\n\n# 1 일반적인 선형 회귀\n\n## 1\nX = df_train[['temp']]\ny = df_train.ice_sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(ice_sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.plot(df_train.temp, df_train.ice_sales_hat, '--', color = 'red')\nplt.show()\n\n\n\n\n\npredictr.coef_\n\narray([-0.64479089])\n\n\n\n기울기가 음수이다.\n- 늘 하던 것처럼 선형회귀로 적합했을 때, 해당 모형은 완전히 언더피팅된 것으로 보여진다…\n\n# 2 의사결정나무를 사용\n(주의!) Lasso와 Ridge는 공선성이 있는 모델에서만 잘 작동할 뿐, 그렇지 않은 경우 선형회귀와 유사하게 적합되니 사용하지 말것!(Ridge는 설명변수가 적을 때 특히 똑같게 적합하는 것 같다.)\n\n## 1\nX = df_train[['temp']]\ny = df_train.ice_sales\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(ice_sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.plot(df_train.temp, df_train.ice_sales_hat, '--', color = 'red')\nplt.show()\n\n\n\n\n\npredictr.score(X, y)\n\n0.9992029367488545\n\n\n\n_df = df_train.drop(0, axis = 0)\nplt.plot(_df.temp, _df.ice_sales, 'o', label = 'observations')\nplt.plot(_df.temp, _df.ice_sales_hat, '--', color = 'red', label = 'prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nDecisionTreeRegressor의 경우 언더라잉만 적합하는 것을 넘어 오버피팅이 되게 하나, 결과는 언더피팅이 되는 경우보단 나름 합리적이다.\n적합에 관여한 구간 외의 값이 인풋으로 들어오면 해당 모형은 예측하지 못한다."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html",
    "title": "다중공선성의 해소",
    "section": "",
    "text": "Ridge와 Lasso를 통해 다중공선성을 극복해보자."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#라이브러리-imports",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#라이브러리-imports",
    "title": "다중공선성의 해소",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#ridge-l2-penalty",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#ridge-l2-penalty",
    "title": "다중공선성의 해소",
    "section": "2. Ridge : L2-penalty",
    "text": "2. Ridge : L2-penalty\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n위와 같은 데이터에서 toeic0~toeic499는 설명변수 간 상관관계가 높은 녀석들이다.\n\n\nA. True World\n\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic']\nyy = df_test[['employment_score']]\n## step2 \npredictr = sklearn.linear_model.LinearRegression()\n## step3\npredictr.fit(X,y)\n## step4 : pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(f'train_score:\\t{predictr.score(X,y):.4f}')\nprint(f'test_score:\\t{predictr.score(XX,yy):.4f}')\n\ntrain_score:    0.9133\ntest_score: 0.9127\n\n\n- 언더라잉만 잘 적합한 결과, 오차항 때문에 1.0은 나오기 힘듦\n\n이 점수는 현실적으로 달성하기 어려워…"
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#b.-무지성",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#b.-무지성",
    "title": "다중공선성의 해소",
    "section": "### B. 무지성…",
    "text": "### B. 무지성…\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.drop(['employment_score'], axis = 1)\ny = df_train[['employment_score']]\nXX = df_test.drop(['employment_score'], axis = 1)\nyy = df_test[['employment_score']]\n## step2 \npredictr = sklearn.linear_model.LinearRegression()\n## step3\npredictr.fit(X,y)\n## step4 : pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\n명백한 오버피팅…\n\n\nC. Ridge\n\n- 통계학자 : 이럴경우 Ridge를 사용하면 됩니다…\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge()  ## 로지스틱의 경우 LogisticRegressionCV(penalty = 'l2')를 사용 가능\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1173\n\n\n\n??? 안되는데요?\n\n- 하이퍼 파라미터를 튜닝하면 됩니다…\n\n## step1 --- 다넣음\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)  ## alpha = 500000000.\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\n오라클에 비할 바는 아니긴 한데, 공선성이 있는 경우라도 적절한 alpha를 고른다면 망하지는 않음."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#d.-ridge의-작동원리",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#d.-ridge의-작동원리",
    "title": "다중공선성의 해소",
    "section": "### D. Ridge의 작동원리",
    "text": "### D. Ridge의 작동원리\n- 정확한 설명…\nSVD를 이용하여 이론적으로 계산하면, sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 가 항상 존재함을 증명할 수 있음…\n\n그렇다네요.\n\n- 직관적 설명(엄밀하지 않은 설명)\n\nLinearRegression은 왜 망했지???\n\n\n취업 자료의 예제를 보면 토익 성적의 계수는 실제로 0.01이다. 적당히… * toeic_coef+toeic0_coef+…+toeic499_coef \\(\\approx\\) 0.01이라면 대충 맞는 답이다.\n\n\n근데 사실 이 0.01이라는 값은 몇 개의 계수만 있어도 만들 순 있을거임… -&gt; 나머지 설명변수가 모두 불필요한 특징이 됨.\n\n\n그래가지고 불필요한 특징은 다중공선성의 문제 때문에 오버피팅을 유발한다.\n\n그래서 Ridge는 몇 개의 계수만 빼고 나머지들이 쓸모없는 게 되지 않도록, 다 유의미하도록 계수에 패널티를 부여한다.\n\nE. \\(\\alpha\\)에 따른 계수값 변화\n\n- 여러 개의 predictor를 alpha의 값을 달리하며 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## 아래에서 배울 RidgeCV에서 이 값들 중 어느 값이 가장 좋을 지 결정하게 할 수 있음\n\n## step3 \nfor predictr in predictrs:  ## 이건 리스트로 만드는 게 아니니까...\n    predictr.fit(X,y)\n## step4 -- pass \n\n\nplt.plot(predictrs[0].coef_[1:], label = r'$\\alpha$ = {}'.format(predictrs[0].alpha))\nplt.plot(predictrs[2].coef_[1:], label = r'$\\alpha$ = {}'.format(predictrs[2].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nalpha의 값이 작을수록, 그 변동 폭이 줄어듦을 알 수 있다.\n- 마지막 predictor의 계수값을 살펴보면…\n\n\ns = pd.Series(predictrs[-1].coef_)\ns.set_axis(X.columns, axis = 0)\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음(한두개로 계수 0.01을 만들 수 없음)\n모든 변수는 대량 2e-5(\\(\\approx\\frac{1}{100}\\frac{1}{501}\\))정도 똑같이 중요하다고 생각된다.\n살짝 (\\(\\frac{1}{100}\\frac{1}{501}\\))보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아니다.\n\n\n[predictr.coef_[1:].sum() for predictr in predictrs]\n\n[0.010274546089787007,\n 0.010157633994689774,\n 0.009948779293105905,\n 0.009866050921714562,\n 0.009854882844936588,\n 0.009820059959693872,\n 0.00949099901512329]\n\n\n\n갈수록 합의 크기가 작아짐…\n\n\n1/100*1/501\n\n1.9960079840319362e-05\n\n\n\n게대가 본래 기대될 회귀계수의 값보다 전체적으로 조금씩 낮은 편"
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#f.-alpha-정리",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#f.-alpha-정리",
    "title": "다중공선성의 해소",
    "section": "### F. \\(\\alpha\\) 정리",
    "text": "### F. \\(\\alpha\\) 정리\n- L2-penalty는 대충 분산같은 것…\n\nx = np.random.randn(5)\nL2_penalty = (x**2).sum()  ## 제곱합, 평균에서 멀어진...\n(L2_penalty, 5*(x.var() + (x.mean()**2)))  ## 2차 적률인듯. E(X**2)\n\n(10.591975556137934, 10.591975556137934)\n\n\n\nfor predictr in predictrs :\n    print(\n        f'alpha={predictr.alpha:.0e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}')\n\nalpha=5e+02 l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5e+03 l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5e+04 l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5e+05 l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5e+06 l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5e+07 l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5e+08 l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n\nalpha의 값이 늘어날수록, penalty의 값이 규모가 작아진다. 그에따라 계수들의총합도 점점 낮아진다…\n게다가 test_score도 어느순간부터 낮아지기 시작한다…"
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#ridgecv",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#ridgecv",
    "title": "다중공선성의 해소",
    "section": "3. RidgeCV",
    "text": "3. RidgeCV\n- 입력한 alpha값들 중에서 가장 적절한 alpha값을 제시해준다.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()  ## 일단 alpha를 지정해주지 않는 모습...\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\nprint(f'df_train score : {predictr.score(X, y):.5f}')\nprint(f'df_test score : {predictr.score(XX, yy):.5f}')\n\ndf_train score : 1.00000\ndf_test score : 0.11915\n\n\n\n아직 overfitting된 모습…\n\n왜냐! alphas의 후보는 0.1, 1.0, 10.0이 디폴트니까…\n- 따라서 이 후보를 직접 넣어주자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 -- 여기서 alpha의 후보들을 alphas에 리스트로 지정해준다.\npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8])\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\n(predictr.score(X, y), predictr.score(XX, yy))\n\n(0.7521268560159359, 0.7450309251010893)\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n\nalpha를 5,000,000로 설정했더니 가장 좋은 결과가 나왔다는 것을 알 수 있다."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#lasso",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#lasso",
    "title": "다중공선성의 해소",
    "section": "4. Lasso",
    "text": "4. Lasso\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\nA. Lasso를 이용한 분석\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\npredictr = sklearn.linear_model.Lasso()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n(0.8600312387900632, 0.8306176063318933)\n\n\n\nprint(f'train_score:\\t {predictr.score(X,y):.4f}')\nprint(f'test_score:\\t {predictr.score(XX,yy):.4f}')\n\ntrain_score:     0.8600\ntest_score:  0.8306\n\n\n\nalpha를 default로 두었음에도 굉장히 우수한 결과가 나왔다."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#b.-lasso의-원리",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#b.-lasso의-원리",
    "title": "다중공선성의 해소",
    "section": "### B. Lasso의 원리",
    "text": "### B. Lasso의 원리\n- 정확한 설명\n\n지금 이해하기엔 어려움…\n\n- 상관성이 짙은 설명변수 몇개로만 그 합의 계수를 만들게 해서는 안된다.\n\n아주 적은 숫자의 coef만 살려두고, 나머지는 0으로 강제한다.\n계수가 0이라는 것은 해당 변수를 제거한 것과 같은 효과를 가진다.\n\n\nplt.plot(predictr.coef_[1:])\n\n\n\n\n\n실제로 계수값이 0인 녀석이 많음을 알 수 있다.\n\n\nC. \\(\\alpha\\)의 값에 따른 변화\n\n- 여러 개의 predictor를 학습시켜 계수값들의 변화를 관찰해보자.\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\nalphas = np.linspace(0.1, 2, 20)\npredictrs = [sklearn.linear_model.Lasso(alpha = alpha) for alpha in alphas]\n\n## 3\nfor predictr in predictrs:\n    predictr.fit(X, y)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.115e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.991e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.375e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.588e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.730e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.671e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.117e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.875e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.192e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n\nplt.plot(predictrs[0].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[9].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[9].alpha.round(5)))\nplt.plot(predictrs[-1].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\n계수값들의 분산이 갈수록 작아지는 것을 느낄 수 있다.\n\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[9].alpha:.4f}\\tsum(toeic_coef)={predictrs[9].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.1000    sum(toeic_coef)=0.010169320378140704\nalpha=1.0000    sum(toeic_coef)=0.009987870459109604\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n\npredictor들의 toeic 계수 합은 여전히 0.01 근처….\n\n\nplt.plot([(predictr.coef_ != 0).sum() for predictr in predictrs])\n\n\n\n\n\nalpha값이 커질수록 0이 아닌 계수의 갯수가 줄어드는 것을 볼 수 있다."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#d.-lassocvcross-validation",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#d.-lassocvcross-validation",
    "title": "다중공선성의 해소",
    "section": "### D. LassoCV(Cross Validation)",
    "text": "### D. LassoCV(Cross Validation)\n- 가장 적합한 \\(\\alpha\\)값을 자동으로 찾아준다.\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\npredictr = sklearn.linear_model.LassoCV(alphas = np.linspace(0.1, 2, 20))\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.989e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.633e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.252e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.352e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.306e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.798e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.872e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.992e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.627e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.897e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.514e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.461e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.234e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.637e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.876e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.340e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.293e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.578e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.930e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.021e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.646e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.779e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.310e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.075e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.837e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.093e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.277e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.617e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.724e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.746e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.480e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.328e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.987e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.132e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.659e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.356e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.074e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.031e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.921e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.669e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.082e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.384e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.782e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.134e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.001e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.107e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.057e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.464e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.704e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.481e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.384e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.173e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.835e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.883e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.713e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n0.9555099850022306\n\n\n\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9555099850022306, 0.8756348559919926)\n\n\n\n살짝 과적합된 면이 있으나, 그래도 상당히 높은 수치이다."
  },
  {
    "objectID": "2023_MP/practice/7. 다중공선성의 해소.html#coef를-0으로-만드는-수학적-장치",
    "href": "2023_MP/practice/7. 다중공선성의 해소.html#coef를-0으로-만드는-수학적-장치",
    "title": "다중공선성의 해소",
    "section": "5. coef를 0으로 만드는 수학적 장치",
    "text": "5. coef를 0으로 만드는 수학적 장치\n\nRidge : L2-penalty\n\ncoef의 값들을 가중치에 따라 분할하는 수학적 장치.\n\n\n\n패널티 : 상관성이 짙은 설명변수들의 계수값을 제곱한 뒤 합치고(L2-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여.\n\n\nLasso : L1-penalty\n\n다수의 coef 값들을 0으로 만드는 수학적 장치\n\n\n\n패널티 : 상관성이 짙은 설명변수들의 계수값의 절대값을 구한 뒤에 합치고(L1-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여."
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "",
    "text": "sklearn.preprocessing을 이용하여 자료의 범위를 전처리해보자."
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#라이브러리-import",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#라이브러리-import",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.preprocessing"
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#minmaxscaler",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#minmaxscaler",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "2. MinMaxScaler",
    "text": "2. MinMaxScaler\n\nA. 모티브\n\n- 예제자료 : 학점, 토익 등이 취업에 미치는 정도\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과… (sklearn.linear_model.Linear_Regression())\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n그래서… * 토익이 중요해? 아니면 학점이 중요해? * 무엇이 얼만큼 중요해?\n- 모티브 : 토익과 gpa 모두 0~1 사이의 척도로 바꾸면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#b.-사용방법",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#b.-사용방법",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "### B. 사용방법",
    "text": "### B. 사용방법\n\nclass를 이용, object를 생성하는 방법(이전과 유사한 방법)\n\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit(df)\n\nscalr.transform(df)  ## 전처리의 경우에는 transform을 사용한다. .impute.SimpleImputer()에서도 그랬잖아?\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n역시 한번에 할 수도 있다.\n\n\nscalr.fit_transform(df)  ## 당연히 원래 자료를 훼손하진 않는다.\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\nsklearn.preprocessing.minmax_scale(df)  ## 한 번에 할 수도 있다.\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n위처럼 할 수도 있는데, 이 경우는 scalr를 test셋에 적용시킬 수 없기 때문에 사용하지 않는다.\n\n\nC. 옳고 그른 방법론\n\n# 1 비효율적인 전환\n- 주어진 자료가 아래와 같이 train/test로 나뉘어있다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 5.0]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 3.5]).reshape(-1,1)\n\nX, XX\n\n(array([[1.],\n        [2.],\n        [3.],\n        [4.],\n        [5.]]),\n array([[1.5],\n        [2.5],\n        [3.5]]))\n\n\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit_transform(X), scalr.fit_transform(XX)\n\n(array([[0.  ],\n        [0.25],\n        [0.5 ],\n        [0.75],\n        [1.  ]]),\n array([[0. ],\n        [0.5],\n        [1. ]]))\n\n\n\n같은 값임에도 다르게 스케일을 변환시키는 것을 볼 수 있다.(X에선 5가 1인데, XX에선 3.5가 1이 됨.\n\n# 2 권장하는 스케일링 방법\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit(X)\n\nscalr.transform(X), scalr.transform(XX)\n\n(array([[0.  ],\n        [0.25],\n        [0.5 ],\n        [0.75],\n        [1.  ]]),\n array([[0.125],\n        [0.375],\n        [0.625]]))\n\n\n\n더 합리적이다.\n\n# 3 변환값의 범위\n- 변환한 값이 무조건 0과 1 사이가 되는 것은 아니다.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n## XX의 5.0은 X에서의 최대값인 4.0을 초과한다.\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X)\n\nsclr.transform(X), sclr.transform(XX)\n\n(array([[0.        ],\n        [0.33333333],\n        [0.66666667],\n        [1.        ],\n        [0.83333333]]),\n array([[0.16666667],\n        [0.5       ],\n        [1.33333333]]))\n\n\n\n스케일링한 값이 1보다 커질 수 있다."
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#d.-아주아주-잘못된-스케일링-방법---정보누수",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#d.-아주아주-잘못된-스케일링-방법---정보누수",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "### D. 아주아주 잘못된 스케일링 방법 - 정보누수",
    "text": "### D. 아주아주 잘못된 스케일링 방법 - 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- train data와 test data를 합친다….????!!!??!?!??\n\nnp.concatenate([X, XX], axis = 0)\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친 데이터에서 스케일링….\n\nsklearn.preprocessing.MinMaxScaler().fit_transform(np.concatenate([X, XX], axis = 0))\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625],\n       [0.125],\n       [0.375],\n       [1.   ]])\n\n\n\n이렇게 전저리하는 것은 정보누수에 해당한다. 본래 test dataset은 알지 못한 상태인데 그것을 합칠 순 없다!\n대회에서 이런 일이 발생하면 cheating으로 간주되어 탈락된다.\n\n\n위에서 minmax_scale()로 처리하는 것은 전략적으로 비효율적인 문제이지 치팅과 관련된 치명적인 문제가 아니다. (만약 어떠한 경우에 minmax_scale 전처리 방식이 유리하다는 생각이 들면 사용해도 무방함)"
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#standardscaler",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#standardscaler",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "3. StandardScaler",
    "text": "3. StandardScaler\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n\n여기서 토익과 gpa가 미치는 영향을 비교하기 위해 각 값들을 표준화해보자.\n\n\nA. 사용법\n\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n\nMinMaxScaler도 마찬가지로 여러 열을 한번에 할 수 있다.\n\n- 원리\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식, 자유도는 0(모분산으로 취급)\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\n그냥 표준화하는 것"
  },
  {
    "objectID": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#비교",
    "href": "2023_MP/practice/5. 연속형 자료의 범위 조정.html#비교",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "4. 비교",
    "text": "4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxSclaer:\n\n장점 : 원하는 범위 내로 데이터를 조정할 때 유용, 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력값을 조정하는 데 유용.\n단점 : 이상치에 매우 민감하다.\n\nStandardScaler:\n\n장점 : 이상치에 덜 민감함, 많은 통계적 기법들 - 선형 알고리즘에서 잘 작동함\n단점 : 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.\n\n\n\n단순히 MinMaxScaler는 데이터가 0~1 또는 -1~1사이의 범위에 있다고 가정한다.\n\n그래서 둘 중 어느 것을 선택해야 하는데???\n\n둘 중 이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 신경망의 경우 MinMaxScaler가 적합할 수 있다.\n\n결론적으로 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n\n결론"
  },
  {
    "objectID": "2023_MP/practice/3. 결측치.html",
    "href": "2023_MP/practice/3. 결측치.html",
    "title": "결측치의 처리",
    "section": "",
    "text": "결측치를 시각화해보고, 계산해서 대치(impute)해보기도 하자!"
  },
  {
    "objectID": "2023_MP/practice/3. 결측치.html#라이브러리-imports",
    "href": "2023_MP/practice/3. 결측치.html#라이브러리-imports",
    "title": "결측치의 처리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#!pip install missingno # missingno 라이브러리가 설치되어있지 않을 경우\n\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport sklearn.impute"
  },
  {
    "objectID": "2023_MP/practice/3. 결측치.html#missingno의-활용",
    "href": "2023_MP/practice/3. 결측치.html#missingno의-활용",
    "title": "결측치의 처리",
    "section": "2. missingno의 활용",
    "text": "2. missingno의 활용\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/msno.csv\")\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n0.383420\n1.385096\nNaN\n-0.545132\n-0.732395\n\n\n1\n1.084175\n0.080613\n-0.770527\n-0.272143\n-0.749881\n\n\n2\n1.142778\n1.258419\nNaN\n-0.072007\n-0.440757\n\n\n3\n0.307894\n0.521400\n0.446974\n0.329530\n-1.457388\n\n\n4\n0.237787\n0.132401\n-0.516630\n0.177995\n0.416182\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.041092\n-1.308165\n1.085820\n1.136210\nNaN\n\n\n996\n-1.286358\n1.547987\nNaN\n-0.174334\n-0.579486\n\n\n997\n0.710257\n1.764058\nNaN\n-0.353928\nNaN\n\n\n998\n-1.908729\n-0.804691\nNaN\nNaN\n-0.066739\n\n\n999\n0.650026\n2.206549\nNaN\n-0.919945\nNaN\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n결측치가 딱봐도 엄청 많아보인다. missingno는 그것을 시각화해준다.\n\n\nmsno.matrix(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n우측 노이즈와 같은 그래프에서 0에 있는 것은 해당 행에 데이터가 하나도 없다는 뜻이고, 5에 있는 것은 다섯개의 데이터가 해당 행에 존재한다는 것이다. 데이터셋이 다섯개니까 그 합이 그래프로 표기된다.\n\n\nmsno.heatmap(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nmsno.dendrogram(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n구조가 비슷한 자료들을 엮어놓는다.\n\n그럼 시각화를 했으니까, 이제 결측치를 처리해야겠지?"
  },
  {
    "objectID": "2023_MP/practice/3. 결측치.html#숫자형-자료의-impute결측치를-대체하는-것",
    "href": "2023_MP/practice/3. 결측치.html#숫자형-자료의-impute결측치를-대체하는-것",
    "title": "결측치의 처리",
    "section": "3. 숫자형 자료의 impute(결측치를 대체하는 것)",
    "text": "3. 숫자형 자료의 impute(결측치를 대체하는 것)\n- 주어진 자료\n\nA = [2.1, 1.9, 2.2, np.nan, 1.9]\nB = [0, 0, np.nan, 0, 0]\n\n\ndf = pd.DataFrame({'A' : A, 'B' : B})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\n결측치를 무엇으로 채워주면 좋을까?\n\n\n일단 평균으로 해보면 얼추 맞을 것 같다.\n\n\ndf2 = df\ndf2.loc[3, 'A'] = df2.A.mean()  ## mean과 같은 메소드는 결측치를 반영하지 않는다.\ndf2.loc[2, 'B'] = df2.B.mean()\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 근데 이게 엄청 많으면 언제 다 일일히 하고 있어? &gt; 자동으로 하려면?\n(방법1) | 평균으로 impute\n\nimputr = sklearn.impute.SimpleImputer()  ## SimpleImputer(strategy = 'mean')\nimputr\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\npredictr.fit하는 것처럼 결측치가 있는 열에 적합해야 한다.\n\n\nimputr.fit(df)\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\npredictr.predict하는 것처럼 인풋시켜야 한다.\n\n\nimputr.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n\n위에서와 똑같은 결과를 산출했다.\n\n해당 과정은 imputr.fit_transform(df)로 한번에 시행할 수 있다.\n- 만약 평균이 아닌 다른 방식으로 결측치를 대체하고 싶다면…\n(방법 2) | median으로 impute\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'median')\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법 3) | 최빈값으로 대체\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법 4) | 정해진 상수값으로 대체\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 999)\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])"
  },
  {
    "objectID": "2023_MP/practice/3. 결측치.html#범주형-자료의-impute",
    "href": "2023_MP/practice/3. 결측치.html#범주형-자료의-impute",
    "title": "결측치의 처리",
    "section": "4. 범주형 자료의 impute",
    "text": "4. 범주형 자료의 impute\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n(방법 1) | 최빈값을 이용\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\nimputr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법 2) | 상수(지정값)로 대체함\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'G')\nA_ = pd.Series(imputr.fit_transform(df[['A']]).reshape(-1))\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'economy')\nB_ = pd.Series(imputr.fit_transform(df[['B']]).reshape(-1))\n\n\npd.concat([A_, B_], axis = 1).set_axis(['A','B'], axis = 1)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\neconomy\n\n\n3\nY\nstat\n\n\n4\nG\nbio\n\n\n\n\n\n\n\n\n## 또는\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'G')\nA_ = imputr.fit_transform(df[['A']])\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'economy')\nB_ = imputr.fit_transform(df[['B']])\n\npd.DataFrame(np.concatenate([A_,B_], axis = 1), columns = ['A','B'])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\neconomy\n\n\n3\nY\nstat\n\n\n4\nG\nbio\n\n\n\n\n\n\n\n\n일반적으로 연속형ㆍ숫자형 자료에는 평균, 범주형 자료에는 최빈값으로 대체한다."
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "",
    "text": "sklearn의 linear_mode.LinearRegression()을 사용하여 선형회귀분석을 해보자!"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#라이브러리-imports",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#라이브러리-imports",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#data",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#data",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "2. Data",
    "text": "2. Data\n\n전주시의 기온 자료\n\n\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()   ## 자료를 크기 순서대로 정렬, sort_values()와 비슷하달까...\n\n\ntemp\n\narray([-4.1, -3.7, -3. , -1.3, -0.5, -0.3,  0.3,  0.4,  0.4,  0.7,  0.7,\n        0.9,  0.9,  1. ,  1.2,  1.4,  1.4,  1.5,  1.5,  2. ,  2. ,  2. ,\n        2.3,  2.5,  2.5,  2.5,  2.6,  2.6,  2.9,  3.2,  3.5,  3.5,  3.6,\n        3.7,  3.8,  4.2,  4.4,  4.5,  4.5,  4.6,  4.9,  4.9,  4.9,  5. ,\n        5. ,  5.1,  5.6,  5.9,  5.9,  6. ,  6. ,  6.1,  6.1,  6.3,  6.3,\n        6.4,  6.4,  6.5,  6.7,  6.8,  6.8,  7. ,  7. ,  7.1,  7.2,  7.4,\n        7.7,  8. ,  8.1,  8.1,  8.3,  8.4,  8.4,  8.4,  8.5,  8.8,  8.9,\n        9.1,  9.2,  9.3,  9.4,  9.4,  9.5,  9.6,  9.6,  9.7,  9.8,  9.9,\n       10.2, 10.3, 10.6, 10.6, 10.8, 11.2, 12.1, 12.4, 13.4, 14.7, 15. ,\n       15.2])\n\n\n- 아래와 같은 모형을 가정하자. \\[\\textup{아이스크림 판매량}= 20 ＋ \\textup{온도} × 2.5 × \\textup{오차(운)}\\]\n\n더미 모형 생성\n\n\nnp.random.seed(43052)\neps = np.random.randn(100)*3  ## 오차\nicecream_sales = 20 + temp * 2.5 + eps\n\n\nplt.plot(temp, icecream_sales, 'o')\nplt.show()\n\n\n\n\n\n상기 결과를 관측했다고 생각합시다.\n\n\ndf = pd.DataFrame({'temp' : temp, 'sales' : icecream_sales})\ndf\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n0\n-4.1\n10.900261\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n...\n...\n...\n\n\n95\n12.4\n54.926065\n\n\n96\n13.4\n54.716129\n\n\n97\n14.7\n56.194791\n\n\n98\n15.0\n60.666163\n\n\n99\n15.2\n61.561043\n\n\n\n\n100 rows × 2 columns"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#게임세팅",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#게임세팅",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "3. 게임세팅",
    "text": "3. 게임세팅\n- 편의상 아래와 같은 기호를 도입하자.\n\n(df.temp[0], df.temp[1], … , df.temp[99]) = \\((x_1,x_2,\\dots,x_{100})=(-4.1,-3.7,\\dots,15.2)\\)\n(df.sales[0], df.sales[1], … , df.sales[99]) = \\((y_1,y_2,\\dots,y_{100})=(10.90,14.00, \\dots,61.56)\\)\n\n\n이 자료 \\(\\big\\{(x_i,y_i)\\big\\}_{i=1}^{100}\\)를 바탕으로 어떠한 패턴을 발견하여 새로운 \\(x\\)에 대한 예측값을 알고 싶다 : \\(\\hat{y}\\)\n\nA. 질문\n- 기온이 \\(x = -2.0\\)일 때, 아이스크림을 얼마정도 판다고 보는 게 타당할까?\nB. 답 1\n- \\(x = -2.0\\) 근처의 데이터를 살펴보자.\n\ndf[(-4.0 &lt; df.temp) & (0.0 &gt; df.temp)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n5\n-0.3\n20.317853\n\n\n\n\n\n\n\n\n\\(-1.3\\)이 제일 가까운데, 대충 \\(17.67\\) 언저리 아닐까…?\n\n\nA. 산점도와 추세선\n\n- 자료를 바탕으로 그림을 그려보자\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot([-2.0],[17.67],'x')     # 이미 들어가있는 플롯에 점을 하나 찍는다. 마커는 X\n\nplt.show()\n\n\n\n\n\n예상한 것(17.67)보다 못팔 것 같은데…?"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-아이디어",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-아이디어",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 아이디어",
    "text": "### B. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자!~(사실 모형을 우리가 만들었으니 이미 추세선을 알고 있긴 함)~\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.temp, 20+df.temp*2.5, '--')  ## 위에서 직접 설정했던 자료의 관계, 절편이 20이고 기울기가 2.5\n\nplt.show()\n\n\n\n\n- 사실 \\(y = 20 + 2.5x\\)라는 추세선을 이미 알고 있었음.\n- 그래서 \\(x = -2\\)라면 \\(y = 20 - 2.5 × 2 = 15\\)라고 보는 게 합리적임(오차를 고려 안하면)\n\n허나, 실제 상황에서 우리는 \\(20, 2.5\\)라는 숫자를 모른다.\n\n- 게임셋팅 * 원래 게임 : 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임 * 변형된 게임 : \\(20, 2.5\\)라는 숫자를 잘 찾는 게임. 즉, 데이처를 보고 최대한 \\(y_i \\approx ax_i+b\\)가 되도록 \\(a, b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#분석",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "4. 분석",
    "text": "4. 분석\n\n그렇다면 늘 했던 것처럼 네 단계로 분석을 해보자.\n\n\nA. 데이터\n\n\n# step 1 -- data\ntrain = pd.DataFrame({'temp' : temp, 'sales' : icecream_sales})\n\nX = train[['temp']]\ny = train['sales']\n\n\n데이터를 학습해서 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄 수 있는 아이(predictor)를 만들자.~(근데 이정도면 학생이 아니라 노예…)~"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-predictor",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-predictor",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. predictor",
    "text": "### B. predictor\n\n# step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n\nsklearn의 linear_model.LinearRegression()을 사용했다. 이러면 가장 기본적인 선형회귀를 진행한다.(LSE를 쓰는 그거 있잖아…)\n\n\nC. 학습\n\n\n# step 3\npredictr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n학생이 train을 완료했다."
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#d.-예측predict",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#d.-예측predict",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### D. 예측(predict)",
    "text": "### D. 예측(predict)\n- 학생(predictr) : 데이터를 살펴보니 True는 이럴 것 같아요.\n\ny_hat = predictr.predict(X)  ## X값에 해당하는 y_hat값을 예측하여 산출.\n\n\nplt.plot(X, y, 'o', alpha = 0.5)\nplt.plot(X, y_hat, 'o--', alpha = 0.5)\n\nplt.show()\n\n\n\n\n- 그럼 기울기와 절편은 어디에 저장된 걸까?\n- predictr : 여깄음.\n\n(predictr.coef_, predictr.intercept_)\n\n(array([2.51561216]), 19.66713126947925)\n\n\n- 새로운 데이터 \\(x = -2\\)에 대한 예측\n\nfloat(predictr.coef_)*(-2) + float(predictr.intercept_)\n\n14.63590694951262\n\n\n\n해당 결과값을 그래프에 나타내면…\n\n\nX_input = pd.DataFrame({'temp' : [-2.0]})\n\n\nplt.plot(X, y, 'o', alpha = 0.5)\nplt.plot(X, y_hat, '--', alpha = 0.5)\nplt.plot(X_input, predictr.predict(X_input), 'xr')  ## 원래는 리스트나 어레이로 넣어주는 게 정배긴 함\n\nplt.show()\n\n\n\n\n\n예측값이 직선상에 위치함을 알 수 있다."
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#두-타입의-아이스크림초코-바닐라에-대한-회귀분석",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#두-타입의-아이스크림초코-바닐라에-대한-회귀분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "5. 두 타입의 아이스크림(초코 / 바닐라)에 대한 회귀분석",
    "text": "5. 두 타입의 아이스크림(초코 / 바닐라)에 대한 회귀분석\n\n이전의 기온 자료를 바꿔 아래와 같은 모형을 가정해보자.\n\n\nA. Data\n\n\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()   ## 자료를 크기 순서대로 정렬\ntemp  ## 전주시의 기온 100개 자료\n\narray([-4.1, -3.7, -3. , -1.3, -0.5, -0.3,  0.3,  0.4,  0.4,  0.7,  0.7,\n        0.9,  0.9,  1. ,  1.2,  1.4,  1.4,  1.5,  1.5,  2. ,  2. ,  2. ,\n        2.3,  2.5,  2.5,  2.5,  2.6,  2.6,  2.9,  3.2,  3.5,  3.5,  3.6,\n        3.7,  3.8,  4.2,  4.4,  4.5,  4.5,  4.6,  4.9,  4.9,  4.9,  5. ,\n        5. ,  5.1,  5.6,  5.9,  5.9,  6. ,  6. ,  6.1,  6.1,  6.3,  6.3,\n        6.4,  6.4,  6.5,  6.7,  6.8,  6.8,  7. ,  7. ,  7.1,  7.2,  7.4,\n        7.7,  8. ,  8.1,  8.1,  8.3,  8.4,  8.4,  8.4,  8.5,  8.8,  8.9,\n        9.1,  9.2,  9.3,  9.4,  9.4,  9.5,  9.6,  9.6,  9.7,  9.8,  9.9,\n       10.2, 10.3, 10.6, 10.6, 10.8, 11.2, 12.1, 12.4, 13.4, 14.7, 15. ,\n       15.2])\n\n\n- 아래와 같은 모형을 가정하자.\n\\[\\textup{초코 아이스크림 판매량} = 20 + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\\[\\textup{바닐라 아이스크림 판매량} = 40 + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\nnp.random.seed(43052)\nchoco = 20 + temp*2.5 + np.random.randn(100)*3  ## random normal distribution\nvanilla = 40 + temp*2.5 + np.random.randn(100)*3\n\n\nplt.plot(temp, choco, 'o', label = 'choco')\nplt.plot(temp, vanilla, 'o', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n\n우리는 위와 같은 정보를 관측했다고 가정하자.\n\n\ndf1 = pd.DataFrame({'temp' : temp, 'type' : ['choco' for i in range(100)], 'sales' : choco})\ndf2 = pd.DataFrame({'temp' : temp, 'type' : ['vanilla' for i in range(100)], 'sales' : vanilla})\n\ndf = pd.concat([df1, df2], axis = 0).reset_index(drop = True)\ndf\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n-4.1\nchoco\n10.900261\n\n\n1\n-3.7\nchoco\n14.002524\n\n\n2\n-3.0\nchoco\n15.928335\n\n\n3\n-1.3\nchoco\n17.673681\n\n\n4\n-0.5\nchoco\n19.463362\n\n\n...\n...\n...\n...\n\n\n195\n12.4\nvanilla\n68.708075\n\n\n196\n13.4\nvanilla\n75.800464\n\n\n197\n14.7\nvanilla\n79.846568\n\n\n198\n15.0\nvanilla\n78.713140\n\n\n199\n15.2\nvanilla\n77.595252\n\n\n\n\n200 rows × 3 columns"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-분석",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 분석",
    "text": "### B. 분석\n- 언제처럼 늘 그랬던 것처럼…\n\n# step 1\n## X = pd.get_dummies(df).drop(['sales'], axis = 1) ## 이게 제일 범용적이긴 함\nX = df.loc[:, ['temp', 'type']].assign(type = (df.type == 'choco')) ## 직관적으로 쓴 코드, 범주형은 인식을 못한다.\ny = df['sales']\n\n# step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\ndf = df.assign(sales_hat = predictr.predict(X));df\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\nsales_hat\n\n\n\n\n0\n-4.1\nchoco\n10.900261\n9.286731\n\n\n1\n-3.7\nchoco\n14.002524\n10.295689\n\n\n2\n-3.0\nchoco\n15.928335\n12.061366\n\n\n3\n-1.3\nchoco\n17.673681\n16.349439\n\n\n4\n-0.5\nchoco\n19.463362\n18.367355\n\n\n...\n...\n...\n...\n...\n\n\n195\n12.4\nvanilla\n68.708075\n71.446479\n\n\n196\n13.4\nvanilla\n75.800464\n73.968875\n\n\n197\n14.7\nvanilla\n79.846568\n77.247989\n\n\n198\n15.0\nvanilla\n78.713140\n78.004708\n\n\n199\n15.2\nvanilla\n77.595252\n78.509187\n\n\n\n\n200 rows × 4 columns\n\n\n\n- 가장 중요한 시각화까지…\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.loc[df.type == 'choco'].temp, df.loc[df.type == 'choco'].sales_hat, '--', color = 'brown', label = 'choco')\nplt.plot(df.loc[df.type == 'vanilla'].temp, df.loc[df.type == 'vanilla'].sales_hat, '--', color = 'yellow', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n\n별다른 뜻 없이 (초코, 바닐라)에 (1, 0)을 넣었는데, 어떻게 뭐가 나오긴 했다.\n\n\n어케했음???\n\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([  2.52239574, -20.54021854]), 40.16877158069265)\n\n\n\ncoef_(기울기)가 2개지요.\n\n온도와 범주형 자료인 아이스크림 종류에 따라 기울기가 다르다. 온도 1도가 변할때마다 판매량은 2.52239574가 변하고, 아이스크림 종류가 1 변할때마다(0에서 1이니까 바닐라에서 초코로 바뀜) -20.54를 곱한 수를 더하여 수식을 설명하였다.\n예측\n- 온도가 \\(-2\\)이고, type이 vanilla(0)라면 예측값은?\n\nXnew = pd.DataFrame({'temp' : [-2], 'type' : [0]})\n\npredictr.predict(Xnew)\n\narray([35.1239801])\n\n\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.loc[df.type == 'choco'].temp, df.loc[df.type == 'choco'].sales_hat, '--', color = 'brown', label = 'choco')\nplt.plot(df.loc[df.type == 'vanilla'].temp, df.loc[df.type == 'vanilla'].sales_hat, '--', color = 'yellow', label = 'vanilla')\nplt.plot(Xnew.temp, predictr.predict(Xnew), 'or', label = 'prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nC. 데이터 전처리\n\n- 아까 pd.get_dummies()를 잠시 본 것 같은데, 이걸 어떻게, 왜 써야 하는 지 알아보자.\n\nX = df[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = pd.get_dummies(X);X\n\n\n\n\n\n\n\n\ntemp\ntype_choco\ntype_vanilla\n\n\n\n\n0\n-4.1\nTrue\nFalse\n\n\n1\n-3.7\nTrue\nFalse\n\n\n2\n-3.0\nTrue\nFalse\n\n\n3\n-1.3\nTrue\nFalse\n\n\n4\n-0.5\nTrue\nFalse\n\n\n...\n...\n...\n...\n\n\n195\n12.4\nFalse\nTrue\n\n\n196\n13.4\nFalse\nTrue\n\n\n197\n14.7\nFalse\nTrue\n\n\n198\n15.0\nFalse\nTrue\n\n\n199\n15.2\nFalse\nTrue\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n원-핫 인코딩 : 표현하고 싶은 단어에는 1을, 그것이 아닌 것에는 0을 부여\n\n- LinearRegression 모델의 경우 범주형 자료를 자동으로 인식하지 못한다. 따라서 구분할 범주형 변수가 많다면, pd.get_dummies()를 통해 범주를 나눠주어야 한다."
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#d.-모형의-평가",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#d.-모형의-평가",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### D. 모형의 평가",
    "text": "### D. 모형의 평가\n- 단순선형회귀분석의 경우 모형을 \\(R^2\\)(결정계수)로 평가한다.\n\n다만 이것이 높다고 해서 무조건적으로 좋은 건 아니고, 명확한 기준도 없다. 모형 간 상대적인 좋음을 비교하는 것 뿐이다.\n\n- LogisticRegression에서는 적중률로 딱 떨어지게 점수를 내줄 수 있겠지만, 이건 그렇게 해버리면 0점이 나와버리겠지…"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#설명변수가-많을-때",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#설명변수가-많을-때",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "6. 설명변수가 많을 때",
    "text": "6. 설명변수가 많을 때\n- kaggle에서 “Medical Cose Personal Datasets”을 다운로드\n\nhttps://www.kaggle.com/datasets/mirichoi0218/insurance\n\n\ndf = pd.read_csv(\".\\data\\insurance.csv\")\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\nA. 분석\n\n\n열 이름을 먼저 알아보자.\n\n\nset(df.columns)\n\n{'age', 'bmi', 'charges', 'children', 'region', 'sex', 'smoker'}\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\n- 대충 여러가지 범주형ㆍ연속형 설명변수들과 보험료의 관계를 요약하고 싶다고 하자.\n\n먼저 범주형 자료(sex, smoker, region)들을 원-핫 인코딩 해주자.\n\n\nX = pd.get_dummies(df.drop(['charges'], axis = 1))\ny = df.charges\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n- 그럼 뭐 늘 하던대로…\n\n# 2\npredictr = sklearn.linear_model.LinearRegression()\n\n# 3\npredictr.fit(X, y)\n\n# 4\ndf = df.assign(y_hat = predictr.predict(X))\n\n\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\ny_hat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns\n\n\n\n- charge와 y_hat이 잘 안맞는 것 같은데…?"
  },
  {
    "objectID": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-평가",
    "href": "2023_MP/practice/1. 회귀분석_아이스크림.html#b.-평가",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 평가",
    "text": "### B. 평가\n\npredictr.score(X, y)\n\n0.7509130345985205\n\n\n\n\\(R^2 = \\frac{SSR}{SST} = 0.7509130345985205\\)\n0.7 이상이면 망한 모형까진 아니지만…\n\n계수 해석\n- 상수항\n\npredictr.intercept_\n\n-666.9377199366372\n\n\n\n기본적인 보험료(다른 모든 것이 0일 때)는 -666이다.~(딱봐도 이상하죠? 그래서 별로 의미는 없다.)~\n\n- 계수\n\npd.DataFrame({'columns' : X.columns, 'coef' : predictr.coef_})\n\n\n\n\n\n\n\n\ncolumns\ncoef\n\n\n\n\n0\nage\n256.856353\n\n\n1\nbmi\n339.193454\n\n\n2\nchildren\n475.500545\n\n\n3\nsex_female\n65.657180\n\n\n4\nsex_male\n-65.657180\n\n\n5\nsmoker_no\n-11924.267271\n\n\n6\nsmoker_yes\n11924.267271\n\n\n7\nregion_northeast\n587.009235\n\n\n8\nregion_northwest\n234.045336\n\n\n9\nregion_southeast\n-448.012814\n\n\n10\nregion_southwest\n-373.041756\n\n\n\n\n\n\n\n\n연속형 : 나이, bmi, 자녀의 수가 많을수록 보험료는 올라갔다.\n범주형 : 여성, 흡연자의 경우 보험료가 더 비쌌다.\n지역은 잘 모르겠으나, 나머지는 꽤 그럴듯해 보인다.(지역에 대한 정보는 알기 어려움…)"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "",
    "text": "주어진 자료에서 입학년도를 추가하고 싶다면 어떻게 해야 할까?"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html#사전작업",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html#사전작업",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 설치\n\n\nimport numpy as np\nimport pandas as pd\n\n\n자료 받아오기 및 확인\n\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = [ '2023-12362', '2022-12471', '2023-12333', '2022-12400', '2022-12377',\n               '2022-12469', '2023-12314', '2022-12363', '2023-12445', '2023-12336',\n               '2023-12426', '2022-12380', '2023-12422', '2022-12488', '2022-12370',\n               '2023-12443', '2022-12463', '2023-12491', '2023-12340', '2022-12312' ]\ndf = pd.DataFrame({'student_id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf.head()\n\n\n\n\n\n\n\n\nstudent_id\natt\nrep\nmid\nfin\n\n\n\n\n0\n2023-12362\n65\n55\n50\n40\n\n\n1\n2022-12471\n95\n100\n50\n80\n\n\n2\n2023-12333\n65\n90\n60\n30\n\n\n3\n2022-12400\n55\n80\n75\n80\n\n\n4\n2022-12377\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n학번(student_id)에서 앞 네자리에 해당하는 숫자를 빼내어 새로운 열로 저장하면 좋을 것 같다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html#가공",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html#가공",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "2. 가공",
    "text": "2. 가공\n\n아래의 코드는 student_id 열을 '-'를 기준으로 앞뒤로 나누고 첫번째 것을 취한다. 숫자형으로 바꾼 뒤, 리스트로 산출한다.\n\n\n[int(i.split('-')[0]) for i in df.student_id]\n\n[2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2022]\n\n\n\nlambda를 이용해 가공할 수도 있다.\n\n\nlist(map((lambda x : int(x.split('-')[0])), df.student_id))\n\n[2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2022]\n\n\n\n첫번째 코드와 똑같은 결과를 산출한다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html#출력",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html#출력",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "3. 출력",
    "text": "3. 출력\n\n상기의 코드를 df에 새로운 열 year에 삽입한다.\n\n\ndf.assign(year = [int(i.split('-')[0]) for i in df.student_id])\n\n\n\n\n\n\n\n\nstudent_id\natt\nrep\nmid\nfin\nyear\n\n\n\n\n0\n2023-12362\n65\n55\n50\n40\n2023\n\n\n1\n2022-12471\n95\n100\n50\n80\n2022\n\n\n2\n2023-12333\n65\n90\n60\n30\n2023\n\n\n3\n2022-12400\n55\n80\n75\n80\n2022\n\n\n4\n2022-12377\n80\n30\n30\n100\n2022\n\n\n5\n2022-12469\n75\n40\n100\n15\n2022\n\n\n6\n2023-12314\n65\n45\n45\n90\n2023\n\n\n7\n2022-12363\n60\n60\n25\n0\n2022\n\n\n8\n2023-12445\n95\n65\n20\n10\n2023\n\n\n9\n2023-12336\n90\n80\n80\n20\n2023\n\n\n10\n2023-12426\n55\n75\n35\n25\n2023\n\n\n11\n2022-12380\n95\n95\n45\n0\n2022\n\n\n12\n2023-12422\n95\n55\n15\n35\n2023\n\n\n13\n2022-12488\n50\n80\n40\n30\n2022\n\n\n14\n2022-12370\n50\n55\n15\n85\n2022\n\n\n15\n2023-12443\n95\n30\n30\n95\n2023\n\n\n16\n2022-12463\n50\n50\n45\n10\n2022\n\n\n17\n2023-12491\n65\n55\n15\n45\n2023\n\n\n18\n2023-12340\n70\n70\n40\n35\n2023\n\n\n19\n2022-12312\n90\n90\n80\n90\n2022\n\n\n\n\n\n\n\n완료\n-감사합니다-"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html",
    "href": "2023_DV/Review/강신성_1113.html",
    "title": "1. 라이브러리 imports",
    "section": "",
    "text": "folium을 활용해보자.\nimport numpy as np\nimport pandas as pd\n#---#\nimport folium\nimport json\nimport requests\n[new !] : folium, json, requests"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#dictionary-view-vs-copy",
    "href": "2023_DV/Review/강신성_1113.html#dictionary-view-vs-copy",
    "title": "1. 라이브러리 imports",
    "section": "2. Dictionary : view vs copy",
    "text": "2. Dictionary : view vs copy\n- 원하지 않는 코드\n\ndct1 = {'a':1, 'b':2, 'c':3}\ndct1\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\n\ndct2 = dct1\ndct1, dct2\n\n({'a': 1, 'b': 2, 'c': 3}, {'a': 1, 'b': 2, 'c': 3})\n\n\n\ndct2['c'] = 9999\ndct1, dct2\n\n({'a': 1, 'b': 2, 'c': 9999}, {'a': 1, 'b': 2, 'c': 9999})\n\n\n\n둘다 바뀌어 버린다…? (아이디가 똑같기 때문임, 리소스가 저장되는 부분이 똑같음)\n\n\ndct1 = {'a':1, 'b':2, 'c':3}\ndct2 = dct1.copy()\ndct2['c'] = 9999\ndct1, dct2\n\n({'a': 1, 'b': 2, 'c': 3}, {'a': 1, 'b': 2, 'c': 9999})"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#choropleth-map",
    "href": "2023_DV/Review/강신성_1113.html#choropleth-map",
    "title": "1. 라이브러리 imports",
    "section": "3. Choropleth map",
    "text": "3. Choropleth map\n- 코로플레스 맵의 예시\n\n\n대충 정의하면, coropleth = polygon + y라고 볼 수 있다.\n\n\npolygon(다각형 구역의 그림 및 큰 지도그림)은 지리적 구역을 표현\ny(색상)는 해당 지리적 구역에 대응하는 측정값"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#folium-기본",
    "href": "2023_DV/Review/강신성_1113.html#folium-기본",
    "title": "1. 라이브러리 imports",
    "section": "4. folium 기본",
    "text": "4. folium 기본\n- 개념 * Map Object를 생성(fig 생성) * Map Object에 이것저것 추가(geom 추가)\n\nA. folium.Map()\n\n# 예시 1 : 기본\n\nfolium??\n\n\nm = folium.Map()\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n세계지도를 불러온다. GIS가 탑재되어 있는듯.\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [37, 127], zoom_start = 10\n)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nscrollWheelZoom을 False로 설정하여 문서를 보다 스크롤로 줌인-줌아웃 되지 않도록 바꿔줌.\nlocation을 지정하여 위도와 경도 설정 가능\nzoom_start를 지정하여 확대 단계를 지정 가능\n\n- 좌표 정보는 구글맵스에서 지역을 더블클릭하여 따올 수 있다.\n35.846737, 127.129374\n\nfolium.Map(\n    scrollWheelZoom = False,\n    location = [35.846737, 127.129374], zoom_start = 18\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n18이 최대값이고 그 이상 값은 18로 설정된다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#b.-folium.marker",
    "href": "2023_DV/Review/강신성_1113.html#b.-folium.marker",
    "title": "1. 라이브러리 imports",
    "section": "### B. folium.Marker()",
    "text": "### B. folium.Marker()\n# 예제1 : Map에 Marker를 추가\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.846737, 127.129374], zoom_start = 18\n)\n\n\nmarker = folium.Marker(location = [35.846737, 127.129374])\n\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n아직은 마커가 Map인스턴스에 표시되지 않음.\n\n\nmarker.add_to(m)\n\n&lt;folium.map.Marker at 0x7d5591b92680&gt;\n\n\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nMarker에서의 메소드로 추가하였음.\n\n# 예제 2 - 통계학과 대학원생의 산책경로\n\n## 통계학과 대학원생의 산책경로..\n[35.8471, 127.1291]\n[35.8468, 127.1289]\n[35.84635, 127.1291]\n[35.84635, 127.1297]\n[35.8468, 127.12995]\n[35.8474, 127.1300]\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.8468,127.1294],  ## 분수대\n    zoom_start = 18\n)\n\nfolium.Marker(location = [35.8471, 127.1291]).add_to(m)\nfolium.Marker(location = [35.8468, 127.1289]).add_to(m)\nfolium.Marker(location = [35.84635, 127.1291]).add_to(m)\nfolium.Marker(location = [35.84635, 127.1297]).add_to(m)\nfolium.Marker(location = [35.8468, 127.12995]).add_to(m)\nfolium.Marker(location = [35.8474, 127.1300]).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n- 한번에…\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.8468,127.1294],  ## 분수대\n    zoom_start = 18\n)\n\nfolium.Marker(location = [[35.8471, 127.1291],\n  [35.8468, 127.1289],\n  [35.84635, 127.1291],\n  [35.84635, 127.1297],\n  [35.8468, 127.12995],\n  [35.8474, 127.1300]]\n).add_to(m)\n\nValueError: ignored\n\n\n\n이런건 안됨\n\n\nC. folium.Polygon\n\n# 예제 1 - 산책경로를 폴리곤으로 표시(더블리스트)\n\nm = folium.Map(\n    scrollWheelZoom=False,\n    location = [35.8468,127.1294], # 분수대\n    zoom_start=18,\n)\nfolium.Polygon(\n    locations = [[35.8471, 127.1291],\n                 [35.8468, 127.1289],\n                 [35.84635, 127.1291],\n                 [35.84635, 127.1297],\n                 [35.8468, 127.12995],\n                 [35.8474, 127.1300]],\n    fill=True   ## 속까지 채워줌\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n# 예제 2 - 2개의 폴리곤으로 표시(3중 리스트)\n\npoly = np.array([[35.8471, 127.1291],\n                 [35.8468, 127.1289],\n                 [35.84635, 127.1291],\n                 [35.84635, 127.1297],\n                 [35.8468, 127.12995],\n                 [35.8474, 127.1300]])\n\n\nlat, lon = poly.T\n\n\npoly2 = np.stack([lat, lon + 0.0011], axis = 1)  ## 경도에다가 0.0011을 더해줬음, concatenate와 달리 이것은 차원을 늘려줌\npoly2\n\narray([[ 35.8471 , 127.1302 ],\n       [ 35.8468 , 127.13   ],\n       [ 35.84635, 127.1302 ],\n       [ 35.84635, 127.1308 ],\n       [ 35.8468 , 127.13105],\n       [ 35.8474 , 127.1311 ]])\n\n\n\nnp.stack([poly, poly2], axis = 0)  ## 두 폴리곤을 삼중 리스트 형태로 변환\n\narray([[[ 35.8471 , 127.1291 ],\n        [ 35.8468 , 127.1289 ],\n        [ 35.84635, 127.1291 ],\n        [ 35.84635, 127.1297 ],\n        [ 35.8468 , 127.12995],\n        [ 35.8474 , 127.13   ]],\n\n       [[ 35.8471 , 127.1302 ],\n        [ 35.8468 , 127.13   ],\n        [ 35.84635, 127.1302 ],\n        [ 35.84635, 127.1308 ],\n        [ 35.8468 , 127.13105],\n        [ 35.8474 , 127.1311 ]]])\n\n\n\nm = folium.Map(\n    scrollWheelZoom=False,\n    location = [35.8468,127.1294], # 분수대\n    zoom_start=18,\n)\nfolium.Polygon(\n    locations = np.stack([poly, poly2], axis = 0),   ## 점이 여러개니까 s 붙여줌\n    fill=True\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n아이디어 : 이걸 잘 이용하면 코로플레스맵처럼 지도를 행정구역별로 나눌 수 있겠음. 근데 실제 구현하려면 엄청난 노가다가 필요할 것 같음… -&gt; 누군가 해놓지 않았을까???"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#south-korea-github",
    "href": "2023_DV/Review/강신성_1113.html#south-korea-github",
    "title": "1. 라이브러리 imports",
    "section": "5. South Korea github",
    "text": "5. South Korea github"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#a.-github-소개",
    "href": "2023_DV/Review/강신성_1113.html#a.-github-소개",
    "title": "1. 라이브러리 imports",
    "section": "### A. github 소개",
    "text": "### A. github 소개\n\nsouthkorea라는 깃허브 유저가 있는데, 이곳엔 southkorea-maps라는 저장소가 있다.\n저장소 https://github.com/southkorea/southkorea-maps에는 kostat/2018/json/이란 폴더가 있고, 아래의 파일들이 있음.\n\nskorea-municipalities-2018-geo.json # &lt;-- 이 파일에 관심있음.\nskorea-municipalities-2018-topo-simple.json\nskorea-municipalities-2018-topo.json\nskorea-provinces-2018-geo.json # &lt;-- 이 파일에 관심있음.\nskorea-provinces-2018-topo-simple.json\nskorea-provinces-2018-topo.json\nskorea-submunicipalities-2018-geo.json\nskorea-submunicipalities-2018-topo-simple.json\nskorea-submunicipalities-2018-topo.json\n이중 관심있는 아래의 두 파일\nskorea-municipalities-2018-geo.json\nskorea-provinces-2018-geo.json\n\n이게 행정구역을 의미하는 폴리곤을 노가다로 정리해 둔 것임.\n\n\nB. json 파일 다운로드\n\n\nglobal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json').text)\nlocal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-municipalities-2018-geo.json').text)\n\n\nglobal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nlocal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nglobal_dict['type']\n\n'FeatureCollection'\n\n\n\nglobal_dict['name']\n\n'sido'\n\n\n\nglobal_dict['crs']\n\n{'type': 'name', 'properties': {'name': 'urn:ogc:def:crs:OGC:1.3:CRS84'}}"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#c.-json-파일의-구조",
    "href": "2023_DV/Review/강신성_1113.html#c.-json-파일의-구조",
    "title": "1. 라이브러리 imports",
    "section": "### C. json 파일의 구조",
    "text": "### C. json 파일의 구조\n\nglobal_dict['features']\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nglobal_dict['features'][0]  ## 딕셔너리임\n\n{'type': 'Feature',\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[127.02214829071995, 37.6997208220174],\n    [127.02531549179129, 37.69958052666555],\n    [127.0268980715665, 37.700251138801015],\n    [127.02702219174589, 37.70111540651664],\n    [127.02768734803148, 37.700937381678756],\n    [127.02899106854076, 37.69965827866671],\n    [127.02928557925306, 37.69929162349923],\n    [127.02960141563474, 37.69817519597225],\n    [127.02965442206329, 37.69780663241529],\n    [127.02972931239567, 37.69627243691011],\n    [127.03016631976621, 37.69519820573226],\n    [127.03097248066267, 37.693556837990705],\n    [127.0324142885382, 37.691839038874726],\n    [127.03577652734599, 37.69237420164817],\n    [127.03676285064714, 37.69263992022006],\n    [127.03790670521317, 37.69327193541787],\n    [127.03999748131439, 37.69468439213186],\n    [127.04110878733236, 37.69529978284375],\n    [127.0418126234525, 37.69538228560054],\n    [127.04307253543809, 37.69523090725608],\n    [127.04481711632289, 37.6929362514031],\n    [127.045094291979, 37.692385199610435],\n    [127.04862943554828, 37.69406226137355],\n    [127.04880746336237, 37.6932813039853],\n    [127.04975102873895, 37.68920713537357],\n    [127.04982002382117, 37.68797700395555],\n    [127.04998471076341, 37.68761035012039],\n    [127.05064314534968, 37.68650780387472],\n    [127.05092067338259, 37.686158552402],\n    [127.05108569439875, 37.686055160768575],\n    [127.05136797709446, 37.685921581506264],\n    [127.0517981363431, 37.6858148464868],\n    [127.05180416049991, 37.68706675968194],\n    [127.05206063036638, 37.687282882739005],\n    [127.05214429018166, 37.687352392956214],\n    [127.05222706432502, 37.68742077669241],\n    [127.05469544726837, 37.68872271744571],\n    [127.05610252968587, 37.68920437206933],\n    [127.05817261245335, 37.689669926579896],\n    [127.05839034056493, 37.689685868260845],\n    [127.05966013532905, 37.69033198280304],\n    [127.06220905346507, 37.6928052648238],\n    [127.06236779518714, 37.693024235595644],\n    [127.06237354393551, 37.693135159327554],\n    [127.06239478737797, 37.693546495381035],\n    [127.06260174913736, 37.69424379234776],\n    [127.06273214287815, 37.694675910354626],\n    [127.06305928833302, 37.694799063961284],\n    [127.0633863131918, 37.694921883623124],\n    [127.06630814019205, 37.69444172108839],\n    [127.06854802331812, 37.69400040092273],\n    [127.0691509700167, 37.69368950125125],\n    [127.0708788225368, 37.693700048667225],\n    [127.07087906859705, 37.693700062181264],\n    [127.07265760034454, 37.69379950124051],\n    [127.07313786558306, 37.69397574643172],\n    [127.07339823304241, 37.69410087667406],\n    [127.07378269914281, 37.69449368954549],\n    [127.07393007343649, 37.69464423236862],\n    [127.07448636301702, 37.695009628645295],\n    [127.07490680588823, 37.69521968047455],\n    [127.0781962787199, 37.695986510360626],\n    [127.08110478631578, 37.696137457947785],\n    [127.08384328792428, 37.69426889016338],\n    [127.08387832248333, 37.69279212349614],\n    [127.08390501861447, 37.69178245187781],\n    [127.08503096598021, 37.69054991893467],\n    [127.08517719556258, 37.69038984858866],\n    [127.08845357891715, 37.68990263950211],\n    [127.09055188620752, 37.68957025151863],\n    [127.09238424524798, 37.689933184218404],\n    [127.09303672225575, 37.689966466063815],\n    [127.09509714278617, 37.68938485003124],\n    [127.09599596058983, 37.68907104690658],\n    [127.09611844498184, 37.68777746180145],\n    [127.09629522668848, 37.687169760670066],\n    [127.09642514740469, 37.68626418153678],\n    [127.09642311415676, 37.685637441138354],\n    [127.09580983621922, 37.68464039310604],\n    [127.09428757876928, 37.683418538095395],\n    [127.09291905210979, 37.6815515024995],\n    [127.09194656333966, 37.67919029908044],\n    [127.09194153856279, 37.67869251579676],\n    [127.09218959069077, 37.67803010766824],\n    [127.09247959854748, 37.67764303460199],\n    [127.092614356114, 37.677474561385274],\n    [127.0939627740268, 37.675617232268024],\n    [127.09644725778618, 37.66968910695227],\n    [127.0962373655161, 37.66882941453695],\n    [127.09497929397519, 37.66760002717652],\n    [127.09458063193003, 37.666172300680316],\n    [127.0946800144686, 37.66492127931861],\n    [127.09541063101342, 37.663889644590746],\n    [127.09414211770347, 37.66329882532109],\n    [127.09217357563625, 37.66079254873924],\n    [127.09120691221572, 37.659334556715166],\n    [127.09113228726125, 37.658243024751805],\n    [127.09157431798847, 37.657772436849626],\n    [127.09235164609564, 37.65549186279581],\n    [127.09286741474956, 37.654453659625155],\n    [127.09356843248374, 37.653318729010714],\n    [127.0939157426287, 37.65296173000377],\n    [127.09400788402047, 37.65272853044388],\n    [127.09404075508206, 37.65254014144715],\n    [127.09403529660045, 37.652418519610805],\n    [127.09399991212685, 37.652295226453205],\n    [127.09326587472968, 37.65061772855914],\n    [127.09246587944706, 37.64970976752811],\n    [127.0926944997674, 37.64857999351487],\n    [127.09438136905902, 37.64495591656824],\n    [127.09456996976148, 37.644570600805835],\n    [127.09755891099569, 37.64396483085298],\n    [127.09890806335117, 37.6440343984249],\n    [127.10159264246101, 37.64477235815439],\n    [127.10280013821811, 37.64520492267967],\n    [127.1028823886103, 37.64540897697184],\n    [127.10386737257883, 37.64548852460528],\n    [127.10411721532803, 37.64548033184725],\n    [127.10657637866768, 37.64537730972521],\n    [127.10769224246286, 37.64496664676661],\n    [127.10786269687772, 37.64469282241332],\n    [127.10928861344723, 37.64285888886917],\n    [127.10989832951653, 37.64248752395126],\n    [127.11125728398851, 37.64237533392683],\n    [127.11142380118652, 37.64208686345102],\n    [127.11178551212458, 37.64073538398819],\n    [127.11113544340976, 37.639943933494294],\n    [127.11090335225738, 37.638262475561504],\n    [127.11155819260641, 37.63803293665921],\n    [127.11163126044404, 37.63797762400025],\n    [127.11163252401063, 37.63797657539423],\n    [127.1116335605215, 37.63797538002365],\n    [127.11163434149125, 37.637974066637646],\n    [127.11248319338027, 37.63648912158805],\n    [127.11220344878163, 37.63264276723643],\n    [127.1116323201253, 37.631503284414286],\n    [127.111371172448, 37.631227883211594],\n    [127.11082549334053, 37.630781845942806],\n    [127.10888476082644, 37.62930151924797],\n    [127.10596351597135, 37.62733070951351],\n    [127.10432790803095, 37.62327070428881],\n    [127.10406621959581, 37.62165311388996],\n    [127.10490909745644, 37.62156734430207],\n    [127.10538285122183, 37.62090189640023],\n    [127.1056619895515, 37.6204162469155],\n    [127.10555591433918, 37.62037072519622],\n    [127.11130504706806, 37.62069184201602],\n    [127.11214286452346, 37.620307017764965],\n    [127.11502106963971, 37.61953760329908],\n    [127.11619333227809, 37.61894778380488],\n    [127.11725242287622, 37.61674187052708],\n    [127.11699384437024, 37.616455741862225],\n    [127.11687861029367, 37.61632493344969],\n    [127.11667194632817, 37.61601542319483],\n    [127.11670352036289, 37.61571891606276],\n    [127.11679649663584, 37.614954676556906],\n    [127.11708589521979, 37.61396022396543],\n    [127.11737381655063, 37.61224525812902],\n    [127.1172872225345, 37.611180500418115],\n    [127.11687784452862, 37.609530700257416],\n    [127.11669806436957, 37.60884925145508],\n    [127.11809512146557, 37.60547242425238],\n    [127.11809557127602, 37.60544383142748],\n    [127.1180703665001, 37.604938459230944],\n    [127.11804701133192, 37.60460229805868],\n    [127.11558445031194, 37.601718190575795],\n    [127.11409999472093, 37.60012853384475],\n    [127.11447064798934, 37.599054605066726],\n    [127.11548924282775, 37.59765428196658],\n    [127.11572670052439, 37.597314488762194],\n    [127.11644313933067, 37.59619235770955],\n    [127.11687748428929, 37.59549705077813],\n    [127.11666448808528, 37.594016545777],\n    [127.11569009390017, 37.59362528500345],\n    [127.11552687588426, 37.593560121449116],\n    [127.11548546147779, 37.593548610635445],\n    [127.11541325076173, 37.593530949622995],\n    [127.1150282067279, 37.59349020815677],\n    [127.11445400975263, 37.59343304169271],\n    [127.1142683890762, 37.593416046920936],\n    [127.11366535163782, 37.593356363149674],\n    [127.11333737292749, 37.593259818471644],\n    [127.11241171452632, 37.59185636470543],\n    [127.1101545340761, 37.58743032790922],\n    [127.109944225286, 37.58595741365269],\n    [127.10973285600105, 37.58524413527232],\n    [127.109376495115, 37.58409232681838],\n    [127.10896665929798, 37.58337922727966],\n    [127.1074133022487, 37.58249485444903],\n    [127.10525566107893, 37.58156565577623],\n    [127.10344046907751, 37.58059529802311],\n    [127.1031362307321, 37.58021462295139],\n    [127.10289434724014, 37.57990679295722],\n    [127.10290869795878, 37.57978627410625],\n    [127.10302529385626, 37.578906983060165],\n    [127.10231819140941, 37.57804309180647],\n    [127.10195757011611, 37.577552361766436],\n    [127.10114351387524, 37.5760709289373],\n    [127.10091888215331, 37.574204101343454],\n    [127.10088786785404, 37.57376349428088],\n    [127.10166464576407, 37.57240065940598],\n    [127.10312974060275, 37.572279426510896],\n    [127.10366088935352, 37.57191791188504],\n    [127.10367642912719, 37.57190732992747],\n    [127.10423020073478, 37.571387655728294],\n    [127.10385348290774, 37.5704937526072],\n    [127.10272467888672, 37.56706421441915],\n    [127.10167082530147, 37.56340484757312],\n    [127.10120468941865, 37.56157990217243],\n    [127.10116259287368, 37.561313298804556],\n    [127.10112209698956, 37.56105064062629],\n    [127.10113406515579, 37.561003040117356],\n    [127.10127747386707, 37.5604659926051],\n    [127.10139885421326, 37.56025021143431],\n    [127.10154158761175, 37.55999837150702],\n    [127.10169403565163, 37.55974146071346],\n    [127.10180078997355, 37.55956709194242],\n    [127.10185922758541, 37.55949382793663],\n    [127.10216726500754, 37.55927810196558],\n    [127.10492928845024, 37.556421585556336],\n    [127.10540370353208, 37.556335336136144],\n    [127.1054039797918, 37.556335335256705],\n    [127.1060404466977, 37.55636800208074],\n    [127.10639619161522, 37.55646172320076],\n    [127.10714458157308, 37.55710610125959],\n    [127.10998660563237, 37.55834211535795],\n    [127.11167162292871, 37.55872797074785],\n    [127.11296064655822, 37.558794346205815],\n    [127.11522556193233, 37.55676018833242],\n    [127.11564563403175, 37.55769987998584],\n    [127.11730331913004, 37.559425641957304],\n    [127.11736706467859, 37.55947936004973],\n    [127.12295557080333, 37.56348990218229],\n    [127.12807612442845, 37.56590930955854],\n    [127.12861049420515, 37.56616043544185],\n    [127.13087184119757, 37.56687794943424],\n    [127.13375309980111, 37.56779331042001],\n    [127.1342663695222, 37.56795394418317],\n    [127.13474102736858, 37.56802464198976],\n    [127.13766579387027, 37.568424209460574],\n    [127.14560001479188, 37.568431169360885],\n    [127.1489495440468, 37.56843431854424],\n    [127.15124335260579, 37.569795301642884],\n    [127.15319599288145, 37.57096985010494],\n    [127.1549810601263, 37.57204353719504],\n    [127.15689916857828, 37.572954115769065],\n    [127.16084531250948, 37.575652662972885],\n    [127.1614681179371, 37.57604621474144],\n    [127.16200668497996, 37.57638652892725],\n    [127.16255503715276, 37.576730120691614],\n    [127.16674716243453, 37.578975666143656],\n    [127.1686475082068, 37.57899747794731],\n    [127.1686477364252, 37.57899709190847],\n    [127.17025143915063, 37.579016497229034],\n    [127.17183744214854, 37.57926199037442],\n    [127.17434341757382, 37.58002897085931],\n    [127.1745646848708, 37.58009734641977],\n    [127.17507432498863, 37.58025539696954],\n    [127.17510636726472, 37.58026661233748],\n    [127.17565914212791, 37.58056537625033],\n    [127.17585094748326, 37.580669273596094],\n    [127.17601761614299, 37.58076165809506],\n    [127.17637876684248, 37.58095906115711],\n    [127.17651851047951, 37.58103431126574],\n    [127.17715482863876, 37.58120032803303],\n    [127.17714288110588, 37.579858265364955],\n    [127.17706037157764, 37.579544339856604],\n    [127.17698860354076, 37.579416335558484],\n    [127.1769358967764, 37.57934067705333],\n    [127.17688336105913, 37.579265297509494],\n    [127.17674096763471, 37.57911572057686],\n    [127.17668903282423, 37.57906934265248],\n    [127.17559141963727, 37.57845491709445],\n    [127.17544227008392, 37.57836278564491],\n    [127.17541541545809, 37.57829863026236],\n    [127.17540428164837, 37.5782080868865],\n    [127.17540423462499, 37.57820770097899],\n    [127.17536098677607, 37.57784747971302],\n    [127.17532131705451, 37.577363342392985],\n    [127.17532434111249, 37.57729990460691],\n    [127.17532755327359, 37.57723513694318],\n    [127.17568308790581, 37.57489869852943],\n    [127.17570202935693, 37.57483166387621],\n    [127.1761038656408, 37.57413533821314],\n    [127.17613152481712, 37.57408856274329],\n    [127.1763084845669, 37.57397427178013],\n    [127.1764980410476, 37.573801389354294],\n    [127.17694363989052, 37.57329364424182],\n    [127.17773113162198, 37.57218791533225],\n    [127.17781926600982, 37.57205770863517],\n    [127.17913286165616, 37.56912369470184],\n    [127.17914903579462, 37.569085099484376],\n    [127.17920371766037, 37.56894649028298],\n    [127.17922017374693, 37.568873819289145],\n    [127.1793312074897, 37.56826577378065],\n    [127.17937794706329, 37.56798582974494],\n    [127.17943875427594, 37.56760084598443],\n    [127.17946467033734, 37.566583016887506],\n    [127.1801941808996, 37.56458787465877],\n    [127.18099458554735, 37.56332274641403],\n    [127.18118573247567, 37.5630169666324],\n    [127.18127812880661, 37.562830429531225],\n    [127.18151410938351, 37.5622675112996],\n    [127.18190992016416, 37.56124879949777],\n    [127.18199542158489, 37.56099244596784],\n    [127.18204081632643, 37.56040549758009],\n    [127.18202084650781, 37.55994782824462],\n    [127.18202079834414, 37.55994743873002],\n    [127.18198419478763, 37.55965551058688],\n    [127.181787374278, 37.55882911648127],\n    [127.18165662833978, 37.557998302305066],\n    [127.18135146515526, 37.553258048765485],\n    [127.1813403273481, 37.55296496478539],\n    [127.18142169421999, 37.552757324685345],\n    [127.18159477250546, 37.552592350538426],\n    [127.18170384487837, 37.552507995663994],\n    [127.182926745995, 37.55023535046519],\n    [127.18284450852022, 37.5494290906802],\n    [127.1826736926933, 37.547747320441374],\n    [127.18265679525105, 37.54752440129038],\n    [127.18276107519915, 37.546499426131824],\n    [127.18279599113094, 37.54638983933193],\n    [127.18353917154177, 37.54517039495601],\n    [127.18169282664165, 37.546444749492956],\n    [127.18125643812643, 37.54657971583249],\n    [127.17991053630526, 37.54657443190896],\n    [127.17933128170148, 37.54656742148427],\n    [127.17608601659725, 37.54561946546925],\n    [127.17571742052262, 37.54520770406822],\n    [127.17538324376515, 37.54520199786245],\n    [127.17463760452904, 37.54525912602242],\n    [127.17447709977255, 37.545277660809106],\n    [127.1743146633164, 37.54529732395536],\n    [127.17390569685948, 37.545600032434535],\n    [127.17157169030703, 37.545385773320305],\n    [127.1651681782903, 37.544631798899346],\n    [127.1631627309092, 37.54499101097656],\n    [127.16009589899915, 37.54151453079874],\n    [127.15958338038794, 37.54102822393515],\n    [127.15671195122711, 37.53757774554748],\n    [127.15603276390642, 37.53734241425835],\n    [127.15402132902574, 37.534514497157076],\n    [127.15356680162624, 37.533665193070505],\n    [127.15374715099493, 37.53160008846154],\n    [127.15353910604473, 37.53043671974301],\n    [127.15323227758938, 37.52923709895612],\n    [127.15316377616772, 37.529108804926516],\n    [127.14947629508237, 37.52502054146565],\n    [127.14779743367326, 37.52214947334571],\n    [127.14593249219926, 37.521955801698766],\n    [127.14578093977661, 37.521941375307804],\n    [127.14480065945018, 37.51938973774546],\n    [127.14509193634692, 37.51683494851859],\n    [127.14532885964844, 37.516607158321115],\n    [127.14538965833438, 37.51651332725961],\n    [127.14543404002671, 37.51642852537821],\n    [127.14544427334503, 37.51633278212625],\n    [127.14543916612115, 37.51606474208284],\n    [127.14485430106286, 37.51567802502981],\n    [127.14475438292835, 37.515645484717986],\n    [127.14460355182592, 37.515606249070395],\n    [127.1433741159653, 37.51557760277298],\n    [127.14210518808247, 37.51466010401819],\n    [127.14023773642688, 37.50985352338447],\n    [127.13996581242264, 37.50864088493049],\n    [127.14001384551528, 37.50852453676255],\n    [127.14032052088919, 37.508273591129395],\n    [127.14135345423192, 37.50694790778293],\n    [127.14147949264665, 37.50677882190354],\n    [127.14151665030897, 37.506708103021836],\n    [127.14151309866016, 37.50670219734203],\n    [127.14126852582045, 37.50641867264958],\n    [127.14117786173351, 37.50631882301717],\n    [127.1409971469675, 37.506163894207944],\n    [127.14081284937815, 37.50597856844039],\n    [127.14113191237908, 37.50546377662881],\n    [127.14223298799321, 37.504884417170004],\n    [127.14342402698134, 37.50441363645751],\n    [127.14392479947718, 37.50440092444333],\n    [127.14410326532739, 37.5043964831474],\n    [127.14550250136601, 37.50330570568455],\n    [127.14595206598852, 37.5032268868512],\n    [127.14769492141562, 37.50321206574927],\n    [127.14860243574782, 37.50372282317089],\n    [127.1490451129403, 37.50400073373371],\n    [127.15082434872058, 37.50406156817608],\n    [127.15165372960253, 37.503395752670144],\n    [127.1521091406241, 37.50297029811745],\n    [127.15323083173114, 37.50267941781355],\n    [127.15643795942692, 37.50185900576393],\n    [127.1565550135419, 37.50189404497823],\n    [127.15772923925586, 37.50317895196306],\n    [127.1588661178982, 37.50239019657405],\n    [127.16050427114095, 37.501094512438115],\n    [127.16121949144342, 37.50039527422741],\n    [127.16139030513257, 37.5002010498375],\n    [127.16143108867095, 37.50002247972045],\n    [127.16142016695896, 37.49970545993054],\n    [127.16135443347227, 37.499537459426215],\n    [127.16125092773778, 37.49939089854903],\n    [127.16110733163866, 37.49927532319294],\n    [127.15986640646732, 37.495706834034365],\n    [127.15996541511976, 37.49527986027531],\n    [127.16001826923471, 37.494707646707646],\n    [127.16002621602986, 37.49437455452455],\n    [127.1597454988479, 37.49363667082564],\n    [127.15970484975863, 37.49354690503217],\n    [127.15965196338816, 37.493434358962624],\n    [127.1595169833914, 37.49327460353943],\n    [127.15931845606401, 37.49313662914927],\n    [127.15764946367052, 37.490182673876625],\n    [127.15763380331077, 37.4901717886922],\n    [127.15740206846664, 37.48908854265012],\n    [127.15043020214158, 37.4852488932469],\n    [127.15021754524786, 37.48510537954825],\n    [127.14992170977906, 37.484905729020184],\n    [127.14868305306886, 37.484043192462515],\n    [127.148579027746, 37.483889863822014],\n    [127.14822520162613, 37.48336825471925],\n    [127.1477703794897, 37.48262416785541],\n    [127.14766978415436, 37.48242576594467],\n    [127.14760689985782, 37.48226916996039],\n    [127.14758794988897, 37.482221953125226],\n    [127.14752532591987, 37.48201386379114],\n    [127.14748230260066, 37.48180264116629],\n    [127.14747861659086, 37.481773480024934],\n    [127.14745907220343, 37.48158945671828],\n    [127.14739684802731, 37.480585692947166],\n    [127.14728211415827, 37.478739628360124],\n    [127.14715916521881, 37.47729511320696],\n    [127.14714533170121, 37.47722179897961],\n    [127.14584352066795, 37.477273068635235],\n    [127.145796878535, 37.47727491723534],\n    [127.14575646485959, 37.47727651768649],\n    [127.1443704748871, 37.47733114309174],\n    [127.14364342913493, 37.47525741763574],\n    [127.14361885800092, 37.47517065376269],\n    [127.14360185852455, 37.475082746354325],\n    [127.14359251006233, 37.47499412650607],\n    [127.1435733636672, 37.47468530404496],\n    [127.14356722898532, 37.474586308344385],\n    [127.14352658956736, 37.473930418522116],\n    [127.1394881494977, 37.474089616658645],\n    [127.13833405279085, 37.47413506191677],\n    [127.13766388419639, 37.474147919140066],\n    [127.13684251991336, 37.47415318717528],\n    [127.13677761017136, 37.47415548714804],\n    [127.1363969537117, 37.47417048239313],\n    [127.13546828445813, 37.47424021251808],\n    [127.1352873110148, 37.47425705042134],\n    [127.13312242962854, 37.4745798598],\n    [127.13285605187727, 37.47462645866824],\n    [127.13262354347293, 37.472395357539355],\n    [127.13282098466541, 37.46919569189179],\n    [127.13282095574344, 37.4691942063499],\n    [127.1327996032295, 37.468393664276086],\n    [127.13087554387387, 37.46775756923912],\n    [127.13083001167365, 37.46774525764664],\n    [127.13078324278253, 37.46773607352042],\n    [127.13073584632706, 37.46773009808275],\n    [127.13068790133586, 37.46772735681814],\n    [127.13063981486039, 37.46772786001192],\n    [127.13059198261257, 37.46773162422421],\n    [127.1305447100502, 37.46773862246795],\n    [127.13018892567416, 37.4678033697074],\n    [127.12933858733388, 37.467959210768726],\n    [127.12930302575272, 37.467966655370866],\n    [127.1266812987817, 37.468621775910755],\n    [127.12644686764371, 37.468750108913085],\n    [127.12602455140079, 37.4691058782371],\n    [127.12570131939023, 37.46931823345992],\n    [127.12551817321834, 37.46943048351345],\n    [127.12537453641595, 37.46951707676626],\n    [127.12531488674534, 37.46955289992616],\n    [127.12520301623321, 37.46961974560557],\n    [127.12510091762285, 37.469613652978204],\n    [127.12494334079366, 37.4696031234281],\n    [127.12487834302604, 37.469597279961135],\n    [127.12485136253184, 37.469523255232446],\n    [127.1248975970592, 37.46938552451682],\n    [127.12504582809731, 37.468979630189054],\n    [127.12516820516782, 37.46875144004594],\n    [127.1251820284445, 37.468352452585854],\n    [127.12519937283692, 37.467833231267484],\n    [127.12498822426089, 37.46724835864308],\n    [127.12482726884033, 37.46700159707097],\n    [127.12475930593294, 37.466916352653705],\n    [127.1244880965488, 37.4666435226522],\n    [127.12420970259667, 37.466517104307265],\n    [127.12420688119097, 37.46652048627629],\n    [127.12414326804209, 37.466495215148015],\n    [127.1237669717385, 37.46633342476096],\n    [127.12320225935616, 37.46597867005132],\n    [127.12185946757938, 37.46513395264438],\n    [127.11747485026633, 37.46220061045074],\n    [127.11747077309776, 37.462194983538076],\n    [127.11711994009005, 37.46167753791065],\n    [127.11698929578043, 37.46148450859329],\n    [127.11693965937852, 37.46090030945957],\n    [127.11689967841377, 37.4604011395989],\n    [127.1168948964395, 37.45864042153465],\n    [127.11669702911543, 37.45862325785094],\n    [127.11623941228295, 37.4585961143597],\n    [127.11589291262797, 37.45859194472567],\n    [127.11549606922648, 37.45869791695614],\n    [127.11547664981921, 37.458704404910726],\n    [127.11395536575394, 37.45951676647907],\n    [127.1136342600001, 37.45973246531095],\n    [127.11353635596203, 37.45980154776803],\n    [127.11341975702304, 37.45990076930371],\n    [127.11276328611841, 37.46053025046058],\n    [127.11290865142644, 37.46064192929295],\n    [127.11318486414389, 37.46075823127989],\n    [127.11329182742885, 37.46083640990825],\n    [127.11333484694086, 37.46090535023642],\n    [127.11333484534424, 37.46090566030155],\n    [127.11333387525471, 37.46108724567093],\n    [127.1122298035669, 37.46150499909431],\n    [127.11182027181094, 37.46164419121139],\n    [127.10868615916183, 37.4621564211832],\n    [127.10688007579863, 37.462369502165004],\n    [127.10667208681129, 37.46241051180783],\n    [127.10646723536935, 37.46242392987725],\n    [127.10616731423701, 37.462406454624805],\n    [127.10434135861443, 37.46217434553484],\n    [127.10447230410983, 37.46102952693268],\n    [127.10394160853244, 37.460050593707535],\n    [127.10381093870869, 37.459927656301566],\n    [127.09930610906709, 37.45667755614937],\n    [127.09823746163788, 37.456371246967336],\n    [127.09522278311739, 37.45639422742016],\n    [127.09431215563166, 37.45622477015542],\n    [127.09377352285104, 37.45607736437658],\n    [127.09354036220854, 37.455888055203964],\n    [127.08882127538375, 37.44974941955953],\n    [127.08843214262606, 37.44899484298624],\n    [127.08832682104673, 37.44862776075761],\n    [127.08832676671251, 37.44862736036078],\n    [127.08836930483001, 37.44724005811312],\n    [127.08816128515241, 37.44538498355397],\n    [127.08785507296487, 37.44489387071301],\n    [127.08777816075089, 37.44487515146447],\n    [127.08730388244146, 37.44464171021886],\n    [127.082904590745, 37.442173808672536],\n    [127.08243181881421, 37.441579468344386],\n    [127.08209733924632, 37.44135078689038],\n    [127.08144410364704, 37.441230999973904],\n    [127.08017160102241, 37.441051093956524],\n    [127.07908335144711, 37.44129874337731],\n    [127.07817890647885, 37.44155161511739],\n    [127.07760188111618, 37.44170909672176],\n    [127.07601057612212, 37.44213893609793],\n    [127.07475598627195, 37.442218846646846],\n    [127.0744976898156, 37.44223280045726],\n    [127.07224980537977, 37.442199811541435],\n    [127.07186137425094, 37.44102970185969],\n    [127.07221171709271, 37.439382901540675],\n    [127.07376019338288, 37.437789433842276],\n    [127.07379537723341, 37.43765707199904],\n    [127.07384351098383, 37.43740786000359],\n    [127.07356006757476, 37.436963727095595],\n    [127.0730242304579, 37.43640683285397],\n    [127.0709099786286, 37.43309491379506],\n    [127.06961542955781, 37.430611692111725],\n    [127.06569800697741, 37.42915810241767],\n    [127.06114498957349, 37.42998528229182],\n    [127.05978112053222, 37.4295872657291],\n    [127.05345832004603, 37.42880858729806],\n    [127.0503615218898, 37.42982129068007],\n    [127.04957975136693, 37.4302701432368],\n    [127.04736923949098, 37.430701820429285],\n    [127.04722475567219, 37.432040442932944],\n    [127.04644166111552, 37.43299046057295],\n    [127.04590560884242, 37.43348142450798],\n    [127.04418573191268, 37.4350554539952],\n    [127.04109031701444, 37.437769124917864],\n    [127.04087975167322, 37.437878441821276],\n    [127.04048048713241, 37.43808242753594],\n    [127.04005277677203, 37.43823714507124],\n    [127.03956828753749, 37.438195909531515],\n    [127.03922186091357, 37.43816448538617],\n    [127.03903241753622, 37.43815131864844],\n    [127.03706776525239, 37.43827496782511],\n    [127.03697772841022, 37.43829639401592],\n    [127.03557372599246, 37.439004097845434],\n    [127.03532026694523, 37.439966564131865],\n    [127.03529507001076, 37.44007215137235],\n    [127.03520801081689, 37.440437652690015],\n    [127.03516906728176, 37.4406057551936],\n    [127.03517186217657, 37.44094194757337],\n    [127.03528263969284, 37.44103476257073],\n    [127.0360722545635, 37.44155099433042],\n    [127.03608762765222, 37.44156253268867],\n    [127.03643099253159, 37.441832734480634],\n    [127.03665445379359, 37.442080160034884],\n    [127.03748294023711, 37.44326670427758],\n    [127.03793486290014, 37.44417883691202],\n    [127.03803531906024, 37.444516404127626],\n    [127.03818353178544, 37.44501754331141],\n    [127.03823039750196, 37.44518026909942],\n    [127.03820566265372, 37.4458588524557],\n    [127.03781196836518, 37.44892239759809],\n    [127.03777449207378, 37.449201719792974],\n    [127.03774440117567, 37.44941881885998],\n    [127.03696804004046, 37.450691157558936],\n    [127.03640970411149, 37.45149687786112],\n    [127.03577642448006, 37.452195909168545],\n    [127.03626740809199, 37.45426216962762],\n    [127.03712770180721, 37.45520768138208],\n    [127.03706742872744, 37.45547434330387],\n    [127.0367406508785, 37.456443867860756],\n    [127.03492422096939, 37.46017540450221],\n    [127.03453266613107, 37.46333522021434],\n    [127.0346601133906, 37.463976592257936],\n    [127.03467091241455, 37.464090458173466],\n    [127.03460314189319, 37.464173939752115],\n    [127.0331405892598, 37.46501738910679],\n    [127.03275757043367, 37.46519318169501],\n    [127.03192176568467, 37.465518883534436],\n    [127.03119554711617, 37.465626061203594],\n    [127.03055508280168, 37.46551416328518],\n    [127.03039682015434, 37.46548632407364],\n    [127.02992820559366, 37.46535158628146],\n    [127.02958338917796, 37.4642881854868],\n    [127.02873126984468, 37.46202826888523],\n    [127.02815696813059, 37.460621993714916],\n    [127.02634678296988, 37.458104359241986],\n    [127.02620458102675, 37.4579878159816],\n    [127.02599541833983, 37.4578169581426],\n    [127.02543481238588, 37.45756450281699],\n    [127.025277616827, 37.4574938687448],\n    [127.02150295539224, 37.45622583459711],\n    [127.01969201000381, 37.45578660642505],\n    [127.01557484735889, 37.454916014104],\n    [127.0150649965327, 37.45483188354524],\n    [127.0148735746752, 37.454841752950394],\n    [127.01450961789199, 37.45486122860588],\n    [127.0107251186711, 37.45577244670041],\n    [127.00923267412264, 37.45722091033481],\n    [127.00704092418567, 37.460215469835134],\n    [127.00496875820613, 37.463186899888576],\n    [127.0044696722443, 37.46407327656836],\n    [127.00476218583773, 37.46492302457033],\n    [127.00490546550544, 37.46584260252905],\n    [127.00452135791603, 37.46679064066523],\n    [127.00424565849995, 37.46714344410636],\n    [127.00367535892853, 37.467720386588596],\n    [127.00273482189448, 37.46712461108707],\n    [127.00196279723106, 37.467087449562925],\n    [127.00047686511023, 37.46706294825534],\n    [126.99742674489028, 37.46724223517637],\n    [126.99717099538091, 37.467214070232885],\n    [126.99698554452699, 37.46717971600085],\n    [126.99675187621652, 37.467072433303954],\n    [126.99628192995874, 37.46665064295208],\n    [126.99719359563045, 37.464184165933666],\n    [126.99725312948726, 37.46401973479701],\n    [126.99737077162193, 37.46368749255192],\n    [126.99676862985268, 37.461873352884574],\n    [126.9942899320261, 37.461421317893326],\n    [126.99315292901082, 37.46127623511324],\n    [126.9916230699418, 37.46044156303114],\n    [126.98999453840017, 37.4594629780069],\n    [126.98863550944588, 37.458165845862126],\n    [126.98678106962474, 37.45740326454205],\n    [126.98238921265964, 37.45591710269429],\n    [126.97775711976738, 37.455199116892956],\n    [126.97457956131598, 37.454412875477026],\n    [126.97175282603096, 37.45173733547508],\n    [126.97056939304684, 37.44945270098554],\n    [126.96959356922594, 37.44909597687449],\n    [126.96866171986065, 37.44875446455986],\n    [126.967662979455, 37.44838054424455],\n    [126.96428976779832, 37.44626723711946],\n    [126.9639418980544, 37.4452458892869],\n    [126.96393361850694, 37.445214631166266],\n    [126.96432082073919, 37.44346229477368],\n    [126.96463404099109, 37.44203964730629],\n    [126.9638033064139, 37.44078692549053],\n    [126.96373604591848, 37.44074688094471],\n    [126.96311396054732, 37.44037652510398],\n    [126.96298353095135, 37.44029960855669],\n    [126.96294292915762, 37.44028045754587],\n    [126.95899886229834, 37.43912694137169],\n    [126.95636819846372, 37.43875401134443],\n    [126.94928410957398, 37.438250273989894],\n    [126.94815539463404, 37.43863037558223],\n    [126.9456762727248, 37.43730530031926],\n    [126.9450906377534, 37.437088221251784],\n    [126.94022016050745, 37.43571237211725],\n    [126.9393643055451, 37.43602581803675],\n    [126.93857452478132, 37.43642042942645],\n    [126.93774638275855, 37.4373857576777],\n    [126.93727268446924, 37.4386322720711],\n    [126.93725777476023, 37.43892734300138],\n    [126.93724856528063, 37.439167511901296],\n    [126.93724711351074, 37.43921284001876],\n    [126.93740216615096, 37.43937426800926],\n    [126.93769358300823, 37.43968442708137],\n    [126.937713857788, 37.43972075748547],\n    [126.93786405751602, 37.44019753476193],\n    [126.9377458076725, 37.440354020623566],\n    [126.93735357677096, 37.440871598648926],\n    [126.93668810996004, 37.44169594611689],\n    [126.93664692478835, 37.441745758401595],\n    [126.9348979978904, 37.443209481797815],\n    [126.9344508816705, 37.44325822606353],\n    [126.93055407014207, 37.44546848090059],\n    [126.93051744833456, 37.445548423023716],\n    [126.9303939359546, 37.44581837200393],\n    [126.93032633869163, 37.4459686910352],\n    [126.93016316403606, 37.4463779831893],\n    [126.93035671199829, 37.44733422226026],\n    [126.92839869373803, 37.450212485076534],\n    [126.9283570058542, 37.44989868951273],\n    [126.9283079026723, 37.44954867104572],\n    [126.9282796601699, 37.4493526845579],\n    [126.92399602716448, 37.446210224228935],\n    [126.92325327636715, 37.44577274628548],\n    [126.92289722984206, 37.445173349643994],\n    [126.92294662379582, 37.44487295199244],\n    [126.9229034040661, 37.444673007921715],\n    [126.92276280882136, 37.44404108321409],\n    [126.9220070215115, 37.443250757259776],\n    [126.92069895346346, 37.44151996843528],\n    [126.91866310009635, 37.43998262017672],\n    [126.91538923818742, 37.43966802735873],\n    [126.91389137436857, 37.43930708270779],\n    [126.91318980966076, 37.43909539212985],\n    [126.91230851419965, 37.43857946078951],\n    [126.91143164721281, 37.43762316839669],\n    [126.91120280868347, 37.43720121439982],\n    [126.91120000044674, 37.437187408644185],\n    [126.91121640637944, 37.4370463602423],\n    [126.91128427742785, 37.43654325238066],\n    [126.9113332747712, 37.4361713431048],\n    [126.91002834759448, 37.43432412191302],\n    [126.90990399563053, 37.43422744640446],\n    [126.90967840112272, 37.43405973794322],\n    [126.90940570432362, 37.43386468248096],\n    [126.90939564142113, 37.43386044896999],\n    [126.90859344653741, 37.433691729024154],\n    [126.90752668322209, 37.43362021944715],\n    [126.90663449172253, 37.43359694340164],\n    [126.90582547214655, 37.43399615235654],\n    [126.90582519408719, 37.43399615186017],\n    [126.90522457694115, 37.433997069199144],\n    [126.90298757638999, 37.434067620178844],\n    [126.90186087029541, 37.436397524817494],\n    [126.89914461684712, 37.43866819351907],\n    [126.89897791331421, 37.43870182579147],\n    [126.89958451332427, 37.44036666498753],\n    [126.8982570102593, 37.44320652479412],\n    [126.89578235329736, 37.44563339801652],\n    [126.89564123458763, 37.44829320383536],\n    [126.8946038195621, 37.451159154897915],\n    [126.89398300003057, 37.45271733991926],\n    [126.89182284732061, 37.45228350409839],\n    [126.8910486017461, 37.45224245321772],\n    [126.88978012488182, 37.45227307520445],\n    [126.88964213513057, 37.45231546387733],\n    [126.88956055650301, 37.45243222840465],\n    [126.88952054982705, 37.45262112176969],\n    [126.88950365869798, 37.4527010699145],\n    [126.88953577253639, 37.452946634786336],\n    [126.88964770418707, 37.45331924221095],\n    [126.88966648039558, 37.453521707197815],\n    [126.88965276996692, 37.45359855847977],\n    [126.889592047684, 37.45370296489857],\n    [126.88876417143807, 37.45476564219567],\n    [126.88828498935065, 37.45516865405293],\n    [126.8863169029548, 37.45628901611006],\n    [126.88626468031083, 37.456362515796506],\n    [126.88618548846422, 37.456522645677246],\n    [126.88588075182155, 37.45750839016862],\n    [126.8853564731724, 37.45953456857106],\n    [126.88534006096899, 37.45963704166101],\n    [126.88534960239055, 37.45986258161532],\n    [126.88539936108317, 37.46000916976716],\n    [126.88543071619748, 37.46009095992281],\n    [126.88544367776255, 37.46012453265269],\n    [126.88551853776474, 37.46025525216233],\n    [126.885650375358, 37.46043079769516],\n    [126.88607198185638, 37.460859466843345],\n    [126.886128084154, 37.460899787651186],\n    [126.88620134657874, 37.46091646478202],\n    [126.88625378849405, 37.46092666112505],\n    [126.88747359786943, 37.4610655269108],\n    [126.88749655889849, 37.46106301666018],\n    [126.88768203443473, 37.46103729223392],\n    [126.88785888520992, 37.460997475157],\n    [126.88795325015386, 37.46095448958872],\n    [126.8883358698455, 37.46078084535764],\n    [126.88863061689209, 37.46078817357594],\n    [126.88876919238663, 37.46083250900126],\n    [126.88887551772156, 37.460946920406975],\n    [126.88853249141253, 37.46133205164132],\n    [126.88810791069389, 37.461806645801666],\n    [126.88712180563425, 37.462904361685275],\n    [126.88704339763547, 37.46290231182216],\n    [126.88692384788376, 37.46289178120236],\n    [126.8866437804029, 37.46286673117086],\n    [126.88624491347673, 37.46279820029081],\n    [126.88485800472498, 37.46254315202208],\n    [126.88285845675665, 37.46426078649855],\n    [126.88457429409604, 37.46564823572887],\n    [126.88399444693059, 37.46666861463424],\n    [126.88153185190849, 37.46929115820678],\n    [126.88020447481996, 37.47093497073136],\n    [126.87987759363838, 37.4714213189637],\n    [126.87978452249763, 37.471539058789205],\n    [126.87955796413078, 37.47182967743504],\n    [126.87896535335604, 37.47259125928221],\n    [126.87873771587651, 37.4728841366197],\n    [126.87845148258745, 37.47325521436153],\n    [126.87801141557026, 37.47382773802628],\n    [126.87758404673444, 37.47451796201532],\n    [126.87614354086422, 37.47684468784561],\n    [126.87598571313652, 37.47710383526691],\n    [126.87532045038951, 37.478372692072554],\n    [126.87527082721985, 37.47846810142021],\n    [126.87495854231086, 37.47907199778555],\n    [126.87428469764085, 37.480380256044086],\n    [126.87413691190955, 37.480757112415716],\n    [126.87337901472617, 37.48243356052865],\n    [126.8727442969783, 37.482424995238794],\n    [126.87277987743597, 37.482794985827105],\n    [126.87278663474126, 37.48348878210105],\n    [126.8717971958008, 37.48499743368912],\n    [126.87176262543268, 37.485269390497095],\n    [126.87244469888192, 37.48611004050176],\n    [126.87267382382166, 37.486220095922036],\n    [126.87273456256477, 37.486240154483085],\n    [126.87289532395594, 37.486251871041624],\n    [126.87304222324997, 37.486197970915136],\n    [126.87313717688521, 37.486151898173034],\n    [126.87455644442305, 37.48536674989791],\n    [126.8749436128769, 37.485699557362956],\n    [126.87626185595225, 37.48723955123044],\n    [126.8766887354242, 37.4878526882681],\n    [126.87676002659364, 37.48811743166551],\n    [126.87680025485933, 37.48835707247939],\n    [126.87679035662485, 37.48857077467621],\n    [126.87666836775517, 37.488731698886916],\n    [126.87638215065101, 37.488941163019874],\n    [126.87604346872821, 37.489047234427446],\n    [126.8758168244155, 37.48902812727765],\n    [126.87471601721661, 37.48857729667705],\n    [126.87307774871593, 37.48825285739754],\n    [126.87295540342764, 37.48830058919606],\n    [126.87292252364195, 37.48831350594885],\n    [126.87274847564402, 37.488430729714324],\n    [126.87266911360857, 37.48855396331768],\n    [126.87241371655864, 37.48933079643515],\n    [126.8724404006732, 37.48953552394017],\n    [126.87244516069696, 37.48954200832903],\n    [126.87267624752327, 37.48985141040301],\n    [126.87277059260437, 37.4899587830918],\n    [126.87283922397087, 37.490014335559685],\n    [126.87336843875514, 37.490386572775556],\n    [126.87357693309845, 37.490481684306566],\n    [126.87375273922521, 37.490475679558024],\n    [126.8739332173435, 37.490428855349435],\n    [126.87402067996526, 37.490316033035164],\n    [126.87409064170969, 37.49021447138926],\n    [126.87414915935193, 37.48998055733015],\n    [126.87411047167724, 37.48977722861176],\n    [126.87432244123868, 37.48969551876888],\n    [126.874579194672, 37.48976928586574],\n    [126.87465124038863, 37.48989887276378],\n    [126.8748395606655, 37.49074546038744],\n    [126.87453459446087, 37.4910683660723],\n    [126.87437268165921, 37.49122136264536],\n    [126.87413007357357, 37.491337668238565],\n    [126.87395185020526, 37.49130790942899],\n    [126.87225463784726, 37.490602443243475],\n    [126.8715772336594, 37.490209200924156],\n    [126.8715477661439, 37.49008528105915],\n    [126.8715056835045, 37.489998794005494],\n    [126.8713423702246, 37.48983362446104],\n    [126.87125961324185, 37.48977074005055],\n    [126.87108027252202, 37.48968325006296],\n    [126.87094589711485, 37.48963721815786],\n    [126.87065821481464, 37.48956087360729],\n    [126.87042098603534, 37.48953583190591],\n    [126.8703347531482, 37.489544746411035],\n    [126.87022812471946, 37.48959615733273],\n    [126.87017041588264, 37.489665641527296],\n    [126.87013621956086, 37.48971909518386],\n    [126.86987690273945, 37.490296013812674],\n    [126.86936626773986, 37.49201860453653],\n    [126.86936892693092, 37.49271744174477],\n    [126.86937342007144, 37.49277207332995],\n    [126.86950267732847, 37.4943284018372],\n    [126.86758862651642, 37.49481588988599],\n    [126.86761040833085, 37.49468809006178],\n    [126.86619406839125, 37.49253860319941],\n    [126.86451819511568, 37.491203026084236],\n    [126.86302813884646, 37.49087608634184],\n    [126.8628110904766, 37.490828236574096],\n    [126.8627337568918, 37.490804494216775],\n    [126.86258684437263, 37.49075871779353],\n    [126.86238768631601, 37.49067907388865],\n    [126.86217497834068, 37.49057746081463],\n    [126.86208955762203, 37.49053119281495],\n    [126.86195048469753, 37.490447969739456],\n    [126.86183032272321, 37.490367864585316],\n    [126.86176820935094, 37.490326119958326],\n    [126.86162303620054, 37.49020374479842],\n    [126.861481767076, 37.490067301502876],\n    [126.86138442651429, 37.489968076240665],\n    [126.86111908480686, 37.48966817910177],\n    [126.86102581461058, 37.48955826657109],\n    [126.86101648222878, 37.489547269155715],\n    [126.86068610548242, 37.48916817605563],\n    [126.86018595731015, 37.48859544427551],\n    [126.85925601817621, 37.48753114874059],\n    [126.8585450417844, 37.48672107555025],\n    [126.85822542932799, 37.486360288148504],\n    [126.85811613511434, 37.48623739417916],\n    [126.85797103348565, 37.48608461582477],\n    [126.85783430290624, 37.485983086909044],\n    [126.85763012230208, 37.485867113767235],\n    [126.85748360128834, 37.48580696465288],\n    [126.85744917364828, 37.48579453432356],\n    [126.8572904242174, 37.48576055327166],\n    [126.85721499765847, 37.48575426248749],\n    [126.85708640467882, 37.48574368934753],\n    [126.85676809597386, 37.485721054345184],\n    [126.85655188308645, 37.48570671540534],\n    [126.85653942998213, 37.48570626540688],\n    [126.85349858643376, 37.482621834752926],\n    [126.85270046935516, 37.481818616044556],\n    [126.85261962553658, 37.481786686082536],\n    [126.85258624296549, 37.48177876052663],\n    [126.85206621905958, 37.48166492424135],\n    [126.851751885319, 37.48160370879597],\n    [126.85166783206732, 37.48158868170002],\n    [126.85161377601116, 37.481583540277235],\n    [126.85099047011765, 37.481527845736494],\n    [126.85084315206339, 37.48152259001683],\n    [126.85070182006966, 37.48152550894454],\n    [126.85057300416253, 37.48153941821166],\n    [126.85041837279233, 37.481566818390434],\n    [126.8493324275512, 37.48184331552959],\n    [126.84698612892635, 37.48189443252598],\n    [126.8466017682575, 37.48167122516655],\n    [126.8465568595493, 37.4816333299397],\n    [126.84651717642556, 37.48159893200954],\n    [126.84642038315994, 37.48150701100949],\n    [126.84640821872374, 37.481495453254205],\n    [126.84626305142321, 37.48130323673433],\n    [126.84621918975228, 37.481240109255275],\n    [126.84604937012155, 37.480925377389326],\n    [126.84600724781465, 37.480798339922416],\n    [126.84598678074923, 37.480695539848334],\n    [126.845976830455, 37.48064175143651],\n    [126.8459342194971, 37.4803147980667],\n    [126.8454343751056, 37.47446358358097],\n    [126.84536592690858, 37.47385949919483],\n    [126.84536055354526, 37.47381246555105],\n    [126.84503323897347, 37.47355891752733],\n    [126.84419346893853, 37.47401499804191],\n    [126.84395991392603, 37.474482912719694],\n    [126.84311200124603, 37.47486984773489],\n    [126.8430773430002, 37.4748849961075],\n    [126.84295255134103, 37.47492481992884],\n    [126.84278163936918, 37.47497161415357],\n    [126.8384196728936, 37.47536790968778],\n    [126.83825923862072, 37.475374710633375],\n    [126.83610875738073, 37.474675546160455],\n    [126.83461359077172, 37.47487025950699],\n    [126.83305167563579, 37.47724609250874],\n    [126.8320789692601, 37.477567839756105],\n    [126.8317480949545, 37.47765099536511],\n    [126.83080142694622, 37.47742382313479],\n    [126.83045751616729, 37.47733913587807],\n    [126.82962156724689, 37.476895297577286],\n    [126.82955329242202, 37.47662461753497],\n    [126.8295367634638, 37.476595599289965],\n    [126.82920434453489, 37.47619529596771],\n    [126.82818195209177, 37.47603415313276],\n    [126.82787204443441, 37.4759892067256],\n    [126.82410830374371, 37.476352189043084],\n    [126.8195759381957, 37.47635542376256],\n    [126.8194233714162, 37.47632759999209],\n    [126.81930759652315, 37.47628940790379],\n    [126.81929647232971, 37.47628516514027],\n    [126.81914316116901, 37.476205532322275],\n    [126.81832133653424, 37.475298753226035],\n    [126.81766855225743, 37.473238400442405],\n    [126.81465604938846, 37.47476019094166],\n    [126.81472979756464, 37.475057306435204],\n    [126.81496487102987, 37.47586011888277],\n    [126.81522179414826, 37.476251063198596],\n    [126.81529553572993, 37.47636211907153],\n    [126.81641141908979, 37.477536748815474],\n    [126.81706064565485, 37.478024854451064],\n    [126.81721035612148, 37.478135181855585],\n    [126.81732392065949, 37.478139295676314],\n    ...]]},\n 'properties': {'name': '서울특별시',\n  'base_year': '2018',\n  'name_eng': 'Seoul',\n  'code': '11'}}\n\n\n\nglobal_dict['features'][0]['geometry']  ## 또 딕셔너리임\n\n{'type': 'Polygon',\n 'coordinates': [[[127.02214829071995, 37.6997208220174],\n   [127.02531549179129, 37.69958052666555],\n   [127.0268980715665, 37.700251138801015],\n   [127.02702219174589, 37.70111540651664],\n   [127.02768734803148, 37.700937381678756],\n   [127.02899106854076, 37.69965827866671],\n   [127.02928557925306, 37.69929162349923],\n   [127.02960141563474, 37.69817519597225],\n   [127.02965442206329, 37.69780663241529],\n   [127.02972931239567, 37.69627243691011],\n   [127.03016631976621, 37.69519820573226],\n   [127.03097248066267, 37.693556837990705],\n   [127.0324142885382, 37.691839038874726],\n   [127.03577652734599, 37.69237420164817],\n   [127.03676285064714, 37.69263992022006],\n   [127.03790670521317, 37.69327193541787],\n   [127.03999748131439, 37.69468439213186],\n   [127.04110878733236, 37.69529978284375],\n   [127.0418126234525, 37.69538228560054],\n   [127.04307253543809, 37.69523090725608],\n   [127.04481711632289, 37.6929362514031],\n   [127.045094291979, 37.692385199610435],\n   [127.04862943554828, 37.69406226137355],\n   [127.04880746336237, 37.6932813039853],\n   [127.04975102873895, 37.68920713537357],\n   [127.04982002382117, 37.68797700395555],\n   [127.04998471076341, 37.68761035012039],\n   [127.05064314534968, 37.68650780387472],\n   [127.05092067338259, 37.686158552402],\n   [127.05108569439875, 37.686055160768575],\n   [127.05136797709446, 37.685921581506264],\n   [127.0517981363431, 37.6858148464868],\n   [127.05180416049991, 37.68706675968194],\n   [127.05206063036638, 37.687282882739005],\n   [127.05214429018166, 37.687352392956214],\n   [127.05222706432502, 37.68742077669241],\n   [127.05469544726837, 37.68872271744571],\n   [127.05610252968587, 37.68920437206933],\n   [127.05817261245335, 37.689669926579896],\n   [127.05839034056493, 37.689685868260845],\n   [127.05966013532905, 37.69033198280304],\n   [127.06220905346507, 37.6928052648238],\n   [127.06236779518714, 37.693024235595644],\n   [127.06237354393551, 37.693135159327554],\n   [127.06239478737797, 37.693546495381035],\n   [127.06260174913736, 37.69424379234776],\n   [127.06273214287815, 37.694675910354626],\n   [127.06305928833302, 37.694799063961284],\n   [127.0633863131918, 37.694921883623124],\n   [127.06630814019205, 37.69444172108839],\n   [127.06854802331812, 37.69400040092273],\n   [127.0691509700167, 37.69368950125125],\n   [127.0708788225368, 37.693700048667225],\n   [127.07087906859705, 37.693700062181264],\n   [127.07265760034454, 37.69379950124051],\n   [127.07313786558306, 37.69397574643172],\n   [127.07339823304241, 37.69410087667406],\n   [127.07378269914281, 37.69449368954549],\n   [127.07393007343649, 37.69464423236862],\n   [127.07448636301702, 37.695009628645295],\n   [127.07490680588823, 37.69521968047455],\n   [127.0781962787199, 37.695986510360626],\n   [127.08110478631578, 37.696137457947785],\n   [127.08384328792428, 37.69426889016338],\n   [127.08387832248333, 37.69279212349614],\n   [127.08390501861447, 37.69178245187781],\n   [127.08503096598021, 37.69054991893467],\n   [127.08517719556258, 37.69038984858866],\n   [127.08845357891715, 37.68990263950211],\n   [127.09055188620752, 37.68957025151863],\n   [127.09238424524798, 37.689933184218404],\n   [127.09303672225575, 37.689966466063815],\n   [127.09509714278617, 37.68938485003124],\n   [127.09599596058983, 37.68907104690658],\n   [127.09611844498184, 37.68777746180145],\n   [127.09629522668848, 37.687169760670066],\n   [127.09642514740469, 37.68626418153678],\n   [127.09642311415676, 37.685637441138354],\n   [127.09580983621922, 37.68464039310604],\n   [127.09428757876928, 37.683418538095395],\n   [127.09291905210979, 37.6815515024995],\n   [127.09194656333966, 37.67919029908044],\n   [127.09194153856279, 37.67869251579676],\n   [127.09218959069077, 37.67803010766824],\n   [127.09247959854748, 37.67764303460199],\n   [127.092614356114, 37.677474561385274],\n   [127.0939627740268, 37.675617232268024],\n   [127.09644725778618, 37.66968910695227],\n   [127.0962373655161, 37.66882941453695],\n   [127.09497929397519, 37.66760002717652],\n   [127.09458063193003, 37.666172300680316],\n   [127.0946800144686, 37.66492127931861],\n   [127.09541063101342, 37.663889644590746],\n   [127.09414211770347, 37.66329882532109],\n   [127.09217357563625, 37.66079254873924],\n   [127.09120691221572, 37.659334556715166],\n   [127.09113228726125, 37.658243024751805],\n   [127.09157431798847, 37.657772436849626],\n   [127.09235164609564, 37.65549186279581],\n   [127.09286741474956, 37.654453659625155],\n   [127.09356843248374, 37.653318729010714],\n   [127.0939157426287, 37.65296173000377],\n   [127.09400788402047, 37.65272853044388],\n   [127.09404075508206, 37.65254014144715],\n   [127.09403529660045, 37.652418519610805],\n   [127.09399991212685, 37.652295226453205],\n   [127.09326587472968, 37.65061772855914],\n   [127.09246587944706, 37.64970976752811],\n   [127.0926944997674, 37.64857999351487],\n   [127.09438136905902, 37.64495591656824],\n   [127.09456996976148, 37.644570600805835],\n   [127.09755891099569, 37.64396483085298],\n   [127.09890806335117, 37.6440343984249],\n   [127.10159264246101, 37.64477235815439],\n   [127.10280013821811, 37.64520492267967],\n   [127.1028823886103, 37.64540897697184],\n   [127.10386737257883, 37.64548852460528],\n   [127.10411721532803, 37.64548033184725],\n   [127.10657637866768, 37.64537730972521],\n   [127.10769224246286, 37.64496664676661],\n   [127.10786269687772, 37.64469282241332],\n   [127.10928861344723, 37.64285888886917],\n   [127.10989832951653, 37.64248752395126],\n   [127.11125728398851, 37.64237533392683],\n   [127.11142380118652, 37.64208686345102],\n   [127.11178551212458, 37.64073538398819],\n   [127.11113544340976, 37.639943933494294],\n   [127.11090335225738, 37.638262475561504],\n   [127.11155819260641, 37.63803293665921],\n   [127.11163126044404, 37.63797762400025],\n   [127.11163252401063, 37.63797657539423],\n   [127.1116335605215, 37.63797538002365],\n   [127.11163434149125, 37.637974066637646],\n   [127.11248319338027, 37.63648912158805],\n   [127.11220344878163, 37.63264276723643],\n   [127.1116323201253, 37.631503284414286],\n   [127.111371172448, 37.631227883211594],\n   [127.11082549334053, 37.630781845942806],\n   [127.10888476082644, 37.62930151924797],\n   [127.10596351597135, 37.62733070951351],\n   [127.10432790803095, 37.62327070428881],\n   [127.10406621959581, 37.62165311388996],\n   [127.10490909745644, 37.62156734430207],\n   [127.10538285122183, 37.62090189640023],\n   [127.1056619895515, 37.6204162469155],\n   [127.10555591433918, 37.62037072519622],\n   [127.11130504706806, 37.62069184201602],\n   [127.11214286452346, 37.620307017764965],\n   [127.11502106963971, 37.61953760329908],\n   [127.11619333227809, 37.61894778380488],\n   [127.11725242287622, 37.61674187052708],\n   [127.11699384437024, 37.616455741862225],\n   [127.11687861029367, 37.61632493344969],\n   [127.11667194632817, 37.61601542319483],\n   [127.11670352036289, 37.61571891606276],\n   [127.11679649663584, 37.614954676556906],\n   [127.11708589521979, 37.61396022396543],\n   [127.11737381655063, 37.61224525812902],\n   [127.1172872225345, 37.611180500418115],\n   [127.11687784452862, 37.609530700257416],\n   [127.11669806436957, 37.60884925145508],\n   [127.11809512146557, 37.60547242425238],\n   [127.11809557127602, 37.60544383142748],\n   [127.1180703665001, 37.604938459230944],\n   [127.11804701133192, 37.60460229805868],\n   [127.11558445031194, 37.601718190575795],\n   [127.11409999472093, 37.60012853384475],\n   [127.11447064798934, 37.599054605066726],\n   [127.11548924282775, 37.59765428196658],\n   [127.11572670052439, 37.597314488762194],\n   [127.11644313933067, 37.59619235770955],\n   [127.11687748428929, 37.59549705077813],\n   [127.11666448808528, 37.594016545777],\n   [127.11569009390017, 37.59362528500345],\n   [127.11552687588426, 37.593560121449116],\n   [127.11548546147779, 37.593548610635445],\n   [127.11541325076173, 37.593530949622995],\n   [127.1150282067279, 37.59349020815677],\n   [127.11445400975263, 37.59343304169271],\n   [127.1142683890762, 37.593416046920936],\n   [127.11366535163782, 37.593356363149674],\n   [127.11333737292749, 37.593259818471644],\n   [127.11241171452632, 37.59185636470543],\n   [127.1101545340761, 37.58743032790922],\n   [127.109944225286, 37.58595741365269],\n   [127.10973285600105, 37.58524413527232],\n   [127.109376495115, 37.58409232681838],\n   [127.10896665929798, 37.58337922727966],\n   [127.1074133022487, 37.58249485444903],\n   [127.10525566107893, 37.58156565577623],\n   [127.10344046907751, 37.58059529802311],\n   [127.1031362307321, 37.58021462295139],\n   [127.10289434724014, 37.57990679295722],\n   [127.10290869795878, 37.57978627410625],\n   [127.10302529385626, 37.578906983060165],\n   [127.10231819140941, 37.57804309180647],\n   [127.10195757011611, 37.577552361766436],\n   [127.10114351387524, 37.5760709289373],\n   [127.10091888215331, 37.574204101343454],\n   [127.10088786785404, 37.57376349428088],\n   [127.10166464576407, 37.57240065940598],\n   [127.10312974060275, 37.572279426510896],\n   [127.10366088935352, 37.57191791188504],\n   [127.10367642912719, 37.57190732992747],\n   [127.10423020073478, 37.571387655728294],\n   [127.10385348290774, 37.5704937526072],\n   [127.10272467888672, 37.56706421441915],\n   [127.10167082530147, 37.56340484757312],\n   [127.10120468941865, 37.56157990217243],\n   [127.10116259287368, 37.561313298804556],\n   [127.10112209698956, 37.56105064062629],\n   [127.10113406515579, 37.561003040117356],\n   [127.10127747386707, 37.5604659926051],\n   [127.10139885421326, 37.56025021143431],\n   [127.10154158761175, 37.55999837150702],\n   [127.10169403565163, 37.55974146071346],\n   [127.10180078997355, 37.55956709194242],\n   [127.10185922758541, 37.55949382793663],\n   [127.10216726500754, 37.55927810196558],\n   [127.10492928845024, 37.556421585556336],\n   [127.10540370353208, 37.556335336136144],\n   [127.1054039797918, 37.556335335256705],\n   [127.1060404466977, 37.55636800208074],\n   [127.10639619161522, 37.55646172320076],\n   [127.10714458157308, 37.55710610125959],\n   [127.10998660563237, 37.55834211535795],\n   [127.11167162292871, 37.55872797074785],\n   [127.11296064655822, 37.558794346205815],\n   [127.11522556193233, 37.55676018833242],\n   [127.11564563403175, 37.55769987998584],\n   [127.11730331913004, 37.559425641957304],\n   [127.11736706467859, 37.55947936004973],\n   [127.12295557080333, 37.56348990218229],\n   [127.12807612442845, 37.56590930955854],\n   [127.12861049420515, 37.56616043544185],\n   [127.13087184119757, 37.56687794943424],\n   [127.13375309980111, 37.56779331042001],\n   [127.1342663695222, 37.56795394418317],\n   [127.13474102736858, 37.56802464198976],\n   [127.13766579387027, 37.568424209460574],\n   [127.14560001479188, 37.568431169360885],\n   [127.1489495440468, 37.56843431854424],\n   [127.15124335260579, 37.569795301642884],\n   [127.15319599288145, 37.57096985010494],\n   [127.1549810601263, 37.57204353719504],\n   [127.15689916857828, 37.572954115769065],\n   [127.16084531250948, 37.575652662972885],\n   [127.1614681179371, 37.57604621474144],\n   [127.16200668497996, 37.57638652892725],\n   [127.16255503715276, 37.576730120691614],\n   [127.16674716243453, 37.578975666143656],\n   [127.1686475082068, 37.57899747794731],\n   [127.1686477364252, 37.57899709190847],\n   [127.17025143915063, 37.579016497229034],\n   [127.17183744214854, 37.57926199037442],\n   [127.17434341757382, 37.58002897085931],\n   [127.1745646848708, 37.58009734641977],\n   [127.17507432498863, 37.58025539696954],\n   [127.17510636726472, 37.58026661233748],\n   [127.17565914212791, 37.58056537625033],\n   [127.17585094748326, 37.580669273596094],\n   [127.17601761614299, 37.58076165809506],\n   [127.17637876684248, 37.58095906115711],\n   [127.17651851047951, 37.58103431126574],\n   [127.17715482863876, 37.58120032803303],\n   [127.17714288110588, 37.579858265364955],\n   [127.17706037157764, 37.579544339856604],\n   [127.17698860354076, 37.579416335558484],\n   [127.1769358967764, 37.57934067705333],\n   [127.17688336105913, 37.579265297509494],\n   [127.17674096763471, 37.57911572057686],\n   [127.17668903282423, 37.57906934265248],\n   [127.17559141963727, 37.57845491709445],\n   [127.17544227008392, 37.57836278564491],\n   [127.17541541545809, 37.57829863026236],\n   [127.17540428164837, 37.5782080868865],\n   [127.17540423462499, 37.57820770097899],\n   [127.17536098677607, 37.57784747971302],\n   [127.17532131705451, 37.577363342392985],\n   [127.17532434111249, 37.57729990460691],\n   [127.17532755327359, 37.57723513694318],\n   [127.17568308790581, 37.57489869852943],\n   [127.17570202935693, 37.57483166387621],\n   [127.1761038656408, 37.57413533821314],\n   [127.17613152481712, 37.57408856274329],\n   [127.1763084845669, 37.57397427178013],\n   [127.1764980410476, 37.573801389354294],\n   [127.17694363989052, 37.57329364424182],\n   [127.17773113162198, 37.57218791533225],\n   [127.17781926600982, 37.57205770863517],\n   [127.17913286165616, 37.56912369470184],\n   [127.17914903579462, 37.569085099484376],\n   [127.17920371766037, 37.56894649028298],\n   [127.17922017374693, 37.568873819289145],\n   [127.1793312074897, 37.56826577378065],\n   [127.17937794706329, 37.56798582974494],\n   [127.17943875427594, 37.56760084598443],\n   [127.17946467033734, 37.566583016887506],\n   [127.1801941808996, 37.56458787465877],\n   [127.18099458554735, 37.56332274641403],\n   [127.18118573247567, 37.5630169666324],\n   [127.18127812880661, 37.562830429531225],\n   [127.18151410938351, 37.5622675112996],\n   [127.18190992016416, 37.56124879949777],\n   [127.18199542158489, 37.56099244596784],\n   [127.18204081632643, 37.56040549758009],\n   [127.18202084650781, 37.55994782824462],\n   [127.18202079834414, 37.55994743873002],\n   [127.18198419478763, 37.55965551058688],\n   [127.181787374278, 37.55882911648127],\n   [127.18165662833978, 37.557998302305066],\n   [127.18135146515526, 37.553258048765485],\n   [127.1813403273481, 37.55296496478539],\n   [127.18142169421999, 37.552757324685345],\n   [127.18159477250546, 37.552592350538426],\n   [127.18170384487837, 37.552507995663994],\n   [127.182926745995, 37.55023535046519],\n   [127.18284450852022, 37.5494290906802],\n   [127.1826736926933, 37.547747320441374],\n   [127.18265679525105, 37.54752440129038],\n   [127.18276107519915, 37.546499426131824],\n   [127.18279599113094, 37.54638983933193],\n   [127.18353917154177, 37.54517039495601],\n   [127.18169282664165, 37.546444749492956],\n   [127.18125643812643, 37.54657971583249],\n   [127.17991053630526, 37.54657443190896],\n   [127.17933128170148, 37.54656742148427],\n   [127.17608601659725, 37.54561946546925],\n   [127.17571742052262, 37.54520770406822],\n   [127.17538324376515, 37.54520199786245],\n   [127.17463760452904, 37.54525912602242],\n   [127.17447709977255, 37.545277660809106],\n   [127.1743146633164, 37.54529732395536],\n   [127.17390569685948, 37.545600032434535],\n   [127.17157169030703, 37.545385773320305],\n   [127.1651681782903, 37.544631798899346],\n   [127.1631627309092, 37.54499101097656],\n   [127.16009589899915, 37.54151453079874],\n   [127.15958338038794, 37.54102822393515],\n   [127.15671195122711, 37.53757774554748],\n   [127.15603276390642, 37.53734241425835],\n   [127.15402132902574, 37.534514497157076],\n   [127.15356680162624, 37.533665193070505],\n   [127.15374715099493, 37.53160008846154],\n   [127.15353910604473, 37.53043671974301],\n   [127.15323227758938, 37.52923709895612],\n   [127.15316377616772, 37.529108804926516],\n   [127.14947629508237, 37.52502054146565],\n   [127.14779743367326, 37.52214947334571],\n   [127.14593249219926, 37.521955801698766],\n   [127.14578093977661, 37.521941375307804],\n   [127.14480065945018, 37.51938973774546],\n   [127.14509193634692, 37.51683494851859],\n   [127.14532885964844, 37.516607158321115],\n   [127.14538965833438, 37.51651332725961],\n   [127.14543404002671, 37.51642852537821],\n   [127.14544427334503, 37.51633278212625],\n   [127.14543916612115, 37.51606474208284],\n   [127.14485430106286, 37.51567802502981],\n   [127.14475438292835, 37.515645484717986],\n   [127.14460355182592, 37.515606249070395],\n   [127.1433741159653, 37.51557760277298],\n   [127.14210518808247, 37.51466010401819],\n   [127.14023773642688, 37.50985352338447],\n   [127.13996581242264, 37.50864088493049],\n   [127.14001384551528, 37.50852453676255],\n   [127.14032052088919, 37.508273591129395],\n   [127.14135345423192, 37.50694790778293],\n   [127.14147949264665, 37.50677882190354],\n   [127.14151665030897, 37.506708103021836],\n   [127.14151309866016, 37.50670219734203],\n   [127.14126852582045, 37.50641867264958],\n   [127.14117786173351, 37.50631882301717],\n   [127.1409971469675, 37.506163894207944],\n   [127.14081284937815, 37.50597856844039],\n   [127.14113191237908, 37.50546377662881],\n   [127.14223298799321, 37.504884417170004],\n   [127.14342402698134, 37.50441363645751],\n   [127.14392479947718, 37.50440092444333],\n   [127.14410326532739, 37.5043964831474],\n   [127.14550250136601, 37.50330570568455],\n   [127.14595206598852, 37.5032268868512],\n   [127.14769492141562, 37.50321206574927],\n   [127.14860243574782, 37.50372282317089],\n   [127.1490451129403, 37.50400073373371],\n   [127.15082434872058, 37.50406156817608],\n   [127.15165372960253, 37.503395752670144],\n   [127.1521091406241, 37.50297029811745],\n   [127.15323083173114, 37.50267941781355],\n   [127.15643795942692, 37.50185900576393],\n   [127.1565550135419, 37.50189404497823],\n   [127.15772923925586, 37.50317895196306],\n   [127.1588661178982, 37.50239019657405],\n   [127.16050427114095, 37.501094512438115],\n   [127.16121949144342, 37.50039527422741],\n   [127.16139030513257, 37.5002010498375],\n   [127.16143108867095, 37.50002247972045],\n   [127.16142016695896, 37.49970545993054],\n   [127.16135443347227, 37.499537459426215],\n   [127.16125092773778, 37.49939089854903],\n   [127.16110733163866, 37.49927532319294],\n   [127.15986640646732, 37.495706834034365],\n   [127.15996541511976, 37.49527986027531],\n   [127.16001826923471, 37.494707646707646],\n   [127.16002621602986, 37.49437455452455],\n   [127.1597454988479, 37.49363667082564],\n   [127.15970484975863, 37.49354690503217],\n   [127.15965196338816, 37.493434358962624],\n   [127.1595169833914, 37.49327460353943],\n   [127.15931845606401, 37.49313662914927],\n   [127.15764946367052, 37.490182673876625],\n   [127.15763380331077, 37.4901717886922],\n   [127.15740206846664, 37.48908854265012],\n   [127.15043020214158, 37.4852488932469],\n   [127.15021754524786, 37.48510537954825],\n   [127.14992170977906, 37.484905729020184],\n   [127.14868305306886, 37.484043192462515],\n   [127.148579027746, 37.483889863822014],\n   [127.14822520162613, 37.48336825471925],\n   [127.1477703794897, 37.48262416785541],\n   [127.14766978415436, 37.48242576594467],\n   [127.14760689985782, 37.48226916996039],\n   [127.14758794988897, 37.482221953125226],\n   [127.14752532591987, 37.48201386379114],\n   [127.14748230260066, 37.48180264116629],\n   [127.14747861659086, 37.481773480024934],\n   [127.14745907220343, 37.48158945671828],\n   [127.14739684802731, 37.480585692947166],\n   [127.14728211415827, 37.478739628360124],\n   [127.14715916521881, 37.47729511320696],\n   [127.14714533170121, 37.47722179897961],\n   [127.14584352066795, 37.477273068635235],\n   [127.145796878535, 37.47727491723534],\n   [127.14575646485959, 37.47727651768649],\n   [127.1443704748871, 37.47733114309174],\n   [127.14364342913493, 37.47525741763574],\n   [127.14361885800092, 37.47517065376269],\n   [127.14360185852455, 37.475082746354325],\n   [127.14359251006233, 37.47499412650607],\n   [127.1435733636672, 37.47468530404496],\n   [127.14356722898532, 37.474586308344385],\n   [127.14352658956736, 37.473930418522116],\n   [127.1394881494977, 37.474089616658645],\n   [127.13833405279085, 37.47413506191677],\n   [127.13766388419639, 37.474147919140066],\n   [127.13684251991336, 37.47415318717528],\n   [127.13677761017136, 37.47415548714804],\n   [127.1363969537117, 37.47417048239313],\n   [127.13546828445813, 37.47424021251808],\n   [127.1352873110148, 37.47425705042134],\n   [127.13312242962854, 37.4745798598],\n   [127.13285605187727, 37.47462645866824],\n   [127.13262354347293, 37.472395357539355],\n   [127.13282098466541, 37.46919569189179],\n   [127.13282095574344, 37.4691942063499],\n   [127.1327996032295, 37.468393664276086],\n   [127.13087554387387, 37.46775756923912],\n   [127.13083001167365, 37.46774525764664],\n   [127.13078324278253, 37.46773607352042],\n   [127.13073584632706, 37.46773009808275],\n   [127.13068790133586, 37.46772735681814],\n   [127.13063981486039, 37.46772786001192],\n   [127.13059198261257, 37.46773162422421],\n   [127.1305447100502, 37.46773862246795],\n   [127.13018892567416, 37.4678033697074],\n   [127.12933858733388, 37.467959210768726],\n   [127.12930302575272, 37.467966655370866],\n   [127.1266812987817, 37.468621775910755],\n   [127.12644686764371, 37.468750108913085],\n   [127.12602455140079, 37.4691058782371],\n   [127.12570131939023, 37.46931823345992],\n   [127.12551817321834, 37.46943048351345],\n   [127.12537453641595, 37.46951707676626],\n   [127.12531488674534, 37.46955289992616],\n   [127.12520301623321, 37.46961974560557],\n   [127.12510091762285, 37.469613652978204],\n   [127.12494334079366, 37.4696031234281],\n   [127.12487834302604, 37.469597279961135],\n   [127.12485136253184, 37.469523255232446],\n   [127.1248975970592, 37.46938552451682],\n   [127.12504582809731, 37.468979630189054],\n   [127.12516820516782, 37.46875144004594],\n   [127.1251820284445, 37.468352452585854],\n   [127.12519937283692, 37.467833231267484],\n   [127.12498822426089, 37.46724835864308],\n   [127.12482726884033, 37.46700159707097],\n   [127.12475930593294, 37.466916352653705],\n   [127.1244880965488, 37.4666435226522],\n   [127.12420970259667, 37.466517104307265],\n   [127.12420688119097, 37.46652048627629],\n   [127.12414326804209, 37.466495215148015],\n   [127.1237669717385, 37.46633342476096],\n   [127.12320225935616, 37.46597867005132],\n   [127.12185946757938, 37.46513395264438],\n   [127.11747485026633, 37.46220061045074],\n   [127.11747077309776, 37.462194983538076],\n   [127.11711994009005, 37.46167753791065],\n   [127.11698929578043, 37.46148450859329],\n   [127.11693965937852, 37.46090030945957],\n   [127.11689967841377, 37.4604011395989],\n   [127.1168948964395, 37.45864042153465],\n   [127.11669702911543, 37.45862325785094],\n   [127.11623941228295, 37.4585961143597],\n   [127.11589291262797, 37.45859194472567],\n   [127.11549606922648, 37.45869791695614],\n   [127.11547664981921, 37.458704404910726],\n   [127.11395536575394, 37.45951676647907],\n   [127.1136342600001, 37.45973246531095],\n   [127.11353635596203, 37.45980154776803],\n   [127.11341975702304, 37.45990076930371],\n   [127.11276328611841, 37.46053025046058],\n   [127.11290865142644, 37.46064192929295],\n   [127.11318486414389, 37.46075823127989],\n   [127.11329182742885, 37.46083640990825],\n   [127.11333484694086, 37.46090535023642],\n   [127.11333484534424, 37.46090566030155],\n   [127.11333387525471, 37.46108724567093],\n   [127.1122298035669, 37.46150499909431],\n   [127.11182027181094, 37.46164419121139],\n   [127.10868615916183, 37.4621564211832],\n   [127.10688007579863, 37.462369502165004],\n   [127.10667208681129, 37.46241051180783],\n   [127.10646723536935, 37.46242392987725],\n   [127.10616731423701, 37.462406454624805],\n   [127.10434135861443, 37.46217434553484],\n   [127.10447230410983, 37.46102952693268],\n   [127.10394160853244, 37.460050593707535],\n   [127.10381093870869, 37.459927656301566],\n   [127.09930610906709, 37.45667755614937],\n   [127.09823746163788, 37.456371246967336],\n   [127.09522278311739, 37.45639422742016],\n   [127.09431215563166, 37.45622477015542],\n   [127.09377352285104, 37.45607736437658],\n   [127.09354036220854, 37.455888055203964],\n   [127.08882127538375, 37.44974941955953],\n   [127.08843214262606, 37.44899484298624],\n   [127.08832682104673, 37.44862776075761],\n   [127.08832676671251, 37.44862736036078],\n   [127.08836930483001, 37.44724005811312],\n   [127.08816128515241, 37.44538498355397],\n   [127.08785507296487, 37.44489387071301],\n   [127.08777816075089, 37.44487515146447],\n   [127.08730388244146, 37.44464171021886],\n   [127.082904590745, 37.442173808672536],\n   [127.08243181881421, 37.441579468344386],\n   [127.08209733924632, 37.44135078689038],\n   [127.08144410364704, 37.441230999973904],\n   [127.08017160102241, 37.441051093956524],\n   [127.07908335144711, 37.44129874337731],\n   [127.07817890647885, 37.44155161511739],\n   [127.07760188111618, 37.44170909672176],\n   [127.07601057612212, 37.44213893609793],\n   [127.07475598627195, 37.442218846646846],\n   [127.0744976898156, 37.44223280045726],\n   [127.07224980537977, 37.442199811541435],\n   [127.07186137425094, 37.44102970185969],\n   [127.07221171709271, 37.439382901540675],\n   [127.07376019338288, 37.437789433842276],\n   [127.07379537723341, 37.43765707199904],\n   [127.07384351098383, 37.43740786000359],\n   [127.07356006757476, 37.436963727095595],\n   [127.0730242304579, 37.43640683285397],\n   [127.0709099786286, 37.43309491379506],\n   [127.06961542955781, 37.430611692111725],\n   [127.06569800697741, 37.42915810241767],\n   [127.06114498957349, 37.42998528229182],\n   [127.05978112053222, 37.4295872657291],\n   [127.05345832004603, 37.42880858729806],\n   [127.0503615218898, 37.42982129068007],\n   [127.04957975136693, 37.4302701432368],\n   [127.04736923949098, 37.430701820429285],\n   [127.04722475567219, 37.432040442932944],\n   [127.04644166111552, 37.43299046057295],\n   [127.04590560884242, 37.43348142450798],\n   [127.04418573191268, 37.4350554539952],\n   [127.04109031701444, 37.437769124917864],\n   [127.04087975167322, 37.437878441821276],\n   [127.04048048713241, 37.43808242753594],\n   [127.04005277677203, 37.43823714507124],\n   [127.03956828753749, 37.438195909531515],\n   [127.03922186091357, 37.43816448538617],\n   [127.03903241753622, 37.43815131864844],\n   [127.03706776525239, 37.43827496782511],\n   [127.03697772841022, 37.43829639401592],\n   [127.03557372599246, 37.439004097845434],\n   [127.03532026694523, 37.439966564131865],\n   [127.03529507001076, 37.44007215137235],\n   [127.03520801081689, 37.440437652690015],\n   [127.03516906728176, 37.4406057551936],\n   [127.03517186217657, 37.44094194757337],\n   [127.03528263969284, 37.44103476257073],\n   [127.0360722545635, 37.44155099433042],\n   [127.03608762765222, 37.44156253268867],\n   [127.03643099253159, 37.441832734480634],\n   [127.03665445379359, 37.442080160034884],\n   [127.03748294023711, 37.44326670427758],\n   [127.03793486290014, 37.44417883691202],\n   [127.03803531906024, 37.444516404127626],\n   [127.03818353178544, 37.44501754331141],\n   [127.03823039750196, 37.44518026909942],\n   [127.03820566265372, 37.4458588524557],\n   [127.03781196836518, 37.44892239759809],\n   [127.03777449207378, 37.449201719792974],\n   [127.03774440117567, 37.44941881885998],\n   [127.03696804004046, 37.450691157558936],\n   [127.03640970411149, 37.45149687786112],\n   [127.03577642448006, 37.452195909168545],\n   [127.03626740809199, 37.45426216962762],\n   [127.03712770180721, 37.45520768138208],\n   [127.03706742872744, 37.45547434330387],\n   [127.0367406508785, 37.456443867860756],\n   [127.03492422096939, 37.46017540450221],\n   [127.03453266613107, 37.46333522021434],\n   [127.0346601133906, 37.463976592257936],\n   [127.03467091241455, 37.464090458173466],\n   [127.03460314189319, 37.464173939752115],\n   [127.0331405892598, 37.46501738910679],\n   [127.03275757043367, 37.46519318169501],\n   [127.03192176568467, 37.465518883534436],\n   [127.03119554711617, 37.465626061203594],\n   [127.03055508280168, 37.46551416328518],\n   [127.03039682015434, 37.46548632407364],\n   [127.02992820559366, 37.46535158628146],\n   [127.02958338917796, 37.4642881854868],\n   [127.02873126984468, 37.46202826888523],\n   [127.02815696813059, 37.460621993714916],\n   [127.02634678296988, 37.458104359241986],\n   [127.02620458102675, 37.4579878159816],\n   [127.02599541833983, 37.4578169581426],\n   [127.02543481238588, 37.45756450281699],\n   [127.025277616827, 37.4574938687448],\n   [127.02150295539224, 37.45622583459711],\n   [127.01969201000381, 37.45578660642505],\n   [127.01557484735889, 37.454916014104],\n   [127.0150649965327, 37.45483188354524],\n   [127.0148735746752, 37.454841752950394],\n   [127.01450961789199, 37.45486122860588],\n   [127.0107251186711, 37.45577244670041],\n   [127.00923267412264, 37.45722091033481],\n   [127.00704092418567, 37.460215469835134],\n   [127.00496875820613, 37.463186899888576],\n   [127.0044696722443, 37.46407327656836],\n   [127.00476218583773, 37.46492302457033],\n   [127.00490546550544, 37.46584260252905],\n   [127.00452135791603, 37.46679064066523],\n   [127.00424565849995, 37.46714344410636],\n   [127.00367535892853, 37.467720386588596],\n   [127.00273482189448, 37.46712461108707],\n   [127.00196279723106, 37.467087449562925],\n   [127.00047686511023, 37.46706294825534],\n   [126.99742674489028, 37.46724223517637],\n   [126.99717099538091, 37.467214070232885],\n   [126.99698554452699, 37.46717971600085],\n   [126.99675187621652, 37.467072433303954],\n   [126.99628192995874, 37.46665064295208],\n   [126.99719359563045, 37.464184165933666],\n   [126.99725312948726, 37.46401973479701],\n   [126.99737077162193, 37.46368749255192],\n   [126.99676862985268, 37.461873352884574],\n   [126.9942899320261, 37.461421317893326],\n   [126.99315292901082, 37.46127623511324],\n   [126.9916230699418, 37.46044156303114],\n   [126.98999453840017, 37.4594629780069],\n   [126.98863550944588, 37.458165845862126],\n   [126.98678106962474, 37.45740326454205],\n   [126.98238921265964, 37.45591710269429],\n   [126.97775711976738, 37.455199116892956],\n   [126.97457956131598, 37.454412875477026],\n   [126.97175282603096, 37.45173733547508],\n   [126.97056939304684, 37.44945270098554],\n   [126.96959356922594, 37.44909597687449],\n   [126.96866171986065, 37.44875446455986],\n   [126.967662979455, 37.44838054424455],\n   [126.96428976779832, 37.44626723711946],\n   [126.9639418980544, 37.4452458892869],\n   [126.96393361850694, 37.445214631166266],\n   [126.96432082073919, 37.44346229477368],\n   [126.96463404099109, 37.44203964730629],\n   [126.9638033064139, 37.44078692549053],\n   [126.96373604591848, 37.44074688094471],\n   [126.96311396054732, 37.44037652510398],\n   [126.96298353095135, 37.44029960855669],\n   [126.96294292915762, 37.44028045754587],\n   [126.95899886229834, 37.43912694137169],\n   [126.95636819846372, 37.43875401134443],\n   [126.94928410957398, 37.438250273989894],\n   [126.94815539463404, 37.43863037558223],\n   [126.9456762727248, 37.43730530031926],\n   [126.9450906377534, 37.437088221251784],\n   [126.94022016050745, 37.43571237211725],\n   [126.9393643055451, 37.43602581803675],\n   [126.93857452478132, 37.43642042942645],\n   [126.93774638275855, 37.4373857576777],\n   [126.93727268446924, 37.4386322720711],\n   [126.93725777476023, 37.43892734300138],\n   [126.93724856528063, 37.439167511901296],\n   [126.93724711351074, 37.43921284001876],\n   [126.93740216615096, 37.43937426800926],\n   [126.93769358300823, 37.43968442708137],\n   [126.937713857788, 37.43972075748547],\n   [126.93786405751602, 37.44019753476193],\n   [126.9377458076725, 37.440354020623566],\n   [126.93735357677096, 37.440871598648926],\n   [126.93668810996004, 37.44169594611689],\n   [126.93664692478835, 37.441745758401595],\n   [126.9348979978904, 37.443209481797815],\n   [126.9344508816705, 37.44325822606353],\n   [126.93055407014207, 37.44546848090059],\n   [126.93051744833456, 37.445548423023716],\n   [126.9303939359546, 37.44581837200393],\n   [126.93032633869163, 37.4459686910352],\n   [126.93016316403606, 37.4463779831893],\n   [126.93035671199829, 37.44733422226026],\n   [126.92839869373803, 37.450212485076534],\n   [126.9283570058542, 37.44989868951273],\n   [126.9283079026723, 37.44954867104572],\n   [126.9282796601699, 37.4493526845579],\n   [126.92399602716448, 37.446210224228935],\n   [126.92325327636715, 37.44577274628548],\n   [126.92289722984206, 37.445173349643994],\n   [126.92294662379582, 37.44487295199244],\n   [126.9229034040661, 37.444673007921715],\n   [126.92276280882136, 37.44404108321409],\n   [126.9220070215115, 37.443250757259776],\n   [126.92069895346346, 37.44151996843528],\n   [126.91866310009635, 37.43998262017672],\n   [126.91538923818742, 37.43966802735873],\n   [126.91389137436857, 37.43930708270779],\n   [126.91318980966076, 37.43909539212985],\n   [126.91230851419965, 37.43857946078951],\n   [126.91143164721281, 37.43762316839669],\n   [126.91120280868347, 37.43720121439982],\n   [126.91120000044674, 37.437187408644185],\n   [126.91121640637944, 37.4370463602423],\n   [126.91128427742785, 37.43654325238066],\n   [126.9113332747712, 37.4361713431048],\n   [126.91002834759448, 37.43432412191302],\n   [126.90990399563053, 37.43422744640446],\n   [126.90967840112272, 37.43405973794322],\n   [126.90940570432362, 37.43386468248096],\n   [126.90939564142113, 37.43386044896999],\n   [126.90859344653741, 37.433691729024154],\n   [126.90752668322209, 37.43362021944715],\n   [126.90663449172253, 37.43359694340164],\n   [126.90582547214655, 37.43399615235654],\n   [126.90582519408719, 37.43399615186017],\n   [126.90522457694115, 37.433997069199144],\n   [126.90298757638999, 37.434067620178844],\n   [126.90186087029541, 37.436397524817494],\n   [126.89914461684712, 37.43866819351907],\n   [126.89897791331421, 37.43870182579147],\n   [126.89958451332427, 37.44036666498753],\n   [126.8982570102593, 37.44320652479412],\n   [126.89578235329736, 37.44563339801652],\n   [126.89564123458763, 37.44829320383536],\n   [126.8946038195621, 37.451159154897915],\n   [126.89398300003057, 37.45271733991926],\n   [126.89182284732061, 37.45228350409839],\n   [126.8910486017461, 37.45224245321772],\n   [126.88978012488182, 37.45227307520445],\n   [126.88964213513057, 37.45231546387733],\n   [126.88956055650301, 37.45243222840465],\n   [126.88952054982705, 37.45262112176969],\n   [126.88950365869798, 37.4527010699145],\n   [126.88953577253639, 37.452946634786336],\n   [126.88964770418707, 37.45331924221095],\n   [126.88966648039558, 37.453521707197815],\n   [126.88965276996692, 37.45359855847977],\n   [126.889592047684, 37.45370296489857],\n   [126.88876417143807, 37.45476564219567],\n   [126.88828498935065, 37.45516865405293],\n   [126.8863169029548, 37.45628901611006],\n   [126.88626468031083, 37.456362515796506],\n   [126.88618548846422, 37.456522645677246],\n   [126.88588075182155, 37.45750839016862],\n   [126.8853564731724, 37.45953456857106],\n   [126.88534006096899, 37.45963704166101],\n   [126.88534960239055, 37.45986258161532],\n   [126.88539936108317, 37.46000916976716],\n   [126.88543071619748, 37.46009095992281],\n   [126.88544367776255, 37.46012453265269],\n   [126.88551853776474, 37.46025525216233],\n   [126.885650375358, 37.46043079769516],\n   [126.88607198185638, 37.460859466843345],\n   [126.886128084154, 37.460899787651186],\n   [126.88620134657874, 37.46091646478202],\n   [126.88625378849405, 37.46092666112505],\n   [126.88747359786943, 37.4610655269108],\n   [126.88749655889849, 37.46106301666018],\n   [126.88768203443473, 37.46103729223392],\n   [126.88785888520992, 37.460997475157],\n   [126.88795325015386, 37.46095448958872],\n   [126.8883358698455, 37.46078084535764],\n   [126.88863061689209, 37.46078817357594],\n   [126.88876919238663, 37.46083250900126],\n   [126.88887551772156, 37.460946920406975],\n   [126.88853249141253, 37.46133205164132],\n   [126.88810791069389, 37.461806645801666],\n   [126.88712180563425, 37.462904361685275],\n   [126.88704339763547, 37.46290231182216],\n   [126.88692384788376, 37.46289178120236],\n   [126.8866437804029, 37.46286673117086],\n   [126.88624491347673, 37.46279820029081],\n   [126.88485800472498, 37.46254315202208],\n   [126.88285845675665, 37.46426078649855],\n   [126.88457429409604, 37.46564823572887],\n   [126.88399444693059, 37.46666861463424],\n   [126.88153185190849, 37.46929115820678],\n   [126.88020447481996, 37.47093497073136],\n   [126.87987759363838, 37.4714213189637],\n   [126.87978452249763, 37.471539058789205],\n   [126.87955796413078, 37.47182967743504],\n   [126.87896535335604, 37.47259125928221],\n   [126.87873771587651, 37.4728841366197],\n   [126.87845148258745, 37.47325521436153],\n   [126.87801141557026, 37.47382773802628],\n   [126.87758404673444, 37.47451796201532],\n   [126.87614354086422, 37.47684468784561],\n   [126.87598571313652, 37.47710383526691],\n   [126.87532045038951, 37.478372692072554],\n   [126.87527082721985, 37.47846810142021],\n   [126.87495854231086, 37.47907199778555],\n   [126.87428469764085, 37.480380256044086],\n   [126.87413691190955, 37.480757112415716],\n   [126.87337901472617, 37.48243356052865],\n   [126.8727442969783, 37.482424995238794],\n   [126.87277987743597, 37.482794985827105],\n   [126.87278663474126, 37.48348878210105],\n   [126.8717971958008, 37.48499743368912],\n   [126.87176262543268, 37.485269390497095],\n   [126.87244469888192, 37.48611004050176],\n   [126.87267382382166, 37.486220095922036],\n   [126.87273456256477, 37.486240154483085],\n   [126.87289532395594, 37.486251871041624],\n   [126.87304222324997, 37.486197970915136],\n   [126.87313717688521, 37.486151898173034],\n   [126.87455644442305, 37.48536674989791],\n   [126.8749436128769, 37.485699557362956],\n   [126.87626185595225, 37.48723955123044],\n   [126.8766887354242, 37.4878526882681],\n   [126.87676002659364, 37.48811743166551],\n   [126.87680025485933, 37.48835707247939],\n   [126.87679035662485, 37.48857077467621],\n   [126.87666836775517, 37.488731698886916],\n   [126.87638215065101, 37.488941163019874],\n   [126.87604346872821, 37.489047234427446],\n   [126.8758168244155, 37.48902812727765],\n   [126.87471601721661, 37.48857729667705],\n   [126.87307774871593, 37.48825285739754],\n   [126.87295540342764, 37.48830058919606],\n   [126.87292252364195, 37.48831350594885],\n   [126.87274847564402, 37.488430729714324],\n   [126.87266911360857, 37.48855396331768],\n   [126.87241371655864, 37.48933079643515],\n   [126.8724404006732, 37.48953552394017],\n   [126.87244516069696, 37.48954200832903],\n   [126.87267624752327, 37.48985141040301],\n   [126.87277059260437, 37.4899587830918],\n   [126.87283922397087, 37.490014335559685],\n   [126.87336843875514, 37.490386572775556],\n   [126.87357693309845, 37.490481684306566],\n   [126.87375273922521, 37.490475679558024],\n   [126.8739332173435, 37.490428855349435],\n   [126.87402067996526, 37.490316033035164],\n   [126.87409064170969, 37.49021447138926],\n   [126.87414915935193, 37.48998055733015],\n   [126.87411047167724, 37.48977722861176],\n   [126.87432244123868, 37.48969551876888],\n   [126.874579194672, 37.48976928586574],\n   [126.87465124038863, 37.48989887276378],\n   [126.8748395606655, 37.49074546038744],\n   [126.87453459446087, 37.4910683660723],\n   [126.87437268165921, 37.49122136264536],\n   [126.87413007357357, 37.491337668238565],\n   [126.87395185020526, 37.49130790942899],\n   [126.87225463784726, 37.490602443243475],\n   [126.8715772336594, 37.490209200924156],\n   [126.8715477661439, 37.49008528105915],\n   [126.8715056835045, 37.489998794005494],\n   [126.8713423702246, 37.48983362446104],\n   [126.87125961324185, 37.48977074005055],\n   [126.87108027252202, 37.48968325006296],\n   [126.87094589711485, 37.48963721815786],\n   [126.87065821481464, 37.48956087360729],\n   [126.87042098603534, 37.48953583190591],\n   [126.8703347531482, 37.489544746411035],\n   [126.87022812471946, 37.48959615733273],\n   [126.87017041588264, 37.489665641527296],\n   [126.87013621956086, 37.48971909518386],\n   [126.86987690273945, 37.490296013812674],\n   [126.86936626773986, 37.49201860453653],\n   [126.86936892693092, 37.49271744174477],\n   [126.86937342007144, 37.49277207332995],\n   [126.86950267732847, 37.4943284018372],\n   [126.86758862651642, 37.49481588988599],\n   [126.86761040833085, 37.49468809006178],\n   [126.86619406839125, 37.49253860319941],\n   [126.86451819511568, 37.491203026084236],\n   [126.86302813884646, 37.49087608634184],\n   [126.8628110904766, 37.490828236574096],\n   [126.8627337568918, 37.490804494216775],\n   [126.86258684437263, 37.49075871779353],\n   [126.86238768631601, 37.49067907388865],\n   [126.86217497834068, 37.49057746081463],\n   [126.86208955762203, 37.49053119281495],\n   [126.86195048469753, 37.490447969739456],\n   [126.86183032272321, 37.490367864585316],\n   [126.86176820935094, 37.490326119958326],\n   [126.86162303620054, 37.49020374479842],\n   [126.861481767076, 37.490067301502876],\n   [126.86138442651429, 37.489968076240665],\n   [126.86111908480686, 37.48966817910177],\n   [126.86102581461058, 37.48955826657109],\n   [126.86101648222878, 37.489547269155715],\n   [126.86068610548242, 37.48916817605563],\n   [126.86018595731015, 37.48859544427551],\n   [126.85925601817621, 37.48753114874059],\n   [126.8585450417844, 37.48672107555025],\n   [126.85822542932799, 37.486360288148504],\n   [126.85811613511434, 37.48623739417916],\n   [126.85797103348565, 37.48608461582477],\n   [126.85783430290624, 37.485983086909044],\n   [126.85763012230208, 37.485867113767235],\n   [126.85748360128834, 37.48580696465288],\n   [126.85744917364828, 37.48579453432356],\n   [126.8572904242174, 37.48576055327166],\n   [126.85721499765847, 37.48575426248749],\n   [126.85708640467882, 37.48574368934753],\n   [126.85676809597386, 37.485721054345184],\n   [126.85655188308645, 37.48570671540534],\n   [126.85653942998213, 37.48570626540688],\n   [126.85349858643376, 37.482621834752926],\n   [126.85270046935516, 37.481818616044556],\n   [126.85261962553658, 37.481786686082536],\n   [126.85258624296549, 37.48177876052663],\n   [126.85206621905958, 37.48166492424135],\n   [126.851751885319, 37.48160370879597],\n   [126.85166783206732, 37.48158868170002],\n   [126.85161377601116, 37.481583540277235],\n   [126.85099047011765, 37.481527845736494],\n   [126.85084315206339, 37.48152259001683],\n   [126.85070182006966, 37.48152550894454],\n   [126.85057300416253, 37.48153941821166],\n   [126.85041837279233, 37.481566818390434],\n   [126.8493324275512, 37.48184331552959],\n   [126.84698612892635, 37.48189443252598],\n   [126.8466017682575, 37.48167122516655],\n   [126.8465568595493, 37.4816333299397],\n   [126.84651717642556, 37.48159893200954],\n   [126.84642038315994, 37.48150701100949],\n   [126.84640821872374, 37.481495453254205],\n   [126.84626305142321, 37.48130323673433],\n   [126.84621918975228, 37.481240109255275],\n   [126.84604937012155, 37.480925377389326],\n   [126.84600724781465, 37.480798339922416],\n   [126.84598678074923, 37.480695539848334],\n   [126.845976830455, 37.48064175143651],\n   [126.8459342194971, 37.4803147980667],\n   [126.8454343751056, 37.47446358358097],\n   [126.84536592690858, 37.47385949919483],\n   [126.84536055354526, 37.47381246555105],\n   [126.84503323897347, 37.47355891752733],\n   [126.84419346893853, 37.47401499804191],\n   [126.84395991392603, 37.474482912719694],\n   [126.84311200124603, 37.47486984773489],\n   [126.8430773430002, 37.4748849961075],\n   [126.84295255134103, 37.47492481992884],\n   [126.84278163936918, 37.47497161415357],\n   [126.8384196728936, 37.47536790968778],\n   [126.83825923862072, 37.475374710633375],\n   [126.83610875738073, 37.474675546160455],\n   [126.83461359077172, 37.47487025950699],\n   [126.83305167563579, 37.47724609250874],\n   [126.8320789692601, 37.477567839756105],\n   [126.8317480949545, 37.47765099536511],\n   [126.83080142694622, 37.47742382313479],\n   [126.83045751616729, 37.47733913587807],\n   [126.82962156724689, 37.476895297577286],\n   [126.82955329242202, 37.47662461753497],\n   [126.8295367634638, 37.476595599289965],\n   [126.82920434453489, 37.47619529596771],\n   [126.82818195209177, 37.47603415313276],\n   [126.82787204443441, 37.4759892067256],\n   [126.82410830374371, 37.476352189043084],\n   [126.8195759381957, 37.47635542376256],\n   [126.8194233714162, 37.47632759999209],\n   [126.81930759652315, 37.47628940790379],\n   [126.81929647232971, 37.47628516514027],\n   [126.81914316116901, 37.476205532322275],\n   [126.81832133653424, 37.475298753226035],\n   [126.81766855225743, 37.473238400442405],\n   [126.81465604938846, 37.47476019094166],\n   [126.81472979756464, 37.475057306435204],\n   [126.81496487102987, 37.47586011888277],\n   [126.81522179414826, 37.476251063198596],\n   [126.81529553572993, 37.47636211907153],\n   [126.81641141908979, 37.477536748815474],\n   [126.81706064565485, 37.478024854451064],\n   [126.81721035612148, 37.478135181855585],\n   [126.81732392065949, 37.478139295676314],\n   ...]]}\n\n\n\nglobal_dict['features'][0]['geometry']['coordinates']  ## 우리가 원하는 3중 리스트\n\n[[[127.02214829071995, 37.6997208220174],\n  [127.02531549179129, 37.69958052666555],\n  [127.0268980715665, 37.700251138801015],\n  [127.02702219174589, 37.70111540651664],\n  [127.02768734803148, 37.700937381678756],\n  [127.02899106854076, 37.69965827866671],\n  [127.02928557925306, 37.69929162349923],\n  [127.02960141563474, 37.69817519597225],\n  [127.02965442206329, 37.69780663241529],\n  [127.02972931239567, 37.69627243691011],\n  [127.03016631976621, 37.69519820573226],\n  [127.03097248066267, 37.693556837990705],\n  [127.0324142885382, 37.691839038874726],\n  [127.03577652734599, 37.69237420164817],\n  [127.03676285064714, 37.69263992022006],\n  [127.03790670521317, 37.69327193541787],\n  [127.03999748131439, 37.69468439213186],\n  [127.04110878733236, 37.69529978284375],\n  [127.0418126234525, 37.69538228560054],\n  [127.04307253543809, 37.69523090725608],\n  [127.04481711632289, 37.6929362514031],\n  [127.045094291979, 37.692385199610435],\n  [127.04862943554828, 37.69406226137355],\n  [127.04880746336237, 37.6932813039853],\n  [127.04975102873895, 37.68920713537357],\n  [127.04982002382117, 37.68797700395555],\n  [127.04998471076341, 37.68761035012039],\n  [127.05064314534968, 37.68650780387472],\n  [127.05092067338259, 37.686158552402],\n  [127.05108569439875, 37.686055160768575],\n  [127.05136797709446, 37.685921581506264],\n  [127.0517981363431, 37.6858148464868],\n  [127.05180416049991, 37.68706675968194],\n  [127.05206063036638, 37.687282882739005],\n  [127.05214429018166, 37.687352392956214],\n  [127.05222706432502, 37.68742077669241],\n  [127.05469544726837, 37.68872271744571],\n  [127.05610252968587, 37.68920437206933],\n  [127.05817261245335, 37.689669926579896],\n  [127.05839034056493, 37.689685868260845],\n  [127.05966013532905, 37.69033198280304],\n  [127.06220905346507, 37.6928052648238],\n  [127.06236779518714, 37.693024235595644],\n  [127.06237354393551, 37.693135159327554],\n  [127.06239478737797, 37.693546495381035],\n  [127.06260174913736, 37.69424379234776],\n  [127.06273214287815, 37.694675910354626],\n  [127.06305928833302, 37.694799063961284],\n  [127.0633863131918, 37.694921883623124],\n  [127.06630814019205, 37.69444172108839],\n  [127.06854802331812, 37.69400040092273],\n  [127.0691509700167, 37.69368950125125],\n  [127.0708788225368, 37.693700048667225],\n  [127.07087906859705, 37.693700062181264],\n  [127.07265760034454, 37.69379950124051],\n  [127.07313786558306, 37.69397574643172],\n  [127.07339823304241, 37.69410087667406],\n  [127.07378269914281, 37.69449368954549],\n  [127.07393007343649, 37.69464423236862],\n  [127.07448636301702, 37.695009628645295],\n  [127.07490680588823, 37.69521968047455],\n  [127.0781962787199, 37.695986510360626],\n  [127.08110478631578, 37.696137457947785],\n  [127.08384328792428, 37.69426889016338],\n  [127.08387832248333, 37.69279212349614],\n  [127.08390501861447, 37.69178245187781],\n  [127.08503096598021, 37.69054991893467],\n  [127.08517719556258, 37.69038984858866],\n  [127.08845357891715, 37.68990263950211],\n  [127.09055188620752, 37.68957025151863],\n  [127.09238424524798, 37.689933184218404],\n  [127.09303672225575, 37.689966466063815],\n  [127.09509714278617, 37.68938485003124],\n  [127.09599596058983, 37.68907104690658],\n  [127.09611844498184, 37.68777746180145],\n  [127.09629522668848, 37.687169760670066],\n  [127.09642514740469, 37.68626418153678],\n  [127.09642311415676, 37.685637441138354],\n  [127.09580983621922, 37.68464039310604],\n  [127.09428757876928, 37.683418538095395],\n  [127.09291905210979, 37.6815515024995],\n  [127.09194656333966, 37.67919029908044],\n  [127.09194153856279, 37.67869251579676],\n  [127.09218959069077, 37.67803010766824],\n  [127.09247959854748, 37.67764303460199],\n  [127.092614356114, 37.677474561385274],\n  [127.0939627740268, 37.675617232268024],\n  [127.09644725778618, 37.66968910695227],\n  [127.0962373655161, 37.66882941453695],\n  [127.09497929397519, 37.66760002717652],\n  [127.09458063193003, 37.666172300680316],\n  [127.0946800144686, 37.66492127931861],\n  [127.09541063101342, 37.663889644590746],\n  [127.09414211770347, 37.66329882532109],\n  [127.09217357563625, 37.66079254873924],\n  [127.09120691221572, 37.659334556715166],\n  [127.09113228726125, 37.658243024751805],\n  [127.09157431798847, 37.657772436849626],\n  [127.09235164609564, 37.65549186279581],\n  [127.09286741474956, 37.654453659625155],\n  [127.09356843248374, 37.653318729010714],\n  [127.0939157426287, 37.65296173000377],\n  [127.09400788402047, 37.65272853044388],\n  [127.09404075508206, 37.65254014144715],\n  [127.09403529660045, 37.652418519610805],\n  [127.09399991212685, 37.652295226453205],\n  [127.09326587472968, 37.65061772855914],\n  [127.09246587944706, 37.64970976752811],\n  [127.0926944997674, 37.64857999351487],\n  [127.09438136905902, 37.64495591656824],\n  [127.09456996976148, 37.644570600805835],\n  [127.09755891099569, 37.64396483085298],\n  [127.09890806335117, 37.6440343984249],\n  [127.10159264246101, 37.64477235815439],\n  [127.10280013821811, 37.64520492267967],\n  [127.1028823886103, 37.64540897697184],\n  [127.10386737257883, 37.64548852460528],\n  [127.10411721532803, 37.64548033184725],\n  [127.10657637866768, 37.64537730972521],\n  [127.10769224246286, 37.64496664676661],\n  [127.10786269687772, 37.64469282241332],\n  [127.10928861344723, 37.64285888886917],\n  [127.10989832951653, 37.64248752395126],\n  [127.11125728398851, 37.64237533392683],\n  [127.11142380118652, 37.64208686345102],\n  [127.11178551212458, 37.64073538398819],\n  [127.11113544340976, 37.639943933494294],\n  [127.11090335225738, 37.638262475561504],\n  [127.11155819260641, 37.63803293665921],\n  [127.11163126044404, 37.63797762400025],\n  [127.11163252401063, 37.63797657539423],\n  [127.1116335605215, 37.63797538002365],\n  [127.11163434149125, 37.637974066637646],\n  [127.11248319338027, 37.63648912158805],\n  [127.11220344878163, 37.63264276723643],\n  [127.1116323201253, 37.631503284414286],\n  [127.111371172448, 37.631227883211594],\n  [127.11082549334053, 37.630781845942806],\n  [127.10888476082644, 37.62930151924797],\n  [127.10596351597135, 37.62733070951351],\n  [127.10432790803095, 37.62327070428881],\n  [127.10406621959581, 37.62165311388996],\n  [127.10490909745644, 37.62156734430207],\n  [127.10538285122183, 37.62090189640023],\n  [127.1056619895515, 37.6204162469155],\n  [127.10555591433918, 37.62037072519622],\n  [127.11130504706806, 37.62069184201602],\n  [127.11214286452346, 37.620307017764965],\n  [127.11502106963971, 37.61953760329908],\n  [127.11619333227809, 37.61894778380488],\n  [127.11725242287622, 37.61674187052708],\n  [127.11699384437024, 37.616455741862225],\n  [127.11687861029367, 37.61632493344969],\n  [127.11667194632817, 37.61601542319483],\n  [127.11670352036289, 37.61571891606276],\n  [127.11679649663584, 37.614954676556906],\n  [127.11708589521979, 37.61396022396543],\n  [127.11737381655063, 37.61224525812902],\n  [127.1172872225345, 37.611180500418115],\n  [127.11687784452862, 37.609530700257416],\n  [127.11669806436957, 37.60884925145508],\n  [127.11809512146557, 37.60547242425238],\n  [127.11809557127602, 37.60544383142748],\n  [127.1180703665001, 37.604938459230944],\n  [127.11804701133192, 37.60460229805868],\n  [127.11558445031194, 37.601718190575795],\n  [127.11409999472093, 37.60012853384475],\n  [127.11447064798934, 37.599054605066726],\n  [127.11548924282775, 37.59765428196658],\n  [127.11572670052439, 37.597314488762194],\n  [127.11644313933067, 37.59619235770955],\n  [127.11687748428929, 37.59549705077813],\n  [127.11666448808528, 37.594016545777],\n  [127.11569009390017, 37.59362528500345],\n  [127.11552687588426, 37.593560121449116],\n  [127.11548546147779, 37.593548610635445],\n  [127.11541325076173, 37.593530949622995],\n  [127.1150282067279, 37.59349020815677],\n  [127.11445400975263, 37.59343304169271],\n  [127.1142683890762, 37.593416046920936],\n  [127.11366535163782, 37.593356363149674],\n  [127.11333737292749, 37.593259818471644],\n  [127.11241171452632, 37.59185636470543],\n  [127.1101545340761, 37.58743032790922],\n  [127.109944225286, 37.58595741365269],\n  [127.10973285600105, 37.58524413527232],\n  [127.109376495115, 37.58409232681838],\n  [127.10896665929798, 37.58337922727966],\n  [127.1074133022487, 37.58249485444903],\n  [127.10525566107893, 37.58156565577623],\n  [127.10344046907751, 37.58059529802311],\n  [127.1031362307321, 37.58021462295139],\n  [127.10289434724014, 37.57990679295722],\n  [127.10290869795878, 37.57978627410625],\n  [127.10302529385626, 37.578906983060165],\n  [127.10231819140941, 37.57804309180647],\n  [127.10195757011611, 37.577552361766436],\n  [127.10114351387524, 37.5760709289373],\n  [127.10091888215331, 37.574204101343454],\n  [127.10088786785404, 37.57376349428088],\n  [127.10166464576407, 37.57240065940598],\n  [127.10312974060275, 37.572279426510896],\n  [127.10366088935352, 37.57191791188504],\n  [127.10367642912719, 37.57190732992747],\n  [127.10423020073478, 37.571387655728294],\n  [127.10385348290774, 37.5704937526072],\n  [127.10272467888672, 37.56706421441915],\n  [127.10167082530147, 37.56340484757312],\n  [127.10120468941865, 37.56157990217243],\n  [127.10116259287368, 37.561313298804556],\n  [127.10112209698956, 37.56105064062629],\n  [127.10113406515579, 37.561003040117356],\n  [127.10127747386707, 37.5604659926051],\n  [127.10139885421326, 37.56025021143431],\n  [127.10154158761175, 37.55999837150702],\n  [127.10169403565163, 37.55974146071346],\n  [127.10180078997355, 37.55956709194242],\n  [127.10185922758541, 37.55949382793663],\n  [127.10216726500754, 37.55927810196558],\n  [127.10492928845024, 37.556421585556336],\n  [127.10540370353208, 37.556335336136144],\n  [127.1054039797918, 37.556335335256705],\n  [127.1060404466977, 37.55636800208074],\n  [127.10639619161522, 37.55646172320076],\n  [127.10714458157308, 37.55710610125959],\n  [127.10998660563237, 37.55834211535795],\n  [127.11167162292871, 37.55872797074785],\n  [127.11296064655822, 37.558794346205815],\n  [127.11522556193233, 37.55676018833242],\n  [127.11564563403175, 37.55769987998584],\n  [127.11730331913004, 37.559425641957304],\n  [127.11736706467859, 37.55947936004973],\n  [127.12295557080333, 37.56348990218229],\n  [127.12807612442845, 37.56590930955854],\n  [127.12861049420515, 37.56616043544185],\n  [127.13087184119757, 37.56687794943424],\n  [127.13375309980111, 37.56779331042001],\n  [127.1342663695222, 37.56795394418317],\n  [127.13474102736858, 37.56802464198976],\n  [127.13766579387027, 37.568424209460574],\n  [127.14560001479188, 37.568431169360885],\n  [127.1489495440468, 37.56843431854424],\n  [127.15124335260579, 37.569795301642884],\n  [127.15319599288145, 37.57096985010494],\n  [127.1549810601263, 37.57204353719504],\n  [127.15689916857828, 37.572954115769065],\n  [127.16084531250948, 37.575652662972885],\n  [127.1614681179371, 37.57604621474144],\n  [127.16200668497996, 37.57638652892725],\n  [127.16255503715276, 37.576730120691614],\n  [127.16674716243453, 37.578975666143656],\n  [127.1686475082068, 37.57899747794731],\n  [127.1686477364252, 37.57899709190847],\n  [127.17025143915063, 37.579016497229034],\n  [127.17183744214854, 37.57926199037442],\n  [127.17434341757382, 37.58002897085931],\n  [127.1745646848708, 37.58009734641977],\n  [127.17507432498863, 37.58025539696954],\n  [127.17510636726472, 37.58026661233748],\n  [127.17565914212791, 37.58056537625033],\n  [127.17585094748326, 37.580669273596094],\n  [127.17601761614299, 37.58076165809506],\n  [127.17637876684248, 37.58095906115711],\n  [127.17651851047951, 37.58103431126574],\n  [127.17715482863876, 37.58120032803303],\n  [127.17714288110588, 37.579858265364955],\n  [127.17706037157764, 37.579544339856604],\n  [127.17698860354076, 37.579416335558484],\n  [127.1769358967764, 37.57934067705333],\n  [127.17688336105913, 37.579265297509494],\n  [127.17674096763471, 37.57911572057686],\n  [127.17668903282423, 37.57906934265248],\n  [127.17559141963727, 37.57845491709445],\n  [127.17544227008392, 37.57836278564491],\n  [127.17541541545809, 37.57829863026236],\n  [127.17540428164837, 37.5782080868865],\n  [127.17540423462499, 37.57820770097899],\n  [127.17536098677607, 37.57784747971302],\n  [127.17532131705451, 37.577363342392985],\n  [127.17532434111249, 37.57729990460691],\n  [127.17532755327359, 37.57723513694318],\n  [127.17568308790581, 37.57489869852943],\n  [127.17570202935693, 37.57483166387621],\n  [127.1761038656408, 37.57413533821314],\n  [127.17613152481712, 37.57408856274329],\n  [127.1763084845669, 37.57397427178013],\n  [127.1764980410476, 37.573801389354294],\n  [127.17694363989052, 37.57329364424182],\n  [127.17773113162198, 37.57218791533225],\n  [127.17781926600982, 37.57205770863517],\n  [127.17913286165616, 37.56912369470184],\n  [127.17914903579462, 37.569085099484376],\n  [127.17920371766037, 37.56894649028298],\n  [127.17922017374693, 37.568873819289145],\n  [127.1793312074897, 37.56826577378065],\n  [127.17937794706329, 37.56798582974494],\n  [127.17943875427594, 37.56760084598443],\n  [127.17946467033734, 37.566583016887506],\n  [127.1801941808996, 37.56458787465877],\n  [127.18099458554735, 37.56332274641403],\n  [127.18118573247567, 37.5630169666324],\n  [127.18127812880661, 37.562830429531225],\n  [127.18151410938351, 37.5622675112996],\n  [127.18190992016416, 37.56124879949777],\n  [127.18199542158489, 37.56099244596784],\n  [127.18204081632643, 37.56040549758009],\n  [127.18202084650781, 37.55994782824462],\n  [127.18202079834414, 37.55994743873002],\n  [127.18198419478763, 37.55965551058688],\n  [127.181787374278, 37.55882911648127],\n  [127.18165662833978, 37.557998302305066],\n  [127.18135146515526, 37.553258048765485],\n  [127.1813403273481, 37.55296496478539],\n  [127.18142169421999, 37.552757324685345],\n  [127.18159477250546, 37.552592350538426],\n  [127.18170384487837, 37.552507995663994],\n  [127.182926745995, 37.55023535046519],\n  [127.18284450852022, 37.5494290906802],\n  [127.1826736926933, 37.547747320441374],\n  [127.18265679525105, 37.54752440129038],\n  [127.18276107519915, 37.546499426131824],\n  [127.18279599113094, 37.54638983933193],\n  [127.18353917154177, 37.54517039495601],\n  [127.18169282664165, 37.546444749492956],\n  [127.18125643812643, 37.54657971583249],\n  [127.17991053630526, 37.54657443190896],\n  [127.17933128170148, 37.54656742148427],\n  [127.17608601659725, 37.54561946546925],\n  [127.17571742052262, 37.54520770406822],\n  [127.17538324376515, 37.54520199786245],\n  [127.17463760452904, 37.54525912602242],\n  [127.17447709977255, 37.545277660809106],\n  [127.1743146633164, 37.54529732395536],\n  [127.17390569685948, 37.545600032434535],\n  [127.17157169030703, 37.545385773320305],\n  [127.1651681782903, 37.544631798899346],\n  [127.1631627309092, 37.54499101097656],\n  [127.16009589899915, 37.54151453079874],\n  [127.15958338038794, 37.54102822393515],\n  [127.15671195122711, 37.53757774554748],\n  [127.15603276390642, 37.53734241425835],\n  [127.15402132902574, 37.534514497157076],\n  [127.15356680162624, 37.533665193070505],\n  [127.15374715099493, 37.53160008846154],\n  [127.15353910604473, 37.53043671974301],\n  [127.15323227758938, 37.52923709895612],\n  [127.15316377616772, 37.529108804926516],\n  [127.14947629508237, 37.52502054146565],\n  [127.14779743367326, 37.52214947334571],\n  [127.14593249219926, 37.521955801698766],\n  [127.14578093977661, 37.521941375307804],\n  [127.14480065945018, 37.51938973774546],\n  [127.14509193634692, 37.51683494851859],\n  [127.14532885964844, 37.516607158321115],\n  [127.14538965833438, 37.51651332725961],\n  [127.14543404002671, 37.51642852537821],\n  [127.14544427334503, 37.51633278212625],\n  [127.14543916612115, 37.51606474208284],\n  [127.14485430106286, 37.51567802502981],\n  [127.14475438292835, 37.515645484717986],\n  [127.14460355182592, 37.515606249070395],\n  [127.1433741159653, 37.51557760277298],\n  [127.14210518808247, 37.51466010401819],\n  [127.14023773642688, 37.50985352338447],\n  [127.13996581242264, 37.50864088493049],\n  [127.14001384551528, 37.50852453676255],\n  [127.14032052088919, 37.508273591129395],\n  [127.14135345423192, 37.50694790778293],\n  [127.14147949264665, 37.50677882190354],\n  [127.14151665030897, 37.506708103021836],\n  [127.14151309866016, 37.50670219734203],\n  [127.14126852582045, 37.50641867264958],\n  [127.14117786173351, 37.50631882301717],\n  [127.1409971469675, 37.506163894207944],\n  [127.14081284937815, 37.50597856844039],\n  [127.14113191237908, 37.50546377662881],\n  [127.14223298799321, 37.504884417170004],\n  [127.14342402698134, 37.50441363645751],\n  [127.14392479947718, 37.50440092444333],\n  [127.14410326532739, 37.5043964831474],\n  [127.14550250136601, 37.50330570568455],\n  [127.14595206598852, 37.5032268868512],\n  [127.14769492141562, 37.50321206574927],\n  [127.14860243574782, 37.50372282317089],\n  [127.1490451129403, 37.50400073373371],\n  [127.15082434872058, 37.50406156817608],\n  [127.15165372960253, 37.503395752670144],\n  [127.1521091406241, 37.50297029811745],\n  [127.15323083173114, 37.50267941781355],\n  [127.15643795942692, 37.50185900576393],\n  [127.1565550135419, 37.50189404497823],\n  [127.15772923925586, 37.50317895196306],\n  [127.1588661178982, 37.50239019657405],\n  [127.16050427114095, 37.501094512438115],\n  [127.16121949144342, 37.50039527422741],\n  [127.16139030513257, 37.5002010498375],\n  [127.16143108867095, 37.50002247972045],\n  [127.16142016695896, 37.49970545993054],\n  [127.16135443347227, 37.499537459426215],\n  [127.16125092773778, 37.49939089854903],\n  [127.16110733163866, 37.49927532319294],\n  [127.15986640646732, 37.495706834034365],\n  [127.15996541511976, 37.49527986027531],\n  [127.16001826923471, 37.494707646707646],\n  [127.16002621602986, 37.49437455452455],\n  [127.1597454988479, 37.49363667082564],\n  [127.15970484975863, 37.49354690503217],\n  [127.15965196338816, 37.493434358962624],\n  [127.1595169833914, 37.49327460353943],\n  [127.15931845606401, 37.49313662914927],\n  [127.15764946367052, 37.490182673876625],\n  [127.15763380331077, 37.4901717886922],\n  [127.15740206846664, 37.48908854265012],\n  [127.15043020214158, 37.4852488932469],\n  [127.15021754524786, 37.48510537954825],\n  [127.14992170977906, 37.484905729020184],\n  [127.14868305306886, 37.484043192462515],\n  [127.148579027746, 37.483889863822014],\n  [127.14822520162613, 37.48336825471925],\n  [127.1477703794897, 37.48262416785541],\n  [127.14766978415436, 37.48242576594467],\n  [127.14760689985782, 37.48226916996039],\n  [127.14758794988897, 37.482221953125226],\n  [127.14752532591987, 37.48201386379114],\n  [127.14748230260066, 37.48180264116629],\n  [127.14747861659086, 37.481773480024934],\n  [127.14745907220343, 37.48158945671828],\n  [127.14739684802731, 37.480585692947166],\n  [127.14728211415827, 37.478739628360124],\n  [127.14715916521881, 37.47729511320696],\n  [127.14714533170121, 37.47722179897961],\n  [127.14584352066795, 37.477273068635235],\n  [127.145796878535, 37.47727491723534],\n  [127.14575646485959, 37.47727651768649],\n  [127.1443704748871, 37.47733114309174],\n  [127.14364342913493, 37.47525741763574],\n  [127.14361885800092, 37.47517065376269],\n  [127.14360185852455, 37.475082746354325],\n  [127.14359251006233, 37.47499412650607],\n  [127.1435733636672, 37.47468530404496],\n  [127.14356722898532, 37.474586308344385],\n  [127.14352658956736, 37.473930418522116],\n  [127.1394881494977, 37.474089616658645],\n  [127.13833405279085, 37.47413506191677],\n  [127.13766388419639, 37.474147919140066],\n  [127.13684251991336, 37.47415318717528],\n  [127.13677761017136, 37.47415548714804],\n  [127.1363969537117, 37.47417048239313],\n  [127.13546828445813, 37.47424021251808],\n  [127.1352873110148, 37.47425705042134],\n  [127.13312242962854, 37.4745798598],\n  [127.13285605187727, 37.47462645866824],\n  [127.13262354347293, 37.472395357539355],\n  [127.13282098466541, 37.46919569189179],\n  [127.13282095574344, 37.4691942063499],\n  [127.1327996032295, 37.468393664276086],\n  [127.13087554387387, 37.46775756923912],\n  [127.13083001167365, 37.46774525764664],\n  [127.13078324278253, 37.46773607352042],\n  [127.13073584632706, 37.46773009808275],\n  [127.13068790133586, 37.46772735681814],\n  [127.13063981486039, 37.46772786001192],\n  [127.13059198261257, 37.46773162422421],\n  [127.1305447100502, 37.46773862246795],\n  [127.13018892567416, 37.4678033697074],\n  [127.12933858733388, 37.467959210768726],\n  [127.12930302575272, 37.467966655370866],\n  [127.1266812987817, 37.468621775910755],\n  [127.12644686764371, 37.468750108913085],\n  [127.12602455140079, 37.4691058782371],\n  [127.12570131939023, 37.46931823345992],\n  [127.12551817321834, 37.46943048351345],\n  [127.12537453641595, 37.46951707676626],\n  [127.12531488674534, 37.46955289992616],\n  [127.12520301623321, 37.46961974560557],\n  [127.12510091762285, 37.469613652978204],\n  [127.12494334079366, 37.4696031234281],\n  [127.12487834302604, 37.469597279961135],\n  [127.12485136253184, 37.469523255232446],\n  [127.1248975970592, 37.46938552451682],\n  [127.12504582809731, 37.468979630189054],\n  [127.12516820516782, 37.46875144004594],\n  [127.1251820284445, 37.468352452585854],\n  [127.12519937283692, 37.467833231267484],\n  [127.12498822426089, 37.46724835864308],\n  [127.12482726884033, 37.46700159707097],\n  [127.12475930593294, 37.466916352653705],\n  [127.1244880965488, 37.4666435226522],\n  [127.12420970259667, 37.466517104307265],\n  [127.12420688119097, 37.46652048627629],\n  [127.12414326804209, 37.466495215148015],\n  [127.1237669717385, 37.46633342476096],\n  [127.12320225935616, 37.46597867005132],\n  [127.12185946757938, 37.46513395264438],\n  [127.11747485026633, 37.46220061045074],\n  [127.11747077309776, 37.462194983538076],\n  [127.11711994009005, 37.46167753791065],\n  [127.11698929578043, 37.46148450859329],\n  [127.11693965937852, 37.46090030945957],\n  [127.11689967841377, 37.4604011395989],\n  [127.1168948964395, 37.45864042153465],\n  [127.11669702911543, 37.45862325785094],\n  [127.11623941228295, 37.4585961143597],\n  [127.11589291262797, 37.45859194472567],\n  [127.11549606922648, 37.45869791695614],\n  [127.11547664981921, 37.458704404910726],\n  [127.11395536575394, 37.45951676647907],\n  [127.1136342600001, 37.45973246531095],\n  [127.11353635596203, 37.45980154776803],\n  [127.11341975702304, 37.45990076930371],\n  [127.11276328611841, 37.46053025046058],\n  [127.11290865142644, 37.46064192929295],\n  [127.11318486414389, 37.46075823127989],\n  [127.11329182742885, 37.46083640990825],\n  [127.11333484694086, 37.46090535023642],\n  [127.11333484534424, 37.46090566030155],\n  [127.11333387525471, 37.46108724567093],\n  [127.1122298035669, 37.46150499909431],\n  [127.11182027181094, 37.46164419121139],\n  [127.10868615916183, 37.4621564211832],\n  [127.10688007579863, 37.462369502165004],\n  [127.10667208681129, 37.46241051180783],\n  [127.10646723536935, 37.46242392987725],\n  [127.10616731423701, 37.462406454624805],\n  [127.10434135861443, 37.46217434553484],\n  [127.10447230410983, 37.46102952693268],\n  [127.10394160853244, 37.460050593707535],\n  [127.10381093870869, 37.459927656301566],\n  [127.09930610906709, 37.45667755614937],\n  [127.09823746163788, 37.456371246967336],\n  [127.09522278311739, 37.45639422742016],\n  [127.09431215563166, 37.45622477015542],\n  [127.09377352285104, 37.45607736437658],\n  [127.09354036220854, 37.455888055203964],\n  [127.08882127538375, 37.44974941955953],\n  [127.08843214262606, 37.44899484298624],\n  [127.08832682104673, 37.44862776075761],\n  [127.08832676671251, 37.44862736036078],\n  [127.08836930483001, 37.44724005811312],\n  [127.08816128515241, 37.44538498355397],\n  [127.08785507296487, 37.44489387071301],\n  [127.08777816075089, 37.44487515146447],\n  [127.08730388244146, 37.44464171021886],\n  [127.082904590745, 37.442173808672536],\n  [127.08243181881421, 37.441579468344386],\n  [127.08209733924632, 37.44135078689038],\n  [127.08144410364704, 37.441230999973904],\n  [127.08017160102241, 37.441051093956524],\n  [127.07908335144711, 37.44129874337731],\n  [127.07817890647885, 37.44155161511739],\n  [127.07760188111618, 37.44170909672176],\n  [127.07601057612212, 37.44213893609793],\n  [127.07475598627195, 37.442218846646846],\n  [127.0744976898156, 37.44223280045726],\n  [127.07224980537977, 37.442199811541435],\n  [127.07186137425094, 37.44102970185969],\n  [127.07221171709271, 37.439382901540675],\n  [127.07376019338288, 37.437789433842276],\n  [127.07379537723341, 37.43765707199904],\n  [127.07384351098383, 37.43740786000359],\n  [127.07356006757476, 37.436963727095595],\n  [127.0730242304579, 37.43640683285397],\n  [127.0709099786286, 37.43309491379506],\n  [127.06961542955781, 37.430611692111725],\n  [127.06569800697741, 37.42915810241767],\n  [127.06114498957349, 37.42998528229182],\n  [127.05978112053222, 37.4295872657291],\n  [127.05345832004603, 37.42880858729806],\n  [127.0503615218898, 37.42982129068007],\n  [127.04957975136693, 37.4302701432368],\n  [127.04736923949098, 37.430701820429285],\n  [127.04722475567219, 37.432040442932944],\n  [127.04644166111552, 37.43299046057295],\n  [127.04590560884242, 37.43348142450798],\n  [127.04418573191268, 37.4350554539952],\n  [127.04109031701444, 37.437769124917864],\n  [127.04087975167322, 37.437878441821276],\n  [127.04048048713241, 37.43808242753594],\n  [127.04005277677203, 37.43823714507124],\n  [127.03956828753749, 37.438195909531515],\n  [127.03922186091357, 37.43816448538617],\n  [127.03903241753622, 37.43815131864844],\n  [127.03706776525239, 37.43827496782511],\n  [127.03697772841022, 37.43829639401592],\n  [127.03557372599246, 37.439004097845434],\n  [127.03532026694523, 37.439966564131865],\n  [127.03529507001076, 37.44007215137235],\n  [127.03520801081689, 37.440437652690015],\n  [127.03516906728176, 37.4406057551936],\n  [127.03517186217657, 37.44094194757337],\n  [127.03528263969284, 37.44103476257073],\n  [127.0360722545635, 37.44155099433042],\n  [127.03608762765222, 37.44156253268867],\n  [127.03643099253159, 37.441832734480634],\n  [127.03665445379359, 37.442080160034884],\n  [127.03748294023711, 37.44326670427758],\n  [127.03793486290014, 37.44417883691202],\n  [127.03803531906024, 37.444516404127626],\n  [127.03818353178544, 37.44501754331141],\n  [127.03823039750196, 37.44518026909942],\n  [127.03820566265372, 37.4458588524557],\n  [127.03781196836518, 37.44892239759809],\n  [127.03777449207378, 37.449201719792974],\n  [127.03774440117567, 37.44941881885998],\n  [127.03696804004046, 37.450691157558936],\n  [127.03640970411149, 37.45149687786112],\n  [127.03577642448006, 37.452195909168545],\n  [127.03626740809199, 37.45426216962762],\n  [127.03712770180721, 37.45520768138208],\n  [127.03706742872744, 37.45547434330387],\n  [127.0367406508785, 37.456443867860756],\n  [127.03492422096939, 37.46017540450221],\n  [127.03453266613107, 37.46333522021434],\n  [127.0346601133906, 37.463976592257936],\n  [127.03467091241455, 37.464090458173466],\n  [127.03460314189319, 37.464173939752115],\n  [127.0331405892598, 37.46501738910679],\n  [127.03275757043367, 37.46519318169501],\n  [127.03192176568467, 37.465518883534436],\n  [127.03119554711617, 37.465626061203594],\n  [127.03055508280168, 37.46551416328518],\n  [127.03039682015434, 37.46548632407364],\n  [127.02992820559366, 37.46535158628146],\n  [127.02958338917796, 37.4642881854868],\n  [127.02873126984468, 37.46202826888523],\n  [127.02815696813059, 37.460621993714916],\n  [127.02634678296988, 37.458104359241986],\n  [127.02620458102675, 37.4579878159816],\n  [127.02599541833983, 37.4578169581426],\n  [127.02543481238588, 37.45756450281699],\n  [127.025277616827, 37.4574938687448],\n  [127.02150295539224, 37.45622583459711],\n  [127.01969201000381, 37.45578660642505],\n  [127.01557484735889, 37.454916014104],\n  [127.0150649965327, 37.45483188354524],\n  [127.0148735746752, 37.454841752950394],\n  [127.01450961789199, 37.45486122860588],\n  [127.0107251186711, 37.45577244670041],\n  [127.00923267412264, 37.45722091033481],\n  [127.00704092418567, 37.460215469835134],\n  [127.00496875820613, 37.463186899888576],\n  [127.0044696722443, 37.46407327656836],\n  [127.00476218583773, 37.46492302457033],\n  [127.00490546550544, 37.46584260252905],\n  [127.00452135791603, 37.46679064066523],\n  [127.00424565849995, 37.46714344410636],\n  [127.00367535892853, 37.467720386588596],\n  [127.00273482189448, 37.46712461108707],\n  [127.00196279723106, 37.467087449562925],\n  [127.00047686511023, 37.46706294825534],\n  [126.99742674489028, 37.46724223517637],\n  [126.99717099538091, 37.467214070232885],\n  [126.99698554452699, 37.46717971600085],\n  [126.99675187621652, 37.467072433303954],\n  [126.99628192995874, 37.46665064295208],\n  [126.99719359563045, 37.464184165933666],\n  [126.99725312948726, 37.46401973479701],\n  [126.99737077162193, 37.46368749255192],\n  [126.99676862985268, 37.461873352884574],\n  [126.9942899320261, 37.461421317893326],\n  [126.99315292901082, 37.46127623511324],\n  [126.9916230699418, 37.46044156303114],\n  [126.98999453840017, 37.4594629780069],\n  [126.98863550944588, 37.458165845862126],\n  [126.98678106962474, 37.45740326454205],\n  [126.98238921265964, 37.45591710269429],\n  [126.97775711976738, 37.455199116892956],\n  [126.97457956131598, 37.454412875477026],\n  [126.97175282603096, 37.45173733547508],\n  [126.97056939304684, 37.44945270098554],\n  [126.96959356922594, 37.44909597687449],\n  [126.96866171986065, 37.44875446455986],\n  [126.967662979455, 37.44838054424455],\n  [126.96428976779832, 37.44626723711946],\n  [126.9639418980544, 37.4452458892869],\n  [126.96393361850694, 37.445214631166266],\n  [126.96432082073919, 37.44346229477368],\n  [126.96463404099109, 37.44203964730629],\n  [126.9638033064139, 37.44078692549053],\n  [126.96373604591848, 37.44074688094471],\n  [126.96311396054732, 37.44037652510398],\n  [126.96298353095135, 37.44029960855669],\n  [126.96294292915762, 37.44028045754587],\n  [126.95899886229834, 37.43912694137169],\n  [126.95636819846372, 37.43875401134443],\n  [126.94928410957398, 37.438250273989894],\n  [126.94815539463404, 37.43863037558223],\n  [126.9456762727248, 37.43730530031926],\n  [126.9450906377534, 37.437088221251784],\n  [126.94022016050745, 37.43571237211725],\n  [126.9393643055451, 37.43602581803675],\n  [126.93857452478132, 37.43642042942645],\n  [126.93774638275855, 37.4373857576777],\n  [126.93727268446924, 37.4386322720711],\n  [126.93725777476023, 37.43892734300138],\n  [126.93724856528063, 37.439167511901296],\n  [126.93724711351074, 37.43921284001876],\n  [126.93740216615096, 37.43937426800926],\n  [126.93769358300823, 37.43968442708137],\n  [126.937713857788, 37.43972075748547],\n  [126.93786405751602, 37.44019753476193],\n  [126.9377458076725, 37.440354020623566],\n  [126.93735357677096, 37.440871598648926],\n  [126.93668810996004, 37.44169594611689],\n  [126.93664692478835, 37.441745758401595],\n  [126.9348979978904, 37.443209481797815],\n  [126.9344508816705, 37.44325822606353],\n  [126.93055407014207, 37.44546848090059],\n  [126.93051744833456, 37.445548423023716],\n  [126.9303939359546, 37.44581837200393],\n  [126.93032633869163, 37.4459686910352],\n  [126.93016316403606, 37.4463779831893],\n  [126.93035671199829, 37.44733422226026],\n  [126.92839869373803, 37.450212485076534],\n  [126.9283570058542, 37.44989868951273],\n  [126.9283079026723, 37.44954867104572],\n  [126.9282796601699, 37.4493526845579],\n  [126.92399602716448, 37.446210224228935],\n  [126.92325327636715, 37.44577274628548],\n  [126.92289722984206, 37.445173349643994],\n  [126.92294662379582, 37.44487295199244],\n  [126.9229034040661, 37.444673007921715],\n  [126.92276280882136, 37.44404108321409],\n  [126.9220070215115, 37.443250757259776],\n  [126.92069895346346, 37.44151996843528],\n  [126.91866310009635, 37.43998262017672],\n  [126.91538923818742, 37.43966802735873],\n  [126.91389137436857, 37.43930708270779],\n  [126.91318980966076, 37.43909539212985],\n  [126.91230851419965, 37.43857946078951],\n  [126.91143164721281, 37.43762316839669],\n  [126.91120280868347, 37.43720121439982],\n  [126.91120000044674, 37.437187408644185],\n  [126.91121640637944, 37.4370463602423],\n  [126.91128427742785, 37.43654325238066],\n  [126.9113332747712, 37.4361713431048],\n  [126.91002834759448, 37.43432412191302],\n  [126.90990399563053, 37.43422744640446],\n  [126.90967840112272, 37.43405973794322],\n  [126.90940570432362, 37.43386468248096],\n  [126.90939564142113, 37.43386044896999],\n  [126.90859344653741, 37.433691729024154],\n  [126.90752668322209, 37.43362021944715],\n  [126.90663449172253, 37.43359694340164],\n  [126.90582547214655, 37.43399615235654],\n  [126.90582519408719, 37.43399615186017],\n  [126.90522457694115, 37.433997069199144],\n  [126.90298757638999, 37.434067620178844],\n  [126.90186087029541, 37.436397524817494],\n  [126.89914461684712, 37.43866819351907],\n  [126.89897791331421, 37.43870182579147],\n  [126.89958451332427, 37.44036666498753],\n  [126.8982570102593, 37.44320652479412],\n  [126.89578235329736, 37.44563339801652],\n  [126.89564123458763, 37.44829320383536],\n  [126.8946038195621, 37.451159154897915],\n  [126.89398300003057, 37.45271733991926],\n  [126.89182284732061, 37.45228350409839],\n  [126.8910486017461, 37.45224245321772],\n  [126.88978012488182, 37.45227307520445],\n  [126.88964213513057, 37.45231546387733],\n  [126.88956055650301, 37.45243222840465],\n  [126.88952054982705, 37.45262112176969],\n  [126.88950365869798, 37.4527010699145],\n  [126.88953577253639, 37.452946634786336],\n  [126.88964770418707, 37.45331924221095],\n  [126.88966648039558, 37.453521707197815],\n  [126.88965276996692, 37.45359855847977],\n  [126.889592047684, 37.45370296489857],\n  [126.88876417143807, 37.45476564219567],\n  [126.88828498935065, 37.45516865405293],\n  [126.8863169029548, 37.45628901611006],\n  [126.88626468031083, 37.456362515796506],\n  [126.88618548846422, 37.456522645677246],\n  [126.88588075182155, 37.45750839016862],\n  [126.8853564731724, 37.45953456857106],\n  [126.88534006096899, 37.45963704166101],\n  [126.88534960239055, 37.45986258161532],\n  [126.88539936108317, 37.46000916976716],\n  [126.88543071619748, 37.46009095992281],\n  [126.88544367776255, 37.46012453265269],\n  [126.88551853776474, 37.46025525216233],\n  [126.885650375358, 37.46043079769516],\n  [126.88607198185638, 37.460859466843345],\n  [126.886128084154, 37.460899787651186],\n  [126.88620134657874, 37.46091646478202],\n  [126.88625378849405, 37.46092666112505],\n  [126.88747359786943, 37.4610655269108],\n  [126.88749655889849, 37.46106301666018],\n  [126.88768203443473, 37.46103729223392],\n  [126.88785888520992, 37.460997475157],\n  [126.88795325015386, 37.46095448958872],\n  [126.8883358698455, 37.46078084535764],\n  [126.88863061689209, 37.46078817357594],\n  [126.88876919238663, 37.46083250900126],\n  [126.88887551772156, 37.460946920406975],\n  [126.88853249141253, 37.46133205164132],\n  [126.88810791069389, 37.461806645801666],\n  [126.88712180563425, 37.462904361685275],\n  [126.88704339763547, 37.46290231182216],\n  [126.88692384788376, 37.46289178120236],\n  [126.8866437804029, 37.46286673117086],\n  [126.88624491347673, 37.46279820029081],\n  [126.88485800472498, 37.46254315202208],\n  [126.88285845675665, 37.46426078649855],\n  [126.88457429409604, 37.46564823572887],\n  [126.88399444693059, 37.46666861463424],\n  [126.88153185190849, 37.46929115820678],\n  [126.88020447481996, 37.47093497073136],\n  [126.87987759363838, 37.4714213189637],\n  [126.87978452249763, 37.471539058789205],\n  [126.87955796413078, 37.47182967743504],\n  [126.87896535335604, 37.47259125928221],\n  [126.87873771587651, 37.4728841366197],\n  [126.87845148258745, 37.47325521436153],\n  [126.87801141557026, 37.47382773802628],\n  [126.87758404673444, 37.47451796201532],\n  [126.87614354086422, 37.47684468784561],\n  [126.87598571313652, 37.47710383526691],\n  [126.87532045038951, 37.478372692072554],\n  [126.87527082721985, 37.47846810142021],\n  [126.87495854231086, 37.47907199778555],\n  [126.87428469764085, 37.480380256044086],\n  [126.87413691190955, 37.480757112415716],\n  [126.87337901472617, 37.48243356052865],\n  [126.8727442969783, 37.482424995238794],\n  [126.87277987743597, 37.482794985827105],\n  [126.87278663474126, 37.48348878210105],\n  [126.8717971958008, 37.48499743368912],\n  [126.87176262543268, 37.485269390497095],\n  [126.87244469888192, 37.48611004050176],\n  [126.87267382382166, 37.486220095922036],\n  [126.87273456256477, 37.486240154483085],\n  [126.87289532395594, 37.486251871041624],\n  [126.87304222324997, 37.486197970915136],\n  [126.87313717688521, 37.486151898173034],\n  [126.87455644442305, 37.48536674989791],\n  [126.8749436128769, 37.485699557362956],\n  [126.87626185595225, 37.48723955123044],\n  [126.8766887354242, 37.4878526882681],\n  [126.87676002659364, 37.48811743166551],\n  [126.87680025485933, 37.48835707247939],\n  [126.87679035662485, 37.48857077467621],\n  [126.87666836775517, 37.488731698886916],\n  [126.87638215065101, 37.488941163019874],\n  [126.87604346872821, 37.489047234427446],\n  [126.8758168244155, 37.48902812727765],\n  [126.87471601721661, 37.48857729667705],\n  [126.87307774871593, 37.48825285739754],\n  [126.87295540342764, 37.48830058919606],\n  [126.87292252364195, 37.48831350594885],\n  [126.87274847564402, 37.488430729714324],\n  [126.87266911360857, 37.48855396331768],\n  [126.87241371655864, 37.48933079643515],\n  [126.8724404006732, 37.48953552394017],\n  [126.87244516069696, 37.48954200832903],\n  [126.87267624752327, 37.48985141040301],\n  [126.87277059260437, 37.4899587830918],\n  [126.87283922397087, 37.490014335559685],\n  [126.87336843875514, 37.490386572775556],\n  [126.87357693309845, 37.490481684306566],\n  [126.87375273922521, 37.490475679558024],\n  [126.8739332173435, 37.490428855349435],\n  [126.87402067996526, 37.490316033035164],\n  [126.87409064170969, 37.49021447138926],\n  [126.87414915935193, 37.48998055733015],\n  [126.87411047167724, 37.48977722861176],\n  [126.87432244123868, 37.48969551876888],\n  [126.874579194672, 37.48976928586574],\n  [126.87465124038863, 37.48989887276378],\n  [126.8748395606655, 37.49074546038744],\n  [126.87453459446087, 37.4910683660723],\n  [126.87437268165921, 37.49122136264536],\n  [126.87413007357357, 37.491337668238565],\n  [126.87395185020526, 37.49130790942899],\n  [126.87225463784726, 37.490602443243475],\n  [126.8715772336594, 37.490209200924156],\n  [126.8715477661439, 37.49008528105915],\n  [126.8715056835045, 37.489998794005494],\n  [126.8713423702246, 37.48983362446104],\n  [126.87125961324185, 37.48977074005055],\n  [126.87108027252202, 37.48968325006296],\n  [126.87094589711485, 37.48963721815786],\n  [126.87065821481464, 37.48956087360729],\n  [126.87042098603534, 37.48953583190591],\n  [126.8703347531482, 37.489544746411035],\n  [126.87022812471946, 37.48959615733273],\n  [126.87017041588264, 37.489665641527296],\n  [126.87013621956086, 37.48971909518386],\n  [126.86987690273945, 37.490296013812674],\n  [126.86936626773986, 37.49201860453653],\n  [126.86936892693092, 37.49271744174477],\n  [126.86937342007144, 37.49277207332995],\n  [126.86950267732847, 37.4943284018372],\n  [126.86758862651642, 37.49481588988599],\n  [126.86761040833085, 37.49468809006178],\n  [126.86619406839125, 37.49253860319941],\n  [126.86451819511568, 37.491203026084236],\n  [126.86302813884646, 37.49087608634184],\n  [126.8628110904766, 37.490828236574096],\n  [126.8627337568918, 37.490804494216775],\n  [126.86258684437263, 37.49075871779353],\n  [126.86238768631601, 37.49067907388865],\n  [126.86217497834068, 37.49057746081463],\n  [126.86208955762203, 37.49053119281495],\n  [126.86195048469753, 37.490447969739456],\n  [126.86183032272321, 37.490367864585316],\n  [126.86176820935094, 37.490326119958326],\n  [126.86162303620054, 37.49020374479842],\n  [126.861481767076, 37.490067301502876],\n  [126.86138442651429, 37.489968076240665],\n  [126.86111908480686, 37.48966817910177],\n  [126.86102581461058, 37.48955826657109],\n  [126.86101648222878, 37.489547269155715],\n  [126.86068610548242, 37.48916817605563],\n  [126.86018595731015, 37.48859544427551],\n  [126.85925601817621, 37.48753114874059],\n  [126.8585450417844, 37.48672107555025],\n  [126.85822542932799, 37.486360288148504],\n  [126.85811613511434, 37.48623739417916],\n  [126.85797103348565, 37.48608461582477],\n  [126.85783430290624, 37.485983086909044],\n  [126.85763012230208, 37.485867113767235],\n  [126.85748360128834, 37.48580696465288],\n  [126.85744917364828, 37.48579453432356],\n  [126.8572904242174, 37.48576055327166],\n  [126.85721499765847, 37.48575426248749],\n  [126.85708640467882, 37.48574368934753],\n  [126.85676809597386, 37.485721054345184],\n  [126.85655188308645, 37.48570671540534],\n  [126.85653942998213, 37.48570626540688],\n  [126.85349858643376, 37.482621834752926],\n  [126.85270046935516, 37.481818616044556],\n  [126.85261962553658, 37.481786686082536],\n  [126.85258624296549, 37.48177876052663],\n  [126.85206621905958, 37.48166492424135],\n  [126.851751885319, 37.48160370879597],\n  [126.85166783206732, 37.48158868170002],\n  [126.85161377601116, 37.481583540277235],\n  [126.85099047011765, 37.481527845736494],\n  [126.85084315206339, 37.48152259001683],\n  [126.85070182006966, 37.48152550894454],\n  [126.85057300416253, 37.48153941821166],\n  [126.85041837279233, 37.481566818390434],\n  [126.8493324275512, 37.48184331552959],\n  [126.84698612892635, 37.48189443252598],\n  [126.8466017682575, 37.48167122516655],\n  [126.8465568595493, 37.4816333299397],\n  [126.84651717642556, 37.48159893200954],\n  [126.84642038315994, 37.48150701100949],\n  [126.84640821872374, 37.481495453254205],\n  [126.84626305142321, 37.48130323673433],\n  [126.84621918975228, 37.481240109255275],\n  [126.84604937012155, 37.480925377389326],\n  [126.84600724781465, 37.480798339922416],\n  [126.84598678074923, 37.480695539848334],\n  [126.845976830455, 37.48064175143651],\n  [126.8459342194971, 37.4803147980667],\n  [126.8454343751056, 37.47446358358097],\n  [126.84536592690858, 37.47385949919483],\n  [126.84536055354526, 37.47381246555105],\n  [126.84503323897347, 37.47355891752733],\n  [126.84419346893853, 37.47401499804191],\n  [126.84395991392603, 37.474482912719694],\n  [126.84311200124603, 37.47486984773489],\n  [126.8430773430002, 37.4748849961075],\n  [126.84295255134103, 37.47492481992884],\n  [126.84278163936918, 37.47497161415357],\n  [126.8384196728936, 37.47536790968778],\n  [126.83825923862072, 37.475374710633375],\n  [126.83610875738073, 37.474675546160455],\n  [126.83461359077172, 37.47487025950699],\n  [126.83305167563579, 37.47724609250874],\n  [126.8320789692601, 37.477567839756105],\n  [126.8317480949545, 37.47765099536511],\n  [126.83080142694622, 37.47742382313479],\n  [126.83045751616729, 37.47733913587807],\n  [126.82962156724689, 37.476895297577286],\n  [126.82955329242202, 37.47662461753497],\n  [126.8295367634638, 37.476595599289965],\n  [126.82920434453489, 37.47619529596771],\n  [126.82818195209177, 37.47603415313276],\n  [126.82787204443441, 37.4759892067256],\n  [126.82410830374371, 37.476352189043084],\n  [126.8195759381957, 37.47635542376256],\n  [126.8194233714162, 37.47632759999209],\n  [126.81930759652315, 37.47628940790379],\n  [126.81929647232971, 37.47628516514027],\n  [126.81914316116901, 37.476205532322275],\n  [126.81832133653424, 37.475298753226035],\n  [126.81766855225743, 37.473238400442405],\n  [126.81465604938846, 37.47476019094166],\n  [126.81472979756464, 37.475057306435204],\n  [126.81496487102987, 37.47586011888277],\n  [126.81522179414826, 37.476251063198596],\n  [126.81529553572993, 37.47636211907153],\n  [126.81641141908979, 37.477536748815474],\n  [126.81706064565485, 37.478024854451064],\n  [126.81721035612148, 37.478135181855585],\n  [126.81732392065949, 37.478139295676314],\n  ...]]\n\n\n\n# local_dict\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates']).shape\n\n(1, 1498, 2)\n\n\n\n서울의 다각형을 의미하는 것 같음(확인해보자)\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates']).shape  ## 쓸데없이 3차원\n\n(1, 1498, 2)\n\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates'])[0,:,:].shape  ## 경도와 위도, 2차원\n\n(1498, 2)\n\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates'])[0,:,1].shape  ## 위도, 1차원\n\n(1498,)\n\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates'])[0, :, 0].shape  ## 경도, 1차원\n\n(1498,)\n\n\n\nlon, lat = np.array(global_dict['features'][0]['geometry']['coordinates'])[0,:,:].T\n\n\nnp.stack([lat, lon], axis = 1).shape\n\n(1498, 2)\n\n\n\n아까는 위도 경도로 저장되었는데, 이제 경도 위도로 저장이 되었음. → 바꿔줌\n\n- 이게 어떤 원리냐면…\n\n[0,:,:] 첫번째 원소만 선택 &gt; 멀티 리스트를 깨줌\nT &gt; 전치시켜서 두 개의 리스트로 만듦\nlon, lat에 각자 저장함(순서를 바꾸기 위함)\nnp.stack([lat, lon], axis = 1) &gt; 열로 스택을 쌓아줌(2차원으로 변경)\n\n\nm = folium.Map(\n    location = [37.55,127],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Polygon(\n    locations = np.stack([lat,lon],axis=1).tolist(),    ## tolist()는 안해도 됨\n    fill = True\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n일상언어에서는 (lat,lon) 순서로 표현한다. 즉 적도를 기준으로 얼마나 위/아래로 있는지, 그리고 그리니치천문대를 기준으로 얼마나 동/서로 있는지를 표현한다. 그런데 lat이 y축의 느낌을 가지고 lon이 x축의 느낌을 가지는데 (lat,lon) 순으로 좌표를 선택하면 컴퓨터로 표현하기에 종종 헷갈릴 수 있다. 그래서 어떤 경우는 (lon,lat) 순서로 좌표를 표현하기도 한다. &gt; 경도, 위도 순"
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#folium.choropleth를-이용한-시각화",
    "href": "2023_DV/Review/강신성_1113.html#folium.choropleth를-이용한-시각화",
    "title": "1. 라이브러리 imports",
    "section": "6. folium.Choropleth를 이용한 시각화",
    "text": "6. folium.Choropleth를 이용한 시각화\n\nA. folium.Choropleth 소개\n\n- folium.Choropleth는 아래와 같은 방식으로 그림을 그린다고 생각하면 편리하다.\n\njson 파일을 바탕으로 폴리곤을 그린다. 폴리곤에 이름을 붙인다.\ndf = [폴리곤의 이름, 통계값(y)]와 같은 형식으로 정리된 데이터프레임을 바탕으로, 각 폴리곤에 대응하는y`값을 색깔로 매핑한다.\n\n서울 - 's' -- 얼만큼 빨간색으로 할지\n경기 - 'g' -- 얼만큼 파란색으로 할지\ndf\n지역 | 값\ns | 10\ng | 20\n\n각 폴리곤의 이름과 통계값들이 정리가 되어있어야 함."
  },
  {
    "objectID": "2023_DV/Review/강신성_1113.html#b.-polygon-시각화",
    "href": "2023_DV/Review/강신성_1113.html#b.-polygon-시각화",
    "title": "1. 라이브러리 imports",
    "section": "### B. Polygon 시각화",
    "text": "### B. Polygon 시각화\n# 예제1 : 전국의 행정구역 시각화(global)\n\nm = folium.Map(\n    location = [36, 128],\n    zoom_start = 7,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = global_dict\n).add_to(m)\nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n그대로 넣어버리면 바로 해준다.\n\n# 예제2 : 전국의 행정구역 시각화(local)\n\nm = folium.Map(\n    location = [36, 128],\n    zoom_start = 7,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data = local_dict\n).add_to(m)\nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n폴리곤을 그릴 수 있는 딕셔너리 타입의 정보가 들어간다.\n\n# 예제3 : 전국의 행정구역 시각화(덕진구/완산구)\n\nlocal_jeonju = local_dict.copy()\n\n\nlocal_jeonju\n\n\nlocal_dict['features'][15]['properties']  ## features의 properties에는 지역 이름에 대한 정보가 포함됨\n\n{'name': '강서구', 'base_year': '2018', 'name_eng': 'Gangseo-gu', 'code': '11160'}\n\n\n\nlen(local_dict['features'])\n\n250\n\n\n\n[local_dict['features'][i] for i in range(250) if local_dict['features'][i]['properties']['name'] == '전주시덕진구' or local_dict['features'][i]['properties']['name'] == '전주시완산구']\n## 아래의 코드가 더 명확하긴 하다.\n#[l for l in local_dict['features'] if l['properties']['name'] == '전주시덕진구' or l['properties']['name'] == '전주시완산구']\n\n[{'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[127.11044780083574, 35.838949832816176],\n      [127.1135948524044, 35.837854486310746],\n      [127.1139393425632, 35.837848233581205],\n      [127.11435513472738, 35.83784184083071],\n      [127.11518661050826, 35.83836645128747],\n      [127.11515332831482, 35.83838977941879],\n      [127.11523061589928, 35.83843384975816],\n      [127.11591558913986, 35.83849511540428],\n      [127.11649941545609, 35.8385449645881],\n      [127.11671724552282, 35.838562492613725],\n      [127.1169183684167, 35.83857831613743],\n      [127.1170210991372, 35.83858252410064],\n      [127.11719355090722, 35.83858623369339],\n      [127.11754826532164, 35.838569128848576],\n      [127.11782249206348, 35.83853668936038],\n      [127.11802163492057, 35.838476833317145],\n      [127.11822440990959, 35.83840345527894],\n      [127.11847743016729, 35.83825242907003],\n      [127.12093095186137, 35.83586689829406],\n      [127.12274668286028, 35.83471784916018],\n      [127.12616610885227, 35.832866304912116],\n      [127.12682631567407, 35.832264339594225],\n      [127.12847330536076, 35.829025121619885],\n      [127.12876563435547, 35.82843832211523],\n      [127.12826447085902, 35.82767532742502],\n      [127.12708179631969, 35.82623167200146],\n      [127.12663244180077, 35.82568514280861],\n      [127.12658216512102, 35.82524671717377],\n      [127.12761826456388, 35.82374313675225],\n      [127.12786387534695, 35.82284610349671],\n      [127.1275101311783, 35.82201176217279],\n      [127.1271661562495, 35.82092409251507],\n      [127.12749962407125, 35.820397419931105],\n      [127.1278968241129, 35.82012099229317],\n      [127.1281327882773, 35.819993711154225],\n      [127.12836211725518, 35.81987000828217],\n      [127.12939561272175, 35.81939300049346],\n      [127.13009068234605, 35.81948339972286],\n      [127.1303173802127, 35.81943866021799],\n      [127.13098529045986, 35.819286325238124],\n      [127.1314210736207, 35.81906558709838],\n      [127.13167133268317, 35.818570916337805],\n      [127.13209060806795, 35.818145988494315],\n      [127.13299964757864, 35.81752641248441],\n      [127.13302455027515, 35.81753123642155],\n      [127.13348167816396, 35.817619820979004],\n      [127.1340042622175, 35.81775109176744],\n      [127.13405804933798, 35.817776501672064],\n      [127.13426372930414, 35.81790606414015],\n      [127.13446544283613, 35.818033132946795],\n      [127.13469571430879, 35.81818312433366],\n      [127.13479380477186, 35.81824701696916],\n      [127.13498567256765, 35.81839527722268],\n      [127.13520179825814, 35.81856228145076],\n      [127.13547621676032, 35.81918903190997],\n      [127.13557122325363, 35.82054960386114],\n      [127.13359620475322, 35.822045745234504],\n      [127.13282852786507, 35.82270308753572],\n      [127.13202354403046, 35.82358144253712],\n      [127.13177782238738, 35.82397663066133],\n      [127.13315098977408, 35.825138256025745],\n      [127.1333912217796, 35.82517201334366],\n      [127.13360143447727, 35.825201345515424],\n      [127.1337351637979, 35.8252164638417],\n      [127.13417697868871, 35.82527420715428],\n      [127.13466364705329, 35.825338713857434],\n      [127.13693842599847, 35.825640468881886],\n      [127.13872942195485, 35.82730893725355],\n      [127.14123992071804, 35.827845740885586],\n      [127.14146327060772, 35.82788911142981],\n      [127.14207241305714, 35.82801499096051],\n      [127.14605804949174, 35.828898933114544],\n      [127.14614554165114, 35.829092695277375],\n      [127.14620845063048, 35.829234675607466],\n      [127.14623968352748, 35.82935785777554],\n      [127.1464110212425, 35.83007675223584],\n      [127.14651183063262, 35.83052956367989],\n      [127.14652037812718, 35.83056797234998],\n      [127.14652265120012, 35.830667070070966],\n      [127.14813325752549, 35.83016527807208],\n      [127.15242364595917, 35.829707768312886],\n      [127.15406124431146, 35.82945174228997],\n      [127.15405668730897, 35.82933183964045],\n      [127.15410291533944, 35.8290777172861],\n      [127.15421727969819, 35.82867793849419],\n      [127.1549341333606, 35.828461210990476],\n      [127.15563556038684, 35.82833220902687],\n      [127.1556399814983, 35.82833495539072],\n      [127.15566106899544, 35.82834805857007],\n      [127.15569084851869, 35.82836657109098],\n      [127.15569270130288, 35.82836772685941],\n      [127.15574123284027, 35.82840895968722],\n      [127.15589192418089, 35.8285590124265],\n      [127.15589793224571, 35.8285650001223],\n      [127.15598861415597, 35.82866749924556],\n      [127.15601915952509, 35.82870203196944],\n      [127.15612907584041, 35.829168766361605],\n      [127.15609818993224, 35.829295225061195],\n      [127.15608950923394, 35.82932257325263],\n      [127.15608860908732, 35.82932540355114],\n      [127.15600065957886, 35.829462830265285],\n      [127.15609932923006, 35.829460728759244],\n      [127.15636163117557, 35.82945512982481],\n      [127.15636253726609, 35.82945502782808],\n      [127.15672503838279, 35.82941402320954],\n      [127.15672963973414, 35.82941350259527],\n      [127.15770798764024, 35.828807684650975],\n      [127.15806758674961, 35.828371772904966],\n      [127.15894549003491, 35.827436949828986],\n      [127.16186022186079, 35.82441392459995],\n      [127.16595279075413, 35.820536069684174],\n      [127.16701880710747, 35.81953497992243],\n      [127.16795358626332, 35.81879487692539],\n      [127.16938684186572, 35.817688783705364],\n      [127.16949212868332, 35.816255336005085],\n      [127.1695479045243, 35.81540313996001],\n      [127.1708906559429, 35.813751221115815],\n      [127.17127728090844, 35.813649914703674],\n      [127.1723736524215, 35.812590324064516],\n      [127.17204131876998, 35.811890160842694],\n      [127.17363669442409, 35.81120288757749],\n      [127.17382619532096, 35.81103839854553],\n      [127.17383775535781, 35.81098544180789],\n      [127.17390845314934, 35.810661555939774],\n      [127.17387417737798, 35.80990022481016],\n      [127.17387105392869, 35.80983087827681],\n      [127.1757343016669, 35.808084452292746],\n      [127.18018765654834, 35.80510947737577],\n      [127.18183149542018, 35.80492216461449],\n      [127.1874030457556, 35.805486532005034],\n      [127.18832488957933, 35.80620517903509],\n      [127.18985569116877, 35.808298470893874],\n      [127.19129462033007, 35.80975136468071],\n      [127.19153090544485, 35.80982307466922],\n      [127.19847961845917, 35.8106815995389],\n      [127.19989728607244, 35.81059727961322],\n      [127.20370757045299, 35.809051831006094],\n      [127.20474852804762, 35.80862635907213],\n      [127.2051173714076, 35.8080615125781],\n      [127.20548093215075, 35.80750203822161],\n      [127.20731651897658, 35.80576528950792],\n      [127.20959113452608, 35.805146026143454],\n      [127.21128613681164, 35.80504580554964],\n      [127.21287142579862, 35.80495282219094],\n      [127.21359777593328, 35.80488501274952],\n      [127.21627186058552, 35.80239918059962],\n      [127.21701155867875, 35.801651168626876],\n      [127.21701670029556, 35.801344844247744],\n      [127.21702004521529, 35.801145042126834],\n      [127.2164857290672, 35.80018080714441],\n      [127.21494728138639, 35.798243594442894],\n      [127.21474064512878, 35.79808764863975],\n      [127.21462182412604, 35.797998300921606],\n      [127.21427283441197, 35.79786711027185],\n      [127.21388199148937, 35.79792623962564],\n      [127.21382769359273, 35.797933689430536],\n      [127.21350703616606, 35.797977316219246],\n      [127.21246262868934, 35.79738277529464],\n      [127.21231548237823, 35.796448403094224],\n      [127.21167266878655, 35.79455168009008],\n      [127.21123223448299, 35.79382298915727],\n      [127.2104140223535, 35.793174807363954],\n      [127.2069115324332, 35.79043046632309],\n      [127.20251993271974, 35.78777720372621],\n      [127.20059716805466, 35.78645794176312],\n      [127.20108974480101, 35.78604790102607],\n      [127.20113421442375, 35.78600191699821],\n      [127.2012369540559, 35.785892892019255],\n      [127.20166498528953, 35.78538649255201],\n      [127.2018255811931, 35.78512752824346],\n      [127.19979423343486, 35.77763749279475],\n      [127.19968778281299, 35.77651954866135],\n      [127.19963345129058, 35.77635177941875],\n      [127.19938280658312, 35.77562963383649],\n      [127.19881526296788, 35.774771412942286],\n      [127.19625685110516, 35.774113571330744],\n      [127.19523376315134, 35.774147904616896],\n      [127.19462352700752, 35.77440334845778],\n      [127.19449094818073, 35.77445890704057],\n      [127.19379314041913, 35.77509301548469],\n      [127.19242099436559, 35.776290778391804],\n      [127.1881140352379, 35.77899596607086],\n      [127.18805181730666, 35.77900866906013],\n      [127.18771018199112, 35.77907827311318],\n      [127.18688935319157, 35.77924387680554],\n      [127.18539218790903, 35.77913732143794],\n      [127.18529428425676, 35.77909353490301],\n      [127.18423644234542, 35.778619588093385],\n      [127.18421376706404, 35.778603851126924],\n      [127.18229285108822, 35.777030964207704],\n      [127.18110648062942, 35.775715208141044],\n      [127.18089186061414, 35.77555795289366],\n      [127.17875703007685, 35.77518131801877],\n      [127.17811511840358, 35.77536476814376],\n      [127.17774077985837, 35.775473448178076],\n      [127.17712062814532, 35.776228477828724],\n      [127.1730842625675, 35.779322806919524],\n      [127.17101644806102, 35.78049864906178],\n      [127.16603194009238, 35.78177902486914],\n      [127.1634816885206, 35.7819765386129],\n      [127.16185006802633, 35.78341298283422],\n      [127.15937180699406, 35.78187019439637],\n      [127.1585556998626, 35.78157737192061],\n      [127.1553255501144, 35.7800605276726],\n      [127.1544468324612, 35.77963637130667],\n      [127.15420872814238, 35.77947346520646],\n      [127.15394726126563, 35.77929156996132],\n      [127.1533995012482, 35.778838291412924],\n      [127.1531223283622, 35.777065509117115],\n      [127.1523072399933, 35.775992060016534],\n      [127.15067093284405, 35.77479176629242],\n      [127.15039700702884, 35.77459368440319],\n      [127.14931680001527, 35.77253535637423],\n      [127.14863003233803, 35.76949500892332],\n      [127.14917277857226, 35.768941618638905],\n      [127.14934000197088, 35.76882523060755],\n      [127.14948704607853, 35.768728444342756],\n      [127.14973345752925, 35.76873532389914],\n      [127.1498200191801, 35.76873846115167],\n      [127.15023069600936, 35.76851910992876],\n      [127.1502333090137, 35.76806961112554],\n      [127.15023087992301, 35.767632897080446],\n      [127.15020017916216, 35.76744819486939],\n      [127.15006028866499, 35.76701829571596],\n      [127.14894344361169, 35.76620417268164],\n      [127.14777607741848, 35.76543993865304],\n      [127.1472217649683, 35.765047310339924],\n      [127.14631461511676, 35.76397618353329],\n      [127.14619418072931, 35.76358991242963],\n      [127.145660062376, 35.76155680134917],\n      [127.14608113508332, 35.760699805974255],\n      [127.14620196688745, 35.76054476097338],\n      [127.14643399589261, 35.760354783801795],\n      [127.14641434225867, 35.75951649785386],\n      [127.14380704857618, 35.76000389671614],\n      [127.14181409778965, 35.76068303321686],\n      [127.14147153965908, 35.7608128528391],\n      [127.14002325561351, 35.76106195415501],\n      [127.13745932283943, 35.761188235158485],\n      [127.13659908383143, 35.76099163133795],\n      [127.13494223997245, 35.759584409512236],\n      [127.13304497837835, 35.75787086362713],\n      [127.1308361835858, 35.75576374447716],\n      [127.13052166905159, 35.755604107250626],\n      [127.13039389617524, 35.75554256109851],\n      [127.12972243997002, 35.755262193290754],\n      [127.12558241224794, 35.7542961262882],\n      [127.12394669602496, 35.752589213450186],\n      [127.120649094401, 35.75165604980791],\n      [127.11818330433017, 35.75160847273217],\n      [127.11696154887598, 35.74967080768674],\n      [127.11682792275872, 35.74948421087846],\n      [127.11587489106222, 35.749265158314905],\n      [127.1155570694397, 35.74953865201555],\n      [127.11433862563585, 35.7502489801349],\n      [127.10454052219262, 35.75254009123712],\n      [127.10258838880486, 35.75249873988192],\n      [127.10069888987405, 35.75027959627791],\n      [127.09929735515682, 35.74773537373314],\n      [127.0960629567337, 35.745489168268385],\n      [127.09569468726191, 35.74476660551508],\n      [127.09558720650233, 35.74356237378983],\n      [127.09553524369747, 35.741540199242124],\n      [127.09580626127057, 35.74036651021287],\n      [127.09597768776867, 35.73896167045323],\n      [127.09572946621935, 35.73748842813409],\n      [127.09546616734698, 35.73728529024795],\n      [127.09492228106285, 35.73686746239988],\n      [127.09460057439446, 35.736625081908336],\n      [127.09449055745841, 35.73663840778115],\n      [127.09137049122891, 35.736167045679444],\n      [127.08753807574742, 35.73240339762132],\n      [127.08643525313707, 35.73089314362657],\n      [127.08598374652547, 35.73019779857921],\n      [127.08537874282977, 35.72903783993472],\n      [127.08513886877897, 35.72857463236245],\n      [127.08513787335829, 35.728574252951795],\n      [127.084795166498, 35.72870828980824],\n      [127.08306847948279, 35.72939951185805],\n      [127.08302376415577, 35.72941742462614],\n      [127.08301013166306, 35.729424760811675],\n      [127.08291917073419, 35.72948579228975],\n      [127.0825270479057, 35.729750246663905],\n      [127.08149028666054, 35.73045282854631],\n      [127.08142598581496, 35.73083743831201],\n      [127.08135972402619, 35.731239114405454],\n      [127.08133448930192, 35.73139389729064],\n      [127.0813104531699, 35.73154445209944],\n      [127.081243518632, 35.73196161118097],\n      [127.08122617371383, 35.73206879393745],\n      [127.08117690993168, 35.73237215699306],\n      [127.08097029306644, 35.733535632640795],\n      [127.08090688370311, 35.73386561906535],\n      [127.07857589840258, 35.736358581081426],\n      [127.0782017389887, 35.73667285718979],\n      [127.07751067203472, 35.73721771599186],\n      [127.0741664480722, 35.73936169928988],\n      [127.07188625600581, 35.74072847432609],\n      [127.0709954994781, 35.74074589182553],\n      [127.07067236636368, 35.74120629107126],\n      [127.07067213114192, 35.741206845633016],\n      [127.07035997221728, 35.74216363769613],\n      [127.07036469491082, 35.74223150184794],\n      [127.07064567049073, 35.743957832763144],\n      [127.07050117190695, 35.74725158808859],\n      [127.07050116289922, 35.747251617810036],\n      [127.0704009968826, 35.74747375850003],\n      [127.0697024656806, 35.74834345361215],\n      [127.0692927428727, 35.74858435118247],\n      [127.06811494587778, 35.74920759401176],\n      [127.06667967561145, 35.74934637340878],\n      [127.06600706035013, 35.74964387394263],\n      [127.06482569565163, 35.750612804438376],\n      [127.06429619333719, 35.7512610097886],\n      [127.06431840716603, 35.75182442367032],\n      [127.06438940553824, 35.752250522915396],\n      [127.0643951257352, 35.752316567702806],\n      [127.06403758717535, 35.752724723067466],\n      [127.06321002374193, 35.75309790530619],\n      [127.06272457741977, 35.753097306576386],\n      [127.06137242212239, 35.752903647100155],\n      [127.0605506605641, 35.752485109679135],\n      [127.06017035660231, 35.752403902711634],\n      [127.0600329931694, 35.75237524631719],\n      [127.05990358139692, 35.7523484058741],\n      [127.05985347694792, 35.752338993545],\n      [127.05836122896223, 35.7521381949362],\n      [127.05742351003217, 35.7523828248584],\n      [127.05079844779175, 35.755128669754576],\n      [127.05076252447756, 35.75514459890692],\n      [127.05057879019874, 35.755282957196236],\n      [127.05041797272311, 35.75559578722978],\n      [127.05041812906742, 35.755603388490094],\n      [127.05043275683455, 35.75631989383581],\n      [127.05043675525798, 35.75636890292901],\n      [127.05110793351275, 35.75839927107823],\n      [127.05166492334021, 35.760025261219276],\n      [127.053500483445, 35.763756272416664],\n      [127.05497311228605, 35.764555784217926],\n      [127.05788220767168, 35.765212513313905],\n      [127.05964420444732, 35.76538939256114],\n      [127.05972416663171, 35.76534612075442],\n      [127.06048024989732, 35.76530322249613],\n      [127.06119221246418, 35.76542510833289],\n      [127.06421033654, 35.76688460179101],\n      [127.0650892120758, 35.767615855547895],\n      [127.06499409722, 35.768016574256634],\n      [127.06602268795213, 35.77026309542991],\n      [127.06997276251124, 35.77287837456019],\n      [127.07103433502562, 35.77353272985972],\n      [127.07214389652451, 35.77372218529967],\n      [127.07258064324078, 35.77342522434521],\n      [127.07267130930005, 35.77336587638447],\n      [127.07346690099051, 35.773754070352936],\n      [127.07347450468292, 35.77376181699417],\n      [127.07415945586673, 35.775242990019954],\n      [127.0742525435819, 35.77561576810263],\n      [127.07424179644714, 35.775629795331696],\n      [127.07289539517652, 35.776793780571744],\n      [127.07223586012528, 35.779218986451426],\n      [127.07215190430023, 35.779878660685114],\n      [127.07130787913576, 35.78188520118829],\n      [127.07109435030168, 35.78230618207151],\n      [127.07086079692446, 35.782627958968405],\n      [127.0681541841511, 35.78455806108573],\n      [127.06544203749256, 35.78616037903986],\n      [127.06508469424192, 35.78905558754583],\n      [127.06579906756068, 35.790329939021774],\n      [127.06509157374575, 35.7909759680311],\n      [127.06503554792052, 35.79311227048502],\n      [127.06468034864005, 35.79441519228142],\n      [127.06512699529485, 35.802012848252694],\n      [127.06517534824398, 35.80215857460577],\n      [127.06522193938684, 35.80228683387173],\n      [127.06637542579742, 35.80477974410523],\n      [127.06658634357424, 35.80522462605486],\n      [127.06747143485487, 35.80624698712845],\n      [127.06785595140342, 35.80673071971099],\n      [127.06821108068303, 35.807204389859365],\n      [127.06828664810152, 35.80744345044361],\n      [127.06833952760879, 35.80761382380613],\n      [127.06832605385418, 35.80762509610728],\n      [127.06808730413367, 35.807824775841716],\n      [127.06779516671472, 35.80806179199545],\n      [127.0677408720109, 35.808464849550866],\n      [127.06768644354905, 35.808919031749404],\n      [127.06766721402474, 35.80908056626466],\n      [127.06758366344047, 35.81006101180299],\n      [127.0674224501195, 35.814751079304386],\n      [127.06742369195022, 35.814751109018324],\n      [127.0676648053255, 35.81474890011787],\n      [127.06913238132275, 35.814744422852236],\n      [127.06922396698396, 35.81474414700581],\n      [127.06922482003365, 35.81492927309212],\n      [127.06922593487145, 35.815173316987895],\n      [127.06805047390591, 35.817872781105486],\n      [127.06736146941184, 35.81899365424702],\n      [127.06709472204298, 35.81929286263067],\n      [127.06610859663733, 35.82071743371596],\n      [127.0657421841288, 35.821265358809946],\n      [127.06555754711309, 35.82156425075195],\n      [127.0655389241155, 35.8215947950067],\n      [127.06553332109354, 35.82160399104296],\n      [127.06551180844308, 35.82164474012402],\n      [127.06549305179185, 35.821686395366804],\n      [127.06547712718742, 35.82172881639586],\n      [127.06546407068743, 35.82177188793621],\n      [127.06546329077804, 35.82177523911711],\n      [127.06545344023166, 35.82181790930701],\n      [127.06542350788986, 35.82196903945642],\n      [127.06541818719349, 35.82201254648707],\n      [127.06541936153577, 35.82205625551942],\n      [127.065427032232, 35.82209952100318],\n      [127.06548793249095, 35.82487592762071],\n      [127.06511751243173, 35.824896709933896],\n      [127.05777726973515, 35.82535487630246],\n      [127.05512727722079, 35.825859356884905],\n      [127.051213216337, 35.82705328324548],\n      [127.05098421851264, 35.82716767377729],\n      [127.05061840822232, 35.82754693638443],\n      [127.05053928573595, 35.82770300341464],\n      [127.0505024065717, 35.82783744905069],\n      [127.05049164473614, 35.82808060227491],\n      [127.05051462530614, 35.828932117746405],\n      [127.0505159424029, 35.82893650810967],\n      [127.05080649066363, 35.82990441557269],\n      [127.05175208935495, 35.83304979887125],\n      [127.05215266517652, 35.83326803043428],\n      [127.05333017334824, 35.83315230347693],\n      [127.05371042698829, 35.833092727538606],\n      [127.05393961164079, 35.83305335209769],\n      [127.05450561596305, 35.83298439190836],\n      [127.05453385984573, 35.83298095507046],\n      [127.0552559397321, 35.83291167894619],\n      [127.05598233038263, 35.83287290328111],\n      [127.05657676916843, 35.83285642918352],\n      [127.05728919506635, 35.83288098275626],\n      [127.05759108072169, 35.832891386402046],\n      [127.05858054465929, 35.83298606569782],\n      [127.05859921661714, 35.83305080323015],\n      [127.05860322145597, 35.838400602276465],\n      [127.06112634325038, 35.8383995266968],\n      [127.06166675601747, 35.838403257148734],\n      [127.06175632258172, 35.83840419689469],\n      [127.06194147450573, 35.838406139599726],\n      [127.06208133046665, 35.83840761583436],\n      [127.06280706010061, 35.83843128732046],\n      [127.06293171477719, 35.83843692860377],\n      [127.06336772589992, 35.838460759862606],\n      [127.06363847060904, 35.83847558115479],\n      [127.06455265799623, 35.83854450186348],\n      [127.06531433617097, 35.838625023204735],\n      [127.06735140356999, 35.83884030533354],\n      [127.06749379105068, 35.83673380959585],\n      [127.06754716364244, 35.834761480584106],\n      [127.06729735165419, 35.834488718614836],\n      [127.06724540053537, 35.83416297057878],\n      [127.06724539846786, 35.834162943523],\n      [127.06724691450488, 35.8341231356338],\n      [127.06724691687593, 35.834123106790784],\n      [127.06731089223592, 35.83373105509539],\n      [127.06731090565924, 35.83373102989861],\n      [127.06774391996393, 35.83299275464586],\n      [127.06795014822568, 35.8326595627534],\n      [127.06944587349126, 35.83233698000071],\n      [127.07106275093973, 35.8319904260318],\n      [127.07104045000884, 35.82837757833322],\n      [127.07100724844841, 35.828248543709876],\n      [127.07088840135759, 35.82750093171181],\n      [127.07087066243267, 35.82733529087564],\n      [127.07086212950826, 35.8271969548738],\n      [127.07083808434726, 35.826642696545385],\n      [127.07080904273658, 35.82596163736545],\n      [127.07079277012944, 35.82557473696133],\n      [127.0711423369316, 35.82524189215536],\n      [127.0712629790765, 35.82524925053834],\n      [127.07180119352732, 35.82528261378914],\n      [127.0722319871153, 35.825328811820405],\n      [127.08330384087344, 35.83106107275879],\n      [127.08632674304147, 35.83328472804396],\n      [127.08641913271174, 35.833468167023874],\n      [127.08639949102998, 35.833981012064065],\n      [127.08638700087516, 35.83430713801223],\n      [127.08638702778403, 35.834333526481835],\n      [127.08638706740888, 35.834369838135544],\n      [127.08681459083887, 35.83603295036747],\n      [127.08688399150556, 35.83612544906636],\n      [127.08695788329834, 35.83622393182868],\n      [127.08742438110755, 35.836749065806494],\n      [127.08765657685609, 35.83698343419853],\n      [127.08847159240739, 35.837472484940136],\n      [127.09010622644345, 35.83821297664334],\n      [127.09084741351093, 35.838015564588574],\n      [127.09089216185863, 35.83800364615355],\n      [127.09572893134965, 35.838266567880275],\n      [127.09842160048386, 35.838990635972216],\n      [127.10169357615828, 35.838523823980054],\n      [127.10361615610132, 35.8399722744053],\n      [127.10382853203615, 35.84041436409811],\n      [127.10441097431786, 35.84329322920314],\n      [127.10790686140354, 35.84047416180194],\n      [127.10819059191797, 35.84012616163443],\n      [127.10910367204059, 35.839553945492945],\n      [127.11044780083574, 35.838949832816176]]]]},\n  'properties': {'name': '전주시완산구',\n   'base_year': '2018',\n   'name_eng': 'Jeonjusiwansangu',\n   'code': '35011'}},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[127.12125606132948, 35.90310705691804],\n      [127.12423640048212, 35.90146481620112],\n      [127.12993195158347, 35.90196703482827],\n      [127.13123672580029, 35.902382359900415],\n      [127.13245159673298, 35.90250214045683],\n      [127.13474798538266, 35.90225796916584],\n      [127.13556426508482, 35.90216636857469],\n      [127.13605899812734, 35.90210245397454],\n      [127.13697833358015, 35.90196284811751],\n      [127.13779267518049, 35.901455827091446],\n      [127.13798973390347, 35.90131197175888],\n      [127.13808643593468, 35.90118314637212],\n      [127.13827861921803, 35.90092325209358],\n      [127.13831175826338, 35.90086408138052],\n      [127.13848723314922, 35.90055069386646],\n      [127.13862578488867, 35.900301851687566],\n      [127.13878554563348, 35.90001101872129],\n      [127.13886026420708, 35.899872362182705],\n      [127.13901240980739, 35.899582387757675],\n      [127.13909297232699, 35.899428243745575],\n      [127.1391674854391, 35.89928114646742],\n      [127.13926252014895, 35.89909289395697],\n      [127.13980795628285, 35.89833804561463],\n      [127.14159286425982, 35.89716519987477],\n      [127.14187451914131, 35.89710995308439],\n      [127.14254977108261, 35.89697763922772],\n      [127.14308368236412, 35.896873078542896],\n      [127.1433906000046, 35.896811323354406],\n      [127.14373297811468, 35.8967399460682],\n      [127.14384312539859, 35.8966860214215],\n      [127.14438642701249, 35.89641753410841],\n      [127.14448429983379, 35.896368417745684],\n      [127.1445653928175, 35.896327486899374],\n      [127.1451058016423, 35.89599450858536],\n      [127.1451917185995, 35.89594088976039],\n      [127.14559915070906, 35.895294042730605],\n      [127.14574963205106, 35.89495899233973],\n      [127.14648939798217, 35.89196992243832],\n      [127.14662653000832, 35.89105780761559],\n      [127.14764784671209, 35.88913832540525],\n      [127.14857264675717, 35.88848154086072],\n      [127.14947682599727, 35.8877346617445],\n      [127.14990653168023, 35.88691005105662],\n      [127.14994406445088, 35.8868088938504],\n      [127.14997876639264, 35.886674233712036],\n      [127.15005350032442, 35.886383488728676],\n      [127.15009523153418, 35.886211358435],\n      [127.15010157022749, 35.88618487688961],\n      [127.15048030896055, 35.8845807523837],\n      [127.15059057133578, 35.88403845390865],\n      [127.15072670872641, 35.88309789887902],\n      [127.1507830363852, 35.88268100280851],\n      [127.15081307431484, 35.88245847079016],\n      [127.15090110273312, 35.881791725103895],\n      [127.15096508659704, 35.88130553195087],\n      [127.15098441661529, 35.88109596809152],\n      [127.1509949356111, 35.880984147088014],\n      [127.15105869343373, 35.880385019653176],\n      [127.15109441980253, 35.88006165382603],\n      [127.15113612739073, 35.8796985697067],\n      [127.1511739234977, 35.879550098200134],\n      [127.15129943421732, 35.87912664406268],\n      [127.15149783012033, 35.87899486926852],\n      [127.15192504235095, 35.8787188961306],\n      [127.15205637894103, 35.87863423160513],\n      [127.15241348359072, 35.878528175222165],\n      [127.15296671413236, 35.87837172658251],\n      [127.153188936293, 35.87830976345067],\n      [127.15402073603752, 35.8777009424598],\n      [127.15426380167486, 35.877504049558844],\n      [127.15478155585727, 35.8770845887924],\n      [127.15498557902842, 35.8769187255295],\n      [127.15611262969013, 35.87582733135044],\n      [127.15975628118638, 35.871450976125395],\n      [127.16136163867239, 35.86884678948912],\n      [127.16274122178146, 35.865349516023514],\n      [127.16277512778841, 35.86526357148209],\n      [127.16458463910281, 35.86252696984427],\n      [127.16554376518684, 35.86128729583338],\n      [127.16598928352258, 35.8607346686731],\n      [127.16655674737379, 35.85992671396448],\n      [127.16659175484835, 35.85987427245684],\n      [127.16672711534581, 35.85966145493844],\n      [127.16677952627306, 35.85957689578185],\n      [127.16687192438536, 35.859420171654314],\n      [127.16687605778523, 35.859411712481524],\n      [127.16726483703151, 35.85861470731257],\n      [127.1673328619457, 35.858470549574314],\n      [127.16744261733693, 35.858178617863345],\n      [127.1678394294171, 35.85709474202287],\n      [127.16793150637643, 35.85637431887536],\n      [127.16794255846477, 35.85628107953899],\n      [127.1679440711739, 35.856263060510464],\n      [127.1679502445134, 35.85618710239605],\n      [127.16840378677989, 35.85363810539166],\n      [127.17118070930867, 35.85398414226719],\n      [127.17127015652345, 35.85399288380238],\n      [127.17203590955575, 35.854063198593714],\n      [127.17239609140714, 35.85408606417716],\n      [127.17425657812794, 35.853903152512125],\n      [127.17421040224876, 35.85359130585533],\n      [127.17482883926259, 35.85353492627981],\n      [127.17792968391214, 35.853339330469055],\n      [127.17840030261404, 35.85339622676275],\n      [127.17867627135084, 35.85342975760051],\n      [127.17948919223493, 35.853711741327565],\n      [127.17962467746344, 35.853807723105284],\n      [127.17980200034008, 35.853974053600666],\n      [127.17999683300151, 35.85415373104421],\n      [127.18027276443581, 35.85440425987588],\n      [127.18148890704116, 35.85475647219647],\n      [127.18194447288428, 35.854733112735346],\n      [127.18206485091557, 35.85472617424453],\n      [127.18385062483921, 35.85440310965099],\n      [127.18467822469245, 35.85420779343935],\n      [127.18539182012309, 35.8543879433337],\n      [127.18549065575364, 35.85441595820303],\n      [127.1875405857056, 35.85544289502653],\n      [127.18754769385538, 35.855453590399335],\n      [127.18771333315243, 35.85571356984076],\n      [127.18782202000031, 35.85588605022559],\n      [127.1879105792966, 35.85602967894472],\n      [127.18792533910131, 35.85605430423626],\n      [127.18780668523038, 35.85635809704533],\n      [127.18771915527192, 35.85657974542616],\n      [127.18763610695531, 35.8567895500923],\n      [127.18760239863735, 35.85687578727013],\n      [127.18935697304585, 35.85886354290074],\n      [127.19007237203961, 35.85926643799766],\n      [127.19007946284367, 35.859269244070795],\n      [127.19073350397261, 35.85940297936958],\n      [127.19249154329543, 35.86003529638075],\n      [127.19409792750875, 35.86086941412405],\n      [127.19554487548731, 35.860771958378784],\n      [127.19653567798056, 35.86031466661824],\n      [127.19807650017624, 35.85800101340818],\n      [127.19815514606327, 35.85601461720298],\n      [127.19814133910042, 35.85596042808594],\n      [127.19810491764069, 35.855781509023345],\n      [127.19808426287054, 35.855679730027376],\n      [127.19807915107043, 35.854590913989504],\n      [127.19810953245745, 35.85421600252861],\n      [127.19811777852804, 35.85419330982678],\n      [127.19811846102664, 35.854191483071546],\n      [127.19839627176343, 35.853563530534856],\n      [127.19845356656032, 35.853511479986885],\n      [127.19874112198184, 35.853539451625565],\n      [127.19888127689461, 35.85355556132339],\n      [127.19905499183544, 35.85357555341],\n      [127.19930182718035, 35.853574579129045],\n      [127.19934835693199, 35.85357436557202],\n      [127.19940525673559, 35.853573984994796],\n      [127.19952461646353, 35.85357308438957],\n      [127.20102119881446, 35.852678653569996],\n      [127.20135615191782, 35.85201709417577],\n      [127.20150477989088, 35.85169464821772],\n      [127.20153379821568, 35.85147167750191],\n      [127.2029182599397, 35.8484618703201],\n      [127.20485467609815, 35.8451061060886],\n      [127.20494630083886, 35.84495639810067],\n      [127.2051909759766, 35.844796715081564],\n      [127.20558207730103, 35.84454736964142],\n      [127.20690924181733, 35.844240088968526],\n      [127.20779359086042, 35.84405156563602],\n      [127.209861820529, 35.841289181430554],\n      [127.20992072093632, 35.84119614174589],\n      [127.21012342123093, 35.84086584285371],\n      [127.20993686217922, 35.832887432568036],\n      [127.20928668609325, 35.832173044983016],\n      [127.20862514250875, 35.83160752545318],\n      [127.20777963582529, 35.82917743103075],\n      [127.20795244519162, 35.82873298809579],\n      [127.20963660715817, 35.826313456146615],\n      [127.20987361232864, 35.82602647578186],\n      [127.21028318364694, 35.82553923360143],\n      [127.21382291935167, 35.822227398552826],\n      [127.2156399170972, 35.82110536122092],\n      [127.21918598967584, 35.81976133299185],\n      [127.22105250660634, 35.82015393103306],\n      [127.22266907491596, 35.82050118449655],\n      [127.22408123050045, 35.8210808684841],\n      [127.22661799922139, 35.82208822349591],\n      [127.22698761067787, 35.822188496691794],\n      [127.22909812099644, 35.822069753706394],\n      [127.22974064840469, 35.82194996671558],\n      [127.2324581717788, 35.82193364444302],\n      [127.23415565961322, 35.821981913660885],\n      [127.23428937480065, 35.82120559418374],\n      [127.23385302164444, 35.82081354649005],\n      [127.23307459943241, 35.82075055546605],\n      [127.23214644790227, 35.820390288682425],\n      [127.23095626413458, 35.81975943710396],\n      [127.23071004972336, 35.81909593656024],\n      [127.22904475783639, 35.81348752776695],\n      [127.22818727211798, 35.809429990870704],\n      [127.22811681557705, 35.80890795321112],\n      [127.22805891997488, 35.808488700611875],\n      [127.22777023682251, 35.80843544854749],\n      [127.227268719812, 35.80837640440498],\n      [127.22645713924545, 35.808280337617816],\n      [127.22604806165805, 35.8082002691374],\n      [127.22478597092326, 35.80717829079498],\n      [127.22391656486373, 35.80636496907527],\n      [127.22328423408095, 35.80569765973084],\n      [127.22235876620167, 35.80466390194142],\n      [127.22219449431283, 35.80452409498123],\n      [127.22047348508447, 35.80348079534846],\n      [127.21913206779017, 35.802728285315936],\n      [127.21701155867875, 35.801651168626876],\n      [127.21627186058552, 35.80239918059962],\n      [127.21359777593328, 35.80488501274952],\n      [127.21287142579862, 35.80495282219094],\n      [127.21128613681164, 35.80504580554964],\n      [127.20959113452608, 35.805146026143454],\n      [127.20731651897658, 35.80576528950792],\n      [127.20548093215075, 35.80750203822161],\n      [127.2051173714076, 35.8080615125781],\n      [127.20474852804762, 35.80862635907213],\n      [127.20370757045299, 35.809051831006094],\n      [127.19989728607244, 35.81059727961322],\n      [127.19847961845917, 35.8106815995389],\n      [127.19153090544485, 35.80982307466922],\n      [127.19129462033007, 35.80975136468071],\n      [127.18985569116877, 35.808298470893874],\n      [127.18832488957933, 35.80620517903509],\n      [127.1874030457556, 35.805486532005034],\n      [127.18183149542018, 35.80492216461449],\n      [127.18018765654834, 35.80510947737577],\n      [127.1757343016669, 35.808084452292746],\n      [127.17387105392869, 35.80983087827681],\n      [127.17387417737798, 35.80990022481016],\n      [127.17390845314934, 35.810661555939774],\n      [127.17383775535781, 35.81098544180789],\n      [127.17382619532096, 35.81103839854553],\n      [127.17363669442409, 35.81120288757749],\n      [127.17204131876998, 35.811890160842694],\n      [127.1723736524215, 35.812590324064516],\n      [127.17127728090844, 35.813649914703674],\n      [127.1708906559429, 35.813751221115815],\n      [127.1695479045243, 35.81540313996001],\n      [127.16949212868332, 35.816255336005085],\n      [127.16938684186572, 35.817688783705364],\n      [127.16795358626332, 35.81879487692539],\n      [127.16701880710747, 35.81953497992243],\n      [127.16595279075413, 35.820536069684174],\n      [127.16186022186079, 35.82441392459995],\n      [127.15894549003491, 35.827436949828986],\n      [127.15806758674961, 35.828371772904966],\n      [127.15770798764024, 35.828807684650975],\n      [127.15672963973414, 35.82941350259527],\n      [127.15672503838279, 35.82941402320954],\n      [127.15636253726609, 35.82945502782808],\n      [127.15636163117557, 35.82945512982481],\n      [127.15609932923006, 35.829460728759244],\n      [127.15600065957886, 35.829462830265285],\n      [127.15608860908732, 35.82932540355114],\n      [127.15608950923394, 35.82932257325263],\n      [127.15609818993224, 35.829295225061195],\n      [127.15612907584041, 35.829168766361605],\n      [127.15601915952509, 35.82870203196944],\n      [127.15598861415597, 35.82866749924556],\n      [127.15589793224571, 35.8285650001223],\n      [127.15589192418089, 35.8285590124265],\n      [127.15574123284027, 35.82840895968722],\n      [127.15569270130288, 35.82836772685941],\n      [127.15569084851869, 35.82836657109098],\n      [127.15566106899544, 35.82834805857007],\n      [127.1556399814983, 35.82833495539072],\n      [127.15563556038684, 35.82833220902687],\n      [127.1549341333606, 35.828461210990476],\n      [127.15421727969819, 35.82867793849419],\n      [127.15410291533944, 35.8290777172861],\n      [127.15405668730897, 35.82933183964045],\n      [127.15406124431146, 35.82945174228997],\n      [127.15242364595917, 35.829707768312886],\n      [127.14813325752549, 35.83016527807208],\n      [127.14652265120012, 35.830667070070966],\n      [127.14652037812718, 35.83056797234998],\n      [127.14651183063262, 35.83052956367989],\n      [127.1464110212425, 35.83007675223584],\n      [127.14623968352748, 35.82935785777554],\n      [127.14620845063048, 35.829234675607466],\n      [127.14614554165114, 35.829092695277375],\n      [127.14605804949174, 35.828898933114544],\n      [127.14207241305714, 35.82801499096051],\n      [127.14146327060772, 35.82788911142981],\n      [127.14123992071804, 35.827845740885586],\n      [127.13872942195485, 35.82730893725355],\n      [127.13693842599847, 35.825640468881886],\n      [127.13466364705329, 35.825338713857434],\n      [127.13417697868871, 35.82527420715428],\n      [127.1337351637979, 35.8252164638417],\n      [127.13360143447727, 35.825201345515424],\n      [127.1333912217796, 35.82517201334366],\n      [127.13315098977408, 35.825138256025745],\n      [127.13177782238738, 35.82397663066133],\n      [127.13202354403046, 35.82358144253712],\n      [127.13282852786507, 35.82270308753572],\n      [127.13359620475322, 35.822045745234504],\n      [127.13557122325363, 35.82054960386114],\n      [127.13547621676032, 35.81918903190997],\n      [127.13520179825814, 35.81856228145076],\n      [127.13498567256765, 35.81839527722268],\n      [127.13479380477186, 35.81824701696916],\n      [127.13469571430879, 35.81818312433366],\n      [127.13446544283613, 35.818033132946795],\n      [127.13426372930414, 35.81790606414015],\n      [127.13405804933798, 35.817776501672064],\n      [127.1340042622175, 35.81775109176744],\n      [127.13348167816396, 35.817619820979004],\n      [127.13302455027515, 35.81753123642155],\n      [127.13299964757864, 35.81752641248441],\n      [127.13209060806795, 35.818145988494315],\n      [127.13167133268317, 35.818570916337805],\n      [127.1314210736207, 35.81906558709838],\n      [127.13098529045986, 35.819286325238124],\n      [127.1303173802127, 35.81943866021799],\n      [127.13009068234605, 35.81948339972286],\n      [127.12939561272175, 35.81939300049346],\n      [127.12836211725518, 35.81987000828217],\n      [127.1281327882773, 35.819993711154225],\n      [127.1278968241129, 35.82012099229317],\n      [127.12749962407125, 35.820397419931105],\n      [127.1271661562495, 35.82092409251507],\n      [127.1275101311783, 35.82201176217279],\n      [127.12786387534695, 35.82284610349671],\n      [127.12761826456388, 35.82374313675225],\n      [127.12658216512102, 35.82524671717377],\n      [127.12663244180077, 35.82568514280861],\n      [127.12708179631969, 35.82623167200146],\n      [127.12826447085902, 35.82767532742502],\n      [127.12876563435547, 35.82843832211523],\n      [127.12847330536076, 35.829025121619885],\n      [127.12682631567407, 35.832264339594225],\n      [127.12616610885227, 35.832866304912116],\n      [127.12274668286028, 35.83471784916018],\n      [127.12093095186137, 35.83586689829406],\n      [127.11847743016729, 35.83825242907003],\n      [127.11822440990959, 35.83840345527894],\n      [127.11802163492057, 35.838476833317145],\n      [127.11782249206348, 35.83853668936038],\n      [127.11754826532164, 35.838569128848576],\n      [127.11719355090722, 35.83858623369339],\n      [127.1170210991372, 35.83858252410064],\n      [127.1169183684167, 35.83857831613743],\n      [127.11671724552282, 35.838562492613725],\n      [127.11649941545609, 35.8385449645881],\n      [127.11591558913986, 35.83849511540428],\n      [127.11523061589928, 35.83843384975816],\n      [127.11515332831482, 35.83838977941879],\n      [127.11518661050826, 35.83836645128747],\n      [127.11435513472738, 35.83784184083071],\n      [127.1139393425632, 35.837848233581205],\n      [127.1135948524044, 35.837854486310746],\n      [127.11044780083574, 35.838949832816176],\n      [127.10910367204059, 35.839553945492945],\n      [127.10819059191797, 35.84012616163443],\n      [127.10790686140354, 35.84047416180194],\n      [127.10441097431786, 35.84329322920314],\n      [127.10382853203615, 35.84041436409811],\n      [127.10361615610132, 35.8399722744053],\n      [127.10169357615828, 35.838523823980054],\n      [127.09842160048386, 35.838990635972216],\n      [127.09572893134965, 35.838266567880275],\n      [127.09089216185863, 35.83800364615355],\n      [127.09084741351093, 35.838015564588574],\n      [127.09010622644345, 35.83821297664334],\n      [127.08847159240739, 35.837472484940136],\n      [127.08765657685609, 35.83698343419853],\n      [127.08742438110755, 35.836749065806494],\n      [127.08695788329834, 35.83622393182868],\n      [127.08688399150556, 35.83612544906636],\n      [127.08681459083887, 35.83603295036747],\n      [127.08638706740888, 35.834369838135544],\n      [127.08638702778403, 35.834333526481835],\n      [127.08638700087516, 35.83430713801223],\n      [127.08639949102998, 35.833981012064065],\n      [127.08641913271174, 35.833468167023874],\n      [127.08632674304147, 35.83328472804396],\n      [127.08330384087344, 35.83106107275879],\n      [127.0722319871153, 35.825328811820405],\n      [127.07180119352732, 35.82528261378914],\n      [127.0712629790765, 35.82524925053834],\n      [127.0711423369316, 35.82524189215536],\n      [127.07079277012944, 35.82557473696133],\n      [127.07080904273658, 35.82596163736545],\n      [127.07083808434726, 35.826642696545385],\n      [127.07086212950826, 35.8271969548738],\n      [127.07087066243267, 35.82733529087564],\n      [127.07088840135759, 35.82750093171181],\n      [127.07100724844841, 35.828248543709876],\n      [127.07104045000884, 35.82837757833322],\n      [127.07106275093973, 35.8319904260318],\n      [127.06944587349126, 35.83233698000071],\n      [127.06795014822568, 35.8326595627534],\n      [127.06774391996393, 35.83299275464586],\n      [127.06731090565924, 35.83373102989861],\n      [127.06731089223592, 35.83373105509539],\n      [127.06724691687593, 35.834123106790784],\n      [127.06724691450488, 35.8341231356338],\n      [127.06724539846786, 35.834162943523],\n      [127.06724540053537, 35.83416297057878],\n      [127.06729735165419, 35.834488718614836],\n      [127.06754716364244, 35.834761480584106],\n      [127.06749379105068, 35.83673380959585],\n      [127.06735140356999, 35.83884030533354],\n      [127.06531433617097, 35.838625023204735],\n      [127.06455265799623, 35.83854450186348],\n      [127.06363847060904, 35.83847558115479],\n      [127.06336772589992, 35.838460759862606],\n      [127.06293171477719, 35.83843692860377],\n      [127.06280706010061, 35.83843128732046],\n      [127.06208133046665, 35.83840761583436],\n      [127.06194147450573, 35.838406139599726],\n      [127.06175632258172, 35.83840419689469],\n      [127.06166675601747, 35.838403257148734],\n      [127.06112634325038, 35.8383995266968],\n      [127.05860322145597, 35.838400602276465],\n      [127.05765680576442, 35.83958631738061],\n      [127.0557735188071, 35.839593612434754],\n      [127.0545115526682, 35.84223496714292],\n      [127.05356318817442, 35.844324376897816],\n      [127.05270852594411, 35.84667739326059],\n      [127.05052037277254, 35.85066260890088],\n      [127.04967831990061, 35.85212292091074],\n      [127.04959549813887, 35.8522508155164],\n      [127.0495213040584, 35.85236525796093],\n      [127.04951701877782, 35.85236730297055],\n      [127.04807378806488, 35.853118931024405],\n      [127.04789168835596, 35.85319336432585],\n      [127.04691922042429, 35.85319189764503],\n      [127.04511749692838, 35.85315468628101],\n      [127.04100727558644, 35.85210488225688],\n      [127.03785713610836, 35.851057615394154],\n      [127.03668939757607, 35.85063029342756],\n      [127.03435470802143, 35.8503133985968],\n      [127.03431751252047, 35.850313968524425],\n      [127.03403855021051, 35.85039347095597],\n      [127.03392302815138, 35.850436178367794],\n      [127.0339209478617, 35.85043702412105],\n      [127.0316290155925, 35.852019450924345],\n      [127.02702891195382, 35.85658605104709],\n      [127.02693937309328, 35.856798565983276],\n      [127.02693695094379, 35.85680434295562],\n      [127.02692537939016, 35.8568330694536],\n      [127.0269042884336, 35.85689631342841],\n      [127.02684164901449, 35.857387209527175],\n      [127.02691278444635, 35.85753957797396],\n      [127.02696092165542, 35.8576265849595],\n      [127.02696367451406, 35.85763151315741],\n      [127.02701421138023, 35.85768839879359],\n      [127.02704138721342, 35.85771895391135],\n      [127.02720318012638, 35.85788958851354],\n      [127.0272715449717, 35.857996174902965],\n      [127.02791937937921, 35.85959983182747],\n      [127.02823624250647, 35.860463836544646],\n      [127.02827570845206, 35.86153631169925],\n      [127.02792980575953, 35.86181972339818],\n      [127.02734096042775, 35.862391007869284],\n      [127.02656750486089, 35.863552376756914],\n      [127.02645623243225, 35.86396922815205],\n      [127.02638695492826, 35.86425032439356],\n      [127.02673020729942, 35.865402433422986],\n      [127.02695651429323, 35.86603608185284],\n      [127.02700608840578, 35.86636249389969],\n      [127.02693812565295, 35.86753441883962],\n      [127.02402852111993, 35.86693340872407],\n      [127.02400499668296, 35.86690299636081],\n      [127.0230114497266, 35.86507421412162],\n      [127.0167811740154, 35.860522449434576],\n      [127.01436725458117, 35.86158817998823],\n      [127.01309438265956, 35.862814845855986],\n      [127.0109907652749, 35.86446798451903],\n      [127.00754646931863, 35.865924854187796],\n      [127.00749387901692, 35.86592880041095],\n      [127.00641289909065, 35.86600656153004],\n      [127.00720171154276, 35.868754217814974],\n      [127.00741780584724, 35.87116533081947],\n      [127.00742906620218, 35.87130953073046],\n      [127.0074451626145, 35.87157172975602],\n      [127.00746058514196, 35.87182577280794],\n      [127.00746441109047, 35.871889696832355],\n      [127.0074711612699, 35.87200770649465],\n      [127.00747964557969, 35.87215642104762],\n      [127.00748103090105, 35.87220034742213],\n      [127.00748294254963, 35.87227414065584],\n      [127.00748867591881, 35.872539450955],\n      [127.00748902065092, 35.87255606079858],\n      [127.00748972058973, 35.87259689546806],\n      [127.00749039960475, 35.87264111106296],\n      [127.00749127988374, 35.87270757724281],\n      [127.00749213729925, 35.87277236814974],\n      [127.00748412830475, 35.874173799540735],\n      [127.00744232569018, 35.8752237537211],\n      [127.00743318296038, 35.87539808979324],\n      [127.00743152810111, 35.87540879945666],\n      [127.00743144384158, 35.87540934728557],\n      [127.00667191587533, 35.87538659087588],\n      [127.0064597724296, 35.875820314446905],\n      [127.00523646617866, 35.87775467573374],\n      [127.00420612845741, 35.87905475179416],\n      [126.99951169855183, 35.88329512720669],\n      [126.99784372026224, 35.88417881752897],\n      [126.99824311360632, 35.88510940236878],\n      [126.99872431759887, 35.88594531835454],\n      [126.99931732252902, 35.88691332129959],\n      [126.99990031107662, 35.887598566452425],\n      [127.00055546723915, 35.88827422550781],\n      [127.0015454970837, 35.889304180991935],\n      [127.00157941052998, 35.890017290992574],\n      [127.00145515597984, 35.89024428688594],\n      [127.00125321707128, 35.89052674838469],\n      [127.00108084273995, 35.89076785513131],\n      [127.00067396902958, 35.89196482160203],\n      [127.00054139298742, 35.8927424231183],\n      [127.00069506801292, 35.89320712449447],\n      [127.00337858727067, 35.893810944532134],\n      [127.00753405594588, 35.894638824760385],\n      [127.0102991676317, 35.89509490666246],\n      [127.01117352021602, 35.89523538017875],\n      [127.0133863860533, 35.895536530062806],\n      [127.01519260382763, 35.89559435154699],\n      [127.02029358990343, 35.89644083017942],\n      [127.02554166079297, 35.89759292112689],\n      [127.02674405805548, 35.89781573179749],\n      [127.0275030278202, 35.897955550688266],\n      [127.0278927448914, 35.897987564637894],\n      [127.02789279363597, 35.8979875666327],\n      [127.02849474060298, 35.89798940851541],\n      [127.02871194013314, 35.897953019569044],\n      [127.0295642590835, 35.8978134197877],\n      [127.03831259890802, 35.897811051719295],\n      [127.04041414307409, 35.89839309476458],\n      [127.04060538308975, 35.89839162655576],\n      [127.04068516479957, 35.8983879412315],\n      [127.04171544856456, 35.89833999733558],\n      [127.04257107539051, 35.898253250780094],\n      [127.04275138150314, 35.89816390176313],\n      [127.04292526031034, 35.89807569287909],\n      [127.0430546855002, 35.8980018628388],\n      [127.04319809918347, 35.89788211865259],\n      [127.04343422297414, 35.897662349762015],\n      [127.04349768788703, 35.89754968174561],\n      [127.04349552988269, 35.89740406861477],\n      [127.04345706011382, 35.89729987944415],\n      [127.04351093973412, 35.897076234788685],\n      [127.04359582046574, 35.89688975650249],\n      [127.0437898366098, 35.89660157924672],\n      [127.04428640179994, 35.89600179728486],\n      [127.0463520505488, 35.89399943440898],\n      [127.04699968270982, 35.893444646794585],\n      [127.04776655336022, 35.89291768448518],\n      [127.04843127575451, 35.892598611303086],\n      [127.04895967568633, 35.89240406749187],\n      [127.0495734278689, 35.892251468427375],\n      [127.0500166036177, 35.89219580040755],\n      [127.05037448052961, 35.89218805797134],\n      [127.05174082148113, 35.89223198828872],\n      [127.05368407335116, 35.892433091397095],\n      [127.05463086710665, 35.89267995925315],\n      [127.05576510290938, 35.89318161700542],\n      [127.0575182452198, 35.89419864929246],\n      [127.05771355295668, 35.89434473035979],\n      [127.05814784016202, 35.89467657122703],\n      [127.05825416336592, 35.89475820027699],\n      [127.05861231700717, 35.895121349723915],\n      [127.0589275394011, 35.89544958368448],\n      [127.05922638025972, 35.89705590678845],\n      [127.05946667565746, 35.89857466508271],\n      [127.0595331407579, 35.89882276266869],\n      [127.05961764922095, 35.899134768757065],\n      [127.05990410377329, 35.89961370353116],\n      [127.06011834169398, 35.8998096192704],\n      [127.06024061359672, 35.89992079991756],\n      [127.06028203205791, 35.89994024764475],\n      [127.06070860783973, 35.90014053519711],\n      [127.06101603070819, 35.900206282498786],\n      [127.06109323575363, 35.900218069118004],\n      [127.06130199254495, 35.90024979647677],\n      [127.06150154749442, 35.900254212834284],\n      [127.061725991324, 35.900214377147535],\n      [127.06245037932067, 35.90005629015512],\n      [127.06283368493636, 35.899970487731096],\n      [127.06297055942488, 35.89993915127129],\n      [127.06344866305784, 35.899800903968455],\n      [127.06378141111367, 35.89970243745522],\n      [127.06732957862586, 35.898569853374745],\n      [127.0684325764306, 35.898589439667795],\n      [127.07300228597413, 35.897874823980295],\n      [127.07460834851497, 35.897640668579776],\n      [127.07542382879215, 35.897606642730544],\n      [127.07635717214231, 35.89756972549754],\n      [127.07694397976226, 35.89755360872949],\n      [127.08008782757244, 35.89767662302018],\n      [127.0814081256771, 35.895719491411555],\n      [127.08159566770073, 35.89497556002756],\n      [127.08197612713248, 35.89418474393197],\n      [127.0825934084058, 35.892952436007775],\n      [127.083184236048, 35.8922730022371],\n      [127.083335532036, 35.89215630618115],\n      [127.08387123262364, 35.891896552461816],\n      [127.08468919337261, 35.89181009071265],\n      [127.08524898956452, 35.89176942618794],\n      [127.08681654167867, 35.89175001699619],\n      [127.08657537880076, 35.892318820049724],\n      [127.08656097592475, 35.892437677509434],\n      [127.08650525440008, 35.89292522955605],\n      [127.08663445516619, 35.89317467544748],\n      [127.08670606029867, 35.89330952761037],\n      [127.0867397116621, 35.89337258972683],\n      [127.09187529027497, 35.89425401666646],\n      [127.09511662379174, 35.89456669825964],\n      [127.09643134311554, 35.894663676427584],\n      [127.09680105090493, 35.89469689082664],\n      [127.09789978526999, 35.89479599833866],\n      [127.0990612026367, 35.89491982959148],\n      [127.09985019124365, 35.895043943866575],\n      [127.10028853036891, 35.89515933913594],\n      [127.10042014949703, 35.89522175873284],\n      [127.10154744005067, 35.895866051663425],\n      [127.10226526425832, 35.896370157004085],\n      [127.10247107696726, 35.89652798064644],\n      [127.10267237750091, 35.89668242963327],\n      [127.1028399056331, 35.89681043438038],\n      [127.10303600330214, 35.89695925397113],\n      [127.1032445825756, 35.89711398210829],\n      [127.1033154300012, 35.897166303554215],\n      [127.10362560478127, 35.89732150360212],\n      [127.1038363715281, 35.89742637477071],\n      [127.10420108221821, 35.897605184979255],\n      [127.10486565583804, 35.89772374867598],\n      [127.10506057064936, 35.897757666181555],\n      [127.10676951264415, 35.897775600635285],\n      [127.10783496455402, 35.897797762122096],\n      [127.10846271614271, 35.897812416570964],\n      [127.11048926672797, 35.89797873084419],\n      [127.11060941262714, 35.898004810738456],\n      [127.11095134022841, 35.89807913268053],\n      [127.11133052621005, 35.89819088257698],\n      [127.11156947106939, 35.89827177571116],\n      [127.11217726611865, 35.89849624664606],\n      [127.1127435146778, 35.898711175920965],\n      [127.11368269097272, 35.89913022655245],\n      [127.1144647969986, 35.899483230876605],\n      [127.11512620340076, 35.89978705580489],\n      [127.11515599751519, 35.89980082499736],\n      [127.11579216877065, 35.90036659828191],\n      [127.11593167296863, 35.900491224098595],\n      [127.11630346166855, 35.90109892808013],\n      [127.11646411271532, 35.90136661505436],\n      [127.11650729944107, 35.901421779600156],\n      [127.11663613744328, 35.90158612760458],\n      [127.1167661365408, 35.90172259439995],\n      [127.11692244014147, 35.90184946535624],\n      [127.11754261816017, 35.90222964281194],\n      [127.117863744881, 35.90238761250073],\n      [127.11840995264154, 35.902590424791455],\n      [127.11881989923033, 35.902740747227746],\n      [127.12002104154396, 35.902971593615945],\n      [127.12125606132948, 35.90310705691804]]]]},\n  'properties': {'name': '전주시덕진구',\n   'base_year': '2018',\n   'name_eng': 'Jeonjusideokjingu',\n   'code': '35012'}}]\n\n\n\nproperties의 name값에는 전주시덕진구가 들어감\n\n\nlocal_jeonju['features'] = [local_dict['features'][i] for i in range(250) if local_dict['features'][i]['properties']['name'] == '전주시덕진구' or local_dict['features'][i]['properties']['name'] == '전주시완산구']\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start = 11,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngeo_data에 포함되어야 할 정보를 모두 가지기 위해 features만 변환한 모습\n\n\nC. (Polygon, Value) 시각화(★★★)\n\n# 예제1 : 덕진구 vs 완산구\n덕진구와 완산구의 전기 사용량이 아래와 같이 정리되었다고 하자.\n\ndf = pd.DataFrame({\n    'key':['전주시덕진구', '전주시완산구'],\n    'elec_use':[20,30]\n})\ndf\n\n\n  \n    \n\n\n\n\n\n\nkey\nelec_use\n\n\n\n\n0\n전주시덕진구\n20\n\n\n1\n전주시완산구\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n(선실습 후이해)\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,    ## Json파일로 폴리곤을 그린다.\n    key_on='properties.name',   ## ['properties']['name'] : 한글이름임, 폴리곤에 이름을 붙인다. 붙이지 않으면 key와 연결되지 못한다.\n    data=df, ## 네임과 데이터 정보\n    columns=['key','elec_use']  ## 해당하는 네임과 y값\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n두 개의 정보를 이용한다.\n\n\n이해를 위해 필요한 약관의 직관\n\n코로플레스맵을 그리기 위해서는 항상 두개의 데이터(geo_data, data)를 연결해야 하는 구조이다.\n두 개의 데이터를 연결하기 위해서는 공유가능한 연결의 매개체가 필요하다.\n코로플레스맵의 연결매개체는 ‘전주시완산구’, ’전주시덕진구’와 같은 지역명이다.\n\n\n# 예제2 : 덕진구 vs 완산구 / key_on에 대한 이해를 위해 만든 억지예제\n덕진구와 완산구 전기 사용량이 아래와 같이 정리되었다고 하자.\n\ndf = pd.DataFrame({\n    'key':['전주시덕진구', 'Jeonjusiwansangu'],\n    'elec_use':[20,30]\n})\ndf\n\n\n  \n    \n\n\n\n\n\n\nkey\nelec_use\n\n\n\n\n0\n전주시덕진구\n20\n\n\n1\nJeonjusiwansangu\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju,\n    key_on = 'properties.name',   ## 한글로 된 이름만 포함함\n    data = df,\n    columns = ['key', 'elec_use']  ## 순서가 뒤바뀌면 안됨\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n깨짐\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,\n    key_on='properties.name_eng',\n    data=df,\n    columns=['key','elec_use']\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이번엔 반대로 깨짐(한글 이름은 name_eng에는 없으니까…)\n\n- 예시2 : code열을 새롭게 할당하고 ’code’열로 key_on\n\ndf.assign(code = ['35012','30511'])\n\n\n  \n    \n\n\n\n\n\n\nkey\nelec_use\ncode\n\n\n\n\n0\n전주시덕진구\n20\n35012\n\n\n1\nJeonjusiwansangu\n30\n30511\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,\n    key_on='properties.code',\n    data=df.assign(code = ['35012','35011']),\n    columns=['code','elec_use']\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n코드로 하면 확실하게 가져올 수 있다. (문자 저장에 따른 문제를 해결할 수 있음)\n\n# 예제 4 : 대한민국 인구수 시각화\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv')\ndf\n\n\n  \n    \n\n\n\n\n\n\n행정구역(시군구)별\n총인구수 (명)\n\n\n\n\n0\n서울특별시\n9532428\n\n\n1\n부산광역시\n3356311\n\n\n2\n대구광역시\n2390721\n\n\n3\n인천광역시\n2945009\n\n\n4\n광주광역시\n1442454\n\n\n5\n대전광역시\n1454228\n\n\n6\n울산광역시\n1122566\n\n\n7\n세종특별자치시\n368276\n\n\n8\n경기도\n13549577\n\n\n9\n강원도\n1537717\n\n\n10\n충청북도\n1596948\n\n\n11\n충청남도\n2118977\n\n\n12\n전라북도\n1789770\n\n\n13\n전라남도\n1834653\n\n\n14\n경상북도\n2627925\n\n\n15\n경상남도\n3318161\n\n\n16\n제주특별자치도\n676569\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nglobal_dict['features'][0]['properties']\n\n{'name': '서울특별시', 'base_year': '2018', 'name_eng': 'Seoul', 'code': '11'}\n\n\n\n이런 식으로 저장되어 있음, 바꿀 건 따로 없어보이니까…\n\n\nm = folium.Map(\n    location = [36,128],\n    zoom_start=7,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=global_dict,\n    key_on='properties.name',\n    data=df,\n    columns=['행정구역(시군구)별','총인구수 (명)']    ## 바로 넣음\n).add_to(m)\nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n# 예제 5 : 대한민국 인수구 시각화(local)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-muni.csv')\ndf\n\n\n  \n    \n\n\n\n\n\n\n행정구역(시군구)별\n총인구수 (명)\n\n\n\n\n0\n종로구\n145346\n\n\n1\n중구\n122781\n\n\n2\n용산구\n223713\n\n\n3\n성동구\n287174\n\n\n4\n광진구\n340814\n\n\n...\n...\n...\n\n\n269\n함양군\n38475\n\n\n270\n거창군\n61242\n\n\n271\n합천군\n43029\n\n\n272\n제주시\n493225\n\n\n273\n서귀포시\n183344\n\n\n\n\n\n274 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    zoom_start = 7,\n    location = [36, 128]\n)\n\nfolium.Choropleth(\n    geo_data = local_dict,\n    key_on = 'properties.name',\n    data = df,\n    columns = ['행정구역(시군구)별', '총인구수 (명)']\n).add_to(m)\n\nm\n\n\n이름의 입력이 고르지 않아 깨진 부분이 있다… 코드로 바꿔야 할 것."
  },
  {
    "objectID": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html",
    "href": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "",
    "text": "심슨의 역설"
  },
  {
    "objectID": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#라이브러리-imports",
    "href": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#라이브러리-imports",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#필요한-코드-비교를-위한-시각화",
    "href": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#필요한-코드-비교를-위한-시각화",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "2. 필요한 코드 | 비교를 위한 시각화",
    "text": "2. 필요한 코드 | 비교를 위한 시각화\n\nA. geom_col()\n\n- 예시1 : geom_col() 기본적인 막대 그래프\n\ndf = pd.DataFrame({'x':[0,1],'y':[40,60]})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n40\n\n\n1\n1\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'x', y = 'y'))   ## geom_bar()는 그냥 없다고 생각하자.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시2 : \\(x\\)축이 범주형인 경우\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score'))   ## 설명변수가 문자열이어도 산출해준다.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시3 : fill = 'index'예시2에서 범주별 색깔로 구분하고 싶은 경우\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score', fill = 'sex'))   ## color 옵션도 있으나, 이것은 바깥의 테두리 색상만 바꾼다.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시4 : 예시3에서 scale_fill_manual()을 이용하여 색상 변경하기\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n  \n    \n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score', fill = 'sex'))\n\nfig + bar + scale_fill_manual(['red','blue'])\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n색상 입력에는 이름이 정해진 색상들 뿐만 아니라 plt.plot(color = option)에서의 옵션과 같이 이미 설정된 C0, C1… 또는 hex code도 입력이 가능하다."
  },
  {
    "objectID": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#b.-facet_warp-한-면을-감싸다.",
    "href": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#b.-facet_warp-한-면을-감싸다.",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "### B. facet_warp() | 한 면을 감싸다.",
    "text": "### B. facet_warp() | 한 면을 감싸다.\n- 예시1 : facet_warp()를 이용한 면분할 – 반별로 면분할\n\ndf = pd.DataFrame({'sex':['male','female','male','female'],'score':[40,60,50,20],'class':['A','A','B','B']})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\nclass\n\n\n\n\n0\nmale\n40\nA\n\n\n1\nfemale\n60\nA\n\n\n2\nmale\n50\nB\n\n\n3\nfemale\n20\nB\n\n\n\n\n\n\n\n\nggplot(df) + geom_col(aes(x='sex',y='score',fill='sex')) + scale_fill_manual(['red','blue']) + facet_wrap('class')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nclass별로 그래프의 면을 분할해서 표시\n\n\nggplot(df) + geom_col(aes(x = 'sex', y = 'score', fill = 'sex')) + scale_fill_manual(['red','blue'])  ## 지정해주지 않을 경우 기본적으로 합산하여 지정된다.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시2 : 성별로 면분할\n\ndf = pd.DataFrame({'sex':['male','female','male','female'],'score':[40,60,50,20],'class':['A','A','B','B']})\ndf\n\n\n  \n    \n\n\n\n\n\n\nsex\nscore\nclass\n\n\n\n\n0\nmale\n40\nA\n\n\n1\nfemale\n60\nA\n\n\n2\nmale\n50\nB\n\n\n3\nfemale\n20\nB\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nggplot(df) + geom_col(aes(x='class',y='score',fill='sex')) + facet_wrap('sex')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n이 경우 'sex'로 color를 구분했으므로 면마다 다른 색의 그래프만이 나온다."
  },
  {
    "objectID": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#심슨의-역설",
    "href": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#심슨의-역설",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "4. 심슨의 역설",
    "text": "4. 심슨의 역설\n- 버클리 대학교의 입학 데이터에서 gender bias가 존재한다는 주장이 있었다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\nA. 시각화 1 : 전체 합격률 시각화 – pandas 초보\n\n- 단순무식하게 query만 이용해서 그룹화\n\ndf.query('gender == \"female\" and result == \"pass\"')['count'].sum()\n\n772\n\n\n\n여성 지원자 중 합격한 사람의 수\n\n\ndf.query('gender == \"female\"')['count'].sum()\n\n1835\n\n\n\n총 여성 지원자 수\n\n\n(df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum(),\n df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum())\n\n(0.420708446866485, 0.5202526941657376)\n\n\n\ntidydata_ = pd.DataFrame({'female' : [df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum()],\n                          'male' : [df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum()]})\n\ntidydata_\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\n0\n0.420708\n0.520253\n\n\n\n\n\n\n\n\n이렇게 하면 시각화 못해요…\n\n\ntidydata = pd.DataFrame({'sex' : ['male', 'female'],\n                         'rate' : [df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum(),\n                                   df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum()]})\n\ntidydata\n\n\n\n\n\n\n\n\nsex\nrate\n\n\n\n\n0\nmale\n0.420708\n\n\n1\nfemale\n0.520253"
  },
  {
    "objectID": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#b.-시각화-1-전체-합격률-시각화-pandas-고수",
    "href": "2023_DV/Review/9. 집단 간 비교_심슨의 역설.html#b.-시각화-1-전체-합격률-시각화-pandas-고수",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "### B. 시각화 1 : 전체 합격률 시각화 – pandas 고수",
    "text": "### B. 시각화 1 : 전체 합격률 시각화 – pandas 고수\ndf.pivot_table(index = row, columns = col, valuse = value_col, aggfunc = func)\n- 피벗 테이블을 만든다. (두 범주형 자료들을 나누는 것)\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n1063\n772\n\n\nmale\n1291\n1400\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count')    ## 집계함수의 디폴트 값이 mean이다\n\n\n  \n    \n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n177.166667\n128.666667\n\n\nmale\n215.166667\n233.333333\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n이 상태에서 reset_index()를 통해 자료를 쉽게 정리할 수 있다.\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\\\n.assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass']))\n## 비율까지 추가한 모습, lambda _df : _df를 통해 현재 데이터프레임까지 호출한 모습\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ngender\n\n\n\n\n\n\n\nfemale\n1063\n772\n0.420708\n\n\nmale\n1291\n1400\n0.520253\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\\\n.assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass'])).reset_index()\n\n\n\n\n\n\n\nresult\ngender\nfail\npass\nrate\n\n\n\n\n0\nfemale\n1063\n772\n0.420708\n\n\n1\nmale\n1291\n1400\n0.520253\n\n\n\n\n\n\n\n- 이제 가공된 데이터를 tidydata라고 하여 그래프를 그려보자.\n\ntidydata = df.pivot_table(index='gender',columns='result',values='count',aggfunc=sum).assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass'])).reset_index()\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x='gender',fill='gender',y='rate'))\nfig+bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n이 그래프를 보고 나온 주장 : 여성이 남성에 비해 합격률이 낮으니까 차별이 있다!!\n\n- 반박\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\n##df.pivot_table(index = ['department', 'gender'], columns = 'result', values = 'count', aggfunc = sum)\n## 위의 두 코드는 완전히 똑같은 동작을 한다. 아마도...\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ndepartment\ngender\n\n\n\n\n\n\nA\nfemale\n19\n89\n\n\nmale\n314\n511\n\n\nB\nfemale\n7\n18\n\n\nmale\n208\n352\n\n\nC\nfemale\n391\n202\n\n\nmale\n204\n121\n\n\nD\nfemale\n244\n131\n\n\nmale\n279\n138\n\n\nE\nfemale\n299\n94\n\n\nmale\n137\n54\n\n\nF\nfemale\n103\n238\n\n\nmale\n149\n224\n\n\n\n\n\n\n\n\ntemp.pass  ## pass 자체가 파이썬에서 특수하게 사용되는 조건어같은 거여서 안된다.\n\nSyntaxError: invalid syntax (2928908933.py, line 1)\n\n\n\ntemp = df.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\ntemp['fail'] + temp['pass']\n\ndepartment  gender\nA           female    108\n            male      825\nB           female     25\n            male      560\nC           female    593\n            male      325\nD           female    375\n            male      417\nE           female    393\n            male      191\nF           female    341\n            male      373\ndtype: int64\n\n\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\\\n.assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass']))\n##df.pivot_table(index = ['gender', 'department'], columns = 'result', values = 'count', aggfunc = sum).reset_index().assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass']))\n\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ndepartment\ngender\n\n\n\n\n\n\n\nA\nfemale\n19\n89\n0.824074\n\n\nmale\n314\n511\n0.619394\n\n\nB\nfemale\n7\n18\n0.720000\n\n\nmale\n208\n352\n0.628571\n\n\nC\nfemale\n391\n202\n0.340641\n\n\nmale\n204\n121\n0.372308\n\n\nD\nfemale\n244\n131\n0.349333\n\n\nmale\n279\n138\n0.330935\n\n\nE\nfemale\n299\n94\n0.239186\n\n\nmale\n137\n54\n0.282723\n\n\nF\nfemale\n103\n238\n0.697947\n\n\nmale\n149\n224\n0.600536\n\n\n\n\n\n\n\n- tidydata 완성\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\\\n.assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass'])).reset_index().drop(['fail', 'pass'], axis = 1)\n\ntidydata = _\ntidydata\n\n\n\n\n\n\n\nresult\ndepartment\ngender\nrate\n\n\n\n\n0\nA\nfemale\n0.824074\n\n\n1\nA\nmale\n0.619394\n\n\n2\nB\nfemale\n0.720000\n\n\n3\nB\nmale\n0.628571\n\n\n4\nC\nfemale\n0.340641\n\n\n5\nC\nmale\n0.372308\n\n\n6\nD\nfemale\n0.349333\n\n\n7\nD\nmale\n0.330935\n\n\n8\nE\nfemale\n0.239186\n\n\n9\nE\nmale\n0.282723\n\n\n10\nF\nfemale\n0.697947\n\n\n11\nF\nmale\n0.600536\n\n\n\n\n\n\n\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x = 'gender', y = 'rate', fill = 'gender'))\n\nfig + bar + scale_fill_manual(['red', 'blue']) + facet_wrap(['department'])\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nC와 E를 제외하고는 오히려 여성의 합격 비율이 더 높은 것을 알 수 있다.\n\n\nD. 해석\n\n- 시각화 1 : 남자의 합격률이 더 높다 -&gt; 성차별이 있어보인다(?)\n- 시각화 2 : 학과별로 살펴보니 오히려 A, B, F, D의 경우 여성의 합격률이 높다.\n- 교재에서 설명한 이유 : 여성이 합격률이 낮은 학과에만 많이 지원하였기 때문.\n\ndf.pivot_table(index='department', columns='gender', values='count',aggfunc='sum')\\\n.stack().reset_index().rename({0:'count'},axis=1)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n\ntidydata = df.pivot_table(index='department', columns='gender', values='count',aggfunc='sum')\\\n.stack().reset_index().rename({0:'count'},axis=1)\n\n \nfig = ggplot(tidydata) \ncol = geom_col(aes(x='department',y='count',fill='gender'),position='dodge')\nfig+col\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nA, B학과가 신체적 역량을 많이 요구로 하거나 하는 학과일 경우 아무래도 기피하겠지…\n은닉변수에 의해 왜곡이 될 수 있다"
  },
  {
    "objectID": "2023_DV/Review/7. Pandas 팁.html",
    "href": "2023_DV/Review/7. Pandas 팁.html",
    "title": "Pandas 사용 팁",
    "section": "",
    "text": "Pandas에서 유용하게 사용할 수 있는 여러가지 메소드들을 알아보자!"
  },
  {
    "objectID": "2023_DV/Review/7. Pandas 팁.html#라이브러리-imports",
    "href": "2023_DV/Review/7. Pandas 팁.html#라이브러리-imports",
    "title": "Pandas 사용 팁",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/7. Pandas 팁.html#pabdas-transform-column",
    "href": "2023_DV/Review/7. Pandas 팁.html#pabdas-transform-column",
    "title": "Pandas 사용 팁",
    "section": "2. pabdas : transform column",
    "text": "2. pabdas : transform column\nA. lambda\nB. map\n\nC. s.apply(변환함수) | 원소들을 각각 변환\n\n\n변환함수 : 원래 형식을 보존하면서 원소들을 바꾸는 함수\n집계함수 : 벡터 -&gt; 스칼라 (평균을 불러주는 함수 : [1,2,3,4,5] -&gt; 3)\n\n\n라고 하자.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')   ## DataFrame\ns = df.Height   ## Series\n\n\ns\n\n0        189cm\n1        179cm\n2        172cm\n3        181cm\n4        172cm\n         ...  \n17655    190cm\n17656    195cm\n17657    190cm\n17658    187cm\n17659    186cm\nName: Height, Length: 17660, dtype: object\n\n\n\n뒤에 cm가 붙어있는 범주형 자료로 저장되어있음.\n\n\ns.apply(lambda x : int(x[:3])) ## 집계함수가 아닌 변환함수만 적용할 수 있음. 각 원소에 함수 적용.\n##s.apply(lambda x : x[:3]).apply(int)    ## 연쇄적으로\n##s.apply(lambda x : x[:3]).astype('int64')   ## astype() 이용\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64\n\n\n\ncm를 제거하고 포맷을 정수형으로 변경하였다."
  },
  {
    "objectID": "2023_DV/Review/7. Pandas 팁.html#d.-s.str-idx.str-string-오브젝트에만-사용할-수-있는-함수를-사용",
    "href": "2023_DV/Review/7. Pandas 팁.html#d.-s.str-idx.str-string-오브젝트에만-사용할-수-있는-함수를-사용",
    "title": "Pandas 사용 팁",
    "section": "### D. s.str, idx.str | string 오브젝트에만 사용할 수 있는 함수를 사용",
    "text": "### D. s.str, idx.str | string 오브젝트에만 사용할 수 있는 함수를 사용\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\n\"180cm\"[:3]\n\n'180'\n\n\n\n'180cm'.replace('cm','')\n\n'180'\n\n\n\n위와 같은 연산을 시리즈에 적용시키고 싶다.\n\n\ns.str[:3]\n##s.str.replace('cm', '')   ## 개별 문자열과 동일하게 메소드를 적용시켜도 된다.\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n\n문자열의 메소드를 그대로 적용 가능\n\n- 예시2 : 원소별로 isupper를 수행(대문자인지 판별)\n\n_s = pd.Series(['A','B','C','d','e','F'])\n_s\n\n0    A\n1    B\n2    C\n3    d\n4    e\n5    F\ndtype: object\n\n\n\n_s.str.isupper()\n\n0     True\n1     True\n2     True\n3    False\n4    False\n5     True\ndtype: bool\n\n\n- 예시3 : 원소별로 공백 제거(pd.Serise 뿐만 아니라 pd.index 자료형에도 사용가능)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\nidx = df.columns\n\n\nidx.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'Joined', 'LoanedFrom',\n       'ContractValidUntil', 'Height', 'Weight', 'ReleaseClause', 'KitNumber',\n       'BestOverallRating'],\n      dtype='object')\n\n\n- 쉽게 말해서 string데이터를 지닌 개체에 string에 사용할 수 있는 메소드를 사용할 수 있도록 하는 게 pandas의 str이라고 보면 된다.\n\nE. s.astype() | 조건을 충족한 시리즈의 타입을 변경\n\n- 예시1 : 원소의 타입을 변경\n\ns = pd.Series(list('12345'))\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: object\n\n\n\ns.astype(int)\n##s.apply(int)\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n- 예시2 : 원소의 타입을 변환한 이후 브로드캐스팅\n\ns1 = pd.Series(list('12345'))\ns2 = pd.Series([-1,-2,-3,-4,-5])\n\n\ns1+s2\n\nTypeError: ignored\n\n\n\nError : 형식이 달라 불가능\n\n\ns1.astype(int) + s2\n\n0    0\n1    0\n2    0\n3    0\n4    0\ndtype: int64\n\n\n\ns2.astype(str) + s1\n\n0    -11\n1    -22\n2    -33\n3    -44\n4    -55\ndtype: object\n\n\n- 예시3 : 원소의 타입을 변환한 이후 브로드캐스팅(str)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Embarked열과 Pclass열을 사용하여 아래와 같은 New Feature를 만들어라.\n\n\n\nEmbarked\nPclass\nNew Feature\n\n\n\n\n‘S’\n3\n‘S3’\n\n\n‘C’\n1\n‘C1’\n\n\n‘S’\n3\n‘S3’\n\n\n‘S’\n1\n‘S1’\n\n\n‘S’\n3\n‘S3’\n\n\n\n둘다 문자열이면 단순히 +를 이용해 브로드캐스팅하면 되지만, 타입이 달라 불가하다.\n\ndf.Embarked + df.Pclass.apply(str)\n##df.Embarked + df.Pclass.astype(str)\n##df.Embarked + pd.Series(list(map(lambda x : str(x)), df.Pclass))\n\n0    S3\n1    C1\n2    S3\n3    S1\n4    S3\ndtype: object"
  },
  {
    "objectID": "2023_DV/Review/7. Pandas 팁.html#f.-컴프리헨션-lambda-map을-무시하지-말-것",
    "href": "2023_DV/Review/7. Pandas 팁.html#f.-컴프리헨션-lambda-map을-무시하지-말-것",
    "title": "Pandas 사용 팁",
    "section": "### F. 컴프리헨션, lambda + map을 무시하지 말 것",
    "text": "### F. 컴프리헨션, lambda + map을 무시하지 말 것\n- 예시1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n위 자료에서 아래와 같은 변환을 하고 싶다면 apply만으로 사용하기에 부담이 된다.\n\\[\nf(\\text{sex}, \\text{sibsp}) =\n\\begin{cases}\n0.7 + 0.25 \\times \\text{sibsp} & \\text{if } \\text{sex} = \\text{'female'} \\\\\n0.2 + 0.15 \\times \\text{sibsp} & \\text{otherwise}\n\\end{cases}\n\\]\n\nlist(map(lambda sex, sibsp : 0.7+0.25*sibsp if sex == 'female' else 0.2+0.15*sibsp, df.Sex, df.SibSp))\n\n[0.35, 0.95, 0.7, 0.95, 0.2]\n\n\n\ndf.assign(Probablity = list(map(lambda sex, sibsp : 0.7+0.25*sibsp if sex == 'female' else 0.2+0.15*sibsp, df.Sex, df.SibSp)))\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\nProbablity\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n0.35\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n0.95\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n0.70\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n0.95\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n0.20\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 예시2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Name열을 아래와 같이 분리하는 작업을 수행하라.\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen\n\n\n\n- 풀이 1\n\ndf.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')\n\n0                            [Braund, Mr, Owen Harris]\n1    [Cumings, Mrs, John Bradley (Florence Briggs T...\n2                             [Heikkinen, Miss, Laina]\n3       [Futrelle, Mrs, Jacques Heath (Lily May Peel)]\n4                           [Allen, Mr, William Henry]\nName: Name, dtype: object\n\n\n\n[[title, name + ' ' + f_name] for f_name, title, name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n- 풀이 2 : 이중 컴프리헨션이 될까 해서 해봤는데… 되네?~(솔직히 안될 이유가 없긴 함, 리스트를 반환하는 거니까…)~\n\n[[names[0], names[1] + ' ' + f_name] for f_name, names in [[f_name, names.split('. ')] for f_name, names in df.Name.str.split(', ')]]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n\nlists = [[names[0], names[1] + ' ' + f_name] for f_name, names in [[f_name, names.split('. ')] for f_name, names in df.Name.str.split(', ')]]\npd.DataFrame({'title' : np.array(lists)[:,0], 'Name' : np.array(lists)[:,1]})\n\n\n\n\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen"
  },
  {
    "objectID": "2023_DV/Review/5. Pandas의 기본.html",
    "href": "2023_DV/Review/5. Pandas의 기본.html",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "",
    "text": "pandas에서 행과 열을 선택하는 기술에 대해서 알아보도록 하자!"
  },
  {
    "objectID": "2023_DV/Review/5. Pandas의 기본.html#라이브러리-import",
    "href": "2023_DV/Review/5. Pandas의 기본.html#라이브러리-import",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "2023_DV/Review/5. Pandas의 기본.html#pandas-행과-열의-선택",
    "href": "2023_DV/Review/5. Pandas의 기본.html#pandas-행과-열의-선택",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "2. pandas : 행과 열의 선택",
    "text": "2. pandas : 행과 열의 선택\n- 같은 자료, 다른 두 형태의 데이터프레임\n\ndf = pd.DataFrame({'date': ['12/30','12/31','01/01','01/02','01/03'], 'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]})\ndf\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n얘는 인덱스는 그저 숫자의 의미이고\n\n\nts = pd.DataFrame({'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]}, index=['12/30','12/31','01/01','01/02','01/03'])\nts  ## 중요한 코드는 아님, 근데 그냥 index 지정해주는 거잖아\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n12/31\n95\n100\n50\n80\n\n\n01/01\n65\n90\n60\n30\n\n\n01/02\n55\n80\n75\n80\n\n\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n얘는 인덱스에 시계열적 표현이 있다.\n\n\nts.reset_index()  ## 결국 이렇게 하는 게 다루기 편하다.\n\n\n  \n    \n\n\n\n\n\n\nindex\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n안바꾸고 냅두는 경우, index가 time seat를 의미하는 경우에는 안바꾸기도 한다. ~근데 위는 왜 다시 바꾼거~\n\n\nA. 열의 선택\n\n1번째 방법 : df.ㆍ\n! 치명적인 단점 : 변수 이름에 공백 등이 있으면 불러올 수 없음.\n\n\ndf.X1   ## df 또한 하나의 object이므로\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\n2번째 방법 : df['ㆍ'], df[['ㆍ']]\n\n\ndf['X1']  ## df를 일종의 딕셔너리처럼 취급하는 방법\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\nSeries로 불러온다.\n\ndictionary?\n\ndct = dict({'date': ['12/30','12/31','01/01','01/02','01/03'], 'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]})\ndct['X1']\n\n[65, 95, 65, 55, 80]\n\n\n\ndf.keys()\n\nIndex(['date', 'X1', 'X2', 'X3', 'X4'], dtype='object')\n\n\n\ndct.keys()\n\ndict_keys(['date', 'X1', 'X2', 'X3', 'X4'])\n\n\n\nkey와 value가 있는 것처럼 column의 한 값(key)에 대한 데이터(value)가 있는 모습이다.\n\n\ndf[['X1']]  ## 프레임으로 산출\n\n\n  \n    \n\n\n\n\n\n\nX1\n\n\n\n\n0\n65\n\n\n1\n95\n\n\n2\n65\n\n\n3\n55\n\n\n4\n80\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf[['X1', 'X2']]  ## 2개 이상 산출 가능\n\n\n  \n    \n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n65\n55\n\n\n1\n95\n100\n\n\n2\n65\n90\n\n\n3\n55\n80\n\n\n4\n80\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n3번째 방법 : df.iloc[:, ㆍ] &gt; 통째로 np.array와 같다고 보면 된다.\n\n\ndf.iloc[:, 0] ## numpy에서 행렬을 다루는 것과 완전히 같게 사용 가능.\n\n0    12/30\n1    12/31\n2    01/01\n3    01/02\n4    01/03\nName: date, dtype: object\n\n\n\n#df.iloc[:,0] ## int - 0번째 행 | [0]이면 데이터프레임으로\n#df.iloc[:,-2:] # int:int, -2번째 행부터 -1번째 행까지(뒤에서 두 개)\n#df.iloc[:,1::2] # int:int:int - 스트라이딩, 1번째(두번째) 행부터 2개 단위로 추출\n#df.iloc[:,[0,2]] # [int,int] - 특정 행(0, 2번째)을 데이터프레임의 형태로 반환\n#df.iloc[:,[True,True,False,False,False]] # bool의 list\n#df.iloc[:,range(2)] # range, 앞에서 2개\n\n\n4번째 방법 : df.loc[:, ㆍ] &gt; 완전히 새로운 방법\n\n\n#df.loc[:,'X1'] # str - 시리즈 | ['X1']이면 데이터프레임으로\n#df.loc[:,'X1':'X3'] # 'str':'str' -- 칼럼이름으로 슬라이싱 **\n#df.loc[:,'X1'::2] # 'str'::int -- 칼럼이름으로 스트라이딩 **\n#df.loc[:,['X1','X4']] # [str,str] - 특정 행만 데이터프레임으로\n#df.loc[:,[True,False,False,True,False]] # bool의 list\n\n\"\"\"\n그냥 왠만해선 다 됨\n\"\"\"\n\n- 'date'행 부터\n\ndf.loc[:, 'date':]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- True가 입력된 행만\n\ndf.loc[:, [True, False, True, False, True]]   ## columns의 이름에 어떤 조건을 걸어서 True에 해당하는 열만 산출 가능\n\n\n  \n    \n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 2칸씩 띄며 스트라이딩\n\ndf.loc[:, ::2]  ## 2 간격으로 스트라이딩\n\n\n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n\n\n\nB. 행의 선택\n\n방법1 : df[]\n\n\ndf[:2]    ## int:int -- 슬라이싱 // df.iloc[:2, :], df.iloc[:2] 와 같음\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf[::2]  ## 스트라이딩, df.iloc[::2]와 같음\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n# df[:2] # int:int -- 슬라이싱 // df.iloc[:2,:], df.iloc[:2] 와 같음\n# df[::2] # int:int -- 스트라이딩\n# ts['12/30':'01/02'] # str:str -- 슬라이싱 &gt; 인덱스가 문자열 등일 경우\n# ts['12/31'::2] # str:str -- 스트라이딩\n# df[['12' in date for date in df.date]] # [bool,bool] `12`가 데이터에 포함되어 있을 경우\n# df[df.X1 &lt; 70] # pd.Series([bool,bool])\n\n\n방법2 : df.iloc[]\n\n\n# df.iloc[0] # int  df.iloc[0, :]에서 생략된 표현\n# df.iloc[-2:] # int:int -- 슬라이싱\n# df.iloc[1::2] # int:int -- 스트라이딩\n# df.iloc[[0]] # [int]\n# df.iloc[[0,1]] # [int,int]\n# df.iloc[['12' in date for date in df.date]] # [bool,bool] 이 경우는 []와 동일하다.\n# df.iloc[range(2)] # range\n\n- 해당 방법은 리스트나 어레이의 원소를 다루는 것과 완전히 동일해서… 아래를 참고하면 된다.\n\nlst = [[1,2,3], [3,4,5]]\nlst[0]\n\n[1, 2, 3]\n\n\n\nary = np.array(lst)\nary[0]\n\narray([1, 2, 3])\n\n\n\n방법3 : df.loc[]\n\n\n# df.loc[0] # int \n# ts.loc['12/30'] # str \n# df.loc[:2] # int:int \n# ts.loc[:'01/02'] # str:str \n# df.loc[[0,1]] # [int,int]\n# ts.loc[['12/30','01/01']] # [str,str]\n# df.loc[['12' in date for date in df.date]] # [bool,bool] 이 경우는 []와 동일하다.\n# df.loc[df.X1&gt;70] # pd.Series([bool,bool]) \n\n\ndf.loc[:2]  ## character와 비슷한 형식이기 때문에 2까지 포함이 된다.\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[df.X1 &gt; 70]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[df.X1 &gt; 70]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n위처럼 튜플로 넣을 수도 있다. 근데 iloc의 경우 위와 같은 코드로 입력하면 오류가 난다.\n\n\n## df.iloc[df.X1 &gt; 70] &gt; 오류, bool을 받을 수 있으나, 튜플의 형태로 들어가면 반환하지 않는다.\ndf.iloc[list(df.X1 &gt; 70)]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nD. 교수님 방식\n-가장 안전한 코드\n\ndf.loc[:,:] ## 해당 코드를 써놓고 시작, generally\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n하나의 col을 뽑으려 할 때\n\n\n# df.X1       ## 제일 간단함. 게다가 눌러보면 변수 이름들이 나옴\n# df['X1']\n# df[['X1']]\n\n\nrow를 슬라이싱\n\n\n# df[:5]\n# ts[:'01/02']  # 시계열일 경우\n\n\n조건에 맞는 row를 뽑을 때 좋은 코드\n\n\n# df[df.X1&lt;60]  ## 이게 좋기는 한데, True, False를 직접 만들어야 하는 경우도 많음.\n# df.loc[['12' in date for date in df.date]]\n\n\n['12' in date for date in df.date]\n\n[True, True, False, False, False]\n\n\n\n하나의 row를 뽑으려 할 때 좋은 코드\n\n\n# df.iloc[0]\n# df.loc[0]\n\n\nts.iloc[[0]]\n# ts.iloc[0]의 경우 오류가 남(인덱스 이름이 숫자열이 아님\n\n\n  \n    \n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\n(row,col)을 뽑으려 할 때 좋은 코드\n\n\n# df.X1[0]    ## &lt;- pd.Series를 뽑고 인덱스로 접근\n# df['X1'][0]\n\n\n# df.iloc[0,0]\n# df.loc[0,'X1']\n\n*위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n\n\n요약\n\n알아두면 좋은 규칙\n.iloc[] 와 .iloc[,:]는 완전히 동등하다.\n.loc[] 와 .loc[,:]는 완전히 동등하다.\n결과를 pd.Series 형태가 아닌 pd.DataFrame 형태로 얻고 싶다면 [[?]]를 사용하면 된다.\n\n\n\nROW\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[3,:]\n\n\nint:int\nX\nO\nO\n\\(\\Delta\\)\ndf[3:5]\n\n\n[int,int]\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[idx,:]\n\n\nstr\nX\nX\nX\nO\nts.loc['time1',:]\n\n\nstr:str\nX\nO\nX\nO\nts.loc['time1':'time2',:]\n\n\n[str,str]\nX\nX\nX\nO\n안할 듯\n\n\n[bool,bool]\nX\nO\nO\nO\ndf[filtered_idx]\n\n\npd.Series([bool,bool])\nX\nO\nX\nO\ndf[df.X1&gt;20]\n\n\n\n\n\nCOL\n\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\ntarget\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\ncol\nX\nX\nO\nX\ndf.iloc[:,0]\n\n\nint:int\ncol\nX\nX\nO\nX\ndf.iloc[:,0:2]\n\n\n[int,int]\ncol\nX\nX\nO\nX\ndf.iloc[:,idx]\n\n\nstr\ncol\nO\nO\nX\nO\ndf.loc[:,'X1']\n\n\nstr:str\ncol\nX\nX\nX\nO\ndf.loc[:,'X1':'X4']\n\n\n[str,str]\ncol\nX\nO\nX\nO\ndf.loc[:,colname_list]\n\n\n[bool,bool]\ncol\nX\nX\nO\nO\ndf.loc[:,bool_list]"
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html",
    "href": "2023_DV/Review/3. Seaborn.html",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "",
    "text": "seaborn을 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html#라이브러리-import",
    "href": "2023_DV/Review/3. Seaborn.html#라이브러리-import",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html#seaborn과-matplotlib",
    "href": "2023_DV/Review/3. Seaborn.html#seaborn과-matplotlib",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "2. Seaborn과 Matplotlib",
    "text": "2. Seaborn과 Matplotlib\n\nmatplotlib : 벡터 친화적\nseaborn : 데이터프레임 친화적\n\n\n분석할 데이터가 태뷸러데이터 형식인 경우가 많다.\nmatplotlib는 여전히 강력하지만, seaborn등 데이터프레임 친화적인 패키지가 우수한 경우가 많다.\n\n\nA. scatter plot\n\n\n## titanic data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x = 'logFare',  ## 요금에 로그를 취한 값(너무 변동이 크니까)\n    y = 'Age',\n    hue = 'Sex',    ## 색상, 색조. 변수 별 색상을 나눠 표기한다.\n    style = 'Survived', style_order = [1,0],  ## Survived 여부로 마커 표시, style_order의 디폴트 값이 [0 =&gt; O,1 =&gt; X]이므로 그 순서를 변경\n    alpha = 0.6     ## 투명도 조절\n)\n\n&lt;Axes: xlabel='logFare', ylabel='Age'&gt;\n\n\n\n\n\n\nplt.hist(df.Age)\nplt.hist(df.Age[df.Survived == 1])  ## 그냥 그리면 겹쳐짐\n\nplt.show()\n\n\n\n\n- seaborn은 데이터과학에서 거의 표준적인 패키지. * 안하는 이유 * 간단한 시각화는 matplotlib가 유리 * seaborn에 대한 고급기능은 matplotlib에 대한 통찰이 있어야 가능 * plotline이 더 우수함(ggplot2) * plotly가 모든 면에서 seaborn을 압도하는 추세임\n\n\nB. seaborn의 고급기능 이해\n\nsns.scatterplot(\n    df,\n    x = 'logFare',\n    y = 'Age',\n    hue = 'Sex',\n    style = 'Survived', style_order = [1,0],\n    alpha = 0.8\n)\n\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scattor Plot')\n\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\n## ax_mini = fig.add_axes([0.6,0.2,0.25,0.25])과 동일\n\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived == 1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nplt.show()\n\n\n\n\n\ntype(fig) ## seaborn으로 제작하였음에도 Figure의 형식을 지닌다.\n\nmatplotlib.figure.Figure"
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html#훌륭한-시각화",
    "href": "2023_DV/Review/3. Seaborn.html#훌륭한-시각화",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "3. 훌륭한 시각화",
    "text": "3. 훌륭한 시각화"
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html#애드워드-터프티",
    "href": "2023_DV/Review/3. Seaborn.html#애드워드-터프티",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "### 애드워드 터프티",
    "text": "### 애드워드 터프티\n- 데이터 시각화계의 거장\n\n엄격한 미니멀리즘\n최소한의 잉크로 많은 정보를 전달할 수 있다면 그것이 바로 좋은 그래프이다.\n\n\n너무 구시대적인 사고일 수도 있음. 적합할 수도 있고."
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html#찰스미나드의-도표",
    "href": "2023_DV/Review/3. Seaborn.html#찰스미나드의-도표",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "### 찰스미나드의 도표",
    "text": "### 찰스미나드의 도표\n\n\n터프티도 극찬하고 ~중국이 놀라고, 일본이 경악하고…~\n\n\n군대의 크기, 2차원 평면상의 위치, 군대의 이동방향, 모스크바에서 퇴각하는 동안의 여러 날짜와 그 시점에서의 온도 -&gt; 6차원의 변수를 한 평면상에 표현\n\n미나드는 여러 그림을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함."
  },
  {
    "objectID": "2023_DV/Review/3. Seaborn.html#미나드처럼-그리는-게-왜-어려운가",
    "href": "2023_DV/Review/3. Seaborn.html#미나드처럼-그리는-게-왜-어려운가",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "4. 미나드처럼 그리는 게 왜 어려운가?",
    "text": "4. 미나드처럼 그리는 게 왜 어려운가?\n- 몸무게, 키, 성별, 국적을 나타내는 자료\n\ndf1=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male1.csv')   ## 남성의 키\ndf2=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male2.csv')   ## 남성의 몸무게\ndf3=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/female.csv')  ## 여성의 키와 몸무게\ndf4=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/foreign.csv') ## 외국인의 키와 몸무게, 성별, 국적\n\n\n_df = pd.concat([pd.concat([df1,df2],axis=1)\\\n                 .assign(g='m'),df3.assign(g='f')])\ndf = pd.concat([_df.assign(g2='korea'),df4.assign(g2='foreign')])\\\n.reset_index(drop=True)\ndf\n\n\n  \n    \n\n\n\n\n\n\nw\nh\ng\ng2\n\n\n\n\n0\n72.788217\n183.486773\nm\nkorea\n\n\n1\n66.606430\n173.599877\nm\nkorea\n\n\n2\n69.806324\n173.237903\nm\nkorea\n\n\n3\n67.449439\n173.223805\nm\nkorea\n\n\n4\n70.463183\n174.931946\nm\nkorea\n\n\n...\n...\n...\n...\n...\n\n\n1525\n78.154632\n188.324350\nm\nforeign\n\n\n1526\n74.754308\n183.017979\nf\nforeign\n\n\n1527\n91.196208\n190.100456\nm\nforeign\n\n\n1528\n87.770394\n187.987255\nm\nforeign\n\n\n1529\n88.021995\n193.456798\nm\nforeign\n\n\n\n\n\n1530 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nsns.scatterplot(\n    data=df,\n    x='w',\n    y='h',\n    hue='g',    ## group 1 : gender\n    style='g2',  ## group 2 : region\n    alpha=0.6\n)\n\n&lt;Axes: xlabel='w', ylabel='h'&gt;\n\n\n\n\n\n\n그래프를 이해하기 어려운 것은 아니지만, 아무래도 난잡한 것은 사실이다.\n\n-어려운 점 :\n\n센스 부족 : 센스가 없어서 그룹 구분할 생각을 못함\n개념 부족 : 타이디데이터( =tidy dataframe, long form dataframe) 형태로 데이터를 정리할 생각을 못함.\n코딩 못함 : 타이디테이터로 데이터를 변형하는 코드를 모름."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html",
    "href": "2023_DV/Review/15. Folium과 월드맵.html",
    "title": "Folium | 월드맵 시각화",
    "section": "",
    "text": "folium을 활용해 그리 대단하진 않지만, 엄청나보이는 차트를 만들어보자."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#라이브러리-imports",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#라이브러리-imports",
    "title": "Folium | 월드맵 시각화",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport folium\nimport json\nimport requests"
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#코로플레스-맵choropleth-map",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#코로플레스-맵choropleth-map",
    "title": "Folium | 월드맵 시각화",
    "section": "2. 코로플레스 맵(Choropleth map)",
    "text": "2. 코로플레스 맵(Choropleth map)\n- 코로플레스 맵의 예시\n\n\n대충 정의하면, coropleth = polygon + y라고 볼 수 있다.(좌표묶음과 측정값)"
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#folium-기본",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#folium-기본",
    "title": "Folium | 월드맵 시각화",
    "section": "3. folium 기본",
    "text": "3. folium 기본\n- 개념\n\nMap Object를 생성(fig)\nMap Object에 이것저것 추가(geom 추가)\n\n\nA. folium.Map()\n\n# 예시 1 : 기본 월드맵\n\nm = folium.Map()\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n세계지도를 불러온다. GIS가 탑재된듯.\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [37, 127], zoom_start = 10\n)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n옵션을 지정하여 시작위치와 초기 줌 정도, 스크롤에 따른 줌인-아웃 기능 비활성화 여부 등을 변경할 수 있다.\n좌표 정보는 구글맵스에서 지역을 더블클릭하여 가져올 수 있다.\nzoom은 18이 최대이며, 그 이상은 18의 값으로 고정된다."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#b.-.folium.marker",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#b.-.folium.marker",
    "title": "Folium | 월드맵 시각화",
    "section": "### B. .folium.Marker()",
    "text": "### B. .folium.Marker()\n# 예제 1 : Map에 Marker를 추가\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.846737, 127.129374], zoom_start = 18\n)\n\nmarker = folium.Marker(location = [35.846737, 127.129374]).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nMap개체에 좌표정보가 포함된 Marker를 추가하는 형태\n\n# 예제 2 : poligon, 통계학과 대학원생의 산책경로\n## 통계학과 대학원생의 산책경로..\n[35.8471, 127.1291]\n[35.8468, 127.1289]\n[35.84635, 127.1291]\n[35.84635, 127.1297]\n[35.8468, 127.12995]\n[35.8474, 127.1300]\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.8468,127.1294],  ## 분수대\n    zoom_start = 18\n)\n\nfolium.Marker(location = [35.8471, 127.1291]).add_to(m)\nfolium.Marker(location = [35.8468, 127.1289]).add_to(m)\nfolium.Marker(location = [35.84635, 127.1291]).add_to(m)\nfolium.Marker(location = [35.84635, 127.1297]).add_to(m)\nfolium.Marker(location = [35.8468, 127.12995]).add_to(m)\nfolium.Marker(location = [35.8474, 127.1300]).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nC. folium.Polygon()\n\n# 예제 1 폴리곤으로 한번에…\n\nm = folium.Map(\n    scrollWheelZoom=False,\n    location = [35.8468,127.1294], # 분수대\n    zoom_start=18,\n)\nfolium.Polygon(\n    locations = [[35.8471, 127.1291],\n                 [35.8468, 127.1289],\n                 [35.84635, 127.1291],\n                 [35.84635, 127.1297],\n                 [35.8468, 127.12995],\n                 [35.8474, 127.1300]],  ## 이중 리스트(2차원)\n    fill=True   ## 속까지 채워줌\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n# 예제 2 : 2개의 폴리곤을 추가(3중 리스트)\n\npoly = np.array([[35.8471, 127.1291],\n                 [35.8468, 127.1289],\n                 [35.84635, 127.1291],\n                 [35.84635, 127.1297],\n                 [35.8468, 127.12995],\n                 [35.8474, 127.1300]])\n\nlat, lon = poly.T\npoly2 = np.stack([lat, lon+0.001], axis = 1)\nnp.stack([poly, poly2], axis = 0)\n\narray([[[ 35.8471 , 127.1291 ],\n        [ 35.8468 , 127.1289 ],\n        [ 35.84635, 127.1291 ],\n        [ 35.84635, 127.1297 ],\n        [ 35.8468 , 127.12995],\n        [ 35.8474 , 127.13   ]],\n\n       [[ 35.8471 , 127.1301 ],\n        [ 35.8468 , 127.1299 ],\n        [ 35.84635, 127.1301 ],\n        [ 35.84635, 127.1307 ],\n        [ 35.8468 , 127.13095],\n        [ 35.8474 , 127.131  ]]])\n\n\n\nm = folium.Map(\n    scrollWheelZoom=False,\n    location = [35.8468,127.1294], # 분수대\n    zoom_start=18,\n)\n\nfolium.Polygon(\n    locations = np.stack([poly, poly2], axis = 0),\n    fill = True\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n삼중 리스트를 잘 넣는다면 코로플레스맵처럼 지도를 행정구역별로 나눌 수 있지 않을까??? \\(\\rightarrow\\) 근데 이 노가다를 다행히도 누군가 해놨다."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#south-korea-github",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#south-korea-github",
    "title": "Folium | 월드맵 시각화",
    "section": "4. South Korea github",
    "text": "4. South Korea github"
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#a.-유저-소개",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#a.-유저-소개",
    "title": "Folium | 월드맵 시각화",
    "section": "### A. 유저 소개",
    "text": "### A. 유저 소개\n\nsouthkorea라는 깃허브 유저가 있는데, 이중 southkorea-maps라는 레포지토리에는…\nkostat/2018/json/이란 폴더가 있고, 아래의 파일들이 있음.\n\nskorea-municipalities-2018-geo.json # &lt;-- 이 파일에 관심있음.\nskorea-municipalities-2018-topo-simple.json\nskorea-municipalities-2018-topo.json\nskorea-provinces-2018-geo.json # &lt;-- 이 파일에 관심있음.\nskorea-provinces-2018-topo-simple.json\nskorea-provinces-2018-topo.json\nskorea-submunicipalities-2018-geo.json\nskorea-submunicipalities-2018-topo-simple.json\nskorea-submunicipalities-2018-topo.json\n이중 관심있는 아래의 두 파일\nskorea-municipalities-2018-geo.json\nskorea-provinces-2018-geo.json\n\n이게 행정구역을 의미하는 폴리곤을 노가다로 정리해 둔 것임.\n\n\nB. json 파일 다운로드\n\n\nglobal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json').text)\nlocal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-municipalities-2018-geo.json').text)\n\n## json파일을 로드할 때 사용하는 코드. 그래서 json과 requests를 import했지! 보면 text로 가져온 것을 알 수 있다.\n\n\nglobal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nlocal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nglobal_dict['type'], global_dict['name'], global_dict['crs']\n\n('FeatureCollection',\n 'sido',\n {'type': 'name', 'properties': {'name': 'urn:ogc:def:crs:OGC:1.3:CRS84'}})\n\n\n\n각각 이러한 키값을 가지고 있는데… 위 셋은 딱히 지역이나 정보와 관련된 것은 없는 것 같다. 그러니까 얘네들은 뭔가 해야될 때 건드리지 않는 게 좋다."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#c.-json-파일의-구조",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#c.-json-파일의-구조",
    "title": "Folium | 월드맵 시각화",
    "section": "### C. json 파일의 구조",
    "text": "### C. json 파일의 구조\n\n#global_dict['features']  ## 굉장히 긴 딕셔너리\n\n\n#global_dict['features'][0]  ## 첫 번째 행정구역의 정보(딕셔너리)\n\n\n#global_dict['features'][0]['geometry']  ## 첫번째 행정구역의 폴리곤 정보(또 딕셔너리...)\n\n\nglobal_dict['features'][0]['geometry']['coordinates']  ## 우리가 원하는 3중 리스트!!(스압주의)\n\n[[[127.02214829071995, 37.6997208220174],\n  [127.02531549179129, 37.69958052666555],\n  [127.0268980715665, 37.700251138801015],\n  [127.02702219174589, 37.70111540651664],\n  [127.02768734803148, 37.700937381678756],\n  [127.02899106854076, 37.69965827866671],\n  [127.02928557925306, 37.69929162349923],\n  [127.02960141563474, 37.69817519597225],\n  [127.02965442206329, 37.69780663241529],\n  [127.02972931239567, 37.69627243691011],\n  [127.03016631976621, 37.69519820573226],\n  [127.03097248066267, 37.693556837990705],\n  [127.0324142885382, 37.691839038874726],\n  [127.03577652734599, 37.69237420164817],\n  [127.03676285064714, 37.69263992022006],\n  [127.03790670521317, 37.69327193541787],\n  [127.03999748131439, 37.69468439213186],\n  [127.04110878733236, 37.69529978284375],\n  [127.0418126234525, 37.69538228560054],\n  [127.04307253543809, 37.69523090725608],\n  [127.04481711632289, 37.6929362514031],\n  [127.045094291979, 37.692385199610435],\n  [127.04862943554828, 37.69406226137355],\n  [127.04880746336237, 37.6932813039853],\n  [127.04975102873895, 37.68920713537357],\n  [127.04982002382117, 37.68797700395555],\n  [127.04998471076341, 37.68761035012039],\n  [127.05064314534968, 37.68650780387472],\n  [127.05092067338259, 37.686158552402],\n  [127.05108569439875, 37.686055160768575],\n  [127.05136797709446, 37.685921581506264],\n  [127.0517981363431, 37.6858148464868],\n  [127.05180416049991, 37.68706675968194],\n  [127.05206063036638, 37.687282882739005],\n  [127.05214429018166, 37.687352392956214],\n  [127.05222706432502, 37.68742077669241],\n  [127.05469544726837, 37.68872271744571],\n  [127.05610252968587, 37.68920437206933],\n  [127.05817261245335, 37.689669926579896],\n  [127.05839034056493, 37.689685868260845],\n  [127.05966013532905, 37.69033198280304],\n  [127.06220905346507, 37.6928052648238],\n  [127.06236779518714, 37.693024235595644],\n  [127.06237354393551, 37.693135159327554],\n  [127.06239478737797, 37.693546495381035],\n  [127.06260174913736, 37.69424379234776],\n  [127.06273214287815, 37.694675910354626],\n  [127.06305928833302, 37.694799063961284],\n  [127.0633863131918, 37.694921883623124],\n  [127.06630814019205, 37.69444172108839],\n  [127.06854802331812, 37.69400040092273],\n  [127.0691509700167, 37.69368950125125],\n  [127.0708788225368, 37.693700048667225],\n  [127.07087906859705, 37.693700062181264],\n  [127.07265760034454, 37.69379950124051],\n  [127.07313786558306, 37.69397574643172],\n  [127.07339823304241, 37.69410087667406],\n  [127.07378269914281, 37.69449368954549],\n  [127.07393007343649, 37.69464423236862],\n  [127.07448636301702, 37.695009628645295],\n  [127.07490680588823, 37.69521968047455],\n  [127.0781962787199, 37.695986510360626],\n  [127.08110478631578, 37.696137457947785],\n  [127.08384328792428, 37.69426889016338],\n  [127.08387832248333, 37.69279212349614],\n  [127.08390501861447, 37.69178245187781],\n  [127.08503096598021, 37.69054991893467],\n  [127.08517719556258, 37.69038984858866],\n  [127.08845357891715, 37.68990263950211],\n  [127.09055188620752, 37.68957025151863],\n  [127.09238424524798, 37.689933184218404],\n  [127.09303672225575, 37.689966466063815],\n  [127.09509714278617, 37.68938485003124],\n  [127.09599596058983, 37.68907104690658],\n  [127.09611844498184, 37.68777746180145],\n  [127.09629522668848, 37.687169760670066],\n  [127.09642514740469, 37.68626418153678],\n  [127.09642311415676, 37.685637441138354],\n  [127.09580983621922, 37.68464039310604],\n  [127.09428757876928, 37.683418538095395],\n  [127.09291905210979, 37.6815515024995],\n  [127.09194656333966, 37.67919029908044],\n  [127.09194153856279, 37.67869251579676],\n  [127.09218959069077, 37.67803010766824],\n  [127.09247959854748, 37.67764303460199],\n  [127.092614356114, 37.677474561385274],\n  [127.0939627740268, 37.675617232268024],\n  [127.09644725778618, 37.66968910695227],\n  [127.0962373655161, 37.66882941453695],\n  [127.09497929397519, 37.66760002717652],\n  [127.09458063193003, 37.666172300680316],\n  [127.0946800144686, 37.66492127931861],\n  [127.09541063101342, 37.663889644590746],\n  [127.09414211770347, 37.66329882532109],\n  [127.09217357563625, 37.66079254873924],\n  [127.09120691221572, 37.659334556715166],\n  [127.09113228726125, 37.658243024751805],\n  [127.09157431798847, 37.657772436849626],\n  [127.09235164609564, 37.65549186279581],\n  [127.09286741474956, 37.654453659625155],\n  [127.09356843248374, 37.653318729010714],\n  [127.0939157426287, 37.65296173000377],\n  [127.09400788402047, 37.65272853044388],\n  [127.09404075508206, 37.65254014144715],\n  [127.09403529660045, 37.652418519610805],\n  [127.09399991212685, 37.652295226453205],\n  [127.09326587472968, 37.65061772855914],\n  [127.09246587944706, 37.64970976752811],\n  [127.0926944997674, 37.64857999351487],\n  [127.09438136905902, 37.64495591656824],\n  [127.09456996976148, 37.644570600805835],\n  [127.09755891099569, 37.64396483085298],\n  [127.09890806335117, 37.6440343984249],\n  [127.10159264246101, 37.64477235815439],\n  [127.10280013821811, 37.64520492267967],\n  [127.1028823886103, 37.64540897697184],\n  [127.10386737257883, 37.64548852460528],\n  [127.10411721532803, 37.64548033184725],\n  [127.10657637866768, 37.64537730972521],\n  [127.10769224246286, 37.64496664676661],\n  [127.10786269687772, 37.64469282241332],\n  [127.10928861344723, 37.64285888886917],\n  [127.10989832951653, 37.64248752395126],\n  [127.11125728398851, 37.64237533392683],\n  [127.11142380118652, 37.64208686345102],\n  [127.11178551212458, 37.64073538398819],\n  [127.11113544340976, 37.639943933494294],\n  [127.11090335225738, 37.638262475561504],\n  [127.11155819260641, 37.63803293665921],\n  [127.11163126044404, 37.63797762400025],\n  [127.11163252401063, 37.63797657539423],\n  [127.1116335605215, 37.63797538002365],\n  [127.11163434149125, 37.637974066637646],\n  [127.11248319338027, 37.63648912158805],\n  [127.11220344878163, 37.63264276723643],\n  [127.1116323201253, 37.631503284414286],\n  [127.111371172448, 37.631227883211594],\n  [127.11082549334053, 37.630781845942806],\n  [127.10888476082644, 37.62930151924797],\n  [127.10596351597135, 37.62733070951351],\n  [127.10432790803095, 37.62327070428881],\n  [127.10406621959581, 37.62165311388996],\n  [127.10490909745644, 37.62156734430207],\n  [127.10538285122183, 37.62090189640023],\n  [127.1056619895515, 37.6204162469155],\n  [127.10555591433918, 37.62037072519622],\n  [127.11130504706806, 37.62069184201602],\n  [127.11214286452346, 37.620307017764965],\n  [127.11502106963971, 37.61953760329908],\n  [127.11619333227809, 37.61894778380488],\n  [127.11725242287622, 37.61674187052708],\n  [127.11699384437024, 37.616455741862225],\n  [127.11687861029367, 37.61632493344969],\n  [127.11667194632817, 37.61601542319483],\n  [127.11670352036289, 37.61571891606276],\n  [127.11679649663584, 37.614954676556906],\n  [127.11708589521979, 37.61396022396543],\n  [127.11737381655063, 37.61224525812902],\n  [127.1172872225345, 37.611180500418115],\n  [127.11687784452862, 37.609530700257416],\n  [127.11669806436957, 37.60884925145508],\n  [127.11809512146557, 37.60547242425238],\n  [127.11809557127602, 37.60544383142748],\n  [127.1180703665001, 37.604938459230944],\n  [127.11804701133192, 37.60460229805868],\n  [127.11558445031194, 37.601718190575795],\n  [127.11409999472093, 37.60012853384475],\n  [127.11447064798934, 37.599054605066726],\n  [127.11548924282775, 37.59765428196658],\n  [127.11572670052439, 37.597314488762194],\n  [127.11644313933067, 37.59619235770955],\n  [127.11687748428929, 37.59549705077813],\n  [127.11666448808528, 37.594016545777],\n  [127.11569009390017, 37.59362528500345],\n  [127.11552687588426, 37.593560121449116],\n  [127.11548546147779, 37.593548610635445],\n  [127.11541325076173, 37.593530949622995],\n  [127.1150282067279, 37.59349020815677],\n  [127.11445400975263, 37.59343304169271],\n  [127.1142683890762, 37.593416046920936],\n  [127.11366535163782, 37.593356363149674],\n  [127.11333737292749, 37.593259818471644],\n  [127.11241171452632, 37.59185636470543],\n  [127.1101545340761, 37.58743032790922],\n  [127.109944225286, 37.58595741365269],\n  [127.10973285600105, 37.58524413527232],\n  [127.109376495115, 37.58409232681838],\n  [127.10896665929798, 37.58337922727966],\n  [127.1074133022487, 37.58249485444903],\n  [127.10525566107893, 37.58156565577623],\n  [127.10344046907751, 37.58059529802311],\n  [127.1031362307321, 37.58021462295139],\n  [127.10289434724014, 37.57990679295722],\n  [127.10290869795878, 37.57978627410625],\n  [127.10302529385626, 37.578906983060165],\n  [127.10231819140941, 37.57804309180647],\n  [127.10195757011611, 37.577552361766436],\n  [127.10114351387524, 37.5760709289373],\n  [127.10091888215331, 37.574204101343454],\n  [127.10088786785404, 37.57376349428088],\n  [127.10166464576407, 37.57240065940598],\n  [127.10312974060275, 37.572279426510896],\n  [127.10366088935352, 37.57191791188504],\n  [127.10367642912719, 37.57190732992747],\n  [127.10423020073478, 37.571387655728294],\n  [127.10385348290774, 37.5704937526072],\n  [127.10272467888672, 37.56706421441915],\n  [127.10167082530147, 37.56340484757312],\n  [127.10120468941865, 37.56157990217243],\n  [127.10116259287368, 37.561313298804556],\n  [127.10112209698956, 37.56105064062629],\n  [127.10113406515579, 37.561003040117356],\n  [127.10127747386707, 37.5604659926051],\n  [127.10139885421326, 37.56025021143431],\n  [127.10154158761175, 37.55999837150702],\n  [127.10169403565163, 37.55974146071346],\n  [127.10180078997355, 37.55956709194242],\n  [127.10185922758541, 37.55949382793663],\n  [127.10216726500754, 37.55927810196558],\n  [127.10492928845024, 37.556421585556336],\n  [127.10540370353208, 37.556335336136144],\n  [127.1054039797918, 37.556335335256705],\n  [127.1060404466977, 37.55636800208074],\n  [127.10639619161522, 37.55646172320076],\n  [127.10714458157308, 37.55710610125959],\n  [127.10998660563237, 37.55834211535795],\n  [127.11167162292871, 37.55872797074785],\n  [127.11296064655822, 37.558794346205815],\n  [127.11522556193233, 37.55676018833242],\n  [127.11564563403175, 37.55769987998584],\n  [127.11730331913004, 37.559425641957304],\n  [127.11736706467859, 37.55947936004973],\n  [127.12295557080333, 37.56348990218229],\n  [127.12807612442845, 37.56590930955854],\n  [127.12861049420515, 37.56616043544185],\n  [127.13087184119757, 37.56687794943424],\n  [127.13375309980111, 37.56779331042001],\n  [127.1342663695222, 37.56795394418317],\n  [127.13474102736858, 37.56802464198976],\n  [127.13766579387027, 37.568424209460574],\n  [127.14560001479188, 37.568431169360885],\n  [127.1489495440468, 37.56843431854424],\n  [127.15124335260579, 37.569795301642884],\n  [127.15319599288145, 37.57096985010494],\n  [127.1549810601263, 37.57204353719504],\n  [127.15689916857828, 37.572954115769065],\n  [127.16084531250948, 37.575652662972885],\n  [127.1614681179371, 37.57604621474144],\n  [127.16200668497996, 37.57638652892725],\n  [127.16255503715276, 37.576730120691614],\n  [127.16674716243453, 37.578975666143656],\n  [127.1686475082068, 37.57899747794731],\n  [127.1686477364252, 37.57899709190847],\n  [127.17025143915063, 37.579016497229034],\n  [127.17183744214854, 37.57926199037442],\n  [127.17434341757382, 37.58002897085931],\n  [127.1745646848708, 37.58009734641977],\n  [127.17507432498863, 37.58025539696954],\n  [127.17510636726472, 37.58026661233748],\n  [127.17565914212791, 37.58056537625033],\n  [127.17585094748326, 37.580669273596094],\n  [127.17601761614299, 37.58076165809506],\n  [127.17637876684248, 37.58095906115711],\n  [127.17651851047951, 37.58103431126574],\n  [127.17715482863876, 37.58120032803303],\n  [127.17714288110588, 37.579858265364955],\n  [127.17706037157764, 37.579544339856604],\n  [127.17698860354076, 37.579416335558484],\n  [127.1769358967764, 37.57934067705333],\n  [127.17688336105913, 37.579265297509494],\n  [127.17674096763471, 37.57911572057686],\n  [127.17668903282423, 37.57906934265248],\n  [127.17559141963727, 37.57845491709445],\n  [127.17544227008392, 37.57836278564491],\n  [127.17541541545809, 37.57829863026236],\n  [127.17540428164837, 37.5782080868865],\n  [127.17540423462499, 37.57820770097899],\n  [127.17536098677607, 37.57784747971302],\n  [127.17532131705451, 37.577363342392985],\n  [127.17532434111249, 37.57729990460691],\n  [127.17532755327359, 37.57723513694318],\n  [127.17568308790581, 37.57489869852943],\n  [127.17570202935693, 37.57483166387621],\n  [127.1761038656408, 37.57413533821314],\n  [127.17613152481712, 37.57408856274329],\n  [127.1763084845669, 37.57397427178013],\n  [127.1764980410476, 37.573801389354294],\n  [127.17694363989052, 37.57329364424182],\n  [127.17773113162198, 37.57218791533225],\n  [127.17781926600982, 37.57205770863517],\n  [127.17913286165616, 37.56912369470184],\n  [127.17914903579462, 37.569085099484376],\n  [127.17920371766037, 37.56894649028298],\n  [127.17922017374693, 37.568873819289145],\n  [127.1793312074897, 37.56826577378065],\n  [127.17937794706329, 37.56798582974494],\n  [127.17943875427594, 37.56760084598443],\n  [127.17946467033734, 37.566583016887506],\n  [127.1801941808996, 37.56458787465877],\n  [127.18099458554735, 37.56332274641403],\n  [127.18118573247567, 37.5630169666324],\n  [127.18127812880661, 37.562830429531225],\n  [127.18151410938351, 37.5622675112996],\n  [127.18190992016416, 37.56124879949777],\n  [127.18199542158489, 37.56099244596784],\n  [127.18204081632643, 37.56040549758009],\n  [127.18202084650781, 37.55994782824462],\n  [127.18202079834414, 37.55994743873002],\n  [127.18198419478763, 37.55965551058688],\n  [127.181787374278, 37.55882911648127],\n  [127.18165662833978, 37.557998302305066],\n  [127.18135146515526, 37.553258048765485],\n  [127.1813403273481, 37.55296496478539],\n  [127.18142169421999, 37.552757324685345],\n  [127.18159477250546, 37.552592350538426],\n  [127.18170384487837, 37.552507995663994],\n  [127.182926745995, 37.55023535046519],\n  [127.18284450852022, 37.5494290906802],\n  [127.1826736926933, 37.547747320441374],\n  [127.18265679525105, 37.54752440129038],\n  [127.18276107519915, 37.546499426131824],\n  [127.18279599113094, 37.54638983933193],\n  [127.18353917154177, 37.54517039495601],\n  [127.18169282664165, 37.546444749492956],\n  [127.18125643812643, 37.54657971583249],\n  [127.17991053630526, 37.54657443190896],\n  [127.17933128170148, 37.54656742148427],\n  [127.17608601659725, 37.54561946546925],\n  [127.17571742052262, 37.54520770406822],\n  [127.17538324376515, 37.54520199786245],\n  [127.17463760452904, 37.54525912602242],\n  [127.17447709977255, 37.545277660809106],\n  [127.1743146633164, 37.54529732395536],\n  [127.17390569685948, 37.545600032434535],\n  [127.17157169030703, 37.545385773320305],\n  [127.1651681782903, 37.544631798899346],\n  [127.1631627309092, 37.54499101097656],\n  [127.16009589899915, 37.54151453079874],\n  [127.15958338038794, 37.54102822393515],\n  [127.15671195122711, 37.53757774554748],\n  [127.15603276390642, 37.53734241425835],\n  [127.15402132902574, 37.534514497157076],\n  [127.15356680162624, 37.533665193070505],\n  [127.15374715099493, 37.53160008846154],\n  [127.15353910604473, 37.53043671974301],\n  [127.15323227758938, 37.52923709895612],\n  [127.15316377616772, 37.529108804926516],\n  [127.14947629508237, 37.52502054146565],\n  [127.14779743367326, 37.52214947334571],\n  [127.14593249219926, 37.521955801698766],\n  [127.14578093977661, 37.521941375307804],\n  [127.14480065945018, 37.51938973774546],\n  [127.14509193634692, 37.51683494851859],\n  [127.14532885964844, 37.516607158321115],\n  [127.14538965833438, 37.51651332725961],\n  [127.14543404002671, 37.51642852537821],\n  [127.14544427334503, 37.51633278212625],\n  [127.14543916612115, 37.51606474208284],\n  [127.14485430106286, 37.51567802502981],\n  [127.14475438292835, 37.515645484717986],\n  [127.14460355182592, 37.515606249070395],\n  [127.1433741159653, 37.51557760277298],\n  [127.14210518808247, 37.51466010401819],\n  [127.14023773642688, 37.50985352338447],\n  [127.13996581242264, 37.50864088493049],\n  [127.14001384551528, 37.50852453676255],\n  [127.14032052088919, 37.508273591129395],\n  [127.14135345423192, 37.50694790778293],\n  [127.14147949264665, 37.50677882190354],\n  [127.14151665030897, 37.506708103021836],\n  [127.14151309866016, 37.50670219734203],\n  [127.14126852582045, 37.50641867264958],\n  [127.14117786173351, 37.50631882301717],\n  [127.1409971469675, 37.506163894207944],\n  [127.14081284937815, 37.50597856844039],\n  [127.14113191237908, 37.50546377662881],\n  [127.14223298799321, 37.504884417170004],\n  [127.14342402698134, 37.50441363645751],\n  [127.14392479947718, 37.50440092444333],\n  [127.14410326532739, 37.5043964831474],\n  [127.14550250136601, 37.50330570568455],\n  [127.14595206598852, 37.5032268868512],\n  [127.14769492141562, 37.50321206574927],\n  [127.14860243574782, 37.50372282317089],\n  [127.1490451129403, 37.50400073373371],\n  [127.15082434872058, 37.50406156817608],\n  [127.15165372960253, 37.503395752670144],\n  [127.1521091406241, 37.50297029811745],\n  [127.15323083173114, 37.50267941781355],\n  [127.15643795942692, 37.50185900576393],\n  [127.1565550135419, 37.50189404497823],\n  [127.15772923925586, 37.50317895196306],\n  [127.1588661178982, 37.50239019657405],\n  [127.16050427114095, 37.501094512438115],\n  [127.16121949144342, 37.50039527422741],\n  [127.16139030513257, 37.5002010498375],\n  [127.16143108867095, 37.50002247972045],\n  [127.16142016695896, 37.49970545993054],\n  [127.16135443347227, 37.499537459426215],\n  [127.16125092773778, 37.49939089854903],\n  [127.16110733163866, 37.49927532319294],\n  [127.15986640646732, 37.495706834034365],\n  [127.15996541511976, 37.49527986027531],\n  [127.16001826923471, 37.494707646707646],\n  [127.16002621602986, 37.49437455452455],\n  [127.1597454988479, 37.49363667082564],\n  [127.15970484975863, 37.49354690503217],\n  [127.15965196338816, 37.493434358962624],\n  [127.1595169833914, 37.49327460353943],\n  [127.15931845606401, 37.49313662914927],\n  [127.15764946367052, 37.490182673876625],\n  [127.15763380331077, 37.4901717886922],\n  [127.15740206846664, 37.48908854265012],\n  [127.15043020214158, 37.4852488932469],\n  [127.15021754524786, 37.48510537954825],\n  [127.14992170977906, 37.484905729020184],\n  [127.14868305306886, 37.484043192462515],\n  [127.148579027746, 37.483889863822014],\n  [127.14822520162613, 37.48336825471925],\n  [127.1477703794897, 37.48262416785541],\n  [127.14766978415436, 37.48242576594467],\n  [127.14760689985782, 37.48226916996039],\n  [127.14758794988897, 37.482221953125226],\n  [127.14752532591987, 37.48201386379114],\n  [127.14748230260066, 37.48180264116629],\n  [127.14747861659086, 37.481773480024934],\n  [127.14745907220343, 37.48158945671828],\n  [127.14739684802731, 37.480585692947166],\n  [127.14728211415827, 37.478739628360124],\n  [127.14715916521881, 37.47729511320696],\n  [127.14714533170121, 37.47722179897961],\n  [127.14584352066795, 37.477273068635235],\n  [127.145796878535, 37.47727491723534],\n  [127.14575646485959, 37.47727651768649],\n  [127.1443704748871, 37.47733114309174],\n  [127.14364342913493, 37.47525741763574],\n  [127.14361885800092, 37.47517065376269],\n  [127.14360185852455, 37.475082746354325],\n  [127.14359251006233, 37.47499412650607],\n  [127.1435733636672, 37.47468530404496],\n  [127.14356722898532, 37.474586308344385],\n  [127.14352658956736, 37.473930418522116],\n  [127.1394881494977, 37.474089616658645],\n  [127.13833405279085, 37.47413506191677],\n  [127.13766388419639, 37.474147919140066],\n  [127.13684251991336, 37.47415318717528],\n  [127.13677761017136, 37.47415548714804],\n  [127.1363969537117, 37.47417048239313],\n  [127.13546828445813, 37.47424021251808],\n  [127.1352873110148, 37.47425705042134],\n  [127.13312242962854, 37.4745798598],\n  [127.13285605187727, 37.47462645866824],\n  [127.13262354347293, 37.472395357539355],\n  [127.13282098466541, 37.46919569189179],\n  [127.13282095574344, 37.4691942063499],\n  [127.1327996032295, 37.468393664276086],\n  [127.13087554387387, 37.46775756923912],\n  [127.13083001167365, 37.46774525764664],\n  [127.13078324278253, 37.46773607352042],\n  [127.13073584632706, 37.46773009808275],\n  [127.13068790133586, 37.46772735681814],\n  [127.13063981486039, 37.46772786001192],\n  [127.13059198261257, 37.46773162422421],\n  [127.1305447100502, 37.46773862246795],\n  [127.13018892567416, 37.4678033697074],\n  [127.12933858733388, 37.467959210768726],\n  [127.12930302575272, 37.467966655370866],\n  [127.1266812987817, 37.468621775910755],\n  [127.12644686764371, 37.468750108913085],\n  [127.12602455140079, 37.4691058782371],\n  [127.12570131939023, 37.46931823345992],\n  [127.12551817321834, 37.46943048351345],\n  [127.12537453641595, 37.46951707676626],\n  [127.12531488674534, 37.46955289992616],\n  [127.12520301623321, 37.46961974560557],\n  [127.12510091762285, 37.469613652978204],\n  [127.12494334079366, 37.4696031234281],\n  [127.12487834302604, 37.469597279961135],\n  [127.12485136253184, 37.469523255232446],\n  [127.1248975970592, 37.46938552451682],\n  [127.12504582809731, 37.468979630189054],\n  [127.12516820516782, 37.46875144004594],\n  [127.1251820284445, 37.468352452585854],\n  [127.12519937283692, 37.467833231267484],\n  [127.12498822426089, 37.46724835864308],\n  [127.12482726884033, 37.46700159707097],\n  [127.12475930593294, 37.466916352653705],\n  [127.1244880965488, 37.4666435226522],\n  [127.12420970259667, 37.466517104307265],\n  [127.12420688119097, 37.46652048627629],\n  [127.12414326804209, 37.466495215148015],\n  [127.1237669717385, 37.46633342476096],\n  [127.12320225935616, 37.46597867005132],\n  [127.12185946757938, 37.46513395264438],\n  [127.11747485026633, 37.46220061045074],\n  [127.11747077309776, 37.462194983538076],\n  [127.11711994009005, 37.46167753791065],\n  [127.11698929578043, 37.46148450859329],\n  [127.11693965937852, 37.46090030945957],\n  [127.11689967841377, 37.4604011395989],\n  [127.1168948964395, 37.45864042153465],\n  [127.11669702911543, 37.45862325785094],\n  [127.11623941228295, 37.4585961143597],\n  [127.11589291262797, 37.45859194472567],\n  [127.11549606922648, 37.45869791695614],\n  [127.11547664981921, 37.458704404910726],\n  [127.11395536575394, 37.45951676647907],\n  [127.1136342600001, 37.45973246531095],\n  [127.11353635596203, 37.45980154776803],\n  [127.11341975702304, 37.45990076930371],\n  [127.11276328611841, 37.46053025046058],\n  [127.11290865142644, 37.46064192929295],\n  [127.11318486414389, 37.46075823127989],\n  [127.11329182742885, 37.46083640990825],\n  [127.11333484694086, 37.46090535023642],\n  [127.11333484534424, 37.46090566030155],\n  [127.11333387525471, 37.46108724567093],\n  [127.1122298035669, 37.46150499909431],\n  [127.11182027181094, 37.46164419121139],\n  [127.10868615916183, 37.4621564211832],\n  [127.10688007579863, 37.462369502165004],\n  [127.10667208681129, 37.46241051180783],\n  [127.10646723536935, 37.46242392987725],\n  [127.10616731423701, 37.462406454624805],\n  [127.10434135861443, 37.46217434553484],\n  [127.10447230410983, 37.46102952693268],\n  [127.10394160853244, 37.460050593707535],\n  [127.10381093870869, 37.459927656301566],\n  [127.09930610906709, 37.45667755614937],\n  [127.09823746163788, 37.456371246967336],\n  [127.09522278311739, 37.45639422742016],\n  [127.09431215563166, 37.45622477015542],\n  [127.09377352285104, 37.45607736437658],\n  [127.09354036220854, 37.455888055203964],\n  [127.08882127538375, 37.44974941955953],\n  [127.08843214262606, 37.44899484298624],\n  [127.08832682104673, 37.44862776075761],\n  [127.08832676671251, 37.44862736036078],\n  [127.08836930483001, 37.44724005811312],\n  [127.08816128515241, 37.44538498355397],\n  [127.08785507296487, 37.44489387071301],\n  [127.08777816075089, 37.44487515146447],\n  [127.08730388244146, 37.44464171021886],\n  [127.082904590745, 37.442173808672536],\n  [127.08243181881421, 37.441579468344386],\n  [127.08209733924632, 37.44135078689038],\n  [127.08144410364704, 37.441230999973904],\n  [127.08017160102241, 37.441051093956524],\n  [127.07908335144711, 37.44129874337731],\n  [127.07817890647885, 37.44155161511739],\n  [127.07760188111618, 37.44170909672176],\n  [127.07601057612212, 37.44213893609793],\n  [127.07475598627195, 37.442218846646846],\n  [127.0744976898156, 37.44223280045726],\n  [127.07224980537977, 37.442199811541435],\n  [127.07186137425094, 37.44102970185969],\n  [127.07221171709271, 37.439382901540675],\n  [127.07376019338288, 37.437789433842276],\n  [127.07379537723341, 37.43765707199904],\n  [127.07384351098383, 37.43740786000359],\n  [127.07356006757476, 37.436963727095595],\n  [127.0730242304579, 37.43640683285397],\n  [127.0709099786286, 37.43309491379506],\n  [127.06961542955781, 37.430611692111725],\n  [127.06569800697741, 37.42915810241767],\n  [127.06114498957349, 37.42998528229182],\n  [127.05978112053222, 37.4295872657291],\n  [127.05345832004603, 37.42880858729806],\n  [127.0503615218898, 37.42982129068007],\n  [127.04957975136693, 37.4302701432368],\n  [127.04736923949098, 37.430701820429285],\n  [127.04722475567219, 37.432040442932944],\n  [127.04644166111552, 37.43299046057295],\n  [127.04590560884242, 37.43348142450798],\n  [127.04418573191268, 37.4350554539952],\n  [127.04109031701444, 37.437769124917864],\n  [127.04087975167322, 37.437878441821276],\n  [127.04048048713241, 37.43808242753594],\n  [127.04005277677203, 37.43823714507124],\n  [127.03956828753749, 37.438195909531515],\n  [127.03922186091357, 37.43816448538617],\n  [127.03903241753622, 37.43815131864844],\n  [127.03706776525239, 37.43827496782511],\n  [127.03697772841022, 37.43829639401592],\n  [127.03557372599246, 37.439004097845434],\n  [127.03532026694523, 37.439966564131865],\n  [127.03529507001076, 37.44007215137235],\n  [127.03520801081689, 37.440437652690015],\n  [127.03516906728176, 37.4406057551936],\n  [127.03517186217657, 37.44094194757337],\n  [127.03528263969284, 37.44103476257073],\n  [127.0360722545635, 37.44155099433042],\n  [127.03608762765222, 37.44156253268867],\n  [127.03643099253159, 37.441832734480634],\n  [127.03665445379359, 37.442080160034884],\n  [127.03748294023711, 37.44326670427758],\n  [127.03793486290014, 37.44417883691202],\n  [127.03803531906024, 37.444516404127626],\n  [127.03818353178544, 37.44501754331141],\n  [127.03823039750196, 37.44518026909942],\n  [127.03820566265372, 37.4458588524557],\n  [127.03781196836518, 37.44892239759809],\n  [127.03777449207378, 37.449201719792974],\n  [127.03774440117567, 37.44941881885998],\n  [127.03696804004046, 37.450691157558936],\n  [127.03640970411149, 37.45149687786112],\n  [127.03577642448006, 37.452195909168545],\n  [127.03626740809199, 37.45426216962762],\n  [127.03712770180721, 37.45520768138208],\n  [127.03706742872744, 37.45547434330387],\n  [127.0367406508785, 37.456443867860756],\n  [127.03492422096939, 37.46017540450221],\n  [127.03453266613107, 37.46333522021434],\n  [127.0346601133906, 37.463976592257936],\n  [127.03467091241455, 37.464090458173466],\n  [127.03460314189319, 37.464173939752115],\n  [127.0331405892598, 37.46501738910679],\n  [127.03275757043367, 37.46519318169501],\n  [127.03192176568467, 37.465518883534436],\n  [127.03119554711617, 37.465626061203594],\n  [127.03055508280168, 37.46551416328518],\n  [127.03039682015434, 37.46548632407364],\n  [127.02992820559366, 37.46535158628146],\n  [127.02958338917796, 37.4642881854868],\n  [127.02873126984468, 37.46202826888523],\n  [127.02815696813059, 37.460621993714916],\n  [127.02634678296988, 37.458104359241986],\n  [127.02620458102675, 37.4579878159816],\n  [127.02599541833983, 37.4578169581426],\n  [127.02543481238588, 37.45756450281699],\n  [127.025277616827, 37.4574938687448],\n  [127.02150295539224, 37.45622583459711],\n  [127.01969201000381, 37.45578660642505],\n  [127.01557484735889, 37.454916014104],\n  [127.0150649965327, 37.45483188354524],\n  [127.0148735746752, 37.454841752950394],\n  [127.01450961789199, 37.45486122860588],\n  [127.0107251186711, 37.45577244670041],\n  [127.00923267412264, 37.45722091033481],\n  [127.00704092418567, 37.460215469835134],\n  [127.00496875820613, 37.463186899888576],\n  [127.0044696722443, 37.46407327656836],\n  [127.00476218583773, 37.46492302457033],\n  [127.00490546550544, 37.46584260252905],\n  [127.00452135791603, 37.46679064066523],\n  [127.00424565849995, 37.46714344410636],\n  [127.00367535892853, 37.467720386588596],\n  [127.00273482189448, 37.46712461108707],\n  [127.00196279723106, 37.467087449562925],\n  [127.00047686511023, 37.46706294825534],\n  [126.99742674489028, 37.46724223517637],\n  [126.99717099538091, 37.467214070232885],\n  [126.99698554452699, 37.46717971600085],\n  [126.99675187621652, 37.467072433303954],\n  [126.99628192995874, 37.46665064295208],\n  [126.99719359563045, 37.464184165933666],\n  [126.99725312948726, 37.46401973479701],\n  [126.99737077162193, 37.46368749255192],\n  [126.99676862985268, 37.461873352884574],\n  [126.9942899320261, 37.461421317893326],\n  [126.99315292901082, 37.46127623511324],\n  [126.9916230699418, 37.46044156303114],\n  [126.98999453840017, 37.4594629780069],\n  [126.98863550944588, 37.458165845862126],\n  [126.98678106962474, 37.45740326454205],\n  [126.98238921265964, 37.45591710269429],\n  [126.97775711976738, 37.455199116892956],\n  [126.97457956131598, 37.454412875477026],\n  [126.97175282603096, 37.45173733547508],\n  [126.97056939304684, 37.44945270098554],\n  [126.96959356922594, 37.44909597687449],\n  [126.96866171986065, 37.44875446455986],\n  [126.967662979455, 37.44838054424455],\n  [126.96428976779832, 37.44626723711946],\n  [126.9639418980544, 37.4452458892869],\n  [126.96393361850694, 37.445214631166266],\n  [126.96432082073919, 37.44346229477368],\n  [126.96463404099109, 37.44203964730629],\n  [126.9638033064139, 37.44078692549053],\n  [126.96373604591848, 37.44074688094471],\n  [126.96311396054732, 37.44037652510398],\n  [126.96298353095135, 37.44029960855669],\n  [126.96294292915762, 37.44028045754587],\n  [126.95899886229834, 37.43912694137169],\n  [126.95636819846372, 37.43875401134443],\n  [126.94928410957398, 37.438250273989894],\n  [126.94815539463404, 37.43863037558223],\n  [126.9456762727248, 37.43730530031926],\n  [126.9450906377534, 37.437088221251784],\n  [126.94022016050745, 37.43571237211725],\n  [126.9393643055451, 37.43602581803675],\n  [126.93857452478132, 37.43642042942645],\n  [126.93774638275855, 37.4373857576777],\n  [126.93727268446924, 37.4386322720711],\n  [126.93725777476023, 37.43892734300138],\n  [126.93724856528063, 37.439167511901296],\n  [126.93724711351074, 37.43921284001876],\n  [126.93740216615096, 37.43937426800926],\n  [126.93769358300823, 37.43968442708137],\n  [126.937713857788, 37.43972075748547],\n  [126.93786405751602, 37.44019753476193],\n  [126.9377458076725, 37.440354020623566],\n  [126.93735357677096, 37.440871598648926],\n  [126.93668810996004, 37.44169594611689],\n  [126.93664692478835, 37.441745758401595],\n  [126.9348979978904, 37.443209481797815],\n  [126.9344508816705, 37.44325822606353],\n  [126.93055407014207, 37.44546848090059],\n  [126.93051744833456, 37.445548423023716],\n  [126.9303939359546, 37.44581837200393],\n  [126.93032633869163, 37.4459686910352],\n  [126.93016316403606, 37.4463779831893],\n  [126.93035671199829, 37.44733422226026],\n  [126.92839869373803, 37.450212485076534],\n  [126.9283570058542, 37.44989868951273],\n  [126.9283079026723, 37.44954867104572],\n  [126.9282796601699, 37.4493526845579],\n  [126.92399602716448, 37.446210224228935],\n  [126.92325327636715, 37.44577274628548],\n  [126.92289722984206, 37.445173349643994],\n  [126.92294662379582, 37.44487295199244],\n  [126.9229034040661, 37.444673007921715],\n  [126.92276280882136, 37.44404108321409],\n  [126.9220070215115, 37.443250757259776],\n  [126.92069895346346, 37.44151996843528],\n  [126.91866310009635, 37.43998262017672],\n  [126.91538923818742, 37.43966802735873],\n  [126.91389137436857, 37.43930708270779],\n  [126.91318980966076, 37.43909539212985],\n  [126.91230851419965, 37.43857946078951],\n  [126.91143164721281, 37.43762316839669],\n  [126.91120280868347, 37.43720121439982],\n  [126.91120000044674, 37.437187408644185],\n  [126.91121640637944, 37.4370463602423],\n  [126.91128427742785, 37.43654325238066],\n  [126.9113332747712, 37.4361713431048],\n  [126.91002834759448, 37.43432412191302],\n  [126.90990399563053, 37.43422744640446],\n  [126.90967840112272, 37.43405973794322],\n  [126.90940570432362, 37.43386468248096],\n  [126.90939564142113, 37.43386044896999],\n  [126.90859344653741, 37.433691729024154],\n  [126.90752668322209, 37.43362021944715],\n  [126.90663449172253, 37.43359694340164],\n  [126.90582547214655, 37.43399615235654],\n  [126.90582519408719, 37.43399615186017],\n  [126.90522457694115, 37.433997069199144],\n  [126.90298757638999, 37.434067620178844],\n  [126.90186087029541, 37.436397524817494],\n  [126.89914461684712, 37.43866819351907],\n  [126.89897791331421, 37.43870182579147],\n  [126.89958451332427, 37.44036666498753],\n  [126.8982570102593, 37.44320652479412],\n  [126.89578235329736, 37.44563339801652],\n  [126.89564123458763, 37.44829320383536],\n  [126.8946038195621, 37.451159154897915],\n  [126.89398300003057, 37.45271733991926],\n  [126.89182284732061, 37.45228350409839],\n  [126.8910486017461, 37.45224245321772],\n  [126.88978012488182, 37.45227307520445],\n  [126.88964213513057, 37.45231546387733],\n  [126.88956055650301, 37.45243222840465],\n  [126.88952054982705, 37.45262112176969],\n  [126.88950365869798, 37.4527010699145],\n  [126.88953577253639, 37.452946634786336],\n  [126.88964770418707, 37.45331924221095],\n  [126.88966648039558, 37.453521707197815],\n  [126.88965276996692, 37.45359855847977],\n  [126.889592047684, 37.45370296489857],\n  [126.88876417143807, 37.45476564219567],\n  [126.88828498935065, 37.45516865405293],\n  [126.8863169029548, 37.45628901611006],\n  [126.88626468031083, 37.456362515796506],\n  [126.88618548846422, 37.456522645677246],\n  [126.88588075182155, 37.45750839016862],\n  [126.8853564731724, 37.45953456857106],\n  [126.88534006096899, 37.45963704166101],\n  [126.88534960239055, 37.45986258161532],\n  [126.88539936108317, 37.46000916976716],\n  [126.88543071619748, 37.46009095992281],\n  [126.88544367776255, 37.46012453265269],\n  [126.88551853776474, 37.46025525216233],\n  [126.885650375358, 37.46043079769516],\n  [126.88607198185638, 37.460859466843345],\n  [126.886128084154, 37.460899787651186],\n  [126.88620134657874, 37.46091646478202],\n  [126.88625378849405, 37.46092666112505],\n  [126.88747359786943, 37.4610655269108],\n  [126.88749655889849, 37.46106301666018],\n  [126.88768203443473, 37.46103729223392],\n  [126.88785888520992, 37.460997475157],\n  [126.88795325015386, 37.46095448958872],\n  [126.8883358698455, 37.46078084535764],\n  [126.88863061689209, 37.46078817357594],\n  [126.88876919238663, 37.46083250900126],\n  [126.88887551772156, 37.460946920406975],\n  [126.88853249141253, 37.46133205164132],\n  [126.88810791069389, 37.461806645801666],\n  [126.88712180563425, 37.462904361685275],\n  [126.88704339763547, 37.46290231182216],\n  [126.88692384788376, 37.46289178120236],\n  [126.8866437804029, 37.46286673117086],\n  [126.88624491347673, 37.46279820029081],\n  [126.88485800472498, 37.46254315202208],\n  [126.88285845675665, 37.46426078649855],\n  [126.88457429409604, 37.46564823572887],\n  [126.88399444693059, 37.46666861463424],\n  [126.88153185190849, 37.46929115820678],\n  [126.88020447481996, 37.47093497073136],\n  [126.87987759363838, 37.4714213189637],\n  [126.87978452249763, 37.471539058789205],\n  [126.87955796413078, 37.47182967743504],\n  [126.87896535335604, 37.47259125928221],\n  [126.87873771587651, 37.4728841366197],\n  [126.87845148258745, 37.47325521436153],\n  [126.87801141557026, 37.47382773802628],\n  [126.87758404673444, 37.47451796201532],\n  [126.87614354086422, 37.47684468784561],\n  [126.87598571313652, 37.47710383526691],\n  [126.87532045038951, 37.478372692072554],\n  [126.87527082721985, 37.47846810142021],\n  [126.87495854231086, 37.47907199778555],\n  [126.87428469764085, 37.480380256044086],\n  [126.87413691190955, 37.480757112415716],\n  [126.87337901472617, 37.48243356052865],\n  [126.8727442969783, 37.482424995238794],\n  [126.87277987743597, 37.482794985827105],\n  [126.87278663474126, 37.48348878210105],\n  [126.8717971958008, 37.48499743368912],\n  [126.87176262543268, 37.485269390497095],\n  [126.87244469888192, 37.48611004050176],\n  [126.87267382382166, 37.486220095922036],\n  [126.87273456256477, 37.486240154483085],\n  [126.87289532395594, 37.486251871041624],\n  [126.87304222324997, 37.486197970915136],\n  [126.87313717688521, 37.486151898173034],\n  [126.87455644442305, 37.48536674989791],\n  [126.8749436128769, 37.485699557362956],\n  [126.87626185595225, 37.48723955123044],\n  [126.8766887354242, 37.4878526882681],\n  [126.87676002659364, 37.48811743166551],\n  [126.87680025485933, 37.48835707247939],\n  [126.87679035662485, 37.48857077467621],\n  [126.87666836775517, 37.488731698886916],\n  [126.87638215065101, 37.488941163019874],\n  [126.87604346872821, 37.489047234427446],\n  [126.8758168244155, 37.48902812727765],\n  [126.87471601721661, 37.48857729667705],\n  [126.87307774871593, 37.48825285739754],\n  [126.87295540342764, 37.48830058919606],\n  [126.87292252364195, 37.48831350594885],\n  [126.87274847564402, 37.488430729714324],\n  [126.87266911360857, 37.48855396331768],\n  [126.87241371655864, 37.48933079643515],\n  [126.8724404006732, 37.48953552394017],\n  [126.87244516069696, 37.48954200832903],\n  [126.87267624752327, 37.48985141040301],\n  [126.87277059260437, 37.4899587830918],\n  [126.87283922397087, 37.490014335559685],\n  [126.87336843875514, 37.490386572775556],\n  [126.87357693309845, 37.490481684306566],\n  [126.87375273922521, 37.490475679558024],\n  [126.8739332173435, 37.490428855349435],\n  [126.87402067996526, 37.490316033035164],\n  [126.87409064170969, 37.49021447138926],\n  [126.87414915935193, 37.48998055733015],\n  [126.87411047167724, 37.48977722861176],\n  [126.87432244123868, 37.48969551876888],\n  [126.874579194672, 37.48976928586574],\n  [126.87465124038863, 37.48989887276378],\n  [126.8748395606655, 37.49074546038744],\n  [126.87453459446087, 37.4910683660723],\n  [126.87437268165921, 37.49122136264536],\n  [126.87413007357357, 37.491337668238565],\n  [126.87395185020526, 37.49130790942899],\n  [126.87225463784726, 37.490602443243475],\n  [126.8715772336594, 37.490209200924156],\n  [126.8715477661439, 37.49008528105915],\n  [126.8715056835045, 37.489998794005494],\n  [126.8713423702246, 37.48983362446104],\n  [126.87125961324185, 37.48977074005055],\n  [126.87108027252202, 37.48968325006296],\n  [126.87094589711485, 37.48963721815786],\n  [126.87065821481464, 37.48956087360729],\n  [126.87042098603534, 37.48953583190591],\n  [126.8703347531482, 37.489544746411035],\n  [126.87022812471946, 37.48959615733273],\n  [126.87017041588264, 37.489665641527296],\n  [126.87013621956086, 37.48971909518386],\n  [126.86987690273945, 37.490296013812674],\n  [126.86936626773986, 37.49201860453653],\n  [126.86936892693092, 37.49271744174477],\n  [126.86937342007144, 37.49277207332995],\n  [126.86950267732847, 37.4943284018372],\n  [126.86758862651642, 37.49481588988599],\n  [126.86761040833085, 37.49468809006178],\n  [126.86619406839125, 37.49253860319941],\n  [126.86451819511568, 37.491203026084236],\n  [126.86302813884646, 37.49087608634184],\n  [126.8628110904766, 37.490828236574096],\n  [126.8627337568918, 37.490804494216775],\n  [126.86258684437263, 37.49075871779353],\n  [126.86238768631601, 37.49067907388865],\n  [126.86217497834068, 37.49057746081463],\n  [126.86208955762203, 37.49053119281495],\n  [126.86195048469753, 37.490447969739456],\n  [126.86183032272321, 37.490367864585316],\n  [126.86176820935094, 37.490326119958326],\n  [126.86162303620054, 37.49020374479842],\n  [126.861481767076, 37.490067301502876],\n  [126.86138442651429, 37.489968076240665],\n  [126.86111908480686, 37.48966817910177],\n  [126.86102581461058, 37.48955826657109],\n  [126.86101648222878, 37.489547269155715],\n  [126.86068610548242, 37.48916817605563],\n  [126.86018595731015, 37.48859544427551],\n  [126.85925601817621, 37.48753114874059],\n  [126.8585450417844, 37.48672107555025],\n  [126.85822542932799, 37.486360288148504],\n  [126.85811613511434, 37.48623739417916],\n  [126.85797103348565, 37.48608461582477],\n  [126.85783430290624, 37.485983086909044],\n  [126.85763012230208, 37.485867113767235],\n  [126.85748360128834, 37.48580696465288],\n  [126.85744917364828, 37.48579453432356],\n  [126.8572904242174, 37.48576055327166],\n  [126.85721499765847, 37.48575426248749],\n  [126.85708640467882, 37.48574368934753],\n  [126.85676809597386, 37.485721054345184],\n  [126.85655188308645, 37.48570671540534],\n  [126.85653942998213, 37.48570626540688],\n  [126.85349858643376, 37.482621834752926],\n  [126.85270046935516, 37.481818616044556],\n  [126.85261962553658, 37.481786686082536],\n  [126.85258624296549, 37.48177876052663],\n  [126.85206621905958, 37.48166492424135],\n  [126.851751885319, 37.48160370879597],\n  [126.85166783206732, 37.48158868170002],\n  [126.85161377601116, 37.481583540277235],\n  [126.85099047011765, 37.481527845736494],\n  [126.85084315206339, 37.48152259001683],\n  [126.85070182006966, 37.48152550894454],\n  [126.85057300416253, 37.48153941821166],\n  [126.85041837279233, 37.481566818390434],\n  [126.8493324275512, 37.48184331552959],\n  [126.84698612892635, 37.48189443252598],\n  [126.8466017682575, 37.48167122516655],\n  [126.8465568595493, 37.4816333299397],\n  [126.84651717642556, 37.48159893200954],\n  [126.84642038315994, 37.48150701100949],\n  [126.84640821872374, 37.481495453254205],\n  [126.84626305142321, 37.48130323673433],\n  [126.84621918975228, 37.481240109255275],\n  [126.84604937012155, 37.480925377389326],\n  [126.84600724781465, 37.480798339922416],\n  [126.84598678074923, 37.480695539848334],\n  [126.845976830455, 37.48064175143651],\n  [126.8459342194971, 37.4803147980667],\n  [126.8454343751056, 37.47446358358097],\n  [126.84536592690858, 37.47385949919483],\n  [126.84536055354526, 37.47381246555105],\n  [126.84503323897347, 37.47355891752733],\n  [126.84419346893853, 37.47401499804191],\n  [126.84395991392603, 37.474482912719694],\n  [126.84311200124603, 37.47486984773489],\n  [126.8430773430002, 37.4748849961075],\n  [126.84295255134103, 37.47492481992884],\n  [126.84278163936918, 37.47497161415357],\n  [126.8384196728936, 37.47536790968778],\n  [126.83825923862072, 37.475374710633375],\n  [126.83610875738073, 37.474675546160455],\n  [126.83461359077172, 37.47487025950699],\n  [126.83305167563579, 37.47724609250874],\n  [126.8320789692601, 37.477567839756105],\n  [126.8317480949545, 37.47765099536511],\n  [126.83080142694622, 37.47742382313479],\n  [126.83045751616729, 37.47733913587807],\n  [126.82962156724689, 37.476895297577286],\n  [126.82955329242202, 37.47662461753497],\n  [126.8295367634638, 37.476595599289965],\n  [126.82920434453489, 37.47619529596771],\n  [126.82818195209177, 37.47603415313276],\n  [126.82787204443441, 37.4759892067256],\n  [126.82410830374371, 37.476352189043084],\n  [126.8195759381957, 37.47635542376256],\n  [126.8194233714162, 37.47632759999209],\n  [126.81930759652315, 37.47628940790379],\n  [126.81929647232971, 37.47628516514027],\n  [126.81914316116901, 37.476205532322275],\n  [126.81832133653424, 37.475298753226035],\n  [126.81766855225743, 37.473238400442405],\n  [126.81465604938846, 37.47476019094166],\n  [126.81472979756464, 37.475057306435204],\n  [126.81496487102987, 37.47586011888277],\n  [126.81522179414826, 37.476251063198596],\n  [126.81529553572993, 37.47636211907153],\n  [126.81641141908979, 37.477536748815474],\n  [126.81706064565485, 37.478024854451064],\n  [126.81721035612148, 37.478135181855585],\n  [126.81732392065949, 37.478139295676314],\n  ...]]\n\n\n\n값이 굉장히 긴 것 같지만 위의 것들에 비하면 새발의 피다… 그래도 체감은 해봐야지(coordinate : 좌표계라는 뜻)\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates']).shape\n\n(1, 1498, 2)\n\n\n\n서울의 다각형을 의미하는 것 같음!(확인해보자)\n\n\nlon, lat = np.array(global_dict['features'][0]['geometry']['coordinates'])[:,:,:].T\n\n\nlon, lat = np.array(global_dict['features'][0]['geometry']['coordinates'])[0,:,:].T\nnp.stack([lat, lon], axis = 1).shape\n\n(1498, 2)\n\n\n\nnp.stack([lat, lon], axis = 1).shape\n\n(1498, 2, 1)\n\n\n\n위도-경도로 저장된 것을 경도-위도로 바꿔줬다.(folium에선 이렇게 인식하니까…)\n\n- 이게 어떤 원리나면…\n\n[0, :, :] 첫번째 원소만 선택 &gt; 멀티 리스트를 깨줌\nT &gt; 전치시켜서 두 개의 리스트로 만듦\nlon, lat에 각자 저장함(순서를 바꾸기 위함)\nnp.stack([lat, lon], axis = 1) &gt; 열로 스택을 쌓아줌(차원을 늘림)\n\n\nm = folium.Map(\n    location = [37.55,127],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Polygon(\n    locations = np.stack([lat,lon],axis=1).tolist(),    ## tolist()는 안해도 됨\n    fill = True\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n일상언어에선 (latitude, longitude) 순서로 표현하는 경우가 다반수이다. 위도, 경도로 나타내지만, 컴퓨터 언어에서 표현의 혼동을 줄이기 위해 \\(x\\), \\(y\\)축에 대응하여 (longitude, latitude)로 표현하기도 한다."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#folium.choropleth를-이용한-시각화",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#folium.choropleth를-이용한-시각화",
    "title": "Folium | 월드맵 시각화",
    "section": "5. folium.Choropleth()를 이용한 시각화",
    "text": "5. folium.Choropleth()를 이용한 시각화\n\nA. folium.Choropleth() 소개\n\n- folium.Choropleth()는 아래와 같은 방식으로 그림을 그린다고 생각하면 편리하다.\n\njson 파일을 바탕으로 폴리곤을 그린다, 폴리곤에 이름을 붙인다.\ndf = [폴리곤의 이름, 통계값(y)]와 같은 형식으로 정리된 데이터프레임을 바탕으로 각 폴리곤에 대응하는 y값을 색깔로 매핑한다.\n\n\n즉, 폴리곤 데이터가 포함된 녀석과 표시될 색생정보가 들어갈 녀석. 데이터프레임이 두 개가 필요함."
  },
  {
    "objectID": "2023_DV/Review/15. Folium과 월드맵.html#b.-polygon-시각화",
    "href": "2023_DV/Review/15. Folium과 월드맵.html#b.-polygon-시각화",
    "title": "Folium | 월드맵 시각화",
    "section": "### B. Polygon 시각화",
    "text": "### B. Polygon 시각화\n# 예제 1 : 전국의 행정구역 시각화(global)\n\nm = folium.Map(\n    location = [36, 128],\n    zoom_start = 7,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = global_dict\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n# 예제 2 : 전국의 행정구역 시각화(local)\n\nm = folium.Map(\n    location = [36, 128],\n    zoom_start = 6,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data = local_dict\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n폴리곤을 그릴 수 있는 딕셔너리 타입의 정보가 들어가고, 그것은 json파일에 온전히 포함되어 있다. 그니까 넣어주기만 하면 구역은 일단 알아서 그려줌…\n\n# 예제 3 : 전국의 행정구역 시각화(덕진구/완산구)\n- 먼저 전주에 해당하는 폴리곤만 추출해보자.(features만 뽑으면 됨)\n\nlocal_jeonju = local_dict.copy()  ## 딕셔너리를 바꿀 때 혹시라도 문제가 생기지 않도록...\n\n\nprint('dictionary formation \\n &gt; '+str(local_jeonju['features'][15]['properties']))\nprint('\\n'+'length of features \\n &gt; '+str(len(local_jeonju['features'])))\n\ndictionary formation \n &gt; {'name': '강서구', 'base_year': '2018', 'name_eng': 'Gangseo-gu', 'code': '11160'}\n\nlength of features \n &gt; 250\n\n\n- 해당 위치에 지역명이 들어가 있는 것을 알 수 있다. 여기서 정확히 하려면 code를 입력해줘야 하겠지만… 일단은 name을 위주로 해보자.\n\n[dic['properties']['name'] for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]\n\n['전주시완산구', '전주시덕진구']\n\n\n\n잘 추출되는 것 같음\n\n\n#[dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]\n\n\n너무길어… 자를래… 하지만 잘 된 것 같음.\n\n- 그래서 이게 무엇이냐?\n\nprint(type(local_dict['features']), type(local_dict['features'][0]))\nprint(type([dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]),\n     type([dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')][0]))\n\n&lt;class 'list'&gt; &lt;class 'dict'&gt;\n&lt;class 'list'&gt; &lt;class 'dict'&gt;\n\n\n- local_dict['features']는 딕셔너리를 원소로 하는 리스트인데, 추출한 리스트는 그것의 부분집합임!\n\n따라서 이 값을 local_dict['features']에 대신 넣어준다면??\n\n\nlocal_jeonju['features'] = [dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start = 11\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이렇게 전주시의 행정구역만 감싼 모습이 보인다.\n\n\nC. (Polygon, Value) 시각화(\\(\\star \\star \\star\\))\n\n# 예제 1 : 덕진구 vs 완산구\n덕진구와 완산구의 전기 사용량이 아래와 같이 정리되었다고 하자.\n\ndf = pd.DataFrame({\n    'key':['전주시덕진구', '전주시완산구'],\n    'elec_use':[20,30]\n})\ndf\n\n\n\n\n\n\n\n\nkey\nelec_use\n\n\n\n\n0\n전주시덕진구\n20\n\n\n1\n전주시완산구\n30\n\n\n\n\n\n\n\n- 그러면…\n\nlocal_jeonju['features'][0]['properties']\n\n{'name': '전주시완산구',\n 'base_year': '2018',\n 'name_eng': 'Jeonjusiwansangu',\n 'code': '35011'}\n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju,   ## json 파일로 폴리곤을 그린다.\n    key_on = 'properties.name',   ## ['properties']['name']의 값으로 폴리곤에 이름을 붙인다.\n    data = df,   ## 이름과 y값의 정보를 가져온다.\n    columns = ['key', 'elec_use']   ## 어떤 열이 (1)이름 (2)y값의 정보인지 알려준다.\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이렇게 아름답게 셰이딩이 된다.\n\n\n이해를 위해 필요한 약간의 직관\n\n코로플레스 맵을 그리기 위해서는 항상 두 개의 데이터(geo_data, data)를 연결해야 한다.\n두 개의 데이터를 연결하기 위해서는 공유 가능한 연결의 매개체(key_on)가 필요하다.\n코로플레스 맵의 연결 매개체는 ‘전주시완산구’, ’전주시덕진구’와 같은 지역명이다.\n\n\n# 예제 2 : 덕진구 vs 완산구 | key_on에 대한 이해를 돕기 위해 만든 억지예제\n덕진구와 완산구 전기 사용량이 입력한 사람에 따라 다른 이름으로 저장이 되어 아래와 같이 정리되었다고 하자.\n\ndf = pd.DataFrame({\n    'key':['전주시덕진구', 'Jeonjusiwansangu'],\n    'elec_use':[20,30]\n})\ndf\n\n\n\n\n\n\n\n\nkey\nelec_use\n\n\n\n\n0\n전주시덕진구\n20\n\n\n1\nJeonjusiwansangu\n30\n\n\n\n\n\n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju,\n    key_on = 'properties.name',   ## 한글로 된 이름만 포함함\n    data = df,\n    columns = ['key', 'elec_use']  ## 순서가 뒤바뀌면 안됨\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n깨졌음.\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,\n    key_on='properties.name_eng',\n    data=df,\n    columns=['key','elec_use']\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이번엔 한글부분이 깨졌음\n\n# 예제 3 : code 열을 새롭게 할당하고 code열로 key_on\n\ndf.assign(code = ['35012','30511'])\n\n\n\n\n\n\n\n\nkey\nelec_use\ncode\n\n\n\n\n0\n전주시덕진구\n20\n35012\n\n\n1\nJeonjusiwansangu\n30\n30511\n\n\n\n\n\n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,\n    key_on='properties.code',   ## code로 폴리곤에 이름을 붙여준다.\n    data=df.assign(code = ['35012','35011']),\n    columns=['code','elec_use']   ## key 값을 code로 바꾼 모습\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n코드는 작성자에 따라 달라질 일이 없기 때문에(오탈자라도 넣지 않는 이상…) 확실하게 가져올 수 있다.\n\n# 예제 4 : 대한민국 인구수 시각화 코로플레스(global)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv')\ndf\n\n\n\n\n\n\n\n\n행정구역(시군구)별\n총인구수 (명)\n\n\n\n\n0\n서울특별시\n9532428\n\n\n1\n부산광역시\n3356311\n\n\n2\n대구광역시\n2390721\n\n\n3\n인천광역시\n2945009\n\n\n4\n광주광역시\n1442454\n\n\n5\n대전광역시\n1454228\n\n\n6\n울산광역시\n1122566\n\n\n7\n세종특별자치시\n368276\n\n\n8\n경기도\n13549577\n\n\n9\n강원도\n1537717\n\n\n10\n충청북도\n1596948\n\n\n11\n충청남도\n2118977\n\n\n12\n전라북도\n1789770\n\n\n13\n전라남도\n1834653\n\n\n14\n경상북도\n2627925\n\n\n15\n경상남도\n3318161\n\n\n16\n제주특별자치도\n676569\n\n\n\n\n\n\n\n\n먹기 좋게 데이터가 저장되어 있다.\n\n\nprint('dictionary formation \\n : '+str(global_dict['features'][0]['properties']))\nprint('\\n length of features \\n : '+str(len(global_dict['features'])))\n\ndictionary formation \n : {'name': '서울특별시', 'base_year': '2018', 'name_eng': 'Seoul', 'code': '11'}\n\n length of features \n : 17\n\n\n\n뭐 바꿀 건 따로 없어보이니 그대로 넣어버리면…\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [36, 128],\n    zoom_start = 6\n)\n\nfolium.Choropleth(\n    geo_data = global_dict,\n    key_on = 'properties.name',  ## 한글이름임\n    data = df,\n    columns = ['행정구역(시군구)별', '총인구수 (명)']\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n깔쌈하게 잘 됐음.\n\n# 예제 5 : 대한민국 인구수 시각화 코로플레스(local)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-muni.csv')\ndf\n\n\n\n\n\n\n\n\n행정구역(시군구)별\n총인구수 (명)\n\n\n\n\n0\n종로구\n145346\n\n\n1\n중구\n122781\n\n\n2\n용산구\n223713\n\n\n3\n성동구\n287174\n\n\n4\n광진구\n340814\n\n\n...\n...\n...\n\n\n269\n함양군\n38475\n\n\n270\n거창군\n61242\n\n\n271\n합천군\n43029\n\n\n272\n제주시\n493225\n\n\n273\n서귀포시\n183344\n\n\n\n\n274 rows × 2 columns\n\n\n\n\n또 예쁘장(?)하게 나온 데이터\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [36, 128],\n    zoom_start = 6\n)\n\nfolium.Choropleth(\n    geo_data = local_dict,\n    key_on = 'properties.name',  ## 한글이름임\n    data = df,\n    columns = ['행정구역(시군구)별', '총인구수 (명)']\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n시꺼멓게 표시된 부분이 있다. 이것이 깨진 부분에 해당함;;;\n\n\n[dic['properties']['name'] for dic in local_dict['features'] if not dic['properties']['name'] in list(df['행정구역(시군구)별'])]\n\n['수원시장안구',\n '수원시권선구',\n '수원시팔달구',\n '수원시영통구',\n '성남시수정구',\n '성남시중원구',\n '성남시분당구',\n '안양시만안구',\n '안양시동안구',\n '안산시상록구',\n '안산시단원구',\n '고양시덕양구',\n '고양시일산동구',\n '고양시일산서구',\n '용인시처인구',\n '용인시기흥구',\n '용인시수지구',\n '청주시상당구',\n '청주시서원구',\n '청주시흥덕구',\n '청주시청원구',\n '천안시동남구',\n '천안시서북구',\n '전주시완산구',\n '전주시덕진구',\n '포항시남구',\n '포항시북구',\n '창원시의창구',\n '창원시성산구',\n '창원시마산합포구',\n '창원시마산회원구',\n '창원시진해구']\n\n\n\n[i for i in df['행정구역(시군구)별'] if '수원' in str(i)]\n\n['수원시']\n\n\n\n애초에 수원시 자체를 구로 나누지 않았으니 없을 수밖에;;; 이런 경우에는… 아직 잘 모르겠다. 합칠 수 있는 방법이 따로 있으려나?"
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html",
    "title": "훌륭한 시각화 2",
    "section": "",
    "text": "훌륭한 시각화란 무엇일까…"
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#presentation-vs-exploration",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#presentation-vs-exploration",
    "title": "훌륭한 시각화 2",
    "section": "1. Presentation vs Exploration",
    "text": "1. Presentation vs Exploration\n\n두 방식의 차이와, 장단점은 무엇일까?\n\n\nA. Presentation\n\n\n- 프리젠테이션 방식의 시각화는 화자가 다듬은 이야기를 전달하기에 좋은 시각화이다. 즉, 잘 정리된 메시지를 전달하기에 좋다."
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#b.-exploration",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#b.-exploration",
    "title": "훌륭한 시각화 2",
    "section": "### B. Exploration",
    "text": "### B. Exploration\n\n\n문학적유기체라는 작품으로 어떤 소설책을 시각화한 것이다.\n수형도와 색깔로 구성되고, 수형도는 단원 \\(\\leftarrow\\) 문단 \\(\\leftarrow\\) 문장 \\(\\leftarrow\\) 단어 순으로 뻗어진다. 색깔은 소설에서 등장하는 소재를 구분(범주형 변수 표현에 적합)한다.\n\n- 익스플로래이션 방식은 독자가 스스로 그림에서 메시지를 찾아내도록 한다. 소설을 읽어보지 않은 사람은 해당 그래픽으로 소설책의 전체 주제를 미리 파악할 수 있고, 읽어본 사람은 분석 및 탐구를 할 수 있을 것이다.\n\nC. 절충\n\n- 카이로 : 사실 프리젠테이션과 익스플로래이션은 절충 가능하다.\n\n## 이 그림을 보면 아래의 코드가 생각나야 함\nfig = ggplot(tidydata)\nline = geom_line(aes(x = '소득', y = '불평등', color = '정부'))\npoint = geom_point(aes(x = '소득', y = '불평등'))\ntext = geom_text(aes(x = '소득', y = '불평등', label = '연도'))\n\nfig + line + point + text\n\n초록색 정부 : 소득이 증가 & 불평등이 훨씬 더 증가\n갈색 정부 : 매우 빠른 경제성장 & 불평등의 해소\n포인트 간의 간격이 조밀하다 = 변화가 더디다 // 포인트 간의 간격이 넓다 = 변화가 빠르다.\n\n- 언뜻 보기에는 익숙한 라인플랏처럼 보이지만, 해석할 만한 정보가 많다.\n\n익스플로레이션 형태의 그래프는 그릴 줄도 알아야 하지만, 남이 그린 그래프를 해석할 수도 있어야 한다."
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#이성적-낙관주의",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#이성적-낙관주의",
    "title": "훌륭한 시각화 2",
    "section": "2. 이성적 낙관주의",
    "text": "2. 이성적 낙관주의"
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#a.-인구문제",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#a.-인구문제",
    "title": "훌륭한 시각화 2",
    "section": "### A. 인구문제",
    "text": "### A. 인구문제\n- 주장 1 : 가난한 나라에서 애를 너무 많이 낳음 \\(\\leftarrow\\) 세계인구가 90억까지 증가할 것이다.\n- 주장 2 : 잘사는 나라에서 애를 적게 낳음 \\(\\leftarrow\\) 극심한 고령화 문제가 발생할 것이다.\n\nB. 리들리의 메시지\n\n둘다 틀렸는뎁쇼???\n- 가난한 나라의 출산율은 점점 감소\n- 잘 사는 사라의 출산율은 점점 증가\n\n따라서 세계의 인구는 안정화되고, 고령화 문제도 오지 않을 것이다."
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#c.-카이로",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#c.-카이로",
    "title": "훌륭한 시각화 2",
    "section": "### C. 카이로",
    "text": "### C. 카이로\n- 리들러의 메시지는 아래의 그림들이 더 잘 전달한다.\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = '연도', y = '인구증가율', color = '국가'))\nfig + line\n\n노르웨이, 영국, 스웨덴, 스페인, 이탈리아의 경우 출산율이 반등함(일본과 독일의 경우는 감소세가 아주 누그러듬)\n인도, 브라질, 중국과 같은 나라는 출산율이 대폭 감소\n\n\nD. 교수님 소감\n\n- 어떠한 현상을 살펴볼 때, 그것의 부분집합들이 역시 그러한 지 살펴보는 것은 기본임.\n- 중요한 선을 제외한 나머지는 일러레를 통해 회색으로 처리한 것이 시각적으로 우수하다.\n- 과학적인 논문작업에 들어갈 그림이라면 임의로 회색처리한 것이 다소 비판을 받을 수 있음."
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#시각화-예시",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#시각화-예시",
    "title": "훌륭한 시각화 2",
    "section": "3. 시각화 예시",
    "text": "3. 시각화 예시"
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#a.-상관관계의-해석",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#a.-상관관계의-해석",
    "title": "훌륭한 시각화 2",
    "section": "### A. 상관관계의 해석",
    "text": "### A. 상관관계의 해석\n\n중졸 여성의 비율과 출산율을 나타냄\n- 해설 : 당신이 더 교육받고 부유할수록, 가질 아이들의 수는 적어집니다.\n\n??? 여중을 다 때려부수면 출산율이 올라가나요?\n\n\nB. 남미 국가들의 국방력\n\n\n- 교수님께선 이를 쓸모없는 그래픽이라고 말씀하셨지…(뭐 기억나는 게 있나요?)\n- 아래가 더 우수한 그림이다.(바 차트)\n\n- 그리고 이게 더 우수한 시각화이다.(분야별 바 차트)\n\n\n분야별 비교가 유용하다. 브라질의 국방력은 모든 지표에서 1등인 것 같다.\n브라질을 하이라이팅한 것도 우아함…(라고 하셨음)\n\n근데 흑막을 제거하면 어떻게 될까.\n\n- 흑막을 제거\n\n\n인구라는 흑막을 제거하고 보니 인구당 군인수도, 국방비 지출도, 군인 당 교육 투자비도 높지 않다.\n즉, 내실이 없다.\n\n- 최종적으로 제안하는 그래프\n\n\n우측하단 : 관심있는 그래프가 아님(지역적 특성을 나타냈을뿐…)\n좌측하단 : 산점도??\n\nfig = ggplot(tidydata)\npoint = geom_point(aes(x = '인구', y = '군인 수', size = '예산'), alpha = 0.5)\npoint2 = geom_point(aes(x = '인구', y = '군인 수'))\nfig + point + point2\n\n교수님 : 사실 저는 아래의 그래프가 좋은 시각화라고 생각 안해요.\n\n- x축과 y축에 비슷한 정보가 들어가있다. 모든 점들이 직선에 몰려있다면 왜 2차원으로 표현해야 할까???\n\n산점도에서 데이터를 한눈에 파악하고 특징을 요약하기 위해서는 \\(x\\)축과 \\(y\\)축을 너무 비슷한 성질의 변수로 설정하지 마라.\n\nfig1 = fig + geom_point(aes(x = '토익', y = '텝스', color = '합격여부', shape = '회사명'))\nfig2 = fig + geom_point(aes(x = '토익', y = 'GPA', color = '합격여부', shape = '회사명'))\nfig2가 더 합리적이다.(토익과 텝스가 유사한 항목이라면…)\n\n\n\nC. 스페인의 실업률\n\n * 명암으로 왜 크기비교를 하는 것인가??? 게다가 명암으로 한 크기비교마저 이상한데???\n\n비교를 위해서는 바플랏이 더 우수하다."
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#d.-은행들의-시가총액",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#d.-은행들의-시가총액",
    "title": "훌륭한 시각화 2",
    "section": "### D. 은행들의 시가총액",
    "text": "### D. 은행들의 시가총액\n- 카이로 교수햄의 강의자료에 등장하는 그림(bubble chart)\n- 회색이 before, 검은색이 after임\n\n\n우리 눈은 부피의 비교를 잘 못하기 때문에, 해당 차트는 결과를 왜곡한다… 버블 차트의 경우 크기를 왜곡시키므로 사용을 지양해야 한다.\n\n\nE. 분열된 유권자들\n\n- 하지만 아래의 버블차트는 우수하다.(왜? 크기비교 자체가 목적이 아니므로)\n\n\n선거 지도의 경우 수치 비교에 별로 관심이 없다.(미국의 경우 간접선거라 그럼…)\n민주당표와 공화당표가 어떤 지역에 몰렸는지 파악하는 것이 중요하므로, aes중 가장 중요한 \\(x\\)와 \\(y\\)를 모두 지역정보를 표현하기 위해 투자함(좌표)"
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#f.-좋은-aes-속성들일반적으로",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#f.-좋은-aes-속성들일반적으로",
    "title": "훌륭한 시각화 2",
    "section": "### F. 좋은 aes 속성들(일반적으로…)",
    "text": "### F. 좋은 aes 속성들(일반적으로…)\n\n아래로 갈수록 뭔가 난잡해지고 있음…\n- 근데 스티븐 잡스는 하위에 해당하는 Volume 방식으로 시각화를 했음.\n\n\n왜???\n\n\n자사의 점유율이 높아보이도록 트릭을 쓴 것이라고 본다."
  },
  {
    "objectID": "2023_DV/Review/13. 훌륭한 시각화.html#시각화의-정석",
    "href": "2023_DV/Review/13. 훌륭한 시각화.html#시각화의-정석",
    "title": "훌륭한 시각화 2",
    "section": "4. 시각화의 정석",
    "text": "4. 시각화의 정석\n그래서, 어떻게 하라는 건데요???\n\n시간 경과에 따른 변화를 보여주고 싶다!(시계열 자료) &gt; 라인플랏\n집단 간 비교를 하고 싶다! &gt; 바플랏\n변수 간 관계를 알고 싶다! &gt; 산점도"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html",
    "href": "2023_DV/Review/11. tidydata 심화실습.html",
    "title": "Tidydata 심화 실습",
    "section": "",
    "text": "Tidydata를 만드는 방법의 모든(?) 것"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html#라이브러리-imports",
    "href": "2023_DV/Review/11. tidydata 심화실습.html#라이브러리-imports",
    "title": "Tidydata 심화 실습",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html#사전학습",
    "href": "2023_DV/Review/11. tidydata 심화실습.html#사전학습",
    "title": "Tidydata 심화 실습",
    "section": "2. 사전학습",
    "text": "2. 사전학습\n\npd.concat()\n\n\ndf1 = pd.DataFrame({'A':[1,2,3],'B':[2,3,4]})\ndf2 = pd.DataFrame({'A':[-1,-2,-3],'B':[-2,-3,-4]})\n\n\ndisplay(\"df1\", df1)\ndisplay(\"df2\", df2)\n\n'df1'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n'df2'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n-1\n-2\n\n\n1\n-2\n-3\n\n\n2\n-3\n-4\n\n\n\n\n\n\n\n\n위 두 개의 데이터프레임을 합치고 싶다면?\n\n\ndisplay(pd.concat([df1, df2], axis = 1))\ndisplay(pd.concat([df1, df2], axis = 0).reset_index(drop = True))\n\n\n\n\n\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n1\n2\n-1\n-2\n\n\n1\n2\n3\n-2\n-3\n\n\n2\n3\n4\n-3\n-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n3\n-1\n-2\n\n\n4\n-2\n-3\n\n\n5\n-3\n-4"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html#df.merge",
    "href": "2023_DV/Review/11. tidydata 심화실습.html#df.merge",
    "title": "Tidydata 심화 실습",
    "section": "### df.merge()",
    "text": "### df.merge()\n- 사이즈가 맞지 않는 두 데이터프레임의 정보를 결합\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'department':['A','B'], 'total':[3,4]})\n\ndisplay('big', big) ## title을 달아주고 아래 산출\ndisplay('small', small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ntotal\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n\ndisplay(big.merge(small))  ## 큰 거를 기준으로 작은거 병합\ndisplay(small.merge(big))  ## 작은거를 기준으로 큰거를 병합\n## 사실 둘 다 비슷하긴 함\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ntotal\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartment\ntotal\ngender\ncount\n\n\n\n\n0\nA\n3\nmale\n1\n\n\n1\nA\n3\nfemale\n2\n\n\n2\nB\n4\nmale\n3\n\n\n3\nB\n4\nfemale\n1\n\n\n\n\n\n\n\n\ndf.applymap()\n\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':np.random.rand(3), 'B':np.random.rand(3)})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.817682\n0.619777\n\n\n1\n0.049532\n0.122541\n\n\n2\n0.838686\n0.117128\n\n\n\n\n\n\n\n\n0.5보다 크면 yes, 0.5보다 작으면 no로 바꾸고 싶다면…\n\n\ndf.applymap(lambda x : 'yes' if x &gt; 0.5 else 'no')\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nyes\nyes\n\n\n1\nno\nno\n\n\n2\nyes\nno"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html#d.-df.astype",
    "href": "2023_DV/Review/11. tidydata 심화실습.html#d.-df.astype",
    "title": "Tidydata 심화 실습",
    "section": "### D. df.astype()",
    "text": "### D. df.astype()\n- 데이터프레임이나 시리즈의 형식을 일괄적으로 변경\n\ndf = pd.DataFrame({'A':[0,1,2],'B':[4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0\n4\n\n\n1\n1\n5\n\n\n2\n2\n6\n\n\n\n\n\n\n\n\ndf.astype(float)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.0\n4.0\n\n\n1\n1.0\n5.0\n\n\n2\n2.0\n6.0\n\n\n\n\n\n\n\n\nE. 데이터프레임 열의 형식\n\n- info()에서의 형식, object는 일괄적으로 문자형이라는 것을 의미하는 게 아님.\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':['1','2','0','1',2], 'B':['2','3','0','0',0]})  ## integer가 포함되어 있다.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   A       5 non-null      object\n 1   B       5 non-null      object\ndtypes: object(2)\nmemory usage: 208.0+ bytes\n\n\n- column의 이름이 이상하게 들어가 있는 경우도 있음.\n\ndf = pd.DataFrame({('A',''):[0,0,0], ('B',''):[1,1,1]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n\n\n\n\n\n\ndf['A']\ndf[('A', '')]\n\n0    0\n1    0\n2    0\nName: (A, ), dtype: int64\n\n\n\n놀랍게도 둘은 같다. 인덱스를 다시 설정해주는 편이 정신건강에 이로움\n\n\ndf.columns ## 쓸모없는 멀티인덱스\n\nMultiIndex([('A', ''),\n            ('B', '')],\n           )"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html#실습-에너지-사용량-시각화",
    "href": "2023_DV/Review/11. tidydata 심화실습.html#실습-에너지-사용량-시각화",
    "title": "Tidydata 심화 실습",
    "section": "3. 실습 : 에너지 사용량 시각화",
    "text": "3. 실습 : 에너지 사용량 시각화\n- 문제\n\npd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv')\n\n\n\n\n\n\n\n\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n종로구\n17,851\n9,204,140\n63,492\n76,653\n799\n\n\n1\n중구\n10,383\n10,078,848\n79,223\n68,210\n497\n\n\n2\n용산구\n17,138\n10,756,612\n51,229\n79,805\n11,128\n\n\n3\n성동구\n13,980\n11,804,313\n59,832\n99,986\n0\n\n\n4\n광진구\n21,556\n12,272,738\n68,756\n123,447\n0\n\n\n5\n동대문구\n21,794\n12,664,554\n65,913\n111,420\n0\n\n\n6\n중랑구\n23,950\n15,182,802\n59,370\n109,284\n7,442\n\n\n7\n성북구\n27,112\n15,938,807\n77,007\n148,376\n0\n\n\n8\n강북구\n23,334\n9,458,987\n47,731\n100,045\n0\n\n\n9\n도봉구\n13,168\n10,644,704\n44,985\n90,379\n5,268\n\n\n10\n노원구\n9,704\n17,197,086\n77,010\n94,340\n50,859\n\n\n11\n은평구\n25,200\n14,735,131\n75,914\n130,159\n14,370\n\n\n12\n서대문구\n17,651\n12,559,425\n65,164\n111,542\n6,330\n\n\n13\n마포구\n18,844\n15,024,186\n92,453\n114,931\n20,148\n\n\n14\n양천구\n14,690\n15,428,339\n70,721\n82,857\n49,258\n\n\n15\n강서구\n20,446\n20,641,866\n86,809\n128,786\n35,896\n\n\n16\n구로구\n17,204\n13,509,894\n59,916\n120,457\n2,963\n\n\n17\n금천구\n12,135\n7,420,441\n34,791\n69,814\n732\n\n\n18\n영등포구\n18,133\n14,914,027\n87,480\n114,238\n13,531\n\n\n19\n동작구\n20,102\n13,612,946\n66,811\n132,285\n899\n\n\n20\n관악구\n26,460\n14,997,859\n85,416\n158,543\n0\n\n\n21\n서초구\n12,856\n21,560,285\n135,491\n121,437\n38,866\n\n\n22\n강남구\n16,129\n29,961,585\n180,121\n149,045\n83,459\n\n\n23\n송파구\n19,331\n26,573,343\n139,117\n143,601\n71,954\n\n\n24\n강동구\n16,636\n15,048,315\n70,341\n121,931\n11,921\n\n\n\n\n\n\n\n에너지 사용량은 2018년부터 2021년까지의 기간 동안 서울, 부산 등 여러 지역에 대해 정리되어 있으며, 아래 주소 형식으로 저장되어 있다.\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2020.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2021.csv\n...\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2018.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2019.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2020.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2021.csv\n\n아래의 url, prov를 참고하여 모든 자료를 불러온 뒤 pd.concat()을 이용하여 하나의 df로 합쳐라.\n\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\n\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\n\n1. 풀이\n\nurl.format('Seoul2018')  ## 이런 식으로 하나하나 리스트로 지정해줘야 함\n\n'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv'\n\n\n\n[url.format(region + str(year)) for year in range(2018,2022) for region in prov]\n\n['https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2021.csv']\n\n\n\ndf = pd.concat([pd.read_csv(url.format(region + str(year))).assign(년도 = year, 시도 = region) for year in range(2018,2022) for region in prov], axis = 0)\n\n\ndf\n\n\n\n\n\n\n\n\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n년도\n시도\n\n\n\n\n0\n종로구\n17,929\n9,141,777\n64,818\n82,015\n111\n2018\nSeoul\n\n\n1\n중구\n10,598\n10,056,233\n81,672\n75,260\n563\n2018\nSeoul\n\n\n2\n용산구\n17,201\n10,639,652\n52,659\n85,220\n12,043\n2018\nSeoul\n\n\n3\n성동구\n14,180\n11,631,770\n60,559\n107,416\n0\n2018\nSeoul\n\n\n4\n광진구\n21,520\n12,054,796\n70,609\n130,308\n0\n2018\nSeoul\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19\n함양군\n12,505\n1,509,149\n6,328\n3,164\n0\n2021\nGyeongsangnam-do\n\n\n20\n거창군\n14,607\n2,322,093\n10,404\n8,850\n0\n2021\nGyeongsangnam-do\n\n\n21\n합천군\n16,039\n1,612,734\n7,587\n0\n0\n2021\nGyeongsangnam-do\n\n\n0\n제주시\n67,053\n20,275,738\n103,217\n25,689\n0\n2021\nJeju-do\n\n\n1\n서귀포시\n35,230\n7,512,206\n37,884\n2,641\n0\n2021\nJeju-do\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n문자열을 pd.read_csv()에 넣어준 후, 컴프리헨션 된 리스트를 행 방향으로 concat했다. 또한 연도와 시도의 정보를 유지시켰다.\n\n\n의미상 숫자형이지만, 문자형으로 입력이 된 자료를 모두 전처리하라.\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1000 entries, 0 to 1\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   지역                1000 non-null   object\n 1   건물동수              1000 non-null   object\n 2   연면적               1000 non-null   object\n 3   에너지사용량(TOE)/전기    1000 non-null   object\n 4   에너지사용량(TOE)/도시가스  1000 non-null   object\n 5   에너지사용량(TOE)/지역난방  1000 non-null   object\n 6   년도                1000 non-null   int64 \n 7   시도                1000 non-null   object\ndtypes: int64(1), object(7)\nmemory usage: 70.3+ KB\n\n\n\n지역, 시도의 경우 문자형으로 입력된 게 맞음. 하지만 나머지는 다 숫자형이 되어야 한다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : x.replace(',',''))\n\nAttributeError: 'int' object has no attribute 'replace'\n\n\n\n문자형이 아닌 숫자형인 녀석이 몇몇 있나보다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   지역                1000 non-null   object\n 1   년도                1000 non-null   int64 \n 2   시도                1000 non-null   object\n 3   건물동수              1000 non-null   int32 \n 4   연면적               1000 non-null   int32 \n 5   에너지사용량(TOE)/전기    1000 non-null   int32 \n 6   에너지사용량(TOE)/도시가스  1000 non-null   int32 \n 7   에너지사용량(TOE)/지역난방  1000 non-null   int32 \ndtypes: int32(5), int64(1), object(2)\nmemory usage: 43.1+ KB\n\n\n\n자료의 형식이 알맞게 설정되었다.\n\n\n년도에는 쉼표가 없으므로 integer로 바꿈(여기선 애초에 숫자형으로 들어가긴 함)\n년도와 시도, 지역을 배제(문자형)\n혹시라도 integer인 녀석들을 string으로 변경 후 문자열 바꾸는 메소드를 통해 ,를 제거, 인덱스 초기화\n열의 이름을 아래와 같이 바꿔라.\n\n\nname_dict = {\n    '년도': 'Year',\n    '시도': 'Prov',\n    '지역': 'Reg',\n    '건물동수': 'BldgCount',\n    '연면적': 'Area',\n    '에너지사용량(TOE)/전기': 'Elec',\n    '에너지사용량(TOE)/도시가스': 'Gas',\n    '에너지사용량(TOE)/지역난방': 'Heat'\n}\n\n\n딕셔너리가 주어졌으므로 그냥 바꾸면 된다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)  ## axis를 꼭 지정해주자.\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n아래와 같은 그림을 시각화하라. \n\n\n가로축이 Year, 세로축이 LogEnergyUse(에너지 사용량에 log를 취한 것)이고, Region으로 면분할했으며, Type으로 라인의 색상을 구분했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n사용해야 할 것은 Prov, Year, Elec Gas Heat.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nlevel_2\n0\n\n\n\n\n0\nSeoul\n2018\nElec\n64818\n\n\n1\nSeoul\n2018\nGas\n82015\n\n\n2\nSeoul\n2018\nHeat\n111\n\n\n3\nSeoul\n2018\nElec\n81672\n\n\n4\nSeoul\n2018\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n2021\nGas\n25689\n\n\n2996\nJeju-do\n2021\nHeat\n0\n\n\n2997\nJeju-do\n2021\nElec\n37884\n\n\n2998\nJeju-do\n2021\nGas\n2641\n\n\n2999\nJeju-do\n2021\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n사용할 두 개의 열을 골라주고, 에너지 관련 세 개 열을 long data로 변환했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nSeoul\n2018\nElec\n64818\n\n\n1\nSeoul\n2018\nGas\n82015\n\n\n2\nSeoul\n2018\nHeat\n111\n\n\n3\nSeoul\n2018\nElec\n81672\n\n\n4\nSeoul\n2018\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n2021\nGas\n25689\n\n\n2996\nJeju-do\n2021\nHeat\n0\n\n\n2997\nJeju-do\n2021\nElec\n37884\n\n\n2998\nJeju-do\n2021\nGas\n2641\n\n\n2999\nJeju-do\n2021\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n이름을 바꾸고…\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n\n\n1\nBusan\n2018\nGas\n708240\n\n\n2\nBusan\n2018\nHeat\n23694\n\n\n3\nBusan\n2019\nElec\n602980\n\n\n4\nBusan\n2019\nGas\n675882\n\n\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n\n\n200\nUlsan\n2020\nHeat\n0\n\n\n201\nUlsan\n2021\nElec\n196412\n\n\n202\nUlsan\n2021\nGas\n312276\n\n\n203\nUlsan\n2021\nHeat\n0\n\n\n\n\n204 rows × 4 columns\n\n\n\n\n지역별로 중복되는 것들을 더했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.assign(LogEnergyUse = lambda _df : _df.EnergyUse.apply(np.log))\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\nLogEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n13.326971\n\n\n1\nBusan\n2018\nGas\n708240\n13.470538\n\n\n2\nBusan\n2018\nHeat\n23694\n10.072977\n\n\n3\nBusan\n2019\nElec\n602980\n13.309639\n\n\n4\nBusan\n2019\nGas\n675882\n13.423774\n\n\n...\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n12.634264\n\n\n200\nUlsan\n2020\nHeat\n0\n-inf\n\n\n201\nUlsan\n2021\nElec\n196412\n12.187970\n\n\n202\nUlsan\n2021\nGas\n312276\n12.651643\n\n\n203\nUlsan\n2021\nHeat\n0\n-inf\n\n\n\n\n204 rows × 5 columns\n\n\n\n\n그리고 로그를 취해준 것을 새로운 열로 할당해줬다. 이정도면 타이디데이터라 할 만 하다.\n\n\ntidydata = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.assign(LogEnergyUse = lambda _df : _df.EnergyUse.apply(np.log))\n\n시각화\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = 'Year', y = 'LogEnergyUse', color = 'Type', linetype = 'Type'))\n\nfig + line + facet_wrap('Prov', scales = 'free')  ## 해당 옵션은 그래프마다 스케일을 따로 적용시킨다.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n뭔가 시각화는 되었는데, 마음에 들지 않는다.\n\n\n## matplotlib로 해당 개체를 이전\nfig = (fig + line + facet_wrap('Prov', scales = 'free')).draw()\nfig\n\n\n\n\n\nfig.set_size_inches(10, 6)\nfig.set_dpi(150)\nfig\n\n\n\n\n\nmatplotlib에서의 메소드를 쉽게 적용시킬 수 있다.\n\n\nProv별로 총 에너지사용량이 많은 상위5개의 Reg을 찾고 아래와 같이 시각화 하라. \n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nReg\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\nSeoul\n64818\n82015\n111\n\n\n1\n중구\nSeoul\n81672\n75260\n563\n\n\n2\n용산구\nSeoul\n52659\n85220\n12043\n\n\n3\n성동구\nSeoul\n60559\n107416\n0\n\n\n4\n광진구\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n거창군\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n합천군\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n제주시\nJeju-do\n103217\n25689\n0\n\n\n999\n서귀포시\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n필요없는 열을 없앤다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nProv\nReg\nType\nEnergyUse\n\n\n\n\n0\nSeoul\n종로구\nElec\n64818\n\n\n1\nSeoul\n종로구\nGas\n82015\n\n\n2\nSeoul\n종로구\nHeat\n111\n\n\n3\nSeoul\n중구\nElec\n81672\n\n\n4\nSeoul\n중구\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n제주시\nGas\n25689\n\n\n2996\nJeju-do\n제주시\nHeat\n0\n\n\n2997\nJeju-do\n서귀포시\nElec\n37884\n\n\n2998\nJeju-do\n서귀포시\nGas\n2641\n\n\n2999\nJeju-do\n서귀포시\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n지역과 구, 타입과 에너지를 표기했다. 이름도 적절히 설정해줬다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Reg'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nReg\nEnergyUse\n\n\n\n\n0\nBusan\n강서구\n200386\n\n\n1\nBusan\n금정구\n451212\n\n\n2\nBusan\n기장군\n287926\n\n\n3\nBusan\n남구\n491030\n\n\n4\nBusan\n동구\n156302\n\n\n...\n...\n...\n...\n\n\n245\nUlsan\n남구\n607820\n\n\n246\nUlsan\n동구\n281094\n\n\n247\nUlsan\n북구\n334844\n\n\n248\nUlsan\n울주군\n394217\n\n\n249\nUlsan\n중구\n395158\n\n\n\n\n250 rows × 3 columns\n\n\n\n\n구역별로 에너지 사용량을 합쳐버렸다.\n\n\ng = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Reg'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.groupby(by = 'Prov')\n\npd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\n\n\n\n\n\n\n\n\nProv\nReg\nEnergyUse\n\n\n\n\n0\nBusan\n부산진구\n690344\n\n\n1\nBusan\n해운대구\n689901\n\n\n2\nBusan\n사하구\n522150\n\n\n3\nBusan\n북구\n493913\n\n\n4\nBusan\n남구\n491030\n\n\n...\n...\n...\n...\n\n\n0\nUlsan\n남구\n607820\n\n\n1\nUlsan\n중구\n395158\n\n\n2\nUlsan\n울주군\n394217\n\n\n3\nUlsan\n북구\n334844\n\n\n4\nUlsan\n동구\n281094\n\n\n\n\n78 rows × 3 columns\n\n\n\n\n구간마다 순위를 정해주기 위해 groupby 함수를 사용, sub-dataframe으로 쪼갠 후에 각각 sort_values() 해주었다.\n\n\npd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\\\n.reset_index().rename({'index' : 'rank'}, axis = 1)\n\n\n\n\n\n\n\n\nrank\nProv\nReg\nEnergyUse\n\n\n\n\n0\n0\nBusan\n부산진구\n690344\n\n\n1\n1\nBusan\n해운대구\n689901\n\n\n2\n2\nBusan\n사하구\n522150\n\n\n3\n3\nBusan\n북구\n493913\n\n\n4\n4\nBusan\n남구\n491030\n\n\n...\n...\n...\n...\n...\n\n\n73\n0\nUlsan\n남구\n607820\n\n\n74\n1\nUlsan\n중구\n395158\n\n\n75\n2\nUlsan\n울주군\n394217\n\n\n76\n3\nUlsan\n북구\n334844\n\n\n77\n4\nUlsan\n동구\n281094\n\n\n\n\n78 rows × 4 columns\n\n\n\n\n인덱스는 랭크와 동일하므로 따로 남겨둔다.\n\n\ntidydata = pd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\\\n.reset_index().rename({'index' : 'rank'}, axis = 1)\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x = 'rank', y = 'EnergyUse', fill = 'Prov'))\n\nfig + bar + facet_wrap('Prov')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n정보는 모두 포함하나, 짜임새가 없으므로 matplotlib로 이전\n\n\nfig = (fig + bar + facet_wrap('Prov')).draw()\n\n\nfig.set_size_inches(12, 6)\nfig.set_dpi(150)\nfig\n\n\n\n\n\n완료\n\n\n(Prov,Year)별 전기에너지 사용량 비율을 구하고 아래와 같이 시각화 하라. + 제주를 제외한 지역으로 한정하고 시각화하라.\n\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n사용해야 할 것\n\nx = ‘Year’, y = ‘ElecRate’, facet_wrap(’Prov”)\nElec, Gas, Heat\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nYear\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n2018\nSeoul\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2021\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n2021\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n2021\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n2021\nJeju-do\n103217\n25689\n0\n\n\n999\n2021\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n필요없는 걸 없애고…\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nYear\nProv\nType\nEnergyUse\n\n\n\n\n0\n2018\nSeoul\nElec\n64818\n\n\n1\n2018\nSeoul\nGas\n82015\n\n\n2\n2018\nSeoul\nHeat\n111\n\n\n3\n2018\nSeoul\nElec\n81672\n\n\n4\n2018\nSeoul\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\n2021\nJeju-do\nGas\n25689\n\n\n2996\n2021\nJeju-do\nHeat\n0\n\n\n2997\n2021\nJeju-do\nElec\n37884\n\n\n2998\n2021\nJeju-do\nGas\n2641\n\n\n2999\n2021\nJeju-do\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\nlong data로 변환했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\n\n\n\n\n\n\n\n\nType\nElec\nGas\nHeat\n\n\nYear\nProv\n\n\n\n\n\n\n\n2018\nBusan\n613522\n708240\n23694\n\n\nChungcheongbuk-do\n361490\n288927\n55002\n\n\nChungcheongnam-do\n456260\n420315\n24286\n\n\nDaegu\n457556\n599115\n77399\n\n\nDaejeon\n309660\n379571\n51341\n\n\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n403399\n4321\n\n\nJeollanam-do\n338032\n281895\n9012\n\n\nSejongsi\n70915\n30533\n61404\n\n\nSeoul\n3486022\n3617731\n546491\n\n\nUlsan\n196412\n312276\n0\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n지역(Prov)과 연도(Year)가 중복되는 값들을 각각 더해줘서 정리했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\n\n\n\n\n\n\n\n\nType\nElec\nGas\nHeat\nEnergyUse\n\n\nYear\nProv\n\n\n\n\n\n\n\n\n2018\nBusan\n613522\n708240\n23694\n1345456\n\n\nChungcheongbuk-do\n361490\n288927\n55002\n705419\n\n\nChungcheongnam-do\n456260\n420315\n24286\n900861\n\n\nDaegu\n457556\n599115\n77399\n1134070\n\n\nDaejeon\n309660\n379571\n51341\n740572\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n403399\n4321\n764778\n\n\nJeollanam-do\n338032\n281895\n9012\n628939\n\n\nSejongsi\n70915\n30533\n61404\n162852\n\n\nSeoul\n3486022\n3617731\n546491\n7650244\n\n\nUlsan\n196412\n312276\n0\n508688\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n총 에너지 사용량 중 전기 에너지만을 구해야 하니 먼저 총 에너지를 구해준다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\n\n\n\n\n\n\n\n\nType\nElec\nEnergyUse\nElecRate\n\n\nYear\nProv\n\n\n\n\n\n\n\n2018\nBusan\n613522\n1345456\n0.455996\n\n\nChungcheongbuk-do\n361490\n705419\n0.512447\n\n\nChungcheongnam-do\n456260\n900861\n0.506471\n\n\nDaegu\n457556\n1134070\n0.403464\n\n\nDaejeon\n309660\n740572\n0.418136\n\n\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n764778\n0.466878\n\n\nJeollanam-do\n338032\n628939\n0.537464\n\n\nSejongsi\n70915\n162852\n0.435457\n\n\nSeoul\n3486022\n7650244\n0.455675\n\n\nUlsan\n196412\n508688\n0.386115\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n필요없는 것을 없애고 비율을 넣어줬다. 이제 필요한 것은 비율 뿐이다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\\\n.drop(['Elec', 'EnergyUse'], axis = 1).reset_index()\n\n\n\n\n\n\n\nType\nYear\nProv\nElecRate\n\n\n\n\n0\n2018\nBusan\n0.455996\n\n\n1\n2018\nChungcheongbuk-do\n0.512447\n\n\n2\n2018\nChungcheongnam-do\n0.506471\n\n\n3\n2018\nDaegu\n0.403464\n\n\n4\n2018\nDaejeon\n0.418136\n\n\n...\n...\n...\n...\n\n\n63\n2021\nJeollabuk-do\n0.466878\n\n\n64\n2021\nJeollanam-do\n0.537464\n\n\n65\n2021\nSejongsi\n0.435457\n\n\n66\n2021\nSeoul\n0.455675\n\n\n67\n2021\nUlsan\n0.386115\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n타이디데이터 같다.\n\n\ntidydata = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\\\n.drop(['Elec', 'EnergyUse'], axis = 1).reset_index()\n\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = 'Year', y = 'ElecRate', color = 'Prov'), linetype = 'dashed')\npoint = geom_point(aes(x = 'Year', y = 'ElecRate', color = 'Prov', shape = 'Prov'))\n\nfig + line + point\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 16 rows containing missing values.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\guides\\guides.py:259: PlotnineWarning: geom_point legend : Removed 4 rows containing missing values.\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n나름 괜찮지만 그래도 matplotlib에 데려와보자.\n\n\nfig_ = (fig + line + point).draw()\nfig_.set_dpi(150)\nfig_\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 16 rows containing missing values.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\guides\\guides.py:259: PlotnineWarning: geom_point legend : Removed 4 rows containing missing values.\n\n\n\n\n\n\n~와 나 너무 잘하는 거 아님?~"
  },
  {
    "objectID": "2023_DV/Review/11. tidydata 심화실습.html#pd.merge의-이용",
    "href": "2023_DV/Review/11. tidydata 심화실습.html#pd.merge의-이용",
    "title": "Tidydata 심화 실습",
    "section": "4. pd.merge()의 이용",
    "text": "4. pd.merge()의 이용\n\n그냥 뇌정지 올 것 같아도 일단 tidydata로 변환하고 시작하자!!\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 6 columns\n\n\n\n\n이런 데이터가 있으면… 일단 value 세 개인 Elec, Gas, Heat를 녹여야 함.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nEnergyUse\n\n\n\n\n0\nBusan\n2018\n1345456\n\n\n1\nBusan\n2019\n1301422\n\n\n2\nBusan\n2020\n1314100\n\n\n3\nBusan\n2021\n1951909\n\n\n4\nChungcheongbuk-do\n2018\n705419\n\n\n...\n...\n...\n...\n\n\n63\nSeoul\n2021\n7650244\n\n\n64\nUlsan\n2018\n512512\n\n\n65\nUlsan\n2019\n491191\n\n\n66\nUlsan\n2020\n500742\n\n\n67\nUlsan\n2021\n508688\n\n\n\n\n68 rows × 3 columns\n\n\n\n\nwidedata로 만들었음. 일단 해!\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n\n\n1\nBusan\n2018\nGas\n708240\n\n\n2\nBusan\n2018\nHeat\n23694\n\n\n3\nBusan\n2019\nElec\n602980\n\n\n4\nBusan\n2019\nGas\n675882\n\n\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n\n\n200\nUlsan\n2020\nHeat\n0\n\n\n201\nUlsan\n2021\nElec\n196412\n\n\n202\nUlsan\n2021\nGas\n312276\n\n\n203\nUlsan\n2021\nHeat\n0\n\n\n\n\n204 rows × 4 columns\n\n\n\n\n큰 데이터와 작은 데이터가 만들어졌다.\n\n\nbig = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\nsmall = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\npd.merge(big, small, on = ['Year', 'Prov']).loc[lambda _df : _df['Type'] == 'Elec'].reset_index(drop = True)\\\n.assign(ElecRate = lambda _df : _df.EnergyUse_x / _df.EnergyUse_y)\\\n.drop(['EnergyUse_x', 'EnergyUse_y'], axis = 1)\n\n\n\n\n\n\n\n\nProv\nYear\nType\nElecRate\n\n\n\n\n0\nBusan\n2018\nElec\n0.455996\n\n\n1\nBusan\n2019\nElec\n0.463324\n\n\n2\nBusan\n2020\nElec\n0.457401\n\n\n3\nBusan\n2021\nElec\n0.534566\n\n\n4\nChungcheongbuk-do\n2018\nElec\n0.512447\n\n\n...\n...\n...\n...\n...\n\n\n63\nSeoul\n2021\nElec\n0.455675\n\n\n64\nUlsan\n2018\nElec\n0.384385\n\n\n65\nUlsan\n2019\nElec\n0.392067\n\n\n66\nUlsan\n2020\nElec\n0.387118\n\n\n67\nUlsan\n2021\nElec\n0.386115\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n타이디데이터가 됐다."
  },
  {
    "objectID": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html",
    "href": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "",
    "text": "파이썬을 이용하여 간단한 그래프를 그려보고, 이미지를 이퀼라이징하는 방법을 알아보도록 하자."
  },
  {
    "objectID": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#사전작업",
    "href": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#사전작업",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 import\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n##--------이퀼라이징을 위한 라이브러리--------\n#!pip install opencv-python\nimport cv2\n\n##--------parameter 설정--------\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (3,2)\nmatplotlib.rcParams['figure.dpi'] = 150 ## 450 : 300\n\n\n오늘 알아볼 함수들\n\nplt.boxplot()      ## 박스플롯 생성\nnp.random.randn()  ## 정규분포 하 확률변수 추출(default : 표준정규분포에서 1개 추출)\nnp.random.seed()   ## 시드 생성\nplt.hist()         ## 히스토그램 생성\ncv2.imread()       ## 이미지를 행렬로 읽어들임\nplt.imshow()       ## 행렬로 저장된 이미지를 시각화\ncv2.equalizeHist() ## 히스토그램 이퀼라이징\n\n!wget link         ## 파일 다운로드(리눅스)"
  },
  {
    "objectID": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#플롯",
    "href": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#플롯",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "2. 플롯",
    "text": "2. 플롯\n\nA. Boxplot\n전북고등학교에는 10명의 학생이 있는 두 개의 학급이 있고, 각 학생들이 받은 점수는 아래와 같다.\n\ny1 = [75,75,76,76,77,77,78,79,79,98]\ny2 = [76,76,77,77,78,78,79,80,80,81]\n\n\ny1_frame = pd.DataFrame(y1)\ny1_frame.describe()\n\n\n  \n    \n\n\n\n\n\n\n0\n\n\n\n\ncount\n10.000000\n\n\nmean\n79.000000\n\n\nstd\n6.831301\n\n\nmin\n75.000000\n\n\n25%\n76.000000\n\n\n50%\n77.000000\n\n\n75%\n78.750000\n\n\nmax\n98.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 1반의 평균은 \\(79\\)\n\ny2_frame = pd.DataFrame(y2)\ny2_frame.describe()\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n10.00000\n\n\nmean\n78.20000\n\n\nstd\n1.75119\n\n\nmin\n76.00000\n\n\n25%\n77.00000\n\n\n50%\n78.00000\n\n\n75%\n79.75000\n\n\nmax\n81.00000\n\n\n\n\n\n\n\n- 2반의 평균은 \\(78.2\\)이다.\n그렇다면 1반(y1)과 2반(y2), 두 반을 지도하는 선생님 중 어떤 선생님이 우수할까?\n\n아마도… : 평균을 중심으로 분석할 시 y1이 더 잘 지도했다고 판단할 수 있다.\n반론 : 평균은 A반이 더 높으나, 편차또한 더 크다. 고득점을 받는 한 학생(outlier)을 제외하면 전체적으로 B반 학생들이 시험을 더 잘 보았다고 해석할 수 있다.\n\n단순한 평균비교보다 학생들이 받은 점수의 분포를 비교하는 것이 중요.\n따라서 그 분포를 알아보기 위해 Boxplot을 그려보자.\n\n\nmatplotlib로 boxplot 그리기\n\nplt.boxplot(y1);  ## 세미콜론을 붙이면 결과값만 출력한다.\n\n\n\n\n\nplt.boxplot(y2);\n\n\n\n\n\nplt.boxplot([y1,y2]); ## 2차원의 리스트를 넣어 여러 개를 동시에 출력시킬 수도 있다.\n\n##np.array([y1, y2]).shape ## &gt; (2, 10)\n\n\n\n\n위처럼 하나의 outlier를 배제한다면, 나머지의 분포는 2반이 더 높게 위치함을 알 수 있다.\n\n박스플롯의 장점 : 단순히 평균만 제공하는 것보다 데이터를 파악하고 직관을 얻기에 유용하다.\n박스플롯이 이용되는 범위 : 초기 자료 분포를 파악하기 용이, 두 개 이상의 방법을 비교\n\n\n\nB. Histogram\n- 중심경향치(평균, 중앙값)만 가지고 집단을 비교할 순 없다.\n이전의 자료도 결과론적으로 중앙값이 더 타당해 보이나, 이것을 근거로 B반이 공부를 더 잘했다는 주장도 비합리적이다.\n\n단순 평균비교로 이러한 질문에 답을 하기 어려움.\n박스플롯으로 전체분포를 파악해도 어떤 반이 공부를 더 잘한다는 기준을 잡기 애매함.\n\nBut!\n특수한 경우에는 두 반 중에 누가 더 공부를 잘하냐는 질문에 명확히 대답할 수 있다.\n정규분포 전북고등학교 : 평균은 좋은 측정값인가?\n\nnp.random.seed(43052)\ny1 = np.random.randn(10000)   ## random.randn, standard normal distribution\ny2 = np.random.randn(10000) + 0.5\n\n- 두 반의 성적은 모두 표준정규분포를 따르는데, 2반의 성적이 일괄적으로 0.5가 높은 상황\n\nnp.mean(y1), np.mean(y2)\n\n(-0.011790879905079434, 0.4979147460611458)\n\n\n\nplt.boxplot([y1,y2]);\n\n\n\n\n\n분포의 모양이 거의 비슷한데, 중앙값(평균)이 2반이 더 높으므로 성적이 더 높다고 말할 수 있다. &gt; 게다가 평균적으로 0.5점 정도 더 공부를 잘한다고 대답할 수 있다!\n\n근데, 위와 같은 경우는 정규분포에서 뽑힌 랜덤샘플이라 분포의 모양이 같다고 하긴 했는데… 실제 데이터를 확인할 때는 박스플롯으로 하긴 어려워보인다.\n따라서!\n히스토그램을 그려 확인해보자\nplt.hist(array, bins = int, range = list)\n\nplt.hist([y1, y2], bins = 100);\n\n\n\n\n\n둘의 분포는 비슷하지만, 2반(주황색)이 조금 더 높은 수준에서 자리함을 알 수 있다."
  },
  {
    "objectID": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#equalization",
    "href": "2023_DV/Review/1. Introduction(그래프, 이미지 이퀼라이징).html#equalization",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "3. Equalization",
    "text": "3. Equalization\n히스토그램이나 이미지를 눈으로 보기 쉽도록 이퀼라이징해보자!\n이미지 자료 다운로드\n\n#!pip install wget\nimport wget\n\nwget.download(\"https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\")\nimg = cv2.imread(\"Unequalized_Hawkes_Bay_NZ.jpg\")\n\n##--------리눅스 환경 충족 시--------\n##!wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\n##img = cv2.imread('https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg')\n##!rm Unequalized_Hawkes_Bay_NZ.jpg\n\n## 파일을 들여오고 인식한 뒤 삭제하는 코드이다.\n\nRequirement already satisfied: wget in c:\\users\\hollyriver\\anaconda3\\envs\\ssk\\lib\\site-packages (3.2)\n100% [............................................................................] 110895 / 110895\n\n\n\nplt.imshow(img) ## image show\n\n&lt;matplotlib.image.AxesImage at 0x184b83fab00&gt;\n\n\n\n\n\n\nplt.imshow()를 통해서 이미지를 가져왔다!\n\n근데, img는 어떤 값으로 저장된 걸까?\n\nA. 사실 이미지는 숫자열이었다!\n\n_img1 = np.array([0,30,90,120,150,180,210,240,255]).reshape(3,-1)  ## 3행 3열로 변경\n_img1\n\narray([[  0,  30,  90],\n       [120, 150, 180],\n       [210, 240, 255]])\n\n\n\nplt.imshow(_img1, cmap = 'gray')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n_img2 = np.array([0,20,40,60,80,100,120,140,160]).reshape(3,3)\n_img2\n\narray([[  0,  20,  40],\n       [ 60,  80, 100],\n       [120, 140, 160]])\n\n\n\nplt.imshow(_img2, cmap = 'gray', vmin = 0, vmax = 255)  ## vmin, vmax를 설정해주지 않으면 가장 큰 값이 max(white)가 된다\nplt.colorbar()\nplt.show()\n\n\n\n\n255에 가까울 수록 하얀색, 0에 가까울 수록 검정색인 이미지로 변환된 것을 볼 수 있다. 숫자만으로 이뤄진 행렬이 이미지가 된 것이다!\n크게, 더 크게 해보자!\n\n_img3 = np.concatenate([_img1,_img2], axis = 1)  ## 열로 병합, default는 행으로 병합\n_img3\n\narray([[  0,  30,  90,   0,  20,  40],\n       [120, 150, 180,  60,  80, 100],\n       [210, 240, 255, 120, 140, 160]])\n\n\n\nplt.imshow(_img3, cmap = 'gray')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\nB. RGB값을 더한 그림 그리기\n\n먼저, RGB값에 해당하는 수를 각각 array로 지정해주자.\n\n\nr = np.array(\n    [[255, 255, 255,  0,   0],\n     [255, 255, 255,  0,   0],\n     [255, 255, 255,  0,   0],\n     [  0,   0,   0,  0,   0],\n     [  0,   0,   0,  0,   0]]\n)\ng = np.array(\n    [[  0,   0, 255, 255, 255],\n     [  0,   0, 255, 255, 255],\n     [  0,   0, 255, 255, 255],\n     [  0,   0,   0,   0,   0],\n     [  0,   0,   0,   0,   0]]\n)\nb = np.array(\n    [[  0,   0,   0,   0,   0],\n     [  0,   0,   0,   0,   0],\n     [255, 255, 255, 255, 255],\n     [255, 255, 255, 255, 255],\n     [255, 255, 255, 255, 255]]\n)\nz = np.array(\n    [[ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0]]\n)\n\n\n그리고 합쳐서 RGB값을 할당해준다.\n\n\nred = np.stack([r,z,z], axis = -1)\ngreen = np.stack([z,g,z], axis = -1)\nblue = np.stack([z,z,b], axis = -1)\n\n\ntemp = np.stack([r,g,b], axis = -1);temp\n\narray([[[255,   0,   0],\n        [255,   0,   0],\n        [255, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[255,   0,   0],\n        [255,   0,   0],\n        [255, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[255,   0, 255],\n        [255,   0, 255],\n        [255, 255, 255],\n        [  0, 255, 255],\n        [  0, 255, 255]],\n\n       [[  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255]],\n\n       [[  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255]]])\n\n\n\n원소 하나 당 3개의 값이 할당된 것을 알 수 있다.\n\n\nnp.stack([], axis = -1) 크기가 동일한 행렬들의 각 원소들을 리스트화 하여 원소로 저장한다.\n\n\nplt.imshow(red+green+blue)\nplt.show()\n\n\n\n\nR, G, B를 같은 비율로 섞으면 다시 흑백이미지가 된다.\n\narr2 = np.array([[10, 40], [80, 60]])\narr2\n\narray([[10, 40],\n       [80, 60]])\n\n\n\narr3 = np.stack([arr2, arr2, arr2], axis = -1)  ## rgb값이 각각 동일\nplt.imshow(arr3)\nplt.show()\n\n\n\n\n\nimg.shape ## 원소의 리스트 수가 3이라는 것으로 rgb가 포함되어 있음을 추측할 수 있음.\n\n(683, 1024, 3)\n\n\n\n\nC. 히스토그램 이퀼라이징\n그래서 이퀼라이징은 뭐냐고!\n\nimg에서 추출해온 행렬로 아래와 같은 히스토그램을 만들어보자.\n\n\nr = img[:, :, 0]  ## 첫 번째 원소\ng = img[:, :, 1]  ## 두 번째 원소\nb = img[:, :, 2]  ## 세 번째 원소\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255])\nplt.show()\n\n\n\n\n\n120~200 사이에 값이 몰려있음\n120~200의 분포된 모양은 그대로 유지하면서 range를 0~255까지 늘린다면?\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\ncv2 라이브러리의 equalizeHist() 사용하면 행렬의 모든 원소들의 분포 정도를 고르게(0~255) 바꾼다.\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='befor');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');  ## cv2.equalizeHist() 사용\nplt.legend()\nplt.show()\n\n\n\n\n그렇다면 이것을 응용하여 위에서의 이미지를 이퀼라이징하면?\n- 이퀼라이징된 각 원소들을 다시 이어붙여 하나의 이미지로 만들어본다.\n\nimg2 = np.stack([rr,gg,bb], axis = -1)  ## axis = -1 &gt; z축(원소 내에서 확장)으로 추가\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(img2)\nplt.show()\n\n\n\n\n\nplt.imshow(np.concatenate([img,img2], axis = 1))\nplt.show()\n\n\n\n\n\n이렇게, 이미지를 조금 더 구별하기 쉽도록 바꿀 수 있다."
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html",
    "href": "2023_DV/Review/10. tidydata, 시각화.html",
    "title": "Tidydata 만들기",
    "section": "",
    "text": "여러가지 방법들을 사용해서 tidydata를 만들어보자!"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#라이브러리-imports",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#라이브러리-imports",
    "title": "Tidydata 만들기",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#pandas---lambda-_df-의-활용",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#pandas---lambda-_df-의-활용",
    "title": "Tidydata 만들기",
    "section": "2. Pandas - lambda _df :의 활용",
    "text": "2. Pandas - lambda _df :의 활용\n\nA. lambda _df : with indexer\n\n- 예시 1 : 아래와 같은 데이터프레임이 있다고 할 때, 표현 1, 2, 3은 모두 같은 문법이다.\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n표현 1\n\ndf[df.A.isna()]  ## 행 슬라이싱, [False, True, False, False]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n표현 2\n\ndf[(lambda _df : _df.A.isna())(df)]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n표현 3\n\ndf[lambda _df : _df.A.isna()]  ## 괄호로 함수를 묶어줘도 되고(그럼 구분이 쉬워진다), 안해도 됨\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n- 예시 2 : .loc, .iloc\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.loc[lambda _df : _df.A.isna()]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\ndf.iloc[lambda _df : list(_df.A.isna())]  ## iloc은 튜플로 입력이 안된다.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\niloc의 경우 범용성이 떨어지긴 한다… 그러니까 왠만해선 .loc을 사용하거나 시리즈를 list로 묶어주자…\n\n근데 왜 이런 문법이 있을까? 연속적으로 DataFrame을 변화시켜야 할 경우 유용하기 때문이다.\n- 예시 3\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.assign(D = df.A + df.B + df.C)  ## 결측치가 있을 경우 합은 NaN이 됨.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\n해당 데이터프레임에서 결측치의 수가 50%가 넘는 열만 고르고 싶다면?\n\n\ndf.assign(D = df.A + df.B + df.C).loc[:, lambda _df : _df.isna().mean() &gt; 0.5]\n\n\n\n\n\n\n\n\nD\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n3\n11.0"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#b.-lambda-df-with-assign",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#b.-lambda-df-with-assign",
    "title": "Tidydata 만들기",
    "section": "### B. lambda df: with assign",
    "text": "### B. lambda df: with assign\n예시 1\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.assign(D = df.A + df.B + df.C)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\n여기에서 결측치의 값을 count하여 새로운 열 E에 할당하고 싶다면?\n\n\ndf.assign(D = df.A + df.B + df.C).assign(E = lambda _df : _df.isna().sum(axis = 1))\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n2\n\n\n1\nNaN\n3.0\n4.0\nNaN\n2\n\n\n2\n1.0\nNaN\n5.0\nNaN\n2\n\n\n3\n1.0\n4.0\n6.0\n11.0\n0\n\n\n\n\n\n\n\n예시 2 : 원본 데이터를 손상시키지 않으며 데이터를 변형하고 싶을 때\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':[12,234,3456,12345,654222]})\ndf\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n\ndf2 = df\ndf2['B'] = np.log(df2.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n???\n\n~이성적으로 이해하기 어려운 결과~ 왜 이렇게 될까? 왜냐면 df2는 df와 같은 녀석을 의미하기 때문이다.(id가 같음)\n\ndf2 = df.copy()\ndf2['B'] = np.log(df2.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n이러면 원본 데이터를 손상시키지 않는다. 또는…\n\n\ndf.assign(B = lambda _df : np.log(df.A)).assign(C = lambda _df : (_df.B - _df.B.mean())/_df.B.std())\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#pandas---multi_index의-이해",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#pandas---multi_index의-이해",
    "title": "Tidydata 만들기",
    "section": "3. Pandas - Multi_index의 이해",
    "text": "3. Pandas - Multi_index의 이해\n\nA. 원래 df, s는 딕셔너리 계열임\n\n- 예시 1 : df는 dict에서 만들 수 있음\n\ndct = {'A': [1,2,3],'B': [2,3,4]}  ## 애초에 데이터프레임에 입력되는 값과 동일...\ndf = pd.DataFrame(dct)\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 예시 2 : s도 dict에서 만들 수 있음.\n\ndct = {'43052': 80, '43053': 90, '43054': 50}   ## key가 index가 된다.\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n- 예시 3 : dict의 키로 올 수 있는 것들\n\n튜플로 dict를 만든다면?\n\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns\n\n43052  4    80\n43053  1    90\n43054  2    50\ndtype: int64\n\n\n\ns.index\n\nMultiIndex([('43052', 4),\n            ('43053', 1),\n            ('43054', 2)],\n           )\n\n\n\n멀쩡하게 잘 작동한다. 그리고 MultiIndex가 나온다."
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#b.-.index혹은-.columns에-name이-있는-경우",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#b.-.index혹은-.columns에-name이-있는-경우",
    "title": "Tidydata 만들기",
    "section": "### B. .index혹은 .columns에 name이 있는 경우",
    "text": "### B. .index혹은 .columns에 name이 있는 경우\n예시 1 : index에 이름이 있는 경우\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns.rename_axis(['id'])  ## set_axis()가 인덱스와 컬럼의 이름을 조정하는 것이라면 이건 그것의 이름을 조정한다.\n\nid\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns.index, s.rename_axis(['id']).index\n\n(Index(['43052', '43053', '43054'], dtype='object'),\n Index(['43052', '43053', '43054'], dtype='object', name='id'))\n\n\n예시 2 : index에 이름이 있는 경우(멀티 인덱스)\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns.rename_axis(['id','year'])\n\nid     year\n43052  4       80\n43053  1       90\n43054  2       50\ndtype: int64\n\n\n\nMultiIndex에서 인덱스의 이름을 각각 지정해준 경우이다.\n\n\n예시 2가 데이터프레임이라면 이렇게 보인다.\n\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ndf = pd.DataFrame(s.rename_axis(['id','year']))  ## index의 이름을 지정해줌\ndf\n\n\n\n\n\n\n\n\n\n0\n\n\nid\nyear\n\n\n\n\n\n43052\n4\n80\n\n\n43053\n1\n90\n\n\n43054\n2\n50\n\n\n\n\n\n\n\n\n만약 여기서 각 원소를 호출하고 싶다면…\n\n\ndf.loc[('43052', 4)]\n\n0    80\nName: (43052, 4), dtype: int64\n\n\n\n그냥 멀티인덱스, 튜플을 입력하여 호출하면 된다."
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#tidydata",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#tidydata",
    "title": "Tidydata 만들기",
    "section": "4. Tidydata",
    "text": "4. Tidydata\n\nA. tidydata의 개념\n\n- 아래의 자료는 불리하다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\n만약 A학과에 해당하는 결과만 뽑고 싶다면? -&gt; department가 column으로 있어야 함…\npass인 사람만 bar plot을 그리고 싶다면? result가 column으로 있어야 함…\n\n원하는 정보를 쉽게 뽑아낼 수 있는 데이터를 tidydata라고 한다.\n\ntidydata = df['pass'].reset_index()\n#---#\nfig = ggplot(tidydata)\ncol = geom_col(aes(x='department',y='pass',fill='gender'),position='dodge')   ## dodge 설정으로 누적으로 표기하지 않고 옆에 늘여서 표시\nfig + col\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#b.-tidydata가-아닌-예시",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#b.-tidydata가-아닌-예시",
    "title": "Tidydata 만들기",
    "section": "### B. tidydata가 아닌 예시",
    "text": "### B. tidydata가 아닌 예시\n\nMultiIndex 구조를 가지면 무조건 tidydata가 아님\n열의 값이 여러 개의 정보를 가지고 있다면 tidydata가 아님\nwide data는 tidydata가 아님 -&gt; melt나 pivot_table등을 활용하여 조정해줘야…\n\n- wide df 예시\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n- 여러 방법을 통해 tidydata로 변환\n\ndf.set_index(['Date']).stack().reset_index().rename({'level_1' : 'Brand', 0 : 'Sales'}, axis = 1)\n\n\n\n\n\n\n\n\nDate\nBrand\nSales\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-10\nApple\n324\n\n\n2\n2019-10\nHuawei\n136\n\n\n3\n2019-10\nXiaomi\n109\n\n\n4\n2019-10\nOppo\n76\n\n\n...\n...\n...\n...\n\n\n203\n2020-10\nNokia\n20\n\n\n204\n2020-10\nLenovo\n22\n\n\n205\n2020-10\nOnePlus\n9\n\n\n206\n2020-10\nSony\n22\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n- melt를 통해 변환\n\ndf.melt(id_vars = ['Date'])  ## 이럼 한번에 되서 편하긴 하다. wide data 한정\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-11\nSamsung\n461\n\n\n2\n2019-12\nSamsung\n426\n\n\n3\n2020-01\nSamsung\n677\n\n\n4\n2020-02\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06\nAsus\n16\n\n\n204\n2020-07\nAsus\n12\n\n\n205\n2020-08\nAsus\n20\n\n\n206\n2020-09\nAsus\n15\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\ndf.melt(id_vars = [])는 index로 지정한 열외의 모든 열들을 한 행에 엮어버린다.\ndf.stack()의 경우 set_index()나 reset_index()등 사용해야 할 게 많을 수 있다. 하지만 직관적이고 사용에 용이하다.\ndf.stack()의 반대로 df.unstack()을 사용하여 인덱스를 컬럼으로 올릴수도 있다."
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#pivot_table-groupby-aggregate",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#pivot_table-groupby-aggregate",
    "title": "Tidydata 만들기",
    "section": "5. pivot_table, groupby + aggregate",
    "text": "5. pivot_table, groupby + aggregate\n\n대부분은 pivot_table로 해결이 된다.\n\n\nA. intro\n\n- tidydata 만드는 개념 : 그룹화 -&gt; 집계\n예제 1 : 아래의 데이터프레임에서 학과, 성별로 count의 합계를 구하라.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\ndf.pivot_table(index = ['department', 'gender'], values = 'count', aggfunc = sum).reset_index()\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n\n인덱스에만 몰아주든 둘다 넣어주든 똑같이 tidydata를 만들어준다.\n\n- 예시에서 본 작업은 아래의 작업들로 세분화할 수 있다.\n\n그룹화(쿼리) : 하나의 DataFrame을 sub-dataframe으로 나누는 과정, 전체 자료를 (학과, 성별)로 묶어 총 12개의 sub-dataframe을 만든다.\n각각집계 : 나눠진 sub-dataframe에서 어떠한 계산을 각각 수행함, 나눠진 sub-dataframe에서 지원자 수의 합계를 각각 구함\n\n- 위와 같은 작업을 하려면 아래와 같은 요소들이 필요하다.\n\n그룹변수~(없는 용어임)~ : 그룹화를 위해 필요한 변수 : DataFrame을 sub-dataframe으로 나누는 역할. &gt;&gt; index and columns &gt; 범주형이거나 범주형으로 바꿀 수 있는 데이터\n집계변수~(이것도 없는 용어임)~ : 집계함수의 대상이 되는 변수 &gt;&gt; values\n집계함수 : 그룹화된 데이터프레임에 수행하는 계산을 정의하는 함수 &gt;&gt; aggfunc"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#b.-pivot_table의-문법",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#b.-pivot_table의-문법",
    "title": "Tidydata 만들기",
    "section": "### B. pivot_table의 문법",
    "text": "### B. pivot_table의 문법\n- pivot_table의 문법\ndf.pivot_table(\n    index = 그룹변수\n    columns = 그룹변수\n    values = 집계변수\n    aggfunc = 집계함수\n)\nindex & columns에 그룹변수를 적절히 나누어 입력한다.\n예시 : 집계함수 전달방법\n\ndf = pd.DataFrame({'category':['A']*5+['B']*5, 'value':np.concatenate([np.random.randn(5), np.random.randn(5)+10])})\ndf\n\n\n\n\n\n\n\n\ncategory\nvalue\n\n\n\n\n0\nA\n0.383420\n\n\n1\nA\n1.084175\n\n\n2\nA\n1.142778\n\n\n3\nA\n0.307894\n\n\n4\nA\n0.237787\n\n\n5\nB\n10.355951\n\n\n6\nB\n8.336925\n\n\n7\nB\n8.617227\n\n\n8\nB\n8.073155\n\n\n9\nB\n8.513784\n\n\n\n\n\n\n\n\ndf.pivot_table(index = 'category', values = 'value', aggfunc = np.sum)\ndf.pivot_table(index = 'category', values = 'value', aggfunc = 'sum')\n## 동일한 코드\n\n\n\n\n\n\n\n\nvalue\n\n\ncategory\n\n\n\n\n\nA\n3.156054\n\n\nB\n43.897041\n\n\n\n\n\n\n\n\ndf.pivot_table(index = 'category', values = 'value', aggfunc = ['sum', 'count'])\n\n\n\n\n\n\n\n\nsum\ncount\n\n\n\nvalue\nvalue\n\n\ncategory\n\n\n\n\n\n\nA\n3.156054\n5\n\n\nB\n43.897041\n5\n\n\n\n\n\n\n\n\n집계함수들의 리스트를 넣는다면 각각의 집계치를 따로 알아서 구해준다.\n\n\nC. groupby + aggregate의 문법\n\n- groupby + aggregate\n\ndf.groupby(그룹변수).aggregate({집계변수:집계함수})\n\n그룹화를 한 후(index, columns), 무엇을 집계할 것인지 dictionary로 지정해준다.(values, aggfunc)\n\n딕셔너리로 지정해주는 것은 pivot_table도 가능하긴 하다…"
  },
  {
    "objectID": "2023_DV/Review/10. tidydata, 시각화.html#연습---airline-data",
    "href": "2023_DV/Review/10. tidydata, 시각화.html#연습---airline-data",
    "title": "Tidydata 만들기",
    "section": "6. 연습 - Airline Data",
    "text": "6. 연습 - Airline Data\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58492 entries, 0 to 58491\nData columns (total 14 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   MONTH      58492 non-null  int64  \n 1   DAY        58492 non-null  int64  \n 2   WEEKDAY    58492 non-null  int64  \n 3   AIRLINE    58492 non-null  object \n 4   ORG_AIR    58492 non-null  object \n 5   DEST_AIR   58492 non-null  object \n 6   SCHED_DEP  58492 non-null  int64  \n 7   DEP_DELAY  57659 non-null  float64\n 8   AIR_TIME   57474 non-null  float64\n 9   DIST       58492 non-null  int64  \n 10  SCHED_ARR  58492 non-null  int64  \n 11  ARR_DELAY  57474 non-null  float64\n 12  DIVERTED   58492 non-null  int64  \n 13  CANCELLED  58492 non-null  int64  \ndtypes: float64(3), int64(8), object(3)\nmemory usage: 6.2+ MB\n\n\n- ChatGPT의 도움을 받아 해당 인포를 정리\n\nMONTH: 비행이 이루어진 월을 나타냄. 1에서 12 사이의 값을 갖음.\nDAY: 비행이 이루어진 일자를 나타냄. 월에 따라 1~28/29/30/31 사이의 값을 1. 가질 수 있음.\nWEEKDAY: 비행이 이루어진 요일을 나타냄. 일반적으로 1(일요일)부터 7(토요일1)까지의 값을 갖음.\nAIRLINE: 해당 항공편을 운영하는 항공사의 약어나 코드를 나타냄.\nORG_AIR: 비행기가 출발하는 공항의 약어나 코드를 나타냄.\nDEST_AIR: 비행기가 도착하는 공항의 약어나 코드를 나타냄.\nSCHED_DEP: 원래의 예정된 출발 시간을 나타냄. 시간은 일반적으로 HHMM 형식으로 표시될 수 있음.\nDEP_DELAY: 출발 지연 시간을 나타냄. 음수 값은 조기 출발, 양수 값은 지연을 의미함.\nAIR_TIME: 실제 공중에서 비행한 시간을 분 단위로 나타냄.\nDIST: 비행 거리를 나타냄. 일반적으로 마일 또는 킬로미터로 표시됨.\nSCHED_ARR: 원래의 예정된 도착 시간을 나타냄. SCHED_DEP와 같은 형식으로 표시될 수 있음.\nARR_DELAY: 도착 지연 시간을 나타냄. 음수는 조기 도착, 양수는 지연을 의미함.\nDIVERTED: 항공편이 다른 곳으로 우회되었는지를 나타냄. 1은 우회, 0은 정상 경로를 의미함.\nCANCELLED: 항공편이 취소되었는지 여부를 나타냄. 1은 취소, 0은 취소되지 않음을 의미함.\n\n# 예제1 : 항공사별로 도착지연시간의 평균을 구하라.\n\ndf.pivot_table(index = 'AIRLINE', values = 'ARR_DELAY', aggfunc = 'mean')\ndf.groupby(by = 'AIRLINE').aggregate({'ARR_DELAY' : 'mean'})\n## 동일한 코드\n\n\n\n\n\n\n\n\nARR_DELAY\n\n\nAIRLINE\n\n\n\n\n\nAA\n5.542661\n\n\nAS\n-0.833333\n\n\nB6\n8.692593\n\n\nDL\n0.339691\n\n\nEV\n7.034580\n\n\nF9\n13.630651\n\n\nHA\n4.972973\n\n\nMQ\n6.860591\n\n\nNK\n18.436070\n\n\nOO\n7.593463\n\n\nUA\n7.765755\n\n\nUS\n1.681105\n\n\nVX\n5.348884\n\n\nWN\n6.397353\n\n\n\n\n\n\n\n# 예제2 : 항공사 별로 비행취소건수의 합계를 구하라. 취소건수가 높은 항공사 순으로 정렬하라.\n\ndf.pivot_table(index = 'AIRLINE', values = 'CANCELLED', aggfunc = 'sum').sort_values('CANCELLED', ascending = False)\ndf.groupby(by = 'AIRLINE').aggregate({'CANCELLED' : 'sum'}).sort_values('CANCELLED', ascending = False)\n## 역시 동일한 코드\n\n\n\n\n\n\n\n\nCANCELLED\n\n\nAIRLINE\n\n\n\n\n\nAA\n154\n\n\nMQ\n152\n\n\nEV\n146\n\n\nOO\n142\n\n\nUA\n93\n\n\nWN\n93\n\n\nDL\n38\n\n\nNK\n25\n\n\nUS\n21\n\n\nF9\n10\n\n\nVX\n6\n\n\nB6\n1\n\n\nAS\n0\n\n\nHA\n0\n\n\n\n\n\n\n\n# 예제3 : 항공사별로 비행취소율을 구하라. 비행취소율이 가장 높은 항공사 순으로 정렬하라\n\ndf.pivot_table(index = 'AIRLINE', values = 'CANCELLED', aggfunc = 'mean').sort_values('CANCELLED', ascending = False)\ndf.groupby(by = 'AIRLINE').aggregate({'CANCELLED' : 'mean'}).sort_values('CANCELLED', ascending = False)\n## 동일한 코드\n\n\n\n\n\n\n\n\nCANCELLED\n\n\nAIRLINE\n\n\n\n\n\nMQ\n0.043791\n\n\nEV\n0.024923\n\n\nOO\n0.021554\n\n\nAA\n0.017303\n\n\nNK\n0.016491\n\n\nUS\n0.013003\n\n\nUA\n0.011935\n\n\nWN\n0.011048\n\n\nF9\n0.007593\n\n\nVX\n0.006042\n\n\nDL\n0.003585\n\n\nB6\n0.001842\n\n\nAS\n0.000000\n\n\nHA\n0.000000\n\n\n\n\n\n\n\n# 예제4 : (항공사, 요일)별 비행취소건수와 비행취소율을 조사하라.\n\ndf.pivot_table(index = ['AIRLINE', 'WEEKDAY'], values = 'CANCELLED', aggfunc = ['sum', 'mean'])\ndf.groupby(by = ['AIRLINE', 'WEEKDAY']).aggregate({'CANCELLED' : ['sum','mean']})\n## 동일\n\n\n\n\n\n\n\n\n\nCANCELLED\n\n\n\n\nsum\nmean\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\nAA\n1\n41\n0.032106\n\n\n2\n9\n0.007341\n\n\n3\n16\n0.011949\n\n\n4\n20\n0.015004\n\n\n5\n18\n0.014151\n\n\n...\n...\n...\n...\n\n\nWN\n3\n18\n0.014118\n\n\n4\n10\n0.007911\n\n\n5\n7\n0.005828\n\n\n6\n10\n0.010132\n\n\n7\n7\n0.006066\n\n\n\n\n98 rows × 2 columns\n\n\n\n# 예제4 : (항공사, 요일)별로 CANCELLED는 평균과 합계를 구하고, AIR_TIME은 평균과 표준편차를 구하여라.\n\ndf.pivot_table(index = ['AIRLINE', 'WEEKDAY'], values = ['CANCELLED', 'AIR_TIME'], aggfunc = {'CANCELLED' : ['sum', 'mean'], 'AIR_TIME' : ['mean', 'std']})\ndf.groupby(by = ['AIRLINE', 'WEEKDAY']).aggregate({'CANCELLED' : ['mean', 'sum'], 'AIR_TIME' : ['mean', 'std']})\n## 거의 유사한 코드임\n\n\n\n\n\n\n\n\n\nCANCELLED\nAIR_TIME\n\n\n\n\nmean\nsum\nmean\nstd\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n41\n147.610569\n73.442540\n\n\n2\n0.007341\n9\n143.851852\n73.211275\n\n\n3\n0.011949\n16\n144.514005\n73.340675\n\n\n4\n0.015004\n20\n141.124618\n69.220840\n\n\n5\n0.014151\n18\n145.430966\n76.711095\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n18\n104.219920\n53.869040\n\n\n4\n0.007911\n10\n107.200800\n54.466218\n\n\n5\n0.005828\n7\n107.893635\n57.172695\n\n\n6\n0.010132\n10\n109.247433\n56.149388\n\n\n7\n0.006066\n7\n107.602273\n56.419207\n\n\n\n\n98 rows × 4 columns\n\n\n\n# 예제5 : 운행구간(거리의 구간)을 그룹화하고, 운행구간 별 비행취소건수와 취소율을 구하여라.\n\npd.qcut(df.DIST, q = 4)  ## 어레이나 시리즈를 넣어주면 해당 시리즈를 분위수로 컷한다.\n\n0          (391.0, 690.0]\n1        (1199.0, 4502.0]\n2          (391.0, 690.0]\n3         (690.0, 1199.0]\n4        (1199.0, 4502.0]\n               ...       \n58487    (1199.0, 4502.0]\n58488      (391.0, 690.0]\n58489     (66.999, 391.0]\n58490     (690.0, 1199.0]\n58491      (391.0, 690.0]\nName: DIST, Length: 58492, dtype: category\nCategories (4, interval[float64, right]): [(66.999, 391.0] &lt; (391.0, 690.0] &lt; (690.0, 1199.0] &lt; (1199.0, 4502.0]]\n\n\n\ndf.assign(DIST_CUT = pd.qcut(df.DIST, q = 4))\\\n.pivot_table(index = 'DIST_CUT', values = 'CANCELLED', aggfunc = ['sum', 'mean'])\n\n\n\n\n\n\n\n\nsum\nmean\n\n\n\nCANCELLED\nCANCELLED\n\n\nDIST_CUT\n\n\n\n\n\n\n(66.999, 391.0]\n334\n0.022659\n\n\n(391.0, 690.0]\n196\n0.013503\n\n\n(690.0, 1199.0]\n203\n0.013637\n\n\n(1199.0, 4502.0]\n148\n0.010313\n\n\n\n\n\n\n\n\n긴 구간일 수록 취소율이 낮은 것 같다."
  },
  {
    "objectID": "2023_DV/Review/12. merge와 concat.html",
    "href": "2023_DV/Review/12. merge와 concat.html",
    "title": "1. 라이브러리 imports",
    "section": "",
    "text": "groupby() 메소드의 심화\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "2023_DV/Review/12. merge와 concat.html#groupby",
    "href": "2023_DV/Review/12. merge와 concat.html#groupby",
    "title": "1. 라이브러리 imports",
    "section": "2. groupby",
    "text": "2. groupby\n\nA. df.groupby\n\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\ng = df.groupby(by = 'department')\ng\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000029243C3B250&gt;\n\n\n\nset(dir(g)) & {'__iter__'} # g는 반복문을 돌리기 유리하게 설계되어 있음\n\n{'__iter__'}\n\n\n\ng에 적용할 수 있는 메소드(dir(g))중에 '__iter__'라는 게 있다. 즉, g는 for문을 돌리려고 만든 오브젝트이다.\n\n\n[i for i in g]  ## list(g)\n\n[('A',\n    department  gender  count\n  0          A    male      1\n  1          A  female      2),\n ('B',\n    department  gender  count\n  2          B    male      3\n  3          B  female      1)]\n\n\n\n두 개의 튜플이 나온다. 튜플의 원소는 그룹화하는 열과 sub_dataframe과 같은 형식이다.\n\n\ndct = {k:df for k, df in g}  ## 딕셔너리 컴프리헨션\ndct\n\n{'A':   department  gender  count\n 0          A    male      1\n 1          A  female      2,\n 'B':   department  gender  count\n 2          B    male      3\n 3          B  female      1}\n\n\n\ndisplay(dct['A'])\ndisplay(dct['B'])\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\n딕셔너리의 원소가 sub-dataframe"
  },
  {
    "objectID": "2023_DV/Review/12. merge와 concat.html#b.-g의-이용법",
    "href": "2023_DV/Review/12. merge와 concat.html#b.-g의-이용법",
    "title": "1. 라이브러리 imports",
    "section": "### B. g의 이용법",
    "text": "### B. g의 이용법\n- g를 이용하여 원래의 df를 복원하라.\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ng = df.groupby('department')\n\n\npd.concat([df for _,df in g])\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\n단순히 묶어주기만 해도 이렇게 나온다.\n\n- g를 이용하여 아래와 동일한 기능을 하는 코드를 작성하라. (agg함수 사용 금지)\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ndf.groupby('department').agg({'count':'sum'})\n\n\n\n\n\n\n\n\ncount\n\n\ndepartment\n\n\n\n\n\nA\n3\n\n\nB\n4\n\n\n\n\n\n\n\n\nlist(g)\n\n[('A',\n    department  gender  count\n  0          A    male      1\n  1          A  female      2),\n ('B',\n    department  gender  count\n  2          B    male      3\n  3          B  female      1)]\n\n\n\npd.DataFrame(pd.Series({i:sum(j['count']) for i, j in g}))  ## 리스트로 먼저 묶어줘야 함.\n##pd.DataFrame(pd.Series({k:df['count'].sum() for k,df in g})) 이게 더 직관적\n\n\n\n\n\n\n\n\n0\n\n\n\n\nA\n3\n\n\nB\n4\n\n\n\n\n\n\n\n- 이 데이터프레임을 class를 기준으로 그룹핑하여 sub-dataframe을 만들고, score가 높은 순서로 정렬하는 코드를 작성하라.\n\ndf = pd.DataFrame({'class':['A']*5+['B']*5, 'id':[0,1,2,3,4]*2, 'score':[60,20,40,60,90,20,30,90,95,95]})\ndf\n\n\n\n\n\n\n\n\nclass\nid\nscore\n\n\n\n\n0\nA\n0\n60\n\n\n1\nA\n1\n20\n\n\n2\nA\n2\n40\n\n\n3\nA\n3\n60\n\n\n4\nA\n4\n90\n\n\n5\nB\n0\n20\n\n\n6\nB\n1\n30\n\n\n7\nB\n2\n90\n\n\n8\nB\n3\n95\n\n\n9\nB\n4\n95\n\n\n\n\n\n\n\n\ng = df.groupby('class')\n\npd.concat([df.sort_values('score', ascending = False) for i, df in g], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nclass\nid\nscore\n\n\n\n\n0\nA\n4\n90\n\n\n1\nA\n0\n60\n\n\n2\nA\n3\n60\n\n\n3\nA\n2\n40\n\n\n4\nA\n1\n20\n\n\n5\nB\n3\n95\n\n\n6\nB\n4\n95\n\n\n7\nB\n2\n90\n\n\n8\nB\n1\n30\n\n\n9\nB\n0\n20"
  },
  {
    "objectID": "2023_DV/Review/12. merge와 concat.html#merge",
    "href": "2023_DV/Review/12. merge와 concat.html#merge",
    "title": "1. 라이브러리 imports",
    "section": "3. merge",
    "text": "3. merge\n\nA. 가장 빈번하게 사용되는 상황\n\n- 예시 : big 데이터프레임에 groupby+agg를 사용하여 small 데이터프레임이 생긴 경우\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'department':['A','B'], 'count_sum':[3,4]})\n## big.groupby('department').aggregate({'count':'sum'}).rename({'count' : 'count_sum'}, axis = 1)\n\n\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ncount_sum\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\ndepartment | gender | count –&gt; big\ndepartment | count_sum –&gt; small\n\ndepartment | gender | count | count_sum\n\n이러한 작업을 하고 싶을 때, pd.merge()를 사용하면 된다.\n\n\npd.merge(big, small)\n## big.merge(small)\n## small.merge(big)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n사실 아래가 정확한 코드이다.\n\npd.merge(big, small, on = 'department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n공통부분인 department에 따라 데이터프레임이 병합됨\n\n\n두 데이터프레임은 IndexLabel에 대하여 서로 다른 정보를 각각 정리한 상황\n두 데티어프레임에서 공통인 열(IndexLabel(을 찾고, 이것을 기준으로 데이터의 정보를 병합한다.\n\n\n\nB. 여러가지 파라메터\n\n# on\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = big.groupby('department').agg({'count':'sum'}).reset_index()\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ncount\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n- 잘못된 코드\n\npd.merge(big, small)  ## department와 count가 겹친다. 이름은 겹치지만 count는 의미가 다름\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n\n\n\n\n\n\n연결고리로 이해\n\n‘A’, 1\n‘A’, 2\n‘B’, 3\n‘B’, 1\n\n‘A’, 3\n‘B’, 4\n두 데이터프레임의 연결고리가 없다. 따라서 아무것도 산출할 수 없다…\n- 제대로 쓴 코드\n\npd.merge(big, small, on = 'department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ncount_y\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n- 열의 이름을 살리면서…\n\npd.merge(big, small.rename({'count' : 'count_sum'}, axis = 1))  ## 공통부분을 없애줌\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n어차피 이름을 바꿔야 하니, 처음부터 양식에 맞게 해주는 게 더 좋을 수 있음…\n\n사실 아래 둘은 같은 코드이다.\n\npd.merge(big,small,on='department')\npd.merge(big,small,left_on='department', right_on='department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ncount_y\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n# left_on, right_on\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'dept':['A','B'], 'count':[3,4]})\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndept\ncount\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n\n공통부분은 count이고, 오히려 다른 부분은 모두 공통되지 않은 상황\n\n\npd.merge(big, small)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ndept\n\n\n\n\n0\nB\nmale\n3\nA\n\n\n\n\n\n\n\n\ncount로 합치면서 지랄이 난다.\n\n- department, dept를 기준으로 병합…\n\npd.merge(big, small, left_on = 'department', right_on = 'dept')\n## 왼쪽 데이터프레임에선 department, 오른쪽 데이터프레임에선 dept를 기준으로 삼음...\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ndept\ncount_y\n\n\n\n\n0\nA\nmale\n1\nA\n3\n\n\n1\nA\nfemale\n2\nA\n3\n\n\n2\nB\nmale\n3\nB\n4\n\n\n3\nB\nfemale\n1\nB\n4\n\n\n\n\n\n\n\n- 더 직관적이고 편하게…\n\npd.merge(big, small.rename({'dept' : 'department', 'count' : 'count_sum'}, axis = 1))\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n강제로 한 열만 이름이 같도록 해서 공통부분을 지정해줬다.(훨씬 편하지요옹?)\n\n# how\n\ndf1 = pd.DataFrame({\n    'dept':['통계','수학','과학','IAB'], \n    'count':[20,30,25,50]\n})\ndf2 = pd.DataFrame({\n    'dept':['통계','수학','과학','신설학과'], \n    'desc':['통계학과는...','수학과는...','과학학과는...','이 학과는 내년에 신설될 예정이고...']\n})\ndisplay(\"df1\",df1)\ndisplay(\"df2\",df2)\n\n'df1'\n\n\n\n\n\n\n\n\n\ndept\ncount\n\n\n\n\n0\n통계\n20\n\n\n1\n수학\n30\n\n\n2\n과학\n25\n\n\n3\nIAB\n50\n\n\n\n\n\n\n\n'df2'\n\n\n\n\n\n\n\n\n\ndept\ndesc\n\n\n\n\n0\n통계\n통계학과는...\n\n\n1\n수학\n수학과는...\n\n\n2\n과학\n과학학과는...\n\n\n3\n신설학과\n이 학과는 내년에 신설될 예정이고...\n\n\n\n\n\n\n\n공통의 열인 dept, 서로 다른 정보인 count와 desc\n\non, left_on, right_on을 사용할 필요가 없다.\n\n\npd.merge(df1, df2)  ## IAB, 신설학과가 미아됨\n\n\n\n\n\n\n\n\ndept\ncount\ndesc\n\n\n\n\n0\n통계\n20\n통계학과는...\n\n\n1\n수학\n30\n수학과는...\n\n\n2\n과학\n25\n과학학과는...\n\n\n\n\n\n\n\n\n근데 겹치지 않는 것이 없어졌음…\n\n겹치지 않는 자료를 처리하는 방식은 4가지 경우로 나누어진다.\n\n#pd.merge(df1, df2, how = 'inner')  ## 보수적으로, 자료가 없으면 없앰, default\n#pd.merge(df1, df2, how = 'left')  ## 왼쪽 거 기준으로(첫 번째)\n#pd.merge(df1, df2, how = 'right')  ## 오른쪽 거 기준으로(두 번째)\npd.merge(df1, df2, how = 'outer')  ## 개방적으로, 자료를 전부 보전\n\n\n\n\n\n\n\n\ndept\ncount\ndesc\n\n\n\n\n0\n통계\n20.0\n통계학과는...\n\n\n1\n수학\n30.0\n수학과는...\n\n\n2\n과학\n25.0\n과학학과는...\n\n\n3\nIAB\n50.0\nNaN\n\n\n4\n신설학과\nNaN\n이 학과는 내년에 신설될 예정이고..."
  },
  {
    "objectID": "2023_DV/Review/12. merge와 concat.html#concat-merge를-이용한-데이터-병합",
    "href": "2023_DV/Review/12. merge와 concat.html#concat-merge를-이용한-데이터-병합",
    "title": "1. 라이브러리 imports",
    "section": "4. concat, merge를 이용한 데이터 병합",
    "text": "4. concat, merge를 이용한 데이터 병합\n\ndf_course2023 = pd.DataFrame({\n    'name':['최규빈']*3+['최혜미']*2+['이영미']+['양성준'],\n    'year':[2023]*7,\n    'course':['파이썬프로그래밍', '데이터시각화', '기계학습활용','수리통계1', '수리통계2','회귀분석1','통계수학']})\ndf_course2023\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n\n\n1\n최규빈\n2023\n데이터시각화\n\n\n2\n최규빈\n2023\n기계학습활용\n\n\n3\n최혜미\n2023\n수리통계1\n\n\n4\n최혜미\n2023\n수리통계2\n\n\n5\n이영미\n2023\n회귀분석1\n\n\n6\n양성준\n2023\n통계수학\n\n\n\n\n\n\n\n\ndf_course2024 = pd.DataFrame({\n    'name':['최규빈','이영미','이영미','양성준','최혜미'],\n    'year':[2024]*5,\n    'course':['기계학습활용','수리통계1', '수리통계2','회귀분석1','통계수학']})\ndf_course2024\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2024\n기계학습활용\n\n\n1\n이영미\n2024\n수리통계1\n\n\n2\n이영미\n2024\n수리통계2\n\n\n3\n양성준\n2024\n회귀분석1\n\n\n4\n최혜미\n2024\n통계수학\n\n\n\n\n\n\n\n\ndf_score = pd.DataFrame({\n    'name':['최규빈','최규빈','이영미','이영미','양성준','양성준','최혜미','최혜미'],\n    'year':[2023,2024]*4,\n    'score':[1, 1.2, 5,5,5,5,5,5]})\ndf_score\n\n\n\n\n\n\n\n\nname\nyear\nscore\n\n\n\n\n0\n최규빈\n2023\n1.0\n\n\n1\n최규빈\n2024\n1.2\n\n\n2\n이영미\n2023\n5.0\n\n\n3\n이영미\n2024\n5.0\n\n\n4\n양성준\n2023\n5.0\n\n\n5\n양성준\n2024\n5.0\n\n\n6\n최혜미\n2023\n5.0\n\n\n7\n최혜미\n2024\n5.0\n\n\n\n\n\n\n\n\ndf_sex = pd.DataFrame({'name':['최규빈','이영미','양성준','최혜미'],\n                        'sex':['male','female','male','female']})\ndf_sex\n\n\n\n\n\n\n\n\nname\nsex\n\n\n\n\n0\n최규빈\nmale\n\n\n1\n이영미\nfemale\n\n\n2\n양성준\nmale\n\n\n3\n최혜미\nfemale\n\n\n\n\n\n\n\n주어진 정보를 바탕으로, 4개의 데이터프레임을 결합하라.\n- 풀이\ndf_course2023  ## 7개 강의목록\ndf_course2024  ## 5개 강의목록\ndf_score  ## 점수\ndf_sex  ## 교수님 성별\n\n위 두개는 열이 똑같다. (concat)\n\n\n아래 두 개는 다른 정보이다. (merge)\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n\n\n1\n최규빈\n2023\n데이터시각화\n\n\n2\n최규빈\n2023\n기계학습활용\n\n\n3\n최혜미\n2023\n수리통계1\n\n\n4\n최혜미\n2023\n수리통계2\n\n\n5\n이영미\n2023\n회귀분석1\n\n\n6\n양성준\n2023\n통계수학\n\n\n7\n최규빈\n2024\n기계학습활용\n\n\n8\n이영미\n2024\n수리통계1\n\n\n9\n이영미\n2024\n수리통계2\n\n\n10\n양성준\n2024\n회귀분석1\n\n\n11\n최혜미\n2024\n통계수학\n\n\n\n\n\n\n\n\n단순히 두 개의 데이터프레임을 합쳤다.\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True).merge(df_score)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\nscore\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n1.0\n\n\n1\n최규빈\n2023\n데이터시각화\n1.0\n\n\n2\n최규빈\n2023\n기계학습활용\n1.0\n\n\n3\n최혜미\n2023\n수리통계1\n5.0\n\n\n4\n최혜미\n2023\n수리통계2\n5.0\n\n\n5\n이영미\n2023\n회귀분석1\n5.0\n\n\n6\n양성준\n2023\n통계수학\n5.0\n\n\n7\n최규빈\n2024\n기계학습활용\n1.2\n\n\n8\n이영미\n2024\n수리통계1\n5.0\n\n\n9\n이영미\n2024\n수리통계2\n5.0\n\n\n10\n양성준\n2024\n회귀분석1\n5.0\n\n\n11\n최혜미\n2024\n통계수학\n5.0\n\n\n\n\n\n\n\n\n연도별로 점수를 넣어줬다. name과 year만 겹치므로 알아서 합쳐진다.\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True).merge(df_score).merge(df_sex)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\nscore\nsex\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n1.0\nmale\n\n\n1\n최규빈\n2023\n데이터시각화\n1.0\nmale\n\n\n2\n최규빈\n2023\n기계학습활용\n1.0\nmale\n\n\n3\n최규빈\n2024\n기계학습활용\n1.2\nmale\n\n\n4\n최혜미\n2023\n수리통계1\n5.0\nfemale\n\n\n5\n최혜미\n2023\n수리통계2\n5.0\nfemale\n\n\n6\n최혜미\n2024\n통계수학\n5.0\nfemale\n\n\n7\n이영미\n2023\n회귀분석1\n5.0\nfemale\n\n\n8\n이영미\n2024\n수리통계1\n5.0\nfemale\n\n\n9\n이영미\n2024\n수리통계2\n5.0\nfemale\n\n\n10\n양성준\n2023\n통계수학\n5.0\nmale\n\n\n11\n양성준\n2024\n회귀분석1\n5.0\nmale\n\n\n\n\n\n\n\n\nname이 겹치므로 알아서 합쳐진다."
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#출산률-시각화",
    "href": "2023_DV/Review/14. Plotly 시작.html#출산률-시각화",
    "title": "RiverFlow",
    "section": "3. 출산률 시각화",
    "text": "3. 출산률 시각화\n\nA. 크롤링 + 데이터 정리\n\n- 대한민국의 저출산 문제\n\nref : https://ko.wikipedia.org/wiki/대한민국의_저출산\n- 위의 url에서 3, 5번째 테이블만 읽고 싶다면 어떻게 해야 할까???\n\n3번째 테이블 : 시도별 출산률\n5번째 테이블 : 시도별 출생아 수\n\n1 데이터 긁어오기\n\ndf_lst = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%A0%80%EC%B6%9C%EC%82%B0')\n\n\n예, ㅈㄴ 단순합니다. 일단 판다스에서 자체적으로 html을 긁어올 수 있어요.\n\n\nlen(df_lst)  ## 22개의 테뷸러 데이터\n\n22\n\n\n\ndf_lst[2]  ## 시도별 합계출산률\n\n\n\n\n\n\n\n\n지역/연도[6]\n2005\n2006[7]\n2007\n2008[8]\n2009[9]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n0.92\n0.97\n1.06\n1.01\n0.96\n1.02\n1.01\n1.06\n0.97\n0.98\n1.00\n0.94\n0.84\n0.76\n0.72\n0.64\n0.63\n\n\n1\n부산\n0.88\n0.91\n1.02\n0.98\n0.94\n1.05\n1.08\n1.14\n1.05\n1.09\n1.14\n1.10\n0.98\n0.90\n0.83\n0.75\n0.73\n\n\n2\n대구\n0.99\n1.00\n1.13\n1.07\n1.03\n1.11\n1.15\n1.22\n1.13\n1.17\n1.22\n1.19\n1.07\n0.99\n0.93\n0.81\n0.78\n\n\n3\n인천\n1.07\n1.11\n1.25\n1.19\n1.14\n1.21\n1.23\n1.30\n1.20\n1.21\n1.22\n1.14\n1.01\n1.01\n0.94\n0.83\n0.78\n\n\n4\n광주\n1.10\n1.14\n1.26\n1.20\n1.14\n1.22\n1.23\n1.30\n1.17\n1.20\n1.21\n1.17\n1.05\n0.97\n0.91\n0.81\n0.90\n\n\n5\n대전\n1.10\n1.15\n1.27\n1.22\n1.16\n1.21\n1.26\n1.32\n1.23\n1.25\n1.28\n1.19\n1.08\n0.95\n0.88\n0.81\n0.81\n\n\n6\n울산\n1.18\n1.24\n1.40\n1.34\n1.31\n1.37\n1.39\n1.48\n1.39\n1.44\n1.49\n1.42\n1.26\n1.13\n1.08\n0.99\n0.94\n\n\n7\n세종\n-\n-\n-\n-\n-\n-\n-\n1.60\n1.44\n1.35\n1.89\n1.82\n1.67\n1.57\n1.47\n1.28\n1.28\n\n\n8\n경기\n1.17\n1.23\n1.35\n1.29\n1.23\n1.31\n1.31\n1.36\n1.23\n1.24\n1.27\n1.19\n1.07\n1.00\n0.94\n0.88\n0.85\n\n\n9\n강원\n1.18\n1.19\n1.35\n1.25\n1.25\n1.31\n1.34\n1.37\n1.25\n1.25\n1.31\n1.24\n1.12\n1.07\n1.08\n1.04\n0.98\n\n\n10\n충북\n1.19\n1.22\n1.39\n1.32\n1.32\n1.40\n1.43\n1.49\n1.37\n1.36\n1.41\n1.36\n1.24\n1.17\n1.05\n0.98\n0.95\n\n\n11\n충남\n1.26\n1.35\n1.50\n1.44\n1.41\n1.48\n1.50\n1.57\n1.44\n1.42\n1.48\n1.40\n1.28\n1.19\n1.11\n1.03\n0.96\n\n\n12\n전북\n1.17\n1.20\n1.37\n1.31\n1.28\n1.37\n1.41\n1.44\n1.32\n1.33\n1.35\n1.25\n1.15\n1.04\n0.97\n0.91\n0.85\n\n\n13\n전남\n1.28\n1.33\n1.53\n1.45\n1.45\n1.54\n1.57\n1.64\n1.52\n1.50\n1.55\n1.47\n1.33\n1.24\n1.23\n1.15\n1.02\n\n\n14\n경북\n1.17\n1.20\n1.36\n1.31\n1.27\n1.38\n1.43\n1.49\n1.38\n1.41\n1.46\n1.40\n1.26\n1.17\n1.09\n1.00\n0.97\n\n\n15\n경남\n1.18\n1.25\n1.43\n1.37\n1.32\n1.41\n1.45\n1.50\n1.37\n1.41\n1.44\n1.36\n1.23\n1.12\n1.05\n0.95\n0.90\n\n\n16\n제주\n1.30\n1.36\n1.48\n1.39\n1.38\n1.46\n1.49\n1.60\n1.43\n1.48\n1.48\n1.43\n1.31\n1.22\n1.15\n1.02\n0.95\n\n\n17\n전국\n1.08\n1.13\n1.25\n1.19\n1.15\n1.23\n1.24\n1.30\n1.19\n1.21\n1.24\n1.17\n1.05\n0.98\n0.92\n0.84\n0.81\n\n\n\n\n\n\n\n\ndf_lst[4]  ## 시도별 출생아 수\n\n\n\n\n\n\n\n\n지역/연도[6]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266\n91526\n93914.000\n84066.000\n83711.000\n83005\n75.536\n65389\n58074\n53.673\n47400\n45531\n\n\n1\n부산\n27415\n27759\n28673.000\n25831.000\n26190.000\n26645\n24906.000\n21480\n19152\n17049.000\n15100\n14446\n\n\n2\n대구\n20557\n20758\n21472.000\n19340.000\n19361.000\n19438\n18298.000\n15946\n14400\n13233.000\n11200\n10661\n\n\n3\n인천\n25752\n20758\n21472.000\n25560.000\n25786.000\n25491\n23609.000\n20445\n20087\n18522.000\n16000\n14947\n\n\n4\n광주\n13979\n13916\n14392.000\n12729.000\n12729.000\n12441\n11580.000\n10120\n9105\n8364.000\n7300\n7956\n\n\n5\n대전\n14314\n14808\n15279.000\n14099.000\n13962.000\n13774\n12436.000\n10851\n9337\n8410.000\n7500\n7414\n\n\n6\n울산\n11432\n11542\n12160.000\n11330.000\n11556.000\n11732\n10910.000\n9381\n8149\n7539.000\n6600\n6127\n\n\n7\n세종\n-\n-\n1054.000\n1111.000\n1344.000\n2708\n3297.000\n3504\n3703\n3819.000\n3500\n3570\n\n\n8\n경기\n121753\n122027\n124746.000\n112129.000\n112.169\n113495\n105643.000\n94088\n83198\n83.198\n77800\n76139\n\n\n9\n강원\n12477\n12408\n12426.000\n10980.000\n10662.000\n10929\n10058.000\n9958\n8351\n8283.000\n7800\n7357\n\n\n10\n충북\n14670\n14804\n15139.000\n13658.000\n13366.000\n13563\n12742.000\n11394\n10586\n9333.000\n8600\n8190\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604\n17302.000\n15670\n14380\n13228.000\n11900\n10984\n\n\n12\n전북\n16100\n16175\n16238.000\n14555.000\n14231.000\n14087\n12698.000\n11348\n10001\n8971.000\n8200\n7745\n\n\n13\n전남\n16654\n16612\n16990.000\n15401.000\n14817.000\n15061\n13980.000\n12354\n11238\n10832.000\n9700\n8430\n\n\n14\n경북\n23700\n24250\n24635.000\n22206.000\n22062.000\n22310\n20616.000\n17957\n16079\n14472.000\n12900\n12045\n\n\n15\n경남\n32203\n32536\n33211.000\n29504.000\n29763.000\n29537\n27138.000\n23849\n21224\n19250.000\n16800\n15562\n\n\n16\n제주\n5657\n5628\n5992.000\n5328.000\n5526.000\n5600\n5494.000\n5037\n4781\n4500.000\n4000\n3728\n\n\n17\n전국\n470171\n471265\n484550.000\n436455.000\n435435.000\n438420\n406243.000\n357771\n326822\n302676.000\n272400\n260562\n\n\n\n\n\n\n\n\n세종시의 경우 중간에 지역이 추가되어 결측치가 -로 들어가 있음\n\n2 데이터 처리\n\ndf = df_lst[4]\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18 entries, 0 to 17\nData columns (total 13 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   지역/연도[6]  18 non-null     object \n 1   2010      18 non-null     object \n 2   2011      18 non-null     object \n 3   2012      18 non-null     float64\n 4   2013      18 non-null     float64\n 5   2014      18 non-null     float64\n 6   2015      18 non-null     int64  \n 7   2016      18 non-null     float64\n 8   2017      18 non-null     int64  \n 9   2018      18 non-null     int64  \n 10  2019      18 non-null     float64\n 11  2020      18 non-null     int64  \n 12  2021      18 non-null     int64  \ndtypes: float64(5), int64(5), object(3)\nmemory usage: 2.0+ KB\n\n\n\n출생아 수이니까 지역빼고 전부 다 int64여야 할텐데, 1-2는 object임\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()  ## 어? 열 이름 영어로 안바꿔요???\n## applymap이 map으로 명령어가 바뀌었네... 근데 일단 이거 써야지 뭐.\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\615695314.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n지역\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266.000\n91526.000\n93914.000\n84066.000\n83711.000\n83005.0\n75.536\n65389.0\n58074.0\n53.673\n47400.0\n45531.0\n\n\n1\n부산\n27415.000\n27759.000\n28673.000\n25831.000\n26190.000\n26645.0\n24906.000\n21480.0\n19152.0\n17049.000\n15100.0\n14446.0\n\n\n2\n대구\n20557.000\n20758.000\n21472.000\n19340.000\n19361.000\n19438.0\n18298.000\n15946.0\n14400.0\n13233.000\n11200.0\n10661.0\n\n\n3\n인천\n25752.000\n20758.000\n21472.000\n25560.000\n25786.000\n25491.0\n23609.000\n20445.0\n20087.0\n18522.000\n16000.0\n14947.0\n\n\n4\n광주\n13979.000\n13916.000\n14392.000\n12729.000\n12729.000\n12441.0\n11580.000\n10120.0\n9105.0\n8364.000\n7300.0\n7956.0\n\n\n5\n대전\n14314.000\n14808.000\n15279.000\n14099.000\n13962.000\n13774.0\n12436.000\n10851.0\n9337.0\n8410.000\n7500.0\n7414.0\n\n\n6\n울산\n11432.000\n11542.000\n12160.000\n11330.000\n11556.000\n11732.0\n10910.000\n9381.0\n8149.0\n7539.000\n6600.0\n6127.0\n\n\n7\n세종\n0.000\n0.000\n1054.000\n1111.000\n1344.000\n2708.0\n3297.000\n3504.0\n3703.0\n3819.000\n3500.0\n3570.0\n\n\n8\n경기\n121753.000\n122027.000\n124746.000\n112129.000\n112.169\n113495.0\n105643.000\n94088.0\n83198.0\n83.198\n77800.0\n76139.0\n\n\n9\n강원\n12477.000\n12408.000\n12426.000\n10980.000\n10662.000\n10929.0\n10058.000\n9958.0\n8351.0\n8283.000\n7800.0\n7357.0\n\n\n10\n충북\n14670.000\n14804.000\n15139.000\n13658.000\n13366.000\n13563.0\n12742.000\n11394.0\n10586.0\n9333.000\n8600.0\n8190.0\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604.0\n17302.000\n15670.0\n14380.0\n13228.000\n11900.0\n10984.0\n\n\n12\n전북\n16100.000\n16175.000\n16238.000\n14555.000\n14231.000\n14087.0\n12698.000\n11348.0\n10001.0\n8971.000\n8200.0\n7745.0\n\n\n13\n전남\n16654.000\n16612.000\n16990.000\n15401.000\n14817.000\n15061.0\n13980.000\n12354.0\n11238.0\n10832.000\n9700.0\n8430.0\n\n\n14\n경북\n23700.000\n24250.000\n24635.000\n22206.000\n22062.000\n22310.0\n20616.000\n17957.0\n16079.0\n14472.000\n12900.0\n12045.0\n\n\n15\n경남\n32203.000\n32536.000\n33211.000\n29504.000\n29763.000\n29537.0\n27138.000\n23849.0\n21224.0\n19250.000\n16800.0\n15562.0\n\n\n16\n제주\n5657.000\n5628.000\n5992.000\n5328.000\n5526.000\n5600.0\n5494.000\n5037.0\n4781.0\n4500.000\n4000.0\n3728.0\n\n\n17\n전국\n470171.000\n471265.000\n484550.000\n436455.000\n435435.000\n438420.0\n406243.000\n357771.0\n326822.0\n302676.000\n272400.0\n260562.0\n\n\n\n\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18 entries, 0 to 17\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   지역      18 non-null     object \n 1   2010    18 non-null     float64\n 2   2011    18 non-null     float64\n 3   2012    18 non-null     float64\n 4   2013    18 non-null     float64\n 5   2014    18 non-null     float64\n 6   2015    18 non-null     float64\n 7   2016    18 non-null     float64\n 8   2017    18 non-null     float64\n 9   2018    18 non-null     float64\n 10  2019    18 non-null     float64\n 11  2020    18 non-null     float64\n 12  2021    18 non-null     float64\ndtypes: float64(12), object(1)\nmemory usage: 2.0+ KB\n\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\1650738614.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n지역만 object로 바뀌고 나머지는 float(???)으로 잘 들어간 모습."
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#b.-시각화-1-전국-출생아-수의-시각화",
    "href": "2023_DV/Review/14. Plotly 시작.html#b.-시각화-1-전국-출생아-수의-시각화",
    "title": "RiverFlow",
    "section": "### B. 시각화 1 : 전국 출생아 수의 시각화",
    "text": "### B. 시각화 1 : 전국 출생아 수의 시각화\n- 전국으로 따로 집계가 되어있긴 하지만, 실습을 위해(?) 따로 집계를 해서 산출해보도록 하자.\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\2390378994.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n지역\n연도\n출생아 수\n\n\n\n\n0\n서울\n2010\n93266.0\n\n\n1\n서울\n2011\n91526.0\n\n\n2\n서울\n2012\n93914.0\n\n\n3\n서울\n2013\n84066.0\n\n\n4\n서울\n2014\n83711.0\n\n\n...\n...\n...\n...\n\n\n199\n제주\n2017\n5037.0\n\n\n200\n제주\n2018\n4781.0\n\n\n201\n제주\n2019\n4500.0\n\n\n202\n제주\n2020\n4000.0\n\n\n203\n제주\n2021\n3728.0\n\n\n\n\n204 rows × 3 columns\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby(by = '연도').aggregate({'출생아 수' : 'sum'}).reset_index()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\4269128748.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n연도\n출생아 수\n\n\n\n\n0\n2010\n449949.242\n\n\n1\n2011\n445527.398\n\n\n2\n2012\n457813.448\n\n\n3\n2013\n417845.628\n\n\n4\n2014\n323378.169\n\n\n5\n2015\n438420.000\n\n\n6\n2016\n330782.536\n\n\n7\n2017\n358771.000\n\n\n8\n2018\n321845.000\n\n\n9\n2019\n165941.871\n\n\n10\n2020\n272300.000\n\n\n11\n2021\n260832.000\n\n\n\n\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby(by = '연도').aggregate({'출생아 수' : 'sum'}).reset_index()\\\n.plot.line(x = '연도', y = '출생아 수', backend = 'plotly')\n\n## 그래프 표기에 한글이 너무나도 잘 나오기 때문에 자신감 있게 한글로 바꿔도 된다!!!\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\2678187976.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\n출생아 수가 팍팍 튀는 구간이 있네용…\n\n\n시각화 2 : 시도별 출생아 수 시각화(line)\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').map(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.pivot_table(index = ['지역', '연도'], values = '출생아 수', aggfunc = 'sum').reset_index()  ## 이미 윗줄만으로 충분해...\n\n\n\n\n\n\n\n\n지역\n연도\n출생아 수\n\n\n\n\n0\n강원\n2010\n12477.0\n\n\n1\n강원\n2011\n12408.0\n\n\n2\n강원\n2012\n12426.0\n\n\n3\n강원\n2013\n10980.0\n\n\n4\n강원\n2014\n10662.0\n\n\n...\n...\n...\n...\n\n\n199\n충북\n2017\n11394.0\n\n\n200\n충북\n2018\n10586.0\n\n\n201\n충북\n2019\n9333.0\n\n\n202\n충북\n2020\n8600.0\n\n\n203\n충북\n2021\n8190.0\n\n\n\n\n204 rows × 3 columns\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').map(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.plot.line(x = '연도', y = '출생아 수', color = '지역', backend = 'plotly')\n\n\n                                                \n\n\n\n경기와 서울이 특정 년도에서 현저히 줄어드는 것을 알 수 있다.\n\n- 두 가지 line plot(전체, 지역별) 각각의 장단점이 있다;; 그 둘을 합쳐서 볼 순 없을까??"
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#d.-시각화-3-시도별-출생아-수-시각화area",
    "href": "2023_DV/Review/14. Plotly 시작.html#d.-시각화-3-시도별-출생아-수-시각화area",
    "title": "RiverFlow",
    "section": "### D. 시각화 3 : 시도별 출생아 수 시각화(area)",
    "text": "### D. 시각화 3 : 시도별 출생아 수 시각화(area)\nplot.area()\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역']).map(lambda x: float(x) if not x=='-' else 0)\\\n.drop('전국',axis=0)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.area(x='연도',y='출생아수',color='지역',backend='plotly')   ## plot.area()\n\n\n                                                \n\n\n\n각각 어떻게 변화했는지와, 그 누적의 변화 정도를 시각화할 수 있다.\n\n- 그래서 자꾸 넘어가는 게, 2014 경기, 2016 서울, 2019 서울ㆍ경기 : 출생아가 없었음. 왜 없었어???\n\ndf\n\n\n\n\n\n\n\n\n지역/연도[6]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266\n91526\n93914.000\n84066.000\n83711.000\n83005\n75.536\n65389\n58074\n53.673\n47400\n45531\n\n\n1\n부산\n27415\n27759\n28673.000\n25831.000\n26190.000\n26645\n24906.000\n21480\n19152\n17049.000\n15100\n14446\n\n\n2\n대구\n20557\n20758\n21472.000\n19340.000\n19361.000\n19438\n18298.000\n15946\n14400\n13233.000\n11200\n10661\n\n\n3\n인천\n25752\n20758\n21472.000\n25560.000\n25786.000\n25491\n23609.000\n20445\n20087\n18522.000\n16000\n14947\n\n\n4\n광주\n13979\n13916\n14392.000\n12729.000\n12729.000\n12441\n11580.000\n10120\n9105\n8364.000\n7300\n7956\n\n\n5\n대전\n14314\n14808\n15279.000\n14099.000\n13962.000\n13774\n12436.000\n10851\n9337\n8410.000\n7500\n7414\n\n\n6\n울산\n11432\n11542\n12160.000\n11330.000\n11556.000\n11732\n10910.000\n9381\n8149\n7539.000\n6600\n6127\n\n\n7\n세종\n-\n-\n1054.000\n1111.000\n1344.000\n2708\n3297.000\n3504\n3703\n3819.000\n3500\n3570\n\n\n8\n경기\n121753\n122027\n124746.000\n112129.000\n112.169\n113495\n105643.000\n94088\n83198\n83.198\n77800\n76139\n\n\n9\n강원\n12477\n12408\n12426.000\n10980.000\n10662.000\n10929\n10058.000\n9958\n8351\n8283.000\n7800\n7357\n\n\n10\n충북\n14670\n14804\n15139.000\n13658.000\n13366.000\n13563\n12742.000\n11394\n10586\n9333.000\n8600\n8190\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604\n17302.000\n15670\n14380\n13228.000\n11900\n10984\n\n\n12\n전북\n16100\n16175\n16238.000\n14555.000\n14231.000\n14087\n12698.000\n11348\n10001\n8971.000\n8200\n7745\n\n\n13\n전남\n16654\n16612\n16990.000\n15401.000\n14817.000\n15061\n13980.000\n12354\n11238\n10832.000\n9700\n8430\n\n\n14\n경북\n23700\n24250\n24635.000\n22206.000\n22062.000\n22310\n20616.000\n17957\n16079\n14472.000\n12900\n12045\n\n\n15\n경남\n32203\n32536\n33211.000\n29504.000\n29763.000\n29537\n27138.000\n23849\n21224\n19250.000\n16800\n15562\n\n\n16\n제주\n5657\n5628\n5992.000\n5328.000\n5526.000\n5600\n5494.000\n5037\n4781\n4500.000\n4000\n3728\n\n\n17\n전국\n470171\n471265\n484550.000\n436455.000\n435435.000\n438420\n406243.000\n357771\n326822\n302676.000\n272400\n260562\n\n\n\n\n\n\n\n\n경기 2014의 경우 인구수인데 소수점이 존재…(솔로몬???), 서울 2016… 등등. 사람이 쓰다보니 오탈자가 있다.\n\n\nE. 위에서의 시각화 수정\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)  ## 소수점이 존재할 경우 그대로, 아닐 경우 1000을 곱해줌.\n\n\n\n\n\n\n\n\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n지역\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n서울\n93266.0\n91526.0\n93914.0\n84066.0\n83711.0\n83005.0\n75536.0\n65389.0\n58074.0\n53673.0\n47400.0\n45531.0\n\n\n부산\n27415.0\n27759.0\n28673.0\n25831.0\n26190.0\n26645.0\n24906.0\n21480.0\n19152.0\n17049.0\n15100.0\n14446.0\n\n\n대구\n20557.0\n20758.0\n21472.0\n19340.0\n19361.0\n19438.0\n18298.0\n15946.0\n14400.0\n13233.0\n11200.0\n10661.0\n\n\n인천\n25752.0\n20758.0\n21472.0\n25560.0\n25786.0\n25491.0\n23609.0\n20445.0\n20087.0\n18522.0\n16000.0\n14947.0\n\n\n광주\n13979.0\n13916.0\n14392.0\n12729.0\n12729.0\n12441.0\n11580.0\n10120.0\n9105.0\n8364.0\n7300.0\n7956.0\n\n\n대전\n14314.0\n14808.0\n15279.0\n14099.0\n13962.0\n13774.0\n12436.0\n10851.0\n9337.0\n8410.0\n7500.0\n7414.0\n\n\n울산\n11432.0\n11542.0\n12160.0\n11330.0\n11556.0\n11732.0\n10910.0\n9381.0\n8149.0\n7539.0\n6600.0\n6127.0\n\n\n세종\n0.0\n0.0\n1054.0\n1111.0\n1344.0\n2708.0\n3297.0\n3504.0\n3703.0\n3819.0\n3500.0\n3570.0\n\n\n경기\n121753.0\n122027.0\n124746.0\n112129.0\n112169.0\n113495.0\n105643.0\n94088.0\n83198.0\n83198.0\n77800.0\n76139.0\n\n\n강원\n12477.0\n12408.0\n12426.0\n10980.0\n10662.0\n10929.0\n10058.0\n9958.0\n8351.0\n8283.0\n7800.0\n7357.0\n\n\n충북\n14670.0\n14804.0\n15139.0\n13658.0\n13366.0\n13563.0\n12742.0\n11394.0\n10586.0\n9333.0\n8600.0\n8190.0\n\n\n충남\n20242.0\n20398.0\n20448.0\n18628.0\n18200.0\n18604.0\n17302.0\n15670.0\n14380.0\n13228.0\n11900.0\n10984.0\n\n\n전북\n16100.0\n16175.0\n16238.0\n14555.0\n14231.0\n14087.0\n12698.0\n11348.0\n10001.0\n8971.0\n8200.0\n7745.0\n\n\n전남\n16654.0\n16612.0\n16990.0\n15401.0\n14817.0\n15061.0\n13980.0\n12354.0\n11238.0\n10832.0\n9700.0\n8430.0\n\n\n경북\n23700.0\n24250.0\n24635.0\n22206.0\n22062.0\n22310.0\n20616.0\n17957.0\n16079.0\n14472.0\n12900.0\n12045.0\n\n\n경남\n32203.0\n32536.0\n33211.0\n29504.0\n29763.0\n29537.0\n27138.0\n23849.0\n21224.0\n19250.0\n16800.0\n15562.0\n\n\n제주\n5657.0\n5628.0\n5992.0\n5328.0\n5526.0\n5600.0\n5494.0\n5037.0\n4781.0\n4500.0\n4000.0\n3728.0\n\n\n전국\n470171.0\n471265.0\n484550.0\n436455.0\n435435.0\n438420.0\n406243.0\n357771.0\n326822.0\n302676.0\n272400.0\n260562.0\n\n\n\n\n\n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)\\\n.stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby('연도').aggregate({'출생아 수' : 'sum'}).reset_index()\\\n.plot.line(x = '연도', y = '출생아 수', backend = 'plotly')\n\n\n                                                \n\n\n\n사실 튀는 구간 따위는 없었다!!!(그치만 줄어들고 있는건 사실이었네…)\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.line(x = '연도', y = '출생아수', color = '지역', backend = 'plotly')\n\n\n                                                \n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.area(x = '연도', y = '출생아수', color = '지역', backend = 'plotly')\n\n\n                                                \n\n\n\n정상적으로 이해할 수 있다."
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#여러가지-플랏",
    "href": "2023_DV/Review/14. Plotly 시작.html#여러가지-플랏",
    "title": "RiverFlow",
    "section": "4. 여러가지 플랏",
    "text": "4. 여러가지 플랏\n\nplotly는 bar, line, scatter, hist, box, area형태의 플랏을 지원한다.(pie나 그런 것들은 지원안함!)"
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#a.-.plot.bar",
    "href": "2023_DV/Review/14. Plotly 시작.html#a.-.plot.bar",
    "title": "RiverFlow",
    "section": "### A. .plot.bar()",
    "text": "### A. .plot.bar()\n- 예제 1 : 성별 별 합격률 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf  ## 파일에 index_column이 존재하고, 첫 행이 열이름인듯.\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\n어디서 많이 봤던 데이터, 집단 간 비교니까 바차트가 좋겠죠잉\n\n\ndf.pivot_table(index = 'gender', columns = 'result', values = 'count', aggfunc = 'sum')\\\n.assign(rate = lambda _df : _df['pass']/(_df.fail + _df['pass'])).reset_index()\\\n.assign(rate = lambda _df : _df.rate.apply(lambda x : round(x, 3)))\\\n.plot.bar(x = 'gender', y = 'rate', color = 'gender', text = 'rate', width = 600)\n\n\n                                                \n\n\n# 예제 2 : (성별, 학과) 별 지원자 수 시각화\n\ndf.pivot_table(index = ['gender', 'department'], columns = 'result', values = 'count', aggfunc = 'sum')\\\n.assign(rate = lambda _df : _df['pass']/(_df.fail + _df['pass'])).reset_index()\\\n.assign(rate = lambda _df : _df.rate.apply(lambda x : round(x, 2)))\\\n.plot.bar(x = 'gender', y = 'rate', color = 'gender', facet_col = 'department', text = 'rate', width = 800)\n\n\n                                                \n\n\n\nB. .plot.line()\n\n# 예제 1 : 핸드폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\n시계열 자료니까 라인차트로 그리면 좋겠다잉\n\n\ndf.melt(id_vars = ['Date']).set_axis(['날짜', '회사', '판매량'], axis = 1)\\\n.plot.line(x = '날짜', y = '판매량', color = '회사')  ## 어떻게 날짜로 잘 읽었네."
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#c.-.plot.scatter",
    "href": "2023_DV/Review/14. Plotly 시작.html#c.-.plot.scatter",
    "title": "RiverFlow",
    "section": "### C. .plot.scatter()",
    "text": "### C. .plot.scatter()\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\\\n.loc[:,lambda df: df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda df: df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda df: df.Wage.str[1:].str.replace('K','000').astype(int))\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n가슴아픈 데이터, 연속형 변수 간 관계를 보고 싶으니 산점도가 적당\n\n\ndf.query('Position == \"DEFENDER\" or Position == \"FORWARD\"')\\\n.plot.scatter(x = 'ShotPower', y = 'StandingTackle',\n             color = 'Position', size = 'Wage', hover_data = ['Name', 'Age'],\n             opacity = 0.5, width = 800)  ## hover_data로 마우스를 가져다 대었을 때 표기되는 정보를 알 수 있으며, alpha대신 opacity 옵션으로 투명도 조절\n\n\n                                                \n\n\n\nD. .plot.box()\n\n# 예제 1 : 렛츠고! 전북고등학교!\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들\n\n\ndf = pd.DataFrame({\n    'Class' : ['A']*len(y1) + ['B']*len(y2),\n    'Score' : y1+y2\n})\ndf\n\n\n\n\n\n\n\n\nClass\nScore\n\n\n\n\n0\nA\n75\n\n\n1\nA\n75\n\n\n2\nA\n76\n\n\n3\nA\n76\n\n\n4\nA\n77\n\n\n5\nA\n77\n\n\n6\nA\n78\n\n\n7\nA\n79\n\n\n8\nA\n79\n\n\n9\nA\n98\n\n\n10\nB\n76\n\n\n11\nB\n76\n\n\n12\nB\n77\n\n\n13\nB\n77\n\n\n14\nB\n78\n\n\n15\nB\n78\n\n\n16\nB\n79\n\n\n17\nB\n80\n\n\n18\nB\n80\n\n\n19\nB\n81\n\n\n\n\n\n\n\n\ndf.plot.box(x = 'Class', y = 'Score', color = 'Class',\n            points = 'all', width = 500, backend = 'plotly')  ## points를 'outlier'로 설정하면 \n\n\n                                                \n\n\n# 예제 2 : (년도, 시도) 별 전기 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_12860\\2652719362.py:9: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.plot.box(x = '시도', y = '에너지사용량(TOE)/전기', color = '시도', facet_row = '년도', width = 800, height = 1000, hover_data = ['지역', '연면적'], points = 'outliers')  ## outliers가 디폴트임"
  },
  {
    "objectID": "2023_DV/Review/14. Plotly 시작.html#e.-.plot.hist",
    "href": "2023_DV/Review/14. Plotly 시작.html#e.-.plot.hist",
    "title": "RiverFlow",
    "section": "### E. .plot.hist()",
    "text": "### E. .plot.hist()\n# 예제 1 : 타이타닉 (연령, 성별) 별 생존자\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n2.564949\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n3.401197\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n3.154870\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n3.401197\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n2.047693\n\n\n\n\n891 rows × 13 columns\n\n\n\n\ndf.Age.hist()\n\n\n                                                \n\n\n\n시리즈를 통해 아무 입력 없이 히스토그램을 그릴 수도 있다.\n\n\ndf.plot.hist(x = 'Age', color = 'Sex',\n            facet_row = 'Sex', facet_col = 'Survived')\n\n\n                                                \n\n\n\n성별 간 효과가 저연령에서는 옅어지는 것이 보인다.(생존률로 시각화해도 좋을듯)\n\n\ndf.plot.hist(x = 'Age', color = 'Survived', facet_col = 'Sex')\n\n\n                                                \n\n\n\ndf.loc[:, ['Age', 'Sex', 'Survived']].assign(Age_cut = lambda _df : pd.qcut(_df.Age, q = 10))\\\n.pivot_table(index = 'Age_cut', columns = 'Sex', values = 'Survived', aggfunc = 'mean')\\\n.stack().reset_index().rename({0 : 'Rate'}, axis = 1)\\\n.plot.bar(x = 'Sex', y = 'Rate', color = 'Sex', facet_col = 'Age_cut', width = 800)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotly\\express\\_core.py:2044: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n                                                \n\n\n\n세부조정은 알아서 하시길…\n\n\nF. .plot.area()\n\n# 예제 1 : 핸드폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars = 'Date').set_axis(['날짜', '회사', '판매량'], axis = 1)\\\n.plot.area(x = '날짜', y = '판매량', color = '회사')\n\n\n                                                \n\n\n\n전체적인 판매량과 개별 판매량의 정도를 알 수 있음. (두 플랏을 한 번에!)\n\n# 예제 2 : 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_12860\\2652719362.py:9: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').apply(lambda x : x[-1]))\\\n.pivot_table(index = ['년도', '시도', '에너지종류'], values = '에너지사용량', aggfunc = 'sum')\\\n.reset_index().plot.area(x = '년도', y = '에너지사용량', facet_col = '에너지종류', color = '시도', width = 600)\n\n\n                                                \n\n\n\n## figure로 저장\nfig = df.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').str[-1])\\\n.pivot_table(index = ['에너지종류', '시도', '년도'], values = '에너지사용량', aggfunc = 'sum').reset_index()\\\n.plot.area(x = '년도', y = '에너지사용량', facet_col = '에너지종류', color = '시도', width = 600)\n\n## 아마도 xaxis의 범위를 한정하는 과정에서 xlabel의 범위가 조정되다보니 이리 되는듯.\nfig.update_layout(\n    xaxis_domain = [0.0, 0.25],\n    xaxis2_domain = [0.35, 0.60],\n    xaxis3_domain = [0.70, 0.95]\n)\n\n\n                                                \n\n\n\n겹칠땐 이렇게 하면 됩니다… 근데 좁아진 걸 보니 다르게 개선할 수도 있을 것 같기는 함…"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "",
    "text": "matplotlib를 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#사전작업",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#사전작업",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 import\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (3, 2)\nmatplotlib.rcParams['figure.dpi'] = 150"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#간단한-꺾은선-그래프",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#간단한-꺾은선-그래프",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "2. 간단한 꺾은선 그래프",
    "text": "2. 간단한 꺾은선 그래프\nplt.plot()을 사용하여 간단하게 그래프를 그릴 수 있다.\n\ny값만 지정한 경우\n\n\nplt.plot([1,2,4,3])\nplt.show()\n\n\n\n\n\nx값과 y값 같이 지정한 경우\n\n\nplt.plot([1,2,3,4],[1,2,4,3])\nplt.show()\n\n\n\n\n\nx값과 y값에 변수를 지정하여 넣어주는 경우\n\n\nx = [1,2,3,4]\ny = [1,2,4,3]\n\nplt.plot(x,y)\nplt.show()\n\n\n\n\n- 이외에도 다양한 옵션을 사용하여 그래프를 다채롭게 그릴 수 있는데, 지금부터 그것들을 알아보도록 하자.\n\nplt.plot의 옵션\nplt.plot()에서 괄호 안에 문자열을 넣음으로서 세 가지 옵션을 간단하게 적용할 수 있다.\nplt.plot(x,y,'--')  ## 파선 그래프\nplt.plot(x,y,':')   ## 점선 그래프\nplt.plot(x,y,'r')   ## 선의 색상이 빨간색\nplt.plot(x,y,'r--') ## 빨간색의 파선 그래프\n...\n- 게다가 세 옵션을 순서 상관없이 집어넣어 적용 가능하다!\n\nLine StylesColorsMarkers\n\n\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\n\n\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\n\n\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\n\n\n그 외에 다른 옵션을 보고 싶다면 아래를 참조하라.\n\nother options or colors\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\nhttps://matplotlib.org/2.0.2/examples/color/named_colors.html\nhex code\nhttps://htmlcolorcodes.com/\nother linestyles\nhttps://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html\n\n- preset에 있는 색상 외 다른 색상을 적용\n\nplt.plot(x,y,'--',color = 'lime')\n\n\n\n\n\nusing color name\n\n\nplt.plot(x,y,color = '#751F9B')\n\n\n\n\n\nusing hex code\n\n- 선의 형태를 다양하게 변경\n\nplt.plot(x,y,linestyle = 'dashed')\nplt.show()\n\n\n\n\n\n문자열로 직접 지정\n\n\nplt.plot(x,y,linestyle = (0, (1,1)))\n\n\n\n\n\n파선의 길이를 직접 지정\n\n\n\nplt.plot()에서 scatter plot을 생성\nmarker 옵션을 변경하여 scatter plot을 손쉽게 그릴 수도 있다.\n\nplt.plot(x,y,'db')  ## diamonds, blue\n\n\n\n\n\n\ndot connected plot\n\nplt.plot(x,y,':or')  ## dotline(:), circle(o), red\n\n\n\n\n\n\npile up\nplt.show()를 입력하기 전 계속해서 그래프를 그리면 중첩된다.\n\nplt.plot([1,2,3,2], '--o', color = 'orange')\nplt.plot([2,3,1,4], '--o', color = 'skyblue')\n\nplt.show()\n\nplt.plot([4,4,2,1], '--o', color = 'cyan')\n\nplt.show()\n\n\n\n\n\n\n\n\nplt.plot([1,2,3,2], '--o', color = 'C1')\nplt.plot([2,3,1,4], '--o', color = 'C0')\n\nplt.show()\n\n\n\n\n\n위와 같은 경우에는 color를 지정하지 않을 경우 먼저 입력한 그래프에 C0가 지정된다.\n\n\n\n응용 : scatter plot and line plot\n- 유사 단순선형회귀\n설명변수와 오차, 반응변수를 지정해주자.\n\nx = np.arange(-5,5,0.1)\neps = np.random.randn(100)\ny = 2*x + eps ## 벗어나도록 겹치게\n\n\nplt.plot(x,y,'.b')     ## 실제 데이터\nplt.plot(x,2*x,'--r')  ## 회귀선\nplt.show()\n\n\n\n\n\n\n적합한 그래프를 그릴 때\n- summary: boxplot, histogram, lineplot, scatterplot\n\n라인플랏: 추세\n☆★☆ 스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#객체지향적-시각화",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#객체지향적-시각화",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "3. 객체지향적 시각화",
    "text": "3. 객체지향적 시각화\n\nA. 배경지식\n- 그림을 저장해둔 뒤 나중에 꺼내보고 싶다면? | plt.gcf() : Get Current Figure.\n\nplt.plot([1, 2, 3, 2],'--o')\nfig = plt.gcf() ## plt.show()를 하기 전, 현재 표기되는 figure를 얻는다.\n\n\n\n\n\nfig\n\n\n\n\n\n위와 같이 변수에 저장된 것을 알 수 있다.\n\n\n\nB. fig의 해체\nfig\nfig.axes\n\nax = fig.axes[0]\nax.yaxis\nax.xaxis\n\nlines = ax.get_lines()[0]\nlines[0]\n\nfig &gt; 그래프 그 자체\naxes &gt; 그래프의 구역\naxis &gt; x축, y축\nline &gt; 직선형 그래프\n\n등등등…\n아무튼 여러 개체가 나뉘어있다.\n\n개념(비유) : * Figure(fig) : 도화지 * Axes(ax) : 도화지에 존재하는 그림틀 * Axis, Lines : 그림틀 위에 올려지는 물체(object)\n\n\n\nC. plt.plot()없이 그래프 그리기\n\nplt.plot([1,2,4,3], '--o')\nplt.show()\n\n\n\n\n위와 같은 그래프를 plt.plot()없이 만들어보자!\n- 아래의 코드를 하나하나 뜯어보자.\n\nfig = plt.Figure()\n\nax = fig.add_axes([0.125,0.11,0.775,0.77])\nax.set_xlim([-0.15, 3.15])  # setting x axis limit\nax.set_ylim([0.9, 3.1])     # setting y axis limit\nline = matplotlib.lines.Line2D(\n    xdata = [0,1,2,3],\n    ydata = [1,2,3,2],\n    linestyle = '--',\n    marker = 'o'\n)\nax.add_line(line)\n\nfig\n\n\n\n\n1. 최상위 하이라이트(figure) 생성\n\nfig = plt.figure(); fig   ## 최상위 하이라이트인 그림만 만들어냄.\n\n&lt;Figure size 450x300 with 0 Axes&gt;\n\n\n&lt;Figure size 450x300 with 0 Axes&gt;\n\n\n2. 그래프가 들어갈 공간(axes) 생성\n\nax = fig.add_axes([0.125,0.11,0.775,0.77]); fig  ## 가로시작, 세로시작, 종횡비\n\n\n\n\n3. 직선을 지정 후 추가\n\nline = matplotlib.lines.Line2D(\n    xdata = [0,1,2,3],\n    ydata = [1,2,3,2],\n    linestyle = '--',\n    marker = 'o'\n)\n\n\nmatplotlib에서 라인을 만드는 함수가 따로 있었다.\n\n\nax.add_line(line)\n\n&lt;matplotlib.lines.Line2D at 0x1c3c591e380&gt;\n\n\n\nfig\n\n\n\n\n4. 직선이 제대로 표기되지 않는 것 같으니 x축과 y축의 한계를 설정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\nfig\n\n\n\n\n\n\nD. 또 코드의 대체\n1. line2D 오브젝트를 쓰지 않는 방법\n\n## genarally\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.plot([1,2,3,2], '--o')\nfig\n\n\n\n\n\nax.plot()을 사용\n\n2. add_axes()를 쓰지 않는 방법(중요!)\n\nfig = plt.Figure()\nax = fig.subplots(1)\nax.plot([1,2,3,2], '--o')\nfig\n\n\n\n\n\nax = fig.subplots()을 사용\n\n3. fig와 ax들을 한번에 지정(중요!)\n\nfig, ax = plt.subplots(1) ## 중요함\nax.plot([1,2,3,2], '--o')\nplt.show()\n\n\n\n\n\n\nE. 정리 (\\(\\star\\star\\star\\))\n아래의 코드는 모두 같은 애들이었다.\n\nplt.plot([1,2,3,2], '--o')\n\n\nfig, ax = plt.subplots()\nax.plot([1,2,3,2], '--o')\n\n\nfig = plt.Figure()\nax = fig.subplots()\nax.plot([1,2,3,2], '--o')\nfig\n\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.plot([1,2,3,2], '--o')\nfig\n\nplt.subplots()과 ax.plot()의 경우 상당히 유용한 코드이니 꼭 숙지할 것!"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#미니맵과-서브플롯",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#미니맵과-서브플롯",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "4. 미니맵과 서브플롯",
    "text": "4. 미니맵과 서브플롯\n\nA. 미니맵\nfig.add_axes()를 사용한다.\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2]); fig\n\n\n\n\n\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])  ## 가로 세로 위치(중심위치), 종횡비\nax.plot([1,5,3,4], '--o')\nax_mini.plot([1,2,3,1], '--or')\n\nfig\n\n\n\n\n\n생성된 fig에 axes를 하나 더 추가하여 만들어냈다.\n\n\n\nB. 서브플롯\nplt.subplots(), fig.subplots()을 이용해보자.\n\nfig, axs = plt.subplots(2)  ## 2행\n\n\n\n\n\naxs\n\narray([&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;], dtype=object)\n\n\n\naxs에 ax들이 array형태로 저장되어 있다.\n\n\naxs[0].plot([1,2,3,2], '--r')\naxs[1].plot([1,2,4,3], '--o')\n\nfig\n\n\n\n\n\n뭔가 레이아웃이 가려져있고 이상하다.\n\n\nfig.tight_layout(); fig\n\n\n\n\n\n왠만해선 fig.tight_layout()을 해주도록 하자.\n\n\n차피 axs가 array 형태로 저장되므로 그것을 따로 지정해주고 싶다면 아래와 같이 사용하는 것을 권장한다.\n\n\nfig, (ax1, ax2) = plt.subplots(2)\nax1.plot([1,2,3,2], '--r')\nax2.plot([1,2,4,3], '--o')\nfig.tight_layout()\n\n\n\n\n\n\nC. 서브플롯 스케일 조정 및 다중화\n- 스케일 변경\n\nfig, (ax1, ax2) = plt.subplots(2, figsize = (3,3))  ## 종횡비\nax1.plot([1,2,3,2], '--r')\nax2.plot([1,2,4,3], '--o')\nfig.tight_layout()\n\n\n\n\n\n미리 설정해줬던 dpi에 의거하여 종횡비가 배수로 적용된다.\n\n- 더 많은 서브플롯 생성\n\nfig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize = (3,3))\nax1.plot([1,2,4,3], 'o', color = 'C0')\nax2.plot([1,2,4,3], 'o', color = 'C1')\nax3.plot([1,2,4,3], 'o', color = 'C2')\nax4.plot([1,2,4,3], 'o', color = 'C3')\nfig.tight_layout()\n\n\n\n\n- 사용자 정의 서브플롯 생성\nplt.subplot() ## s가 없는 subplot(), 즉, 하나만 만들어진다.\n\nplt.figure(figsize=(3,3))\nplt.subplot(2,2,1)  ## 2×2의 1\nplt.plot([1,2,4,3],'o', color='C0')\nplt.subplot(1,2,2)\nplt.plot([1,2,4,3],'o', color='C1')\nplt.subplot(2,2,3)\nplt.plot([1,2,4,3],'o', color='C2')\nplt.tight_layout()\n\nfig = plt.gcf()\n\n\n\n\n\n이미 생성된 figure의 크기를 조정\n\n\nfig.set_size_inches(2,2); fig"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#title",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#title",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "5. title",
    "text": "5. title\n\ntitle을 만드는 함수는 어떤 오브젝트에 소속되는 게 좋을까? 1. plt -&gt; subplot의 제목을 설정 가능 2. fig -&gt; 전체제목(super title)을 설정할 수 있음 3. ax -&gt; subplot들의 제목을 설정할 수 있음\n\n\nA. plt.title()\nfigure를 생성하지 않은 기본적인 환경에서 타이틀을 달아준다.\n\n## 가장 평범한 플롯\nplt.plot([1,2,3,2])\nplt.title('asdf')\nplt.show()\n\n\n\n\n\n\nB. ax.set_title()\nfigure와 axes를 생성했을 경우, 각 ax마다 타이틀을 달아줄 수 있다.\n\n## title이 axes에 존재\nfig, ax = plt.subplots()\nax.set_title('asdf')\nax.plot([1,2,3,2])\n\nplt.show()\n\n\n\n\n\n\nC. fig.suptitle() | 권장하지 않는 방법\n원래 figure 자체에 타이틀을 붙이는 것은 불가능하다.\n\n##--------fig : 원래는 불가능--------\nplt.plot([1,2,3,2])\nfig = plt.gcf()\nfig.suptitle('asdf')\n\nplt.show()\n\n\n\n\n\n\nD. 응용\n\nplt.subplots()과 set_title()을 이용\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (4,2))\nax1.set_title('asdf')\nax2.set_title('1234')\nax1.plot([1,2,3,2])\nax2.plot([1,2,3,2])\nfig.tight_layout()\n\n\n\n\n\nfigure를 생성하지 않고 plt.subplot()과 plt.title()을 이용하여 손수 지정\n\n\nplt.subplot(1,2,1)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(1,2,2)\nplt.plot([1,2,3])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\nfig.suptitle()을 이용한 방법\n\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\nE. plt.gca()\nplt.gca()를 통해 ax개체를 다룰 수도 있다.\n\nplt.plot([1,2,3,2])\nax = plt.gca()\nax.set_title('asdf')  ## 현재의 axis에 바로 타이틀을 설정해준다.\n\nText(0.5, 1.0, 'asdf')"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도의-응용-표본상관계수",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도의-응용-표본상관계수",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "6. 산점도의 응용 | 표본상관계수",
    "text": "6. 산점도의 응용 | 표본상관계수\n\nA. 산점도와 표본상관계수\n아래처럼 두 연속형 자료가 주어질 경우 산점도로 나타낼 수 있다.\n\nweight = [44,48,49,58,62,68,69,70,76,79]\nheight = [159,160,162,165,167,162,165,175,165,172]\n\nplt.plot(weight,height,'.')  ## option : '.' marker가 .인 산점도 산출\nplt.show()\n\n\n\n\n아래 표본상관계수의 정의에 따라 데이터에서의 표본상관계수를 구해보자.\n- (표본)상관계수의 정의\n\\[r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2 }}=\\sum_{i=1}^{n}\\tilde{x}_i\\tilde{y}_i \\]\n\\[단,~\\tilde{x}_i=\\frac{(x_i-\\bar{x})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}},~ \\tilde{y}_i=\\frac{(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}\\]\n\n위 식에서 \\(\\tilde{x}_i\\)와 \\(\\tilde{y}_i\\)는 \\(x_i\\)와 \\(y_i\\)를 표준화한 것이다.\n\n(데이터를 불러오자)\n\nx=[44,48,49,58,62,68,69,70,76,79]\ny=[159,160,162,165,167,162,165,175,165,172]\n\n(평균을 0으로)\n\nxx = x - np.mean(x); print(xx)\nyy = y - np.mean(y); print(yy)\n\n[-18.3 -14.3 -13.3  -4.3  -0.3   5.7   6.7   7.7  13.7  16.7]\n[-6.2 -5.2 -3.2 -0.2  1.8 -3.2 -0.2  9.8 -0.2  6.8]\n\n\n(퍼진 정도를 표준화)\n\nx_standard = xx/np.sqrt(np.sum(xx**2))\ny_standard = yy/np.sqrt(np.sum(yy**2))\n\n(표본상관계수 산출)\n\nnp.sum(x_standard*y_standard)\n\n0.7138620583559141\n\n\n\n이미 정의된 코드를 통해 해당 결과가 맞는지 확인해보자.\n\n\nnp.corrcoef(x,y)\n\narray([[1.        , 0.71386206],\n       [0.71386206, 1.        ]])"
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#b.-산점도를-보고-상관계수의-부호를-해석",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#b.-산점도를-보고-상관계수의-부호를-해석",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "### B. 산점도를 보고 상관계수의 부호를 해석",
    "text": "### B. 산점도를 보고 상관계수의 부호를 해석\n- 아래의 그림은 상관계수 r의 값이 양수인가 음수인가?\n\nx=[44,48,49,58,62,68,69,70,76,79]\ny=[159,160,162,165,167,162,165,175,165,172]\n\nplt.plot(x, y, 'o')\nplt.show()\n\n\n\n\n\nxx = x-np.mean(x)\nyy = y-np.mean(y) \nxxx = xx/np.sqrt(np.sum(xx**2))\nyyy = yy/np.sqrt(np.sum(yy**2))\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (10,3))\nax1.plot(x,y, 'o')\nax1.set_title(r'$(x_i,y_i)$')\nax2.plot(xx,yy,'o') ## mean to 0\nax2.set_title(r'$(x_i-\\bar{x}, y_i-\\bar{y})$')\nax3.plot(xxx,yyy,'o') ## standarized\nax3.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\n\nplt.show()\n\n\n\n\n\n마지막 \\(\\tilde{x}_i\\), \\(\\tilde{y}_i\\)를 곱한 값이 양수인 것과 음수인 것을 체크해보자.\n\n\n1,3사분면에 점들이 많으므로 상관계수의 부호는 양수일 것이다."
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#d.-산점도를-보고-상관계수의-절대값을-해석",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#d.-산점도를-보고-상관계수의-절대값을-해석",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "### D. 산점도를 보고 상관계수의 절대값을 해석",
    "text": "### D. 산점도를 보고 상관계수의 절대값을 해석\n- 기울기가 동일하지만 직선 근처의 퍼짐이 다른 두 개의 자료\n\nx=np.arange(0,10,0.1)\ny1=x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## N(0,1)\ny2=x+np.random.normal(loc=0,scale=7.0,size=len(x))  ## N(0,7)\n\nplt.plot(x,y1,'.')\nplt.plot(x,y2,'x')\nplt.show()\n\n\n\n\n\n표준화하는 함수 tilde() 정의\n\n\ndef tilde(x):\n    xx = x-np.mean(x)\n    xxx = xx / np.sqrt(np.sum(xx**2))\n    return xxx\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (4,2))\nax1.plot(x,y1,'.'); ax1.plot(x,y2,'x'); ax1.set_title(r'$(x_i,y_i)$')\nax2.plot(tilde(x), tilde(y1),'.'); ax2.plot(tilde(x), tilde(y2), 'x'); ax2.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\nfig.tight_layout()\n\n\n\n\n- 직선 근처의 퍼짐은 동일하지만, 직선의 기울기가 다른 경우\n\nx=np.arange(0,10,0.1)\ny1=x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## 기울기가 1\ny2=0.2*x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## 기울기가 0.2\n\nplt.plot(x,y1,'.')\nplt.plot(x,y2,'x')\n\nplt.show()\n\n\n\n\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(4,2))\nax1.plot(x,y1,'.'); ax1.plot(x,y2,'x'); ax1.set_title(r'$(x_i,y_i)$')\nax2.plot(tilde(x),tilde(y1),'.'); ax2.plot(tilde(x),tilde(y2),'x'); ax2.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\nfig.tight_layout()\n\n\n\n\n기울기가 클수록, 퍼짐 정도가 작을수록 상관계수의 절댓값이 높다."
  },
  {
    "objectID": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도-응용예제2---앤스콤의-4분할",
    "href": "2023_DV/Review/2. 꺾은선, 산점도, 객체지향화.html#산점도-응용예제2---앤스콤의-4분할",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "7. 산점도 응용예제2 - 앤스콤의 4분할",
    "text": "7. 산점도 응용예제2 - 앤스콤의 4분할\n- 표본상관계수가 모두 동일한 네 자료를 보라.\n\nx1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n\nx2 = x1\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n\nx3 = x1\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\nnp.corrcoef(x1,y1),np.corrcoef(x2,y2),np.corrcoef(x3,y3),np.corrcoef(x4,y4)\n\n(array([[1.        , 0.81642052],\n        [0.81642052, 1.        ]]),\n array([[1.        , 0.81623651],\n        [0.81623651, 1.        ]]),\n array([[1.        , 0.81628674],\n        [0.81628674, 1.        ]]),\n array([[1.        , 0.81652144],\n        [0.81652144, 1.        ]]))\n\n\n\n음, 다 비슷한 자료겠구나… 양의 상관관계를 띄겠네?\n\n라고 속단하긴 이르다.\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(6,4))\nax1.plot(x1,y1,'o'); ax1.set_title(f'corrcoef = {np.corrcoef(x1,y1)[0,1] : .6f}')\nax2.plot(x2,y2,'o'); ax2.set_title(f'corrcoef = {np.corrcoef(x2,y2)[0,1] : .6f}')\nax3.plot(x3,y3,'o'); ax3.set_title(f'corrcoef = {np.corrcoef(x3,y3)[0,1] : .6f}')\nax4.plot(x4,y4,'o'); ax4.set_title(f'corrcoef = {np.corrcoef(x4,y4)[0,1] : .6f}')\nfig.tight_layout()\n\n\n\n\n4개의 그림은 모두 같은 상관계수를 가지나, 그 느낌이 전혀 다르다.\n- 앤스콤플랏의 4개의 그림은 모두 같은 상관계수를 가진다. 하지만, 4개의 그림은 느낌이 전혀 다르다.\n- 같은 표본상관계수를 가진다고 하여 같은 관계성을 가지는 것은 아니다. 표본상관계수는 x,y의 비례정도를 측정하는데 그 값이 1에 가깝다고 하여 꼭 정비례의 관계가 있음을 의미하는 건 아니다.\n\\((x_i,y_i)\\)의 산점도가 선형성을 보일 때만 “표본상관계수가 1에 가까우므로 정비례의 관계에 있다”라는 논리전개가 성립한다.\n\n앤스콤의 첫번째 플랏 : 산점도가 선형 -&gt; 표본상관계수가 0.816 = 정비례의 관계가 0.816 정도\n앤스콤의 두번째 플랏 : 산점도가 선형이 아님 -&gt; 표본상관계수가 크게 의미없음.\n앤스콤의 세번째 플랏 : 산점도가 선형인듯 보이나 하나의 이상치가 있음 -&gt; 하나의 이상치가 표본상관계수의 값을 무너뜨릴 수 있으므로 표본상관계수 값을 신뢰할 수 없음.\n앤스콤의 네번째 플랏 : 산점도를 그려보니 이상한 그림 -&gt; 표본상관계수를 계산할 수는 있으나, 그게 무슨 의미가 있을까?\n\n산점도가 선형성을 보일 때만 표본상관계수가 1에 가까우므로 정비례의 관계에 있다라는 논리전개가 성립한다.\n\n1번만 의미가 있음. 3번의 경우 이상치가 존재하여 신뢰할 수 없음.\n\n\n교훈\n상관계수를 해석하기에 앞서서 산점도가 선형성을 보이는 지 체크할 것! 항상 통계량은 적절한 가정하에서만 말이 된다는 사실을 기억할 것!"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html",
    "href": "2023_DV/Review/4. Plotnine.html",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "",
    "text": "plotnine : R에서의 문법을 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#라이브러리-import",
    "href": "2023_DV/Review/4. Plotnine.html#라이브러리-import",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\n##!pip install plotnine   ## plotnine이 구축되지 않은 경우 설치해야 한다.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n\n\nimport plotnine\n\n\nplotnine.options.dpi= 150\nplotnine.options.figure_size = (6, 5)\n\n\n간단한 그래프 설정이다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#mpg-data",
    "href": "2023_DV/Review/4. Plotnine.html#mpg-data",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "2. mpg data",
    "text": "2. mpg data\n\nA. read data\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/mpg.csv')\ndf\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#b.-descriptions",
    "href": "2023_DV/Review/4. Plotnine.html#b.-descriptions",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. descriptions",
    "text": "### B. descriptions\n\ndf.columns\n\nIndex(['manufacturer', 'model', 'displ', 'year', 'cyl', 'trans', 'drv', 'cty',\n       'hwy', 'fl', 'class'],\n      dtype='object')\n\n\n- 각 행들이 어떤 의미를 가지는 지 Chat GPT에게 분석을 요청해봤다.\n\n- 그렇단다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#mpg의-시각화-2차원",
    "href": "2023_DV/Review/4. Plotnine.html#mpg의-시각화-2차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "3. mpg의 시각화 : 2차원",
    "text": "3. mpg의 시각화 : 2차원\n\nA. x=displ, y=hwy\n\n- 예시 1 : 정직하게 메뉴얼대로…\n\n파라미터를 직접 지정해주는 경우\n\n\nggplot(data = df) + geom_point(mapping = aes(x = 'displ', y = 'hwy')) ## aes : dictionary와 유사하다고 생각하면 된다.\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n파라미터 생략\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#b.-rpy2-코랩-아닌-경우-실습-금지",
    "href": "2023_DV/Review/4. Plotnine.html#b.-rpy2-코랩-아닌-경우-실습-금지",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. rpy2 : 코랩 아닌 경우 실습 금지",
    "text": "### B. rpy2 : 코랩 아닌 경우 실습 금지\n- R에서도 거의 똑같은 문법으로 그릴 수 있음\n\n#import rpy2\n#%load_ext rpy2.ipython\n\n\n#%%R\n#library(tidyverse)\n#df = mpg\n#ggplot(df)+geom_point(aes(x=displ,y=hwy))"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#mpg의-시각화-3차원",
    "href": "2023_DV/Review/4. Plotnine.html#mpg의-시각화-3차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "4. mpg의 시각화 : 3차원",
    "text": "4. mpg의 시각화 : 3차원\n\nA. x=displ, y=hwy, shape=class\n\n\nset(df['class'])  ## 중복되지 않은 값이 어느 것이 있는 지 산출\n## df['class'].unique() : 이건 array로 산출된다. 동일한 코드\n\n{'2seater', 'compact', 'midsize', 'minivan', 'pickup', 'subcompact', 'suv'}\n\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', shape = 'class'))   ## class를 shape로 구분 &gt; 불편함\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#b.-xdispl-yhwy-colorclass",
    "href": "2023_DV/Review/4. Plotnine.html#b.-xdispl-yhwy-colorclass",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. x=displ, y=hwy, color=class",
    "text": "### B. x=displ, y=hwy, color=class\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n모양까지 class별로 달랐으면 좋겠다.\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n전체적으로 포인트의 사이즈를 키우고 싶다.\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'), size = 5)  ## 외부 파라미터\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n너무 커서 겹치니까 투명도 조정\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'), size = 5, alpha = 0.5)\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\ngeom_point()에서 내부 aes()에 넣은 값들은 값들을 구분하도록 되며, 외부에 입력된 값은 전체 개체들을 바꿔버린다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#mpg의-시각화-4차원-5차원",
    "href": "2023_DV/Review/4. Plotnine.html#mpg의-시각화-4차원-5차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "5. mpg의 시각화 : 4차원, 5차원",
    "text": "5. mpg의 시각화 : 4차원, 5차원\n\nset(df['drv'])  ## 4륜구동, 전륜구동(front), 후륜구동(r)\n\n{'4', 'f', 'r'}\n\n\n\nA. drive metiod에 더 중점\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'drv', shape = 'class'), size = 4, alpha = 0.5)\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n4륜구동이 연비가 낮은 걸 확인할 수 있다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#b.-5차원-시각화",
    "href": "2023_DV/Review/4. Plotnine.html#b.-5차원-시각화",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 5차원 시각화",
    "text": "### B. 5차원 시각화\n\nset(df['cyl'])  ## 실린더 수, 4,5,6,8\n\n{4, 5, 6, 8}\n\n\n\nggplot(df) + geom_point(aes(x='displ',y='hwy',color='drv',shape='class', size = 'cyl'), alpha = 0.5)  ## 외부 파라미터에 size는 제거\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n여기까지가 기본적인 사용 방법이다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#객체지향적-시각화",
    "href": "2023_DV/Review/4. Plotnine.html#객체지향적-시각화",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "6. 객체지향적 시각화",
    "text": "6. 객체지향적 시각화\n- ggplot의 정체는 뭐지?\n\ntype(ggplot)\n\ntype\n\n\n\nclass. 어떤 물체를 만들어내는 함수와 비슷. matplotlib에서의 plt.figure()와 유사하다고 보면 된다.\n\n- 그럼 geom_point는 정체가 뭐지?\n\ntype(geom_point)  ## class, 생성함수.\n\nplotnine.utils.Registry\n\n\n\ngeom은 그림, 그래프라고 보면 된다. ’fig.add_axes()후 추가된ax`에 그래프를 그리는 것과 유사"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#a.-fig-geom_point-geom_smooth",
    "href": "2023_DV/Review/4. Plotnine.html#a.-fig-geom_point-geom_smooth",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### A. fig + geom_point + geom_smooth",
    "text": "### A. fig + geom_point + geom_smooth\n\nfig  = ggplot(df)\npoint = geom_point(aes(x = 'displ', y = 'hwy'))\n\n\npoint ## 아무것도 나오지 않음\n\n&lt;plotnine.geoms.geom_point.geom_point at 0x121c8015390&gt;\n\n\n\nfig + point\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n두 개체를 합치니 피규어에 그래프가 들어가버린 형태가 되었다.\n\n\ngeom_smooth() | 산점도가 아닌 직선 그래프를 그려준다.\n\n\nsmooth = geom_smooth(aes(x = 'displ', y = 'hwy'))\n\n\nfig + smooth  ## ggplot(df) + geom_smooth(aes(x = 'displ', y = 'hwy')), 추세선 산출\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n그럼 셋을 합쳐보면…\n\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy')) + geom_smooth(aes(x = 'displ', y = 'hwy'))과 동일하다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#b.-시각화-개선",
    "href": "2023_DV/Review/4. Plotnine.html#b.-시각화-개선",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 시각화 개선",
    "text": "### B. 시각화 개선\n\ngeom_point()를 개선\n\n\npoint_better = geom_point(aes(x='displ',y='hwy',color='drv',size='cyl'),alpha=0.5)  ## 색상과 크기로 구분\n\n\nfig + point_better\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\ngeom_smooth() 개선\n\n\nsmooth_better = geom_smooth(aes(x = 'displ',  y = 'hwy', color = 'drv'), linetype = 'dashed')  ## 차종별로 추세선\n\n\nfig + smooth_better\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\nassemble\n\n\nfig + smooth_better + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#c.-다양한-조합",
    "href": "2023_DV/Review/4. Plotnine.html#c.-다양한-조합",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### C. 다양한 조합",
    "text": "### C. 다양한 조합\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\nfig + smooth_better + point_better\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n전체 추세선 추가\n\n\nfig + smooth_better + point_better + geom_smooth(aes(x = 'displ', y = 'hwy'), color = 'white', linetype = 'dashed', size = 3)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#아이스크림을-많이-먹으면-걸리는-병---인과관계와-상관관계",
    "href": "2023_DV/Review/4. Plotnine.html#아이스크림을-많이-먹으면-걸리는-병---인과관계와-상관관계",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "7. 아이스크림을 많이 먹으면 걸리는 병 - 인과관계와 상관관계",
    "text": "7. 아이스크림을 많이 먹으면 걸리는 병 - 인과관계와 상관관계"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#a.-교회의-수와-범죄-아이스크림과-소아마비",
    "href": "2023_DV/Review/4. Plotnine.html#a.-교회의-수와-범죄-아이스크림과-소아마비",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### A. 교회의 수와 범죄, 아이스크림과 소아마비",
    "text": "### A. 교회의 수와 범죄, 아이스크림과 소아마비\n\n\n\n\n교회의 개수\n범죄건수\n\n\n\n\n전주\n100\n20\n\n\n부산\n1000\n200\n\n\n서울\n5000\n1000\n\n\n\n\n결론(?) : 교회가 많을 수록 범죄도 많아진다???\n\n\n배경없이 숫자만 비교할 경우, 상관관계를 인과관계로 착각할 수도 있다.\n인구에 대한 인과를 착각\n\n- 내용요약\n\n여름 → 수영장 → 소아마비\n여름 → 아이스크림\n아이스크림과 소아마비는 상관관계가 높다. 따라서 아이스크림 성분 중에서 소아마비를 유발하는 유해물질이 있을 것이다(?)\n\n\n다른 변인을 통제하고(인구가 동일한 지역), 비교하려는 대상만 차이를 부여해야 한다."
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#b.-기상자료",
    "href": "2023_DV/Review/4. Plotnine.html#b.-기상자료",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 기상자료",
    "text": "### B. 기상자료\n- 기상자료 다운로드\n\ntemp=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()\n## 판다스 데이터의 4번째 열만 가져와 numpy.array로 만든다.\n\n\nplt.plot(temp)    ## 이럴 때는 ggplot보다 matplotlib가 훨씬 편하다.\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#c.-숨은-진짜-상황-1-온도---아이스크림-판매량",
    "href": "2023_DV/Review/4. Plotnine.html#c.-숨은-진짜-상황-1-온도---아이스크림-판매량",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### C. 숨은 진짜 상황 1 : 온도 -> 아이스크림 판매량",
    "text": "### C. 숨은 진짜 상황 1 : 온도 -&gt; 아이스크림 판매량\n-아래와 같은 관계를 가정하자. \\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\text{오차}\\]\n\nnp.random.seed(1)   ## 결과가 같도록 시드 설정\nicecream = 20 + 2 * temp + np.random.randn(len(temp))*10  ## N(0, 10^2)\nplt.plot(temp, icecream, 'o', alpha = 0.5)\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#d.-숨은-진짜-상황-2-온도---소아마비-반응수치",
    "href": "2023_DV/Review/4. Plotnine.html#d.-숨은-진짜-상황-2-온도---소아마비-반응수치",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### D. 숨은 진짜 상황 2 : 온도 -> 소아마비 반응수치",
    "text": "### D. 숨은 진짜 상황 2 : 온도 -&gt; 소아마비 반응수치\n- 아래와 같은 관계를 가정하자. \\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\text{오차}\\]\n\nnp.random.seed(2)\n\ndisease = 30 + 0.5 * temp + np.random.randn(len(temp))*1  ## N(0,1)\nplt.plot(temp, disease, 'o', alpha = 0.5)\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#e.-우리가-관측한-상황온도는-은닉되어-있음",
    "href": "2023_DV/Review/4. Plotnine.html#e.-우리가-관측한-상황온도는-은닉되어-있음",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### E. 우리가 관측한 상황(온도는 은닉되어 있음)",
    "text": "### E. 우리가 관측한 상황(온도는 은닉되어 있음)\n\nplt.plot(icecream, disease, 'o', alpha=0.3)\nplt.show()\n\n\n\n\n\nnp.corrcoef(icecream,disease)\n\narray([[1.        , 0.86298975],\n       [0.86298975, 1.        ]])\n\n\n여름만 뽑아서 플랏한다면?\n\nplt.plot(icecream,disease,'o',alpha=0.3)\nplt.plot(icecream[temp&gt;25], disease[temp&gt;25],'o') ## 기온이 25도 이상, 즉, 여름(아마도)\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#f.-ggplot으로-온도구간을-세분화하여-시각화하자.",
    "href": "2023_DV/Review/4. Plotnine.html#f.-ggplot으로-온도구간을-세분화하여-시각화하자.",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### F. ggplot으로 온도구간을 세분화하여 시각화하자.",
    "text": "### F. ggplot으로 온도구간을 세분화하여 시각화하자.\n- 데이터를 데이터프레임으로\n\ndf = pd.DataFrame({'temp' : temp, 'ice' : icecream, 'dis' : disease})\ndf\n\n\n\n\n\n\n\n\ntemp\nice\ndis\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n\n\n1\n1.4\n16.682436\n30.643733\n\n\n2\n2.6\n19.918282\n29.163804\n\n\n3\n2.0\n13.270314\n32.640271\n\n\n4\n2.5\n33.654076\n29.456564\n\n\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n\n\n652\n20.4\n76.554679\n38.920443\n\n\n653\n18.3\n68.666079\n39.882650\n\n\n654\n12.8\n42.771364\n36.613159\n\n\n655\n6.7\n30.736731\n34.902513\n\n\n\n\n656 rows × 3 columns\n\n\n\n- 구간별로 나눈 변수를 추가 : pd.cut(df, bins = int)\n\ndf.assign(temp_cut = pd.cut(df.temp, bins = 5))   ## 온도를 4구간으로 분할한다\n\n\n\n\n\n\n\n\ntemp\nice\ndis\ntemp_cut\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n(-3.92, 4.56]\n\n\n1\n1.4\n16.682436\n30.643733\n(-3.92, 4.56]\n\n\n2\n2.6\n19.918282\n29.163804\n(-3.92, 4.56]\n\n\n3\n2.0\n13.270314\n32.640271\n(-3.92, 4.56]\n\n\n4\n2.5\n33.654076\n29.456564\n(-3.92, 4.56]\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n(13.04, 21.52]\n\n\n652\n20.4\n76.554679\n38.920443\n(13.04, 21.52]\n\n\n653\n18.3\n68.666079\n39.882650\n(13.04, 21.52]\n\n\n654\n12.8\n42.771364\n36.613159\n(4.56, 13.04]\n\n\n655\n6.7\n30.736731\n34.902513\n(4.56, 13.04]\n\n\n\n\n656 rows × 4 columns\n\n\n\n\ncut_df = df.assign(temp_cut = pd.cut(df.temp, bins = 7))\n\nfig = ggplot(cut_df)\npoint = geom_point(aes(x = 'ice', y = 'dis', color = 'temp_cut'), alpha = 0.2)\nsmooth = geom_smooth(aes(x = 'ice', y = 'dis', color = 'temp_cut'), linetype = 'dashed')\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n실제로 보니 상관관계가 없어보인다.\n\n\n진짜 아이스크림을 먹고 배탈이 났다면?\n\nnp.random.seed(1)\nicecream_sales = 30 + 2 * temp + np.random.randn(len(temp))*10\n\n\nnp.random.seed(2)\ndisease = 30 + 0 * temp + 0.15 * icecream + np.random.randn(len(temp))*1  ## temp, 온도가 미치는 영향을 제로로\n\n\ndf2 = pd.DataFrame({'temp' : temp, 'ice' : icecream_sales, 'dis' : disease})\ndf2.assign(temp_cut = pd.cut(df2.temp, bins = 7))\n\n\n\n\n\n\n\n\ntemp\nice\ndis\ntemp_cut\n\n\n\n\n0\n-0.5\n45.243454\n34.869760\n(-6.343, -0.286]\n\n\n1\n1.4\n26.682436\n32.446099\n(-0.286, 5.771]\n\n\n2\n2.6\n29.918282\n30.851546\n(-0.286, 5.771]\n\n\n3\n2.0\n23.270314\n33.630818\n(-0.286, 5.771]\n\n\n4\n2.5\n43.654076\n33.254676\n(-0.286, 5.771]\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n78.839992\n40.009905\n(17.886, 23.943]\n\n\n652\n20.4\n86.554679\n40.203645\n(17.886, 23.943]\n\n\n653\n18.3\n78.666079\n41.032562\n(17.886, 23.943]\n\n\n654\n12.8\n52.771364\n36.628863\n(11.829, 17.886]\n\n\n655\n6.7\n40.736731\n36.163023\n(5.771, 11.829]\n\n\n\n\n656 rows × 4 columns\n\n\n\n\nfig = ggplot(df2.assign(temp_cut = pd.cut(df2.temp,bins=7)))\npoint = geom_point(aes(x='ice',y='dis',color='temp_cut'),alpha=0.2)\nsmooth = geom_smooth(aes(x='ice',y='dis',color='temp_cut'),linetype='dashed')\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n무친 인과관계"
  },
  {
    "objectID": "2023_DV/Review/4. Plotnine.html#결론",
    "href": "2023_DV/Review/4. Plotnine.html#결론",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "8. 결론",
    "text": "8. 결론\n\n아이스크림 먹어도 소아마비 안걸려!\n\n\n온도라는 흑막(은닉변수)을 잘 찾았고, 결과적으로 온도 -&gt; 아이스크림 판매량 & 소아마비라는 합리적인 진리를 얻을 수 있었다.\n\n\n고려할 흑막이 온도뿐이라는 보장이 있나?\n\n\n이론적으로는 모든 은닉변수들을 통제하였을 경우에도 corr(X,Y)의 절댓값이 1에 가깝다면 그때는 인과성이 있음이라고 주장할 수 있다.(이 경우에도 둘 중 어느것이 원인인지 파악하는 것은 불가)\n즉, 모든 은닉변수를 제거하면 상관성 = 인과성이다.\n\n\n모든 흑막을 제거하는 건 사실상 불가능하지 않나?\n\n\n실험계획을 잘 하면 흑막을 제거한 효과가 있음(무작위 추출 등)\n인과추론 : 실험계획이 사실상 불가능한 경우가 있음 -&gt; 모인 데이터에서 최대한 흑막2ㆍ3ㆍ4ㆍㆍㆍ등이 비슷한 그룹끼리 “매칭”을 시킨 뒤, 그룹간 corr을 구하여 규명한다!\n\n\n데이터의 수가 방대해지면서 가능해졌다."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "",
    "text": "열 이름 변경, 열 추가, 리스트 컴프리헨션, 결측치 파악, query, 매핑 등등… 해당 내용은 왠만해선 다 알아두는 게 좋다."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#import",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#import",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "1. import",
    "text": "1. import\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-기본기능",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-기본기능",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "2. Pandas 기본기능",
    "text": "2. Pandas 기본기능\n\nA. 열의 이름 변경\n\n\ndf = pd.DataFrame(np.random.randn(3, 2))\ndf\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n-0.000655\n0.686701\n\n\n1\n0.591774\n0.842045\n\n\n2\n-0.027722\n-0.703161\n\n\n\n\n\n\n\n- 방법1 : df.columns에 대입\n\ndf.columns = ['A', 'B']\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n-0.000655\n0.686701\n\n\n1\n0.591774\n0.842045\n\n\n2\n-0.027722\n-0.703161\n\n\n\n\n\n\n\n- 방법2 : df.set_axis() \\(\\star\\star\\star\\)\n\ndf2 = pd.DataFrame(np.random.randn(5,3))\ndf2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.200618\n-0.567175\n-0.249051\n\n\n1\n0.805185\n-0.479624\n0.797904\n\n\n2\n-1.278647\n-0.061503\n1.048704\n\n\n3\n0.308626\n-3.294418\n0.326681\n\n\n4\n1.585979\n-1.200001\n0.386765\n\n\n\n\n\n\n\n\ndf2 = df2.set_axis(['A','B','C'], axis = 1)\ndf2\n\n#df2.set_axis(['a','b','c','d,',e'], axis = 0)으로 하면 인덱스가 바뀐다.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n0.200618\n-0.567175\n-0.249051\n\n\n1\n0.805185\n-0.479624\n0.797904\n\n\n2\n-1.278647\n-0.061503\n1.048704\n\n\n3\n0.308626\n-3.294418\n0.326681\n\n\n4\n1.585979\n-1.200001\n0.386765\n\n\n\n\n\n\n\n- 방법3 : df.rename()\n\ndf3 = pd.DataFrame(np.random.randn(5,3))\ndf3\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.202540\n0.265273\n1.855420\n\n\n1\n-0.422516\n-0.954117\n-0.050532\n\n\n2\n-0.010961\n-1.681503\n-1.613766\n\n\n3\n0.855199\n0.773191\n1.149413\n\n\n4\n0.310184\n-0.063591\n-0.572836\n\n\n\n\n\n\n\n\ndf3.rename({0 : 'A'}, axis = 1) ## dictionary 형태로 지정, 특정 열만 바꿈\n##df3.rename(columns = {0 : 'A'})와 동일\n\n\n\n\n\n\n\n\nA\n1\n2\n\n\n\n\n0\n0.202540\n0.265273\n1.855420\n\n\n1\n-0.422516\n-0.954117\n-0.050532\n\n\n2\n-0.010961\n-1.681503\n-1.613766\n\n\n3\n0.855199\n0.773191\n1.149413\n\n\n4\n0.310184\n-0.063591\n-0.572836"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-행의-이름-변경",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-행의-이름-변경",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. 행의 이름 변경",
    "text": "### B. 행의 이름 변경\n- 방법1 : df.index에 대입\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.108275\n-0.802206\n-3.011323\n\n\n1\n-1.437775\n-1.868590\n-0.079212\n\n\n\n\n\n\n\n\ndf.index = ['a', 'b']\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n0.108275\n-0.802206\n-3.011323\n\n\nb\n-1.437775\n-1.868590\n-0.079212\n\n\n\n\n\n\n\n- 방법2 : df.set_axis() \\(\\star\\star\\star\\)\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-0.179379\n0.684650\n1.678079\n\n\n1\n0.487614\n-1.358992\n-0.661587\n\n\n\n\n\n\n\n\ndf.set_axis(['1','2'], axis = 0)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n1\n-0.179379\n0.684650\n1.678079\n\n\n2\n0.487614\n-1.358992\n-0.661587\n\n\n\n\n\n\n\n- 방법3 : df.rename()\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-0.051285\n1.185885\n0.841335\n\n\n1\n0.118555\n1.527457\n0.544870\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({0 : 'A'}, axis = 0)    ## default = 0\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nA\n-0.051285\n1.185885\n0.841335\n\n\n1\n0.118555\n1.527457\n0.544870\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 방법 4 : df.set_index() &gt; 임의의 열을 행이름으로 지정, 이미 있던 열 하나를 인덱스로 잡고 싶을 시 사용\n\ndf = pd.DataFrame({'id':['2020-43052','2021-43053'], 'X1':[1,2],'X2':[2,3]})\ndf\n\n\n\n\n\n\n\n\nid\nX1\nX2\n\n\n\n\n0\n2020-43052\n1\n2\n\n\n1\n2021-43053\n2\n3\n\n\n\n\n\n\n\n\ndf.set_index('id')\n\n\n\n\n\n\n\n\nX1\nX2\n\n\nid\n\n\n\n\n\n\n2020-43052\n1\n2\n\n\n2021-43053\n2\n3\n\n\n\n\n\n\n\n\n# A~B에 대한 연습문제\n\n- 데이터 load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n# 예제1 : 열의 이름을 출력하고, 열의 이름중 공백()이 있을 경우 언더바(_) 로 바꾸자.\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n- 방법1 : df.columns에 직접 대입\n\ndf_ = df\ndf_.columns = [i.replace(' ', '_') for i in df_.columns]\n\n\ndf_.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n- 방법2 : set_axis() 이용 \\(\\star\\star\\star\\)\n\ndf_ = df\ndf_.set_axis([col.replace(' ', '_') for col in df_.columns], axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n- 방법 3 : rename() 이용(안중요함)\n\ntemp3 = df\n\ndic = {i:i.replace(' ','_') for i in df.columns}\ntemp3.rename(dic, axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n# 예제2: ID를 row-index로 지정하라.\n\ndf.ID\n\n0        209658\n1        212198\n2        224334\n3        192985\n4        224232\n          ...  \n17655    269526\n17656    267946\n17657    270567\n17658    256624\n17659    256376\nName: ID, Length: 17660, dtype: int64\n\n\n- 방법1 : 직접지정\n\ndf_ = df\ndf_.index = df.ID\ndf_.index\n\nIndex([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961, 208333,\n       210514,\n       ...\n       256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567, 256624,\n       256376],\n      dtype='int64', name='ID', length=17660)\n\n\n- 방법2 : set_axis() \\(\\star\\star\\star\\)\n\ndf_ = df\ndf_ = df_.set_axis(df.ID)   ## default : axis = 0, df_.set_axis(df.ID, axis = 0)과 동일\ndf_.index\n\nInt64Index([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961,\n            208333, 210514,\n            ...\n            256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567,\n            256624, 256376],\n           dtype='int64', name='ID', length=17660)\n\n\n- 방법3 : set_index()\n\n이 경우 해당 열을 나중에 따로 드랍하지 않아도 됨\n\n\ndf_ = df\ndf_ = df_.set_index('ID')\ndf_.index\n\nIndex([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961, 208333,\n       210514,\n       ...\n       256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567, 256624,\n       256376],\n      dtype='int64', name='ID', length=17660)"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#c.-df.t-데이터프레임을-전치",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#c.-df.t-데이터프레임을-전치",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. df.T | 데이터프레임을 전치",
    "text": "### C. df.T | 데이터프레임을 전치\ndf.T를 이용하여 데이터를 살피면 편리함\n- data load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n- df.T : 데이터프레임을 전치(transition)한다.\n\ndf.T.loc[:,:3]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nID\n209658\n212198\n224334\n192985\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\nK. De Bruyne\n\n\nAge\n27\n27\n30\n31\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nhttps://cdn.sofifa.net/players/192/985/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\nBelgium\n\n\nFlag\nhttps://cdn.sofifa.net/flags/de.png\nhttps://cdn.sofifa.net/flags/pt.png\nhttps://cdn.sofifa.net/flags/ar.png\nhttps://cdn.sofifa.net/flags/be.png\n\n\nOverall\n87\n86\n85\n91\n\n\nPotential\n88\n87\n85\n91\n\n\nClub\nFC Bayern München\nManchester United\nSevilla FC\nManchester City\n\n\nClub Logo\nhttps://cdn.sofifa.net/teams/21/30.png\nhttps://cdn.sofifa.net/teams/11/30.png\nhttps://cdn.sofifa.net/teams/481/30.png\nhttps://cdn.sofifa.net/teams/10/30.png\n\n\nValue\n€91M\n€78.5M\n€46.5M\n€107.5M\n\n\nWage\n€115K\n€190K\n€46K\n€350K\n\n\nSpecial\n2312\n2305\n2303\n2303\n\n\nPreferred Foot\nRight\nRight\nLeft\nRight\n\n\nInternational Reputation\n4.0\n3.0\n2.0\n4.0\n\n\nWeak Foot\n4.0\n3.0\n3.0\n5.0\n\n\nSkill Moves\n3.0\n4.0\n3.0\n4.0\n\n\nWork Rate\nHigh/ Medium\nHigh/ High\nHigh/ High\nHigh/ High\n\n\nBody Type\nUnique\nUnique\nStocky (170-185)\nUnique\n\n\nReal Face\nYes\nYes\nNo\nYes\n\n\nPosition\n&lt;span class=\"pos pos28\"&gt;SUB\n&lt;span class=\"pos pos15\"&gt;LCM\n&lt;span class=\"pos pos7\"&gt;LB\n&lt;span class=\"pos pos13\"&gt;RCM\n\n\nJoined\nJul 1, 2018\nJan 30, 2020\nSep 14, 2020\nAug 30, 2015\n\n\nLoaned From\nNaN\nNaN\nNaN\nNaN\n\n\nContract Valid Until\n2026\n2026\n2024\n2025\n\n\nHeight\n189cm\n179cm\n172cm\n181cm\n\n\nWeight\n82kg\n69kg\n69kg\n70kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n€198.9M\n\n\nKit Number\n8.0\n8.0\n19.0\n17.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n- 출력옵션 조정\n\npd.options.display.max_rows = 10\ndisplay(df.T.iloc[:, :3])\npd.reset_option('display.max_rows')   ## 디폴트 옵션으로 변경\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\n209658\n212198\n224334\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\n\n\nAge\n27\n27\n30\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\n\n\n...\n...\n...\n...\n\n\nHeight\n189cm\n179cm\n172cm\n\n\nWeight\n82kg\n69kg\n69kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n\n\nKit Number\n8.0\n8.0\n19.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\n\n\n\n\n29 rows × 3 columns\n\n\n\n\n여기선 설명을 위해 줄이는 옵션을 사용했지만, 보통은 늘려서 사용함."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#d.-df.dtypes-sdtype-데이터의-타입-산출",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#d.-df.dtypes-sdtype-데이터의-타입-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### D. df.dtypes, s,dtype | 데이터의 타입 산출",
    "text": "### D. df.dtypes, s,dtype | 데이터의 타입 산출\n- df.dtypes\n\n데이터프레임 각 열에 저장된 데이터들의 타입을 알려준다.\n\n\ndf.dtypes\n\nID                            int64\nName                         object\nAge                           int64\nPhoto                        object\nNationality                  object\nFlag                         object\nOverall                       int64\nPotential                     int64\nClub                         object\nClub Logo                    object\nValue                        object\nWage                         object\nSpecial                       int64\nPreferred Foot               object\nInternational Reputation    float64\nWeak Foot                   float64\nSkill Moves                 float64\nWork Rate                    object\nBody Type                    object\nReal Face                    object\nPosition                     object\nJoined                       object\nLoaned From                  object\nContract Valid Until         object\nHeight                       object\nWeight                       object\nRelease Clause               object\nKit Number                  float64\nBest Overall Rating          object\ndtype: object\n\n\n\nobject : string이라고 생각해도 무방. 범주형 자료.\n\n- s.dtype Series에 붙여 사용\n\ndf.Name.dtype   ## 한 행의 데이터 타입만을 산출\n\ndtype('O')\n\n\n\n다양한 활용이 가능\n\n\ndf.Name.dtype == np.object_\n\nTrue\n\n\n\ndf.Age.dtype == np.int64\n\nTrue\n\n\n\ndf['International Reputation'].dtype == np.float64\n\nTrue\n\n\n\nbool을 산출하니까 컴프리헨션에 조건문 걸어서 해도 되고… 활용의 여지가 넓다.\n\n# 예제: df에서 int64 자료형만 출력\n- 풀이 1 : 표를 보고 직접 뽑음\n\npd.Series(list(df.dtypes))\n\n0       int64\n1      object\n2       int64\n3      object\n4      object\n5      object\n6       int64\n7       int64\n8      object\n9      object\n10     object\n11     object\n12      int64\n13     object\n14    float64\n15    float64\n16    float64\n17     object\n18     object\n19     object\n20     object\n21     object\n22     object\n23     object\n24     object\n25     object\n26     object\n27    float64\n28     object\ndtype: object\n\n\n\ndf.iloc[:, [0,2,6,7,12]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns\n\n\n\n- 풀이 2 : 리스트 컴프리핸션 이용\n\ndf.loc[:, [o == np.int64 for o in df.dtypes]]\n##df.loc[:, [o == 'int64' for o in df.dtypes]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#e.-df.sort_values-데이터들을-정렬",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#e.-df.sort_values-데이터들을-정렬",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### E. df.sort_values() | 데이터들을 정렬",
    "text": "### E. df.sort_values() | 데이터들을 정렬\n- 예시1 : 순서대로 정렬\n\ndf.sort_values('Age')   ## 나이가 어린 순서대로 오름차순 정렬 / 인덱스 개판\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n13712\n271072\nE. Topcu\n16\nhttps://cdn.sofifa.net/players/271/072/23_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.net/flags/ie.png\n48\n58\nDrogheda United\nhttps://cdn.sofifa.net/teams/1572/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 8, 2022\nNaN\n2022\n183cm\n65kg\n€175K\n20.0\nNaN\n\n\n13078\n259442\n22 R. van den Berg\n16\nhttps://cdn.sofifa.net/players/259/442/22_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n60\n81\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 24, 2020\nNaN\n2024\n190cm\n73kg\n€1.8M\n33.0\nNaN\n\n\n11257\n266205\n22 Y. Koré\n16\nhttps://cdn.sofifa.net/players/266/205/22_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n59\n74\nParis FC\nhttps://cdn.sofifa.net/teams/111817/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nAug 11, 2022\nNaN\n2025\n187cm\n75kg\n€1.1M\n34.0\nNaN\n\n\n11278\n261873\n21 H. Kumagai\n16\nhttps://cdn.sofifa.net/players/261/873/21_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n52\n70\nVegalta Sendai\nhttps://cdn.sofifa.net/teams/112836/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 16, 2021\nNaN\n2023\n174cm\n64kg\n€375K\n48.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시 2 : 내림차순으로 정렬\n\ndf.sort_values('Age', ascending = False)  ## default : ascending = True\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17360\n261023\n21 H. Broun\n16\nhttps://cdn.sofifa.net/players/261/023/21_60.png\nScotland\nhttps://cdn.sofifa.net/flags/gb-sct.png\n52\n72\nKilmarnock\nhttps://cdn.sofifa.net/teams/82/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 17, 2020\nNaN\n2022\n182cm\n70kg\n€523K\n40.0\nNaN\n\n\n15536\n263639\n22 M. Pavel\n16\nhttps://cdn.sofifa.net/players/263/639/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n51\n69\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2021\nNaN\n2023\n178cm\n66kg\n€277K\n77.0\nNaN\n\n\n11398\n256405\n21 W. Essanoussi\n16\nhttps://cdn.sofifa.net/players/256/405/21_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n59\n75\nVVV-Venlo\nhttps://cdn.sofifa.net/teams/100651/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2019\nNaN\n2022\n178cm\n70kg\n€1.1M\n24.0\nNaN\n\n\n15030\n270594\nT. Walczak\n16\nhttps://cdn.sofifa.net/players/270/594/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n54\n68\nWisła Płock\nhttps://cdn.sofifa.net/teams/1569/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 7, 2021\nNaN\n2023\n191cm\n88kg\n€494K\n99.0\nNaN\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시 3 : 능력치가 좋은 순서대로 정렬\n\ndf.sort_values(by = 'Overall', ascending = False)    ## 수가 높을수록 위로 가니까, by 생략해도 됨.\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n41\n188545\nR. Lewandowski\n33\nhttps://cdn.sofifa.net/players/188/545/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n91\n91\nFC Barcelona\nhttps://cdn.sofifa.net/teams/241/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 18, 2022\nNaN\n2025\n185cm\n81kg\n€172.2M\n9.0\nNaN\n\n\n124\n165153\nK. Benzema\n34\nhttps://cdn.sofifa.net/players/165/153/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n91\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos21\"&gt;CF\nJul 9, 2009\nNaN\n2023\n185cm\n81kg\n€131.2M\n9.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n56\n158023\nL. Messi\n35\nhttps://cdn.sofifa.net/players/158/023/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n91\n91\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos23\"&gt;RW\nAug 10, 2021\nNaN\n2023\n169cm\n67kg\n€99.9M\n30.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15513\n266751\n22 Jung Ho Yeon\n20\nhttps://cdn.sofifa.net/players/266/751/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n45\n53\nGwangJu FC\nhttps://cdn.sofifa.net/teams/112258/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 20, 2022\nNaN\n2026\n180cm\n73kg\n€145K\n23.0\nNaN\n\n\n16215\n268279\n22 J. Looschen\n24\nhttps://cdn.sofifa.net/players/268/279/22_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n44\n47\nSV Meppen\nhttps://cdn.sofifa.net/teams/110597/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMar 19, 2022\nNaN\n2026\n178cm\n78kg\n€92K\n42.0\nNaN\n\n\n16042\n255283\n20 Kim Yeong Geun\n22\nhttps://cdn.sofifa.net/players/255/283/20_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n44\n49\nGyeongnam FC\nhttps://cdn.sofifa.net/teams/111588/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 9, 2020\nNaN\n2020\n174cm\n71kg\n€53K\n43.0\nNaN\n\n\n14634\n269038\n22 Zhang Wenxuan\n16\nhttps://cdn.sofifa.net/players/269/038/22_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n44\n59\nGuangzhou FC\nhttps://cdn.sofifa.net/teams/111839/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 1, 2022\nNaN\n2022\n175cm\n70kg\n€239K\n29.0\nNaN\n\n\n17618\n168933\n07 I. Paskov\n33\nhttps://cdn.sofifa.net/players/168/933/07_60.png\nBulgaria\nhttps://cdn.sofifa.net/flags/bg.png\n43\n42\nNaN\nhttps://cdn.sofifa.net/flags/bg.png\n...\nNaN\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n184cm\n79kg\nNaN\n24.0\nNaN\n\n\n\n\n\n17660 rows × 29 columns"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#f.-df.info-행별-information-산출",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#f.-df.info-행별-information-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### F. df.info() | 행별 information 산출",
    "text": "### F. df.info() | 행별 information 산출\n시험에는 절대 안 낼 거지만 매우 중요한 것\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17660 entries, 0 to 17659\nData columns (total 29 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   ID                        17660 non-null  int64  \n 1   Name                      17660 non-null  object \n 2   Age                       17660 non-null  int64  \n 3   Photo                     17660 non-null  object \n 4   Nationality               17660 non-null  object \n 5   Flag                      17660 non-null  object \n 6   Overall                   17660 non-null  int64  \n 7   Potential                 17660 non-null  int64  \n 8   Club                      17449 non-null  object \n 9   Club Logo                 17660 non-null  object \n 10  Value                     17660 non-null  object \n 11  Wage                      17660 non-null  object \n 12  Special                   17660 non-null  int64  \n 13  Preferred Foot            17660 non-null  object \n 14  International Reputation  17660 non-null  float64\n 15  Weak Foot                 17660 non-null  float64\n 16  Skill Moves               17660 non-null  float64\n 17  Work Rate                 17660 non-null  object \n 18  Body Type                 17622 non-null  object \n 19  Real Face                 17622 non-null  object \n 20  Position                  17625 non-null  object \n 21  Joined                    16562 non-null  object \n 22  Loaned From               694 non-null    object \n 23  Contract Valid Until      17299 non-null  object \n 24  Height                    17660 non-null  object \n 25  Weight                    17660 non-null  object \n 26  Release Clause            16509 non-null  object \n 27  Kit Number                17625 non-null  float64\n 28  Best Overall Rating       21 non-null     object \ndtypes: float64(4), int64(5), object(20)\nmemory usage: 3.9+ MB\n\n\n\ndata들의 현황을 한눈에 파악하기 좋다.\n\n\ndf.iloc[:, [28]].sort_values('Best Overall Rating')\n\n\n  \n    \n\n\n\n\n\n\nBest Overall Rating\n\n\n\n\n13299\n&lt;span class=\"bp3-tag p p-54\"&gt;54&lt;/span&gt;\n\n\n14366\n&lt;span class=\"bp3-tag p p-56\"&gt;56&lt;/span&gt;\n\n\n16779\n&lt;span class=\"bp3-tag p p-58\"&gt;58&lt;/span&gt;\n\n\n16968\n&lt;span class=\"bp3-tag p p-58\"&gt;58&lt;/span&gt;\n\n\n16835\n&lt;span class=\"bp3-tag p p-59\"&gt;59&lt;/span&gt;\n\n\n...\n...\n\n\n17655\nNaN\n\n\n17656\nNaN\n\n\n17657\nNaN\n\n\n17658\nNaN\n\n\n17659\nNaN\n\n\n\n\n\n17660 rows × 1 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[:, ['Best Overall Rating']].isna().sum()\n\nBest Overall Rating    17639\ndtype: int64\n\n\n\n결측치가 매우 많은 것을 볼 수 있다."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#g.-df.isna-각-원소가-결측치인지-아닌지-산출",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#g.-df.isna-각-원소가-결측치인지-아닌지-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### G. df.isna()| 각 원소가 결측치인지 아닌지 산출",
    "text": "### G. df.isna()| 각 원소가 결측치인지 아닌지 산출\n- 예시 1 : 열별로 결측치 카운트\n\ndf.isna()   ## NaN 값이 있다면 True 산출\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17656\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17657\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17658\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17659\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\ndf.isna().sum(axis = 0)   ## default : axis = 0\n\nID                              0\nName                            0\nAge                             0\nPhoto                           0\nNationality                     0\nFlag                            0\nOverall                         0\nPotential                       0\nClub                          211\nClub Logo                       0\nValue                           0\nWage                            0\nSpecial                         0\nPreferred Foot                  0\nInternational Reputation        0\nWeak Foot                       0\nSkill Moves                     0\nWork Rate                       0\nBody Type                      38\nReal Face                      38\nPosition                       35\nJoined                       1098\nLoaned From                 16966\nContract Valid Until          361\nHeight                          0\nWeight                          0\nRelease Clause               1151\nKit Number                     35\nBest Overall Rating         17639\ndtype: int64\n\n\n\narr = np.array([(True, False), (True, False), (False, True)])\narr\n\narray([[ True, False],\n       [ True, False],\n       [False,  True]])\n\n\n\narr.shape\n\n(3, 2)\n\n\n\narr.sum(axis = 0) ## 열별로 합\n\narray([2, 1])\n\n\n\narr.sum(axis = 1)   ## 행별로 합\n\narray([1, 1, 1])\n\n\n- 예시2 : 결측치가 50% 이상인 열 출력\n\ntype(df.isna().mean() &gt; 0.5)  ## 이 값 자체가 시리즈이므로 리스트로 넣으면 안된다.\n\npandas.core.series.Series\n\n\n\ndf.loc[:, df.isna().mean() &gt; 0.5]  ## [df.isna().mean() &gt; 0.5]는 에러뜸\n\n\n\n\n\n\n\n\nLoaned From\nBest Overall Rating\n\n\n\n\n0\nNaN\nNaN\n\n\n1\nNaN\nNaN\n\n\n2\nNaN\nNaN\n\n\n3\nNaN\nNaN\n\n\n4\nNaN\nNaN\n\n\n...\n...\n...\n\n\n17655\nNaN\nNaN\n\n\n17656\nNaN\nNaN\n\n\n17657\nNaN\nNaN\n\n\n17658\nNaN\nNaN\n\n\n17659\nNaN\nNaN\n\n\n\n\n17660 rows × 2 columns"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#h.-df.drop-특정-행이나-열을-drop",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#h.-df.drop-특정-행이나-열을-drop",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### H. df.drop() | 특정 행이나 열을 drop",
    "text": "### H. df.drop() | 특정 행이나 열을 drop\n- 예시 1 : [0,1,2,3] 행을 drop\n\ndf.drop([0,1,2,3])\n## df.drop([0,1,2,3], axis = 0)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n5\n212622\nJ. Kimmich\n27\nhttps://cdn.sofifa.net/players/212/622/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n89\n90\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos9\"&gt;RDM\nJul 1, 2015\nNaN\n2025\n177cm\n75kg\n€182M\n6.0\nNaN\n\n\n6\n197445\nD. Alaba\n30\nhttps://cdn.sofifa.net/players/197/445/23_60.png\nAustria\nhttps://cdn.sofifa.net/flags/at.png\n86\n86\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos6\"&gt;LCB\nJul 1, 2021\nNaN\n2026\n180cm\n78kg\n€113.8M\n4.0\nNaN\n\n\n7\n187961\n22 Paulinho\n32\nhttps://cdn.sofifa.net/players/187/961/22_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n83\n83\nAl Ahli\nhttps://cdn.sofifa.net/teams/112387/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJul 22, 2021\nNaN\n2024\n183cm\n80kg\n€48.5M\n15.0\nNaN\n\n\n8\n208333\nE. Can\n28\nhttps://cdn.sofifa.net/players/208/333/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n82\n82\nBorussia Dortmund\nhttps://cdn.sofifa.net/teams/22/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nFeb 18, 2020\nNaN\n2024\n186cm\n86kg\n€51.9M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17656 rows × 29 columns\n\n\n\n- 예시 2 : ['Name', 'Age']열을 drop\n\ndf.drop(columns = ['Name', 'Age'])\n## df.drop(['Name', 'Age'], axis = 1)\n\n\n  \n    \n\n\n\n\n\n\nID\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\nValue\nWage\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n€91M\n€115K\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n€78.5M\n€190K\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n€46.5M\n€46K\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n€107.5M\n€350K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n€89.5M\n€110K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n€70K\n€2K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n\n17660 rows × 27 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n결국에 axis 옵션만 기억하면 다른 parameter를 기억하지 않아도 된다.\n\n\n# G~H 에 대한 연습문제\n# 예제: 결측치가 50퍼 이상인 열을 제외하라.\n- 풀이 1 : 무지성 직접 제외\n\ndf.isna().mean()  ## := df.isna().sum() / len(df). Series\n\nID                          0.000000\nName                        0.000000\nAge                         0.000000\nPhoto                       0.000000\nNationality                 0.000000\nFlag                        0.000000\nOverall                     0.000000\nPotential                   0.000000\nClub                        0.011948\nClub Logo                   0.000000\nValue                       0.000000\nWage                        0.000000\nSpecial                     0.000000\nPreferred Foot              0.000000\nInternational Reputation    0.000000\nWeak Foot                   0.000000\nSkill Moves                 0.000000\nWork Rate                   0.000000\nBody Type                   0.002152\nReal Face                   0.002152\nPosition                    0.001982\nJoined                      0.062174\nLoaned From                 0.960702\nContract Valid Until        0.020442\nHeight                      0.000000\nWeight                      0.000000\nRelease Clause              0.065176\nKit Number                  0.001982\nBest Overall Rating         0.998811\ndtype: float64\n\n\n\ndf.drop(columns=['Loaned From','Best Overall Rating'])\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns\n\n\n\n- 풀이 2 : 이성적인 풀이\n\ndf.loc[:, df.isna().mean() &lt; 0.5]  ## 시리즈니까 리스트로 묶지 말것!!\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-missing",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-missing",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "3. Pandas : missing",
    "text": "3. Pandas : missing"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-numpy",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-numpy",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. Numpy",
    "text": "### A. Numpy\n- 발생 : np.nan\n\nnp.nan\n\nnan\n\n\n\narr = np.array([1,2,3,np.nan])\narr\n\narray([ 1.,  2.,  3., nan])\n\n\n\narr.mean()\n\nnan\n\n\n\nprint(type(np.nan))  ## np.nan 자체는 일종의 float로 취급된다.\nprint(type(arr[0]))\n\n&lt;class 'float'&gt;\n&lt;class 'numpy.float64'&gt;"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-pandas",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-pandas",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. Pandas",
    "text": "### B. Pandas\n- 발생 : np.nan, pd.NA\n\npd.Series([np.nan,1,2,3])\n\n0    NaN\n1    1.0\n2    2.0\n3    3.0\ndtype: float64\n\n\n\npd.Series([pd.NA, 1, 2, 3])\n\n0    &lt;NA&gt;\n1       1\n2       2\n3       3\ndtype: object\n\n\n\n위 두 개의 코드는 동일하다고 봐도 무방하다.\n\n- pd.Serise에 NaN 또는 &lt;NA&gt;가 있다면 연산할 때 제외함\n\nprint(f'np.nan을 넣은 시리즈의 평균 : {pd.Series([np.nan,1,2,3]).mean()} = pd.NA를 넣은 시리즈의 평균 : {pd.Series([pd.NA,1,2,3]).mean()}')\n\nnp.nan을 넣은 시리즈의 평균 : 2.0 = pd.NA를 넣은 시리즈의 평균 : 2.0\n\n\n\ns1 = pd.Series([np.nan,1,2,3])\ntype(s1[0])\n\nnumpy.float64\n\n\n\ns2 = pd.Series([pd.NA, 1,2,3])\ntype(s2[0])\n\npandas._libs.missing.NAType\n\n\n\nmissing은 그냥 NaN이라고 보자.\n\n- 검출(\\(\\star\\))(중요한가?)\n\ns1.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\ns2.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\npd.isna(s1[0]), pd.isnull(s1[0])  ## 결측치가 있느냐?\n\n(True, True)\n\n\n\npd.isna(s2[0]), pd.isnull(s2[0])  ## 결측치가 있느냐?\n\n(True, True)\n\n\n\nid(pd.isna), id(pd.isnull) # 같은함수\n\n(135146401797248, 135146401797248)\n\n\n\nid를 찍었을 때 같다면 같은 함수이다."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-query",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "5. Pandas : query",
    "text": "5. Pandas : query\n\n개 간단하고 쉽지만 고점은 낮은 데이터 처리방식\n\n\nts = pd.DataFrame(np.random.normal(size=(20,4)),columns=list('ABCD'),index=pd.date_range('20221226',periods=20)).assign(E=['A']*10+['B']*10)\nts\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-02\n-1.136712\n0.595607\n-1.938775\n0.201931\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-06\n-0.095612\n-0.692796\n0.456627\n-0.395918\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-08\n-0.939328\n0.745621\n0.632724\n0.032088\nB\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n2023-01-10\n-0.339565\n-0.460976\n0.320097\n0.482333\nB\n\n\n2023-01-11\n-0.117493\n-1.964531\n-1.867120\n2.325713\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB\n\n\n2023-01-14\n-1.069801\n-0.659982\n-0.368828\n1.286645\nB"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-기본-query",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-기본-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. 기본 query",
    "text": "### A. 기본 query\n- 예시1: A&gt;0 and B&lt;0\n\nts.query('A&gt;0 and B&lt;0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n\n\n\n\n\n- 예시2: A&lt;B&lt;C\n\nts.query('A&lt;B&lt;C')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-14\n-1.069801\n-0.659982\n-0.368828\n1.286645\nB\n\n\n\n\n\n\n\n- 예시3: (A+B/2) &gt; 0\n\nts.query('(A+B)/2 &gt; 0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB\n\n\n\n\n\n\n\n- 예시4: (A+B/2) &gt; 0 and E=='A'\n\nts.query('(A+B)/2 &gt; 0 and E == \"A\"')   ## string 조건을 넣어주고 싶으면 다른 따옴표로 구분하면 된다.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n\n\n\n\n\n\n그냥 스트링으로 된 것들 중에는 생각헀던 건 왠만해선 다 된다."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-외부변수를-이용",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-외부변수를-이용",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. 외부변수를 이용",
    "text": "### B. 외부변수를 이용\n- 예시1: A &gt; mean(A)\n\nmean = ts.A.mean()\nmean\n\n0.14414224086779243\n\n\n\n#ts.query('A &gt; np.mean(A)')   ## 이건 안됨\n#ts.query('A &gt; A.mean()')      ## 이건 되긴 함\n\n#ts.query('A &gt; mean')    ## column 중 하나인지 뭔지 헷갈림, 그래서 안됨\n\nts.query('A &gt; @mean')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n\n\n\n\n\n\nA.mean()보다 작은 값들을 산출했다.\n\n\nvalue = np.percentile(ts.B, 77)  ## ts.B에서 77백분위수에 해당하는 숫자\nts.query('B &gt; @value')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-01-02\n-1.136712\n0.595607\n-1.938775\n0.201931\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-08\n-0.939328\n0.745621\n0.632724\n0.032088\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#c.-index로-query",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#c.-index로-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. Index로 query",
    "text": "### C. Index로 query\n- 예시: (2022년 12월30일 보다 이전 날짜) \\(\\cup\\) (2023년 1월10일)\n\nts.query('index &lt; \"2022-12-30\" or index == \"2023-01-10\"')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2023-01-10\n-0.339565\n-0.460976\n0.320097\n0.482333\nB"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#d.-열의-이름에-공백이-있을-경우",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#d.-열의-이름에-공백이-있을-경우",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### D. 열의 이름에 공백이 있을 경우",
    "text": "### D. 열의 이름에 공백이 있을 경우\n열의 이름에 공백이 있으면 백틱을 이용하면 된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n\ndf.query('`Skill Moves` &gt; 4')  ## `를 사용\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n19\n193082\nJ. Cuadrado\n34\nhttps://cdn.sofifa.net/players/193/082/23_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n83\n83\nJuventus\nhttps://cdn.sofifa.net/teams/45/30.png\n...\nYes\n&lt;span class=\"pos pos3\"&gt;RB\nJul 1, 2017\nNaN\n2023\n179cm\n72kg\n€23M\n11.0\nNaN\n\n\n27\n189509\nThiago\n31\nhttps://cdn.sofifa.net/players/189/509/23_60.png\nSpain\nhttps://cdn.sofifa.net/flags/es.png\n86\n86\nLiverpool\nhttps://cdn.sofifa.net/teams/9/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nSep 18, 2020\nNaN\n2024\n174cm\n70kg\n€102.7M\n6.0\nNaN\n\n\n44\n232411\nC. Nkunku\n24\nhttps://cdn.sofifa.net/players/232/411/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n86\n89\nRB Leipzig\nhttps://cdn.sofifa.net/teams/112172/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n175cm\n73kg\n€166.9M\n12.0\nNaN\n\n\n62\n233927\nLucas Paquetá\n24\nhttps://cdn.sofifa.net/players/233/927/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n82\n87\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nOct 1, 2020\nNaN\n2025\n180cm\n72kg\n€90.9M\n10.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4516\n253755\nTalles Magno\n20\nhttps://cdn.sofifa.net/players/253/755/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n71\n83\nNew York City FC\nhttps://cdn.sofifa.net/teams/112828/30.png\n...\nNo\n&lt;span class=\"pos pos16\"&gt;LM\nMay 18, 2021\nNaN\n2026\n186cm\n70kg\n€7.7M\n43.0\nNaN\n\n\n4643\n246548\nO. Sahraoui\n21\nhttps://cdn.sofifa.net/players/246/548/23_60.png\nNorway\nhttps://cdn.sofifa.net/flags/no.png\n67\n78\nVålerenga Fotball\nhttps://cdn.sofifa.net/teams/920/30.png\n...\nNo\n&lt;span class=\"pos pos27\"&gt;LW\nMay 15, 2019\nNaN\n2023\n170cm\n65kg\n€3.3M\n10.0\nNaN\n\n\n4872\n251570\nR. Cherki\n18\nhttps://cdn.sofifa.net/players/251/570/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n73\n88\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 7, 2019\nNaN\n2023\n176cm\n71kg\n€17.7M\n18.0\nNaN\n\n\n5361\n225712\nD. Bahamboula\n27\nhttps://cdn.sofifa.net/players/225/712/23_60.png\nCongo\nhttps://cdn.sofifa.net/flags/cg.png\n63\n63\nLivingston FC\nhttps://cdn.sofifa.net/teams/621/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 9, 2022\nNaN\n2024\n185cm\n70kg\n€875K\n7.0\nNaN\n\n\n10452\n212455\n17 H. Mastour\n18\nhttps://cdn.sofifa.net/players/212/455/17_60.png\nMorocco\nhttps://cdn.sofifa.net/flags/ma.png\n65\n76\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\n&lt;a href=\"/team/47/ac-milan/\"&gt;AC Milan&lt;/a&gt;\nJun 30, 2017\n175cm\n63kg\nNaN\n98.0\nNaN\n\n\n\n\n65 rows × 29 columns"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-할당",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-할당",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "5. Pandas : 할당",
    "text": "5. Pandas : 할당\n아래와 같은 자료를 조건에 맞게 가공해서 새로운 열을 추가해보자.\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\ndf = pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-df.assign",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-df.assign",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. df.assign()",
    "text": "### A. df.assign()\n- 예시: total = att*0.1 + rep*0.2 + mid*0.35 + fin*0.35 를 계산하여 할당\n\ndf.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35)\n## 원래 데이터 손상시키지 않음\n\ndf_total = df.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35)\ndf_total\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n49.00\n\n\n1\n95\n100\n50\n80\n75.00\n\n\n2\n65\n90\n60\n30\n56.00\n\n\n3\n55\n80\n75\n80\n75.75\n\n\n4\n80\n30\n30\n100\n59.50\n\n\n5\n75\n40\n100\n15\n55.75\n\n\n6\n65\n45\n45\n90\n62.75\n\n\n7\n60\n60\n25\n0\n26.75\n\n\n8\n95\n65\n20\n10\n33.00\n\n\n9\n90\n80\n80\n20\n60.00\n\n\n10\n55\n75\n35\n25\n41.50\n\n\n11\n95\n95\n45\n0\n44.25\n\n\n12\n95\n55\n15\n35\n38.00\n\n\n13\n50\n80\n40\n30\n45.50\n\n\n14\n50\n55\n15\n85\n51.00\n\n\n15\n95\n30\n30\n95\n59.25\n\n\n16\n50\n50\n45\n10\n34.25\n\n\n17\n65\n55\n15\n45\n38.50\n\n\n18\n70\n70\n40\n35\n47.25\n\n\n19\n90\n90\n80\n90\n86.50"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-df.eval",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-df.eval",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. df.eval()",
    "text": "### B. df.eval()\n- A에서와 동일한 할당\n\ndf.eval('total = att*0.1 + rep*0.2 + mid*0.3 + fin*0.4')    ## query를 쓰는 것처럼, 원본 데이터를 변화시키지 않음\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0\n\n\n\n\n\n\n\n\nbut, 사칙연산과 같은 기초연산 수준에서만 잘 작동한다."
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#c.-dfcolname-xxx",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#c.-dfcolname-xxx",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. df[colname] = xxx",
    "text": "### C. df[colname] = xxx\n\n별로 안쓰는 방법\n\n\ndf['total'] = df.att*0.1 + df.rep*0.2 + df.mid*0.3 + df.fin*0.4   ## 원래의 데이터프레임을 손상시킨다.\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-transform-columnstarstarstar",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#pandas-transform-columnstarstarstar",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "6. Pandas : transform column(\\(\\star\\star\\star\\))",
    "text": "6. Pandas : transform column(\\(\\star\\star\\star\\))"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-lambda",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#a.-lambda",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. lambda",
    "text": "### A. lambda\n- 예시1: \\(x \\to x+2\\)\n\n\"\"\"\ndef f(x) :\n  return x + 2\n\n해당 코드와 동일하다.\n\"\"\"\n\nf = lambda x: x+2\nprint(f(3))\n\nprint((lambda x : x+2)(3))    ## (lambda x : x+2) 자체가 함수이므로, 뒤에 변수만 지정해주면 된다.\n\n5\n5\n\n\n- 예시2: \\(x,y \\to x+y\\)\n\n(lambda x,y : x+y)(1,3)\n\n4\n\n\n- 예시3: ‘2023-09’ \\(\\to\\) 9 (format : int)\n\n(lambda x : int(x[-2:]))('2023-09')   ## -1번째(뒤에서 두번째 원소까지 추출)\n\n9\n\n\n- 예시4: ‘2023-09’ \\(\\to\\) (2023, 9) (format : tuple)\n\n(lambda x : (int(x[:4]), int(x[-2:])))('2023-09')\n\n(2023, 9)\n\n\n- 예시5: 문자열이 ‘cat’이면 1 ’dog’ 이면 0 // ’cat이면 1 ’cat’이 아니면 0\n\n(lambda x : 1 if x == 'cat' else 0)('cat')\n## (lambda x : pd.Series(x == 'cat').sum())('cat') ## 이것도 된다.\n\n1"
  },
  {
    "objectID": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-map",
    "href": "2023_DV/Review/6. Pandas, 데이터프레임 핸들링.html#b.-map",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. map",
    "text": "### B. map\n\n함수의 output들을 엮는다. 매핑하는 거\n\n:- 개념: map(f,[x1,x2,...xn])=[f(x1),f(x2),...,f(xn)]\n- 예시1: x-&gt;x+1을 [1,2,3]에 적용\n\nf = lambda x: x+1\nlist(map(f,[1,2,3]))\n\n[2, 3, 4]\n\n\n\nlist(map(lambda x : x + 1, [1,2,3]))\n\n[2, 3, 4]\n\n\n\n매핑하는 것 자체는 수나 리스트가 아니기 때문에 리스트로 엮어줘야 값을 알 수 있다.\n\n- 예시2 df.Height열 변환하기 (xxxcm 라고 적혀있는 것을 cm 없애고 키만 뽑기)\ns.str.replace('cm', '')\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height[:5]\ns\n\n0    189cm\n1    179cm\n2    172cm\n3    181cm\n4    172cm\nName: Height, dtype: object\n\n\n\nlist(map(lambda x : int(x[:-2]), s))\n\n[189, 179, 172, 181, 172]\n\n\n- 풀이 1 : map 이용\n\ndf.assign(Height = list(map(lambda x: int(x.replace('cm','')), df.Height)))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 풀이 2 : 사실 수틀리면 컴프리헨션 쓰면 된다.\n\ndf.assign(Height = [int(s.replace('cm', '')) for s in df.Height])\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n\n17660 rows × 29 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n# 예시4 – df.Position 열에 아래와 같은 변환을 수행하고, 변환된 열을 할당하라.\n\n\n\nbefore\nafter\n\n\n\n\n&lt;span class=\"pos pos28\"&gt;SUB\nSUB\n\n\n&lt;span class=\"pos pos15\"&gt;LCM\nLCM\n\n\n&lt;span class=\"pos pos7\"&gt;LB\nLB\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n\n- 풀이 1\n\n데이터를 불러와서…\n\n\nlist(map(lambda x : x.str.split('&gt;')[-1] if x.isna() == False else pd.NA, df.Position))\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\n\n\n저장된 꼬라지를 보면…\n\n\nx = df.Position[0]\nx\n\n'&lt;span class=\"pos pos28\"&gt;SUB'\n\n\n\n게다가 결측치까지 있네???\n\n\ndf.Position.isna().sum()\n\n35\n\n\n\n결측치 처리 + 데이터 변환\n\n\ndf.assign(Position = list(map(lambda x : x.split('&gt;')[-1] if not pd.isna(x) else pd.NA, df.Position))).Position\n\n0        SUB\n1        LCM\n2         LB\n3        RCM\n4        RCM\n        ... \n17655    RES\n17656    RES\n17657    RES\n17658    RES\n17659    RES\nName: Position, Length: 17660, dtype: object\n\n\n- (풀이2) – 수틀리면 리스트컴프리헨션\n\nf = lambda x: x.split(\"&gt;\")[-1] if not pd.isna(x) else pd.NA\n\n\ndf.assign(Position = [f(s) for s in df.Position]).Position\n\n0        SUB\n1        LCM\n2         LB\n3        RCM\n4        RCM\n        ... \n17655    RES\n17656    RES\n17657    RES\n17658    RES\n17659    RES\nName: Position, Length: 17660, dtype: object\n\n\n\nmapping하는 게 조금 더 자연스럽고 한번에 쓸 수 있다. ~(어차피 이미 람다로 함수 만들었잖아?)~"
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html",
    "href": "2023_DV/Review/8. 실습_FIFA23.html",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "",
    "text": "FIFA23 선수 데이터에서 결측치를 처리하고 여러 열을 가공하여 시각화해보자!"
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#라이브러리-import",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#라이브러리-import",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#학습할-코드",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#학습할-코드",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "2. 학습할 코드",
    "text": "2. 학습할 코드\n\nA. dropna()\n\n- 행에서 결측치가 하나라도 있으면 제거한다.\n\ndf = pd.DataFrame({\n    'A': [1,2,3,np.nan,5,6,7],\n    'B': [11,np.nan,33,np.nan,55,66,77], \n    'C': [111,222,333,np.nan,555,666,np.nan]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n1\n2.0\nNaN\n222.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n6\n7.0\n77.0\nNaN\n\n\n\n\n\n\n\n\n이러한 데이터가 있다고 할 때…\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n\n\n\n\n\n\n결측치가 하나라도 있는 행은 모두 드롭된다. (원본 데이터 손상 X)"
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#b.-_",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#b.-_",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### B. _",
    "text": "### B. _\n- 파이썬에서 가장 최근 콘솔에 띄워진 결과는 _로 불러올 수 있다.\n\na = [1,2,3]\na + [4] \n\n[1, 2, 3, 4]\n\n\n\n_\n\n[1, 2, 3, 4]\n\n\n\n_ + [5]\n\n[1, 2, 3, 4, 5]\n\n\n\n_.pop()  ## 마지막 요소를 리턴하고 그 요소는 삭제\n\n5\n\n\n\n_ + 1\n\n6\n\n\n\n리스트에서 숫자까지… _의 다사다난한 모험"
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#fifa23-시각화-문제",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#fifa23-시각화-문제",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "3. FIFA23 시각화 문제",
    "text": "3. FIFA23 시각화 문제\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n포지션별로 선수들의 능력치(ShotPower,SlidingTackle)와 급여(Wage)를 시각화하고 싶다. 아래의 세부지침에 맞추어 포지션별 ShotPower와 SlidingTackle의 산점도를 그려라.\n\ndf.Position\n\n0        &lt;span class=\"pos pos18\"&gt;CAM\n1        &lt;span class=\"pos pos11\"&gt;LDM\n2         &lt;span class=\"pos pos24\"&gt;RS\n3        &lt;span class=\"pos pos13\"&gt;RCM\n4          &lt;span class=\"pos pos7\"&gt;LB\n                    ...             \n16705    &lt;span class=\"pos pos29\"&gt;RES\n16706    &lt;span class=\"pos pos29\"&gt;RES\n16707    &lt;span class=\"pos pos29\"&gt;RES\n16708    &lt;span class=\"pos pos28\"&gt;SUB\n16709    &lt;span class=\"pos pos28\"&gt;SUB\nName: Position, Length: 16710, dtype: object\n\n\n세부지침\nA. Column의 이름에서 공백을 제거하라.\nB. 결측치가 50%이상인 컬럼을 찾고 이를 제거하라. 그 뒤에 .dropna()를 사용하여 결측치가 포함된 행을 제거하라.\nC. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\n\nD. df.Wage를 적절하게 변환하라.\nE. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.\n시각화 예시"
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#fifa23-시각화---풀이",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#fifa23-시각화---풀이",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "4. FIFA23 시각화 - 풀이",
    "text": "4. FIFA23 시각화 - 풀이\n\nA. Column의 이름에서 공백을 제거하라.\n\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Jersey Number', 'Joined', 'Loaned From', 'Contract Valid Until',\n       'Height', 'Weight', 'Crossing', 'Finishing', 'HeadingAccuracy',\n       'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy',\n       'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility',\n       'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength',\n       'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision',\n       'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle',\n       'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'Best Position', 'Best Overall Rating', 'Release Clause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.columns.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'JerseyNumber',\n       'Joined', 'LoanedFrom', 'ContractValidUntil', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'BestPosition', 'BestOverallRating', 'ReleaseClause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.set_axis(df.columns.str.replace(' ', ''), axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'JerseyNumber',\n       'Joined', 'LoanedFrom', 'ContractValidUntil', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'BestPosition', 'BestOverallRating', 'ReleaseClause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\n공백이 없어진 것을 확인할 수 있다.\n\n\n\nB. 결측치가 50%이상인 컬럼을 찾고 이를 제거하라. 그 뒤에 .dropna()를 사용하여 결측치가 포함된 행을 제거하라.\n\n\ndf_b = df.set_axis(df.columns.str.replace(' ', ''), axis = 1)\n\n\ndf_b.loc[:, df_b.isna().mean() &gt;= 0.5].columns\n\nIndex(['LoanedFrom', 'Marking'], dtype='object')\n\n\n\n위 두 개의 컬럼이 결측치가 50% 이상이다.\n\n\ndf_b.loc[:, df_b.isna().mean() &lt; 0.5].dropna()\n##df_b.loc[:, [s &lt; 0.5 for s in df_b.isna().mean()]].dropna()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n따라서 해당 조건에 반대를 슬라이싱하는 방식으로 해당 컬럼을 제거하였다."
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#c.-position_dict를-이용하여-df.position을-적절하게-변환하라.-변환된-값을-df.position에-저장하라.",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#c.-position_dict를-이용하여-df.position을-적절하게-변환하라.-변환된-값을-df.position에-저장하라.",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### C. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.",
    "text": "### C. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.\n\nposition_dict\n\n{'GOALKEEPER': {'GK'},\n 'DEFENDER': {'CB', 'LB', 'LCB', 'LWB', 'RB', 'RCB', 'RWB'},\n 'MIDFIELDER': {'CAM',\n  'CDM',\n  'CM',\n  'LAM',\n  'LCM',\n  'LDM',\n  'LM',\n  'RAM',\n  'RCM',\n  'RDM',\n  'RM'},\n 'FORWARD': {'CF', 'LF', 'LS', 'LW', 'RF', 'RS', 'RW', 'ST'},\n 'SUB': {'SUB'},\n 'RES': {'RES'}}\n\n\n\ndf_c = df_b.loc[:, df_b.isna().mean() &lt; 0.5].dropna()\ndf_c.Position\n\n0        &lt;span class=\"pos pos18\"&gt;CAM\n1        &lt;span class=\"pos pos11\"&gt;LDM\n2         &lt;span class=\"pos pos24\"&gt;RS\n3        &lt;span class=\"pos pos13\"&gt;RCM\n4          &lt;span class=\"pos pos7\"&gt;LB\n                    ...             \n16703    &lt;span class=\"pos pos29\"&gt;RES\n16704    &lt;span class=\"pos pos29\"&gt;RES\n16706    &lt;span class=\"pos pos29\"&gt;RES\n16707    &lt;span class=\"pos pos29\"&gt;RES\n16708    &lt;span class=\"pos pos28\"&gt;SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n뒤의 &gt;를 제외한 문자열을 추출해서 바꿔줘야 할 것 같다.\n\n\ndf_c.assign(Position = df_c.Position.str.split('&gt;').str[-1]).Position\n\n0        CAM\n1        LDM\n2         RS\n3        RCM\n4         LB\n        ... \n16703    RES\n16704    RES\n16706    RES\n16707    RES\n16708    SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n뒤의 문자열만 추출\n\n- 무지성으로 쥐어짜내본 아이디어\n\nx = df_c.Position.str.split('&gt;').str[-1][2]\nlst = [i != 0 for i in [key if x in value else 0 for key, value in position_dict.items()]];lst\n\n[False, False, False, True, False, False]\n\n\n\n[i != 0 for i in [key if x in value else 0 for key, value in position_dict.items()]].index(True)\n\n3\n\n\n\nlst = [[key if i in value else np.nan if i == np.nan else 1 for key, value in position_dict.items()] for i in df_c.Position.str.split('&gt;').str[-1]]\n\n\nvalue와 같은 값이면 key, 아니면 1, 결측치면 np.nan을 넣어줘봤음.\n\n\ndef cutting_1(lst):\n    for i in lst:\n        if i != 1:\n            return i\n\nPosition_s = pd.Series(lst).apply(cutting_1); Position_s\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n14393           RES\n14394           RES\n14395           RES\n14396           RES\n14397           SUB\nLength: 14398, dtype: object\n\n\n\n잘 된듯~(근데 앞에서 .dropna()를 안해서 쓸데없는 코드까지 작성해버렸다.)~\n\n\nlst_2 = list(map(lambda x : [key for key, value in position_dict.items() if x in value], df_c.Position.str.split('&gt;').str[-1]))\n## [i[0] for i in lst_2] : 왜인지 안됨\n## [i.pop() for i in lst_2] : 이건 뭐 객체 저장인지 뭔지 문제라는데, 다른 변수에 copy()해서 넣어봐도 안됨\n\n\ndf_c.reset_index(drop = True).assign(Position = Position_s).Position\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n14393           RES\n14394           RES\n14395           RES\n14396           RES\n14397           SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n잘못된 코드\n\n\ndf_c.assign(Position = Position_s).Position  ## 문제가 있는 코드, reset_index(drop = True)를 안해줘서 인덱스가 꼬임\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n16703           NaN\n16704           NaN\n16706           NaN\n16707           NaN\n16708           NaN\nName: Position, Length: 14398, dtype: object\n\n\n\n뭘 잘못했는 지 알겠지? index가 달라서 값이 엮이지가 않잖아…\n\n- 매우 간단한 교수님의 해법\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda _df: _df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n아마도 모든 문제의 원흉은 dropna()를 하지 않은 너에게 있었다. (결측치가 있으면 작동이 힘든가봄)\n\n\nD. df.Wage를 적절하게 변환하라.\n\n\ndf.Wage\n\n0        €250K\n1        €140K\n2        €135K\n3        €350K\n4         €45K\n         ...  \n16705      €1K\n16706     €550\n16707     €700\n16708     €500\n16709       €0\nName: Wage, Length: 16710, dtype: object\n\n\n\n시각화 해주려면 숫자형 자료여야 하는데, 범주형으로 들어가있다.\n\n- 앞의 유로를 없애고, K는 1000을 곱해주자.\n\ndf_d = df_c.reset_index(drop = True).assign(Position = Position_s)\n\n\ndf_d.Wage.str.replace('€','').str.replace('K','000')\n##[int(i) if i[-1] != 'K' else int(i.replace('K',''))*1000 for i in df_d.Wage.str.replace('€','')]와 동일\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n14393       650\n14394       950\n14395       550\n14396       700\n14397       500\nName: Wage, Length: 14398, dtype: object\n\n\n\ndf_d.assign(Wage = df_d.Wage.str.replace('€','').str.replace('K','000').astype(int)).Wage\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n14393       650\n14394       950\n14395       550\n14396       700\n14397       500\nName: Wage, Length: 14398, dtype: int32\n\n\n\n잘 된 것을 볼 수 있다."
  },
  {
    "objectID": "2023_DV/Review/8. 실습_FIFA23.html#e.-positiondefender-or-positionforward에-해당하는-관측치를-고른-뒤-x축에-shotpower-y축에-slidingtackle을-시각화하라.-이때-position은-color로-구분하고-wage는-size와-alpha로-구분하라.",
    "href": "2023_DV/Review/8. 실습_FIFA23.html#e.-positiondefender-or-positionforward에-해당하는-관측치를-고른-뒤-x축에-shotpower-y축에-slidingtackle을-시각화하라.-이때-position은-color로-구분하고-wage는-size와-alpha로-구분하라.",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### E. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.",
    "text": "### E. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.\n\ndf_e = df_d.assign(Wage = df_d.Wage.str.replace('€','').str.replace('K','000').astype(int))\n\n\ndf_e.loc[(df_e.Position == \"DEFENDER\") | (df_e.Position == \"FORWARD\")]\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n11\n155862\nSergio Ramos\n35\nhttps://cdn.sofifa.com/players/155/862/22_60.png\nSpain\nhttps://cdn.sofifa.com/flags/es.png\n88\n88\nParis Saint-Germain\nhttps://cdn.sofifa.com/teams/73/30.png\n...\n91.0\n11.0\n8.0\n9.0\n7.0\n11.0\nCB\n88.0\n€44.4M\n84.0\n\n\n12\n197445\nD. Alaba\n29\nhttps://cdn.sofifa.com/players/197/445/22_60.png\nAustria\nhttps://cdn.sofifa.com/flags/at.png\n84\n84\nReal Madrid CF\nhttps://cdn.sofifa.com/teams/243/30.png\n...\n82.0\n5.0\n7.0\n14.0\n15.0\n9.0\nCB\n84.0\n€72.8M\n86.0\n\n\n20\n210514\nJoão Cancelo\n27\nhttps://cdn.sofifa.com/players/210/514/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n86\n87\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n80.0\n6.0\n9.0\n15.0\n14.0\n14.0\nRB\n86.0\n€137.6M\n79.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13109\n203430\nG. Ray\n27\nhttps://cdn.sofifa.com/players/203/430/22_60.png\nWales\nhttps://cdn.sofifa.com/flags/gb-wls.png\n59\n60\nExeter City\nhttps://cdn.sofifa.com/teams/143/30.png\n...\n59.0\n13.0\n10.0\n12.0\n9.0\n8.0\nCB\n60.0\n€420K\n58.0\n\n\n13124\n187154\nN. Canavan\n30\nhttps://cdn.sofifa.com/players/187/154/22_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.com/flags/ie.png\n63\n63\nBradford City\nhttps://cdn.sofifa.com/teams/1804/30.png\n...\n62.0\n6.0\n10.0\n11.0\n14.0\n6.0\nCB\n63.0\n€700K\n63.0\n\n\n13183\n263968\nK. Sow\n18\nhttps://cdn.sofifa.com/players/263/968/22_60.png\nSwitzerland\nhttps://cdn.sofifa.com/flags/ch.png\n54\n76\nFC Lausanne-Sport\nhttps://cdn.sofifa.com/teams/1862/30.png\n...\n55.0\n6.0\n9.0\n13.0\n7.0\n14.0\nCB\n56.0\n€796K\n54.0\n\n\n13238\n263022\nM. Rosenfelder\n18\nhttps://cdn.sofifa.com/players/263/022/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n57\n71\nSC Freiburg II\nhttps://cdn.sofifa.com/teams/110691/30.png\n...\n60.0\n10.0\n10.0\n8.0\n10.0\n11.0\nCB\n59.0\n€726K\n58.0\n\n\n13405\n261062\nLee Han Beom\n19\nhttps://cdn.sofifa.com/players/261/062/22_60.png\nKorea Republic\nhttps://cdn.sofifa.com/flags/kr.png\n53\n72\nFC Seoul\nhttps://cdn.sofifa.com/teams/982/30.png\n...\n53.0\n7.0\n14.0\n5.0\n6.0\n15.0\nCB\n55.0\n€431K\n52.0\n\n\n\n\n3298 rows × 63 columns\n\n\n\n\ntidydata = df_e.loc[(df_e.Position == \"DEFENDER\") | (df_e.Position == \"FORWARD\")]\n\n\nfig = ggplot(tidydata)\npoint = geom_point(aes(x = 'ShotPower', y = 'SlidingTackle', color = 'Position', size = 'Wage', alpha = 'Wage'), position = 'jitter')\n\nfig + point\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n해치웠나…?\n\n결론\n\n- 데이터의 결측치를 반드시 먼저 처리하고 하자!(dropna()의 필요성)\n- pop()을 사용하기 전에는 결측치를 반드시 모두 없애자!\n- 데이터를 가공하여 순서가 바뀐 경우 왠만해선 인덱스를 초기화해주자!(reset_index()의 필요성)"
  },
  {
    "objectID": "2023_DV/Review/강신성_1108.html",
    "href": "2023_DV/Review/강신성_1108.html",
    "title": "1. 라이브러리 imports",
    "section": "",
    "text": "Plotly with pandas-backend\n- pandas에서 plotly를 이용하여 플롯을 그려보자.\nimport pandas as pd\nimport numpy as np\nimport plotly.io as pio\npd.options.plotting.backend = 'plotly'\npio.templates.default = 'plotly_white'\nprint(pio.templates)\n\nTemplates configuration\n-----------------------\n    Default template: 'plotly_white'\n    Available templates:\n        ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n         'ygridoff', 'gridon', 'none']\n- backend = plotly를 입력하지 않아도 되게 만들었음.\n_df = pd.DataFrame({'x' : [1,2,3]})\n_df.plot.line()\n- pie chart같은 것은 지원하지 않음."
  },
  {
    "objectID": "2023_DV/Review/강신성_1108.html#여러가지-플랏",
    "href": "2023_DV/Review/강신성_1108.html#여러가지-플랏",
    "title": "1. 라이브러리 imports",
    "section": "2. 여러가지 플랏",
    "text": "2. 여러가지 플랏\n\nA. .plot.bar()\n\n- 예제 1 : 성별 합격률 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf  ## 파일에 index_column이 존재하고, 첫 행이 열이름인듯.\n\n\n  \n    \n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.pivot_table(index = 'gender', columns = 'result', values = 'count', aggfunc = 'sum')\n\n\n  \n    \n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n1063\n772\n\n\nmale\n1291\n1400\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.pivot_table(index = 'gender', columns = 'result', values = 'count', aggfunc = 'sum').assign(total = lambda _df : _df.fail + _df['pass'])\\\n.assign(rate = lambda _df : _df['pass']/_df.total)\n\n\n  \n    \n\n\n\n\n\nresult\nfail\npass\ntotal\nrate\n\n\ngender\n\n\n\n\n\n\n\n\nfemale\n1063\n772\n1835\n0.420708\n\n\nmale\n1291\n1400\n2691\n0.520253\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.pivot_table(index = 'gender', columns = 'result', values = 'count', aggfunc = 'sum').assign(total = lambda _df : _df.fail + _df['pass'])\\\n.assign(rate = lambda _df : _df['pass']/_df.total)\\\n.reset_index().drop(['fail', 'pass', 'total'], axis = 1)\n\n\n  \n    \n\n\n\n\n\nresult\ngender\nrate\n\n\n\n\n0\nfemale\n0.420708\n\n\n1\nmale\n0.520253\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 이상태에서 바로 시각화\n\ndf.pivot_table(index = 'gender', columns = 'result', values = 'count', aggfunc = 'sum').assign(total = lambda _df : _df.fail + _df['pass'])\\\n.assign(rate = lambda _df : _df['pass']/_df.total)\\\n.reset_index().drop(['fail', 'pass', 'total'], axis = 1)\\\n.assign(rate = lambda _df : _df.rate.apply(lambda x : round(x, 2)))\\\n.plot.bar(x = 'gender', y = 'rate', color = 'gender', text = 'rate', width = 600)    ## text 옵션으로 개체에 라벨링 가능, width나 height 옵션으로 크기 조절 가능\n\n\n\n\n\n                                \n                                            \n\n\n\n\n- 예제 2 : (성별, 학과) 별 지원자 수 시각화\n\ndf.pivot_table(index = ['department', 'gender'], values = 'count', aggfunc = 'sum')\n\n\n  \n    \n\n\n\n\n\n\n\ncount\n\n\ndepartment\ngender\n\n\n\n\n\nA\nfemale\n108\n\n\nmale\n825\n\n\nB\nfemale\n25\n\n\nmale\n560\n\n\nC\nfemale\n593\n\n\nmale\n325\n\n\nD\nfemale\n375\n\n\nmale\n417\n\n\nE\nfemale\n393\n\n\nmale\n191\n\n\nF\nfemale\n341\n\n\nmale\n373\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.pivot_table(index = ['department', 'gender'], values = 'count', aggfunc = 'sum').reset_index()\\\n.plot.bar(x = 'gender', y = 'count', color = 'gender', facet_col = 'department', text = 'count', width = 800)  ## 면분할도 됨. facet_row는 지양할것"
  },
  {
    "objectID": "2023_DV/Review/강신성_1108.html#b.-plot.line",
    "href": "2023_DV/Review/강신성_1108.html#b.-plot.line",
    "title": "1. 라이브러리 imports",
    "section": "### B. plot.line()",
    "text": "### B. plot.line()\n- 예제 1 : 핸드폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n  \n    \n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.melt(id_vars = 'Date').set_axis(['날짜','회사','판매량'], axis = 1)\n\n\n  \n    \n\n\n\n\n\n\n날짜\n회사\n판매량\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-11\nSamsung\n461\n\n\n2\n2019-12\nSamsung\n426\n\n\n3\n2020-01\nSamsung\n677\n\n\n4\n2020-02\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06\nAsus\n16\n\n\n204\n2020-07\nAsus\n12\n\n\n205\n2020-08\nAsus\n20\n\n\n206\n2020-09\nAsus\n15\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n\n208 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.melt(id_vars = 'Date').set_axis(['날짜','회사','판매량'], axis = 1)\\\n.plot.line(x = '날짜', y = '판매량', color = '회사', width = 800)\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\nC. .plot.scatter()\n\n- 예제 1 : FIFA23 data\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\\\n.loc[:,lambda df: df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda df: df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda df: df.Wage.str[1:].str.replace('K','000').astype(int))\ndf\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n\n14398 rows × 63 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.query('Position == \"DEFENDER\" or Position == \"FORWARD\"')\\\n.plot.scatter(x = 'ShotPower', y = 'StandingTackle',\n              color = 'Position', size = 'Wage', hover_data = ['Name', 'Age'],\n              opacity = 0.5, width = 800)    ## alpha가 아니라 opacity(불투명)로 설정함, hover_data로 마우스를 갖다댔을 때 추가적인 정보를 넣어줄 수 있음."
  },
  {
    "objectID": "2023_DV/Review/강신성_1108.html#d.-.plot.box",
    "href": "2023_DV/Review/강신성_1108.html#d.-.plot.box",
    "title": "1. 라이브러리 imports",
    "section": "### D. .plot.box()",
    "text": "### D. .plot.box()\n- 예제 1 : 전북고등학교\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들\n\n\ndf = pd.DataFrame({\n    'Class' : ['A']*len(y1) + ['B']*len(y2),\n    'Score' : y1+y2\n})\ndf\n\n\n  \n    \n\n\n\n\n\n\nClass\nScore\n\n\n\n\n0\nA\n75\n\n\n1\nA\n75\n\n\n2\nA\n76\n\n\n3\nA\n76\n\n\n4\nA\n77\n\n\n5\nA\n77\n\n\n6\nA\n78\n\n\n7\nA\n79\n\n\n8\nA\n79\n\n\n9\nA\n98\n\n\n10\nB\n76\n\n\n11\nB\n76\n\n\n12\nB\n77\n\n\n13\nB\n77\n\n\n14\nB\n78\n\n\n15\nB\n78\n\n\n16\nB\n79\n\n\n17\nB\n80\n\n\n18\nB\n80\n\n\n19\nB\n81\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.plot.box(x = 'Class', y = 'Score', color = 'Class',\n            points = 'all', width = 500)   ## points 옵션을 사용해서 점도 직접 띄워줄 수 있음\n\n\n\n\n\n                                \n                                            \n\n\n\n\n- 예제 2 : (년도, 시도)별 전기에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n## 그냥 바로 해도 됨(이미 타이디데이터임)\ndf.plot.box(x = '시도', y = '에너지사용량(TOE)/전기', color = '시도', facet_row = '년도', height = 1600, width = 800, hover_data = ['지역','연면적'])\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\nE. .plot.hist()\n\n- 예제 1 : 타이타닉 (연령, 성별) 생존자\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n2.564949\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n3.401197\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n3.154870\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n3.401197\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n2.047693\n\n\n\n\n\n891 rows × 13 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.Age.hist()  ## 바로 히스토그램 그릴 수도 있음(시리즈에서)\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\ndf.plot.hist(\n    x = 'Age', color = 'Sex',\n    facet_row = 'Sex',\n    facet_col = 'Survived')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n성별 효과가 15세 이상에서는 있었고, 그 아래에서는 없는 것 같다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1108.html#f.-.plot.area",
    "href": "2023_DV/Review/강신성_1108.html#f.-.plot.area",
    "title": "1. 라이브러리 imports",
    "section": "### F. .plot.area()",
    "text": "### F. .plot.area()\n- 예제 1 : 핸드폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n  \n    \n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.melt(id_vars = 'Date').set_axis(['날짜', '회사', '판매량'], axis = 1)\\\n.plot.area(x = '날짜', y = '판매량', color = '회사')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n전체적인 판매량과 비중을 알 수 있음과 동시에 경향성을 알 수 있음\n\n- 예제 2 : 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').str[-1])\n\n\n  \n    \n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지종류\n에너지사용량\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n전기\n64818\n\n\n1\n2018\nSeoul\n종로구\n17929\n9141777\n도시가스\n82015\n\n\n2\n2018\nSeoul\n종로구\n17929\n9141777\n지역난방\n111\n\n\n3\n2018\nSeoul\n중구\n10598\n10056233\n전기\n81672\n\n\n4\n2018\nSeoul\n중구\n10598\n10056233\n도시가스\n75260\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2995\n2021\nJeju-do\n제주시\n67053\n20275738\n도시가스\n25689\n\n\n2996\n2021\nJeju-do\n제주시\n67053\n20275738\n지역난방\n0\n\n\n2997\n2021\nJeju-do\n서귀포시\n35230\n7512206\n전기\n37884\n\n\n2998\n2021\nJeju-do\n서귀포시\n35230\n7512206\n도시가스\n2641\n\n\n2999\n2021\nJeju-do\n서귀포시\n35230\n7512206\n지역난방\n0\n\n\n\n\n\n3000 rows × 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').str[-1])\\\n.pivot_table(index = ['에너지종류', '시도', '년도'], values = '에너지사용량', aggfunc = 'sum').reset_index()\n\n\n  \n    \n\n\n\n\n\n\n에너지종류\n시도\n년도\n에너지사용량\n\n\n\n\n0\n도시가스\nBusan\n2018\n708240\n\n\n1\n도시가스\nBusan\n2019\n675882\n\n\n2\n도시가스\nBusan\n2020\n690015\n\n\n3\n도시가스\nBusan\n2021\n878874\n\n\n4\n도시가스\nChungcheongbuk-do\n2018\n288927\n\n\n...\n...\n...\n...\n...\n\n\n199\n지역난방\nSeoul\n2021\n546491\n\n\n200\n지역난방\nUlsan\n2018\n0\n\n\n201\n지역난방\nUlsan\n2019\n0\n\n\n202\n지역난방\nUlsan\n2020\n0\n\n\n203\n지역난방\nUlsan\n2021\n0\n\n\n\n\n\n204 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').str[-1])\\\n.pivot_table(index = ['에너지종류', '시도', '년도'], values = '에너지사용량', aggfunc = 'sum').reset_index()\\\n.plot.area(x = '년도', y = '에너지사용량', facet_col = '에너지종류', color = '시도', width = 600)\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n연도가 겹치는 문제 해결을 위한 미세조정\n\n\n## figure로 저장\nfig = df.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').str[-1])\\\n.pivot_table(index = ['에너지종류', '시도', '년도'], values = '에너지사용량', aggfunc = 'sum').reset_index()\\\n.plot.area(x = '년도', y = '에너지사용량', facet_col = '에너지종류', color = '시도', width = 600)\n\n## 아마도 xaxis3에서 년도가 들어갈 수 있는 영역을 한정해주는듯\nfig.update_layout(\n    xaxis_domain = [0.0, 0.25],\n    xaxis2_domain = [0.35, 0.60],\n    xaxis3_domain = [0.70, 0.95]\n)"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "",
    "text": "데이터프레임 df의 열이름에 actor라는 단어가 포함된 column만을 선택하는 코드를 작성하라"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#사전작업",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#사전작업",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 설치\n\n\nimport pandas as pd\nimport numpy as np\n\n\n데이터 불러오기 및 확인\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\n...\nnum_user_for_reviews\nlanguage\ncountry\ncontent_rating\nbudget\ntitle_year\nactor_2_facebook_likes\nimdb_score\naspect_ratio\nmovie_facebook_likes\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n...\n3054.0\nEnglish\nUSA\nPG-13\n237000000.0\n2009.0\n936.0\n7.9\n1.78\n33000\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n...\n1238.0\nEnglish\nUSA\nPG-13\n300000000.0\n2007.0\n5000.0\n7.1\n2.35\n0\n\n\n2\nColor\nSam Mendes\n602.0\n148.0\n0.0\n161.0\nRory Kinnear\n11000.0\n200074175.0\nAction|Adventure|Thriller\n...\n994.0\nEnglish\nUK\nPG-13\n245000000.0\n2015.0\n393.0\n6.8\n2.35\n85000\n\n\n3\nColor\nChristopher Nolan\n813.0\n164.0\n22000.0\n23000.0\nChristian Bale\n27000.0\n448130642.0\nAction|Thriller\n...\n2701.0\nEnglish\nUSA\nPG-13\n250000000.0\n2012.0\n23000.0\n8.5\n2.35\n164000\n\n\n4\nNaN\nDoug Walker\nNaN\nNaN\n131.0\nNaN\nRob Walker\n131.0\nNaN\nDocumentary\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n7.1\nNaN\n0\n\n\n\n\n5 rows × 28 columns\n\n\n\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n열 이름에서 단어의 구분이 모두 '_'로 되어있으므로, 열이름에 split()함수를 적용시킬 수 있을 것 같다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#풀이",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#풀이",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "2. 풀이",
    "text": "2. 풀이\n\nfor 문을 이용하여 풀이해보자.\n\n\n['actor' in i.split('_') for i in df.columns]\n\n[False,\n False,\n False,\n False,\n False,\n True,\n True,\n True,\n False,\n False,\n True,\n False,\n False,\n False,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n False,\n False,\n False]"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#결과",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#결과",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "3. 결과",
    "text": "3. 결과\n\ndf.loc[:, ['actor' in i.split('_') for i in df.columns]]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n완료"
  },
  {
    "objectID": "2023_DV.html",
    "href": "2023_DV.html",
    "title": "2023 DV",
    "section": "",
    "text": "전북대학교 2023년 2학기 통계학부 최규빈 교수님의 “데이터시각화(Data Visualization)” 강의를 듣고 그 내용을 나름대로 정리한 내용입니다.\n\n:::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\n라이브러리 imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n라이브러리 imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n라이브러리 imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFolium | 월드맵 시각화\n\n\n\n\n\n\n\npython\n\n\nfolium\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n훌륭한 시각화 2\n\n\n\n\n\n\n\nvisuallization\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPlotly : pandas backend\n\n\n\n\n\n\n\npython\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nTidydata 심화 실습\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nTidydata 만들기\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n집단 간 비교 : 심슨의 역설\n\n\n\n\n\n\n\npython\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n데이터 시각화 실습 : FIFA23 선수 데이터\n\n\n\n\n\n\n\npython\n\n\npractice\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPandas 사용 팁\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPandas 기본기 | 데이터프레임 핸들링\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPandas 기본기 | 행과 열의 선택\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPlotnine : R에서 비롯한 패키지\n\n\n\n\n\n\n\npython\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nSeaborn | 데이터프레임 친화적 패키지\n\n\n\n\n\n\n\npython\n\n\nseaborn\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPlot | 꺾은선, 산점도, 객체지향화\n\n\n\n\n\n\n\npython\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction | 그래프, 이미지 이퀼라이징\n\n\n\n\n\n\n\npython\n\n\nimage\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n[문제 풀이] 데이터프레임 : 특정 열의 재가공\n\n\n\n\n\n\n\npython\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n[문제 풀이] 특정 단어를 포함하는 열 선택\n\n\n\n\n\n\n\npython\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\n강신성\n\n\n\n\n:::\n\n\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "",
    "text": "예측해야 할 유형이 범주형일 때 사용할 수 있는 분석 기법 중 하나인 로지스틱 회귀분석을 해보자!"
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html#라이브러리-imports",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html#라이브러리-imports",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "2. 로지스틱 회귀분석",
    "text": "2. 로지스틱 회귀분석\n- 연속형 설명변수와 범주형 반응변수와의 관계\n\nA. 성적과 취업 여부 데이터\n\n\n학점과 토익 성적, 그리고 취업 여부를 나타낸 데이터가 있다.(교수님이 만드신 페이크 데이터이다.)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf  ## 페이크 데이터입니다.\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nplt.plot(df.toeic[df.employment == 0], df.gpa[df.employment == 0], 'o')\nplt.plot(df.toeic[df.employment == 1], df.gpa[df.employment == 1], 'o')\nplt.show()\n\n\n\n\n- 뭔가 관련성을 찾을 수 있을 것 같지 않은가?~(당연하지 그렇게 만드셨으니까)~\n\n그래서 토익 성적ㆍ학점과 취업여부의 관계를 구하고 싶다."
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html#b.-분석",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html#b.-분석",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### B. 분석",
    "text": "### B. 분석\n\n# step 1\nX = pd.get_dummies(df[['toeic', 'gpa']])\ny = df.employment\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\ndf = df.assign(employment_hat = predictr.predict(X))\n\n\nsklearn.linear_model.LogisticRegression()\n\n\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemployment_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n로지스틱 회귀분석으로 적합 및 예측이 완료되었다.\n\n\nC. 평가\n\n\npredictr.score(X, y)\n\n0.882\n\n\n\n이건 y와 y_hat이 동일한 정도를 나타낸다, 나름 잘 맞춘 것 같지 않은가?\n\n- 시각화를 해야 정확히 알 수 있겠지? 현재 예측치와 기존 예측치를 비교해보자.\n\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemployment_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\ndf_filtered = df[predictr.predict(X) == 1]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,5))\n\nfig.suptitle('Constrat Pradiction and Real')\n\nax1.plot(df.toeic, df.gpa, 'o', color = 'C0', label = 'employed')\nax1.plot(df.loc[df.employment == 1].toeic, df.loc[df.employment == 1].gpa, 'o', color = 'C1', label = 'not yet employed')\nax1.set_title('Real Data')\n\nax2.plot(df.toeic, df.gpa, 'o', color = 'C0', label = 'employed')\nax2.plot(df_filtered.toeic, df_filtered.gpa, 'o', color = 'C1', label = 'not yet employed')\nax2.set_title('Estimated Data')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n어때요, 나름 합리적이지 않나요?"
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석의-실적용",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html#로지스틱-회귀분석의-실적용",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "3. 로지스틱 회귀분석의 실적용",
    "text": "3. 로지스틱 회귀분석의 실적용\n\n그럼 타이타닉 데이터에서 로지스틱 회귀분석을 통해 결과를 잘 예측할 수 있지 않을까요?\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/HollyRiver/Machine_learning_in_practice/main/kaggle/titanic/data/train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/HollyRiver/Machine_learning_in_practice/main/kaggle/titanic/data/test.csv')\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nkaggle 입문하기에서 보았던 타이타닉 데이터이다.\n- 여기서 반응변수를 쉽게 구하려면…\n\nset(df_train.columns) - set(df_test.columns)\n\n{'Survived'}\n\n\n- 하나만 남는 것을 볼 수 있다.\n\n아! 테스트 셋에 없는 열이니까 저게 y겠구나!"
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html#a.-늘-해왔던-것처럼-분석하면-안된다",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html#a.-늘-해왔던-것처럼-분석하면-안된다",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### A. 늘 해왔던 것처럼 분석…~(하면 안된다)~",
    "text": "### A. 늘 해왔던 것처럼 분석…~(하면 안된다)~\n\n# step 1\nX = pd.get_dummies(df_train.drop(['Survived'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(df_test)\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n\n\n오류가 나온다.\n\nInput X contains NaN.\n선형 회귀에서 설명변수의 input값에는 결측치가 있으면 안된다!!!\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\n결측치가 있는 열을 제거, 행을 제거, 둘 다. 또는 결측치를 impute해야 하는데…\n\n- 일단 Cabin 열은 결측치가 너무 많으니까 빼자!\n- Name이나 Ticket과 같은 변수는 이성적으로 봤을 때 바로 one-hot 인코딩 하기에는 어색하니 빼자!\n\nlen(set(df_train.Name)), len(set(df_train.Ticket))  ## 다 다름, 거의 다 다름\n\n(891, 681)\n\n\n- Age, Embarked에 포함된 약간의 결측치가 마음에 걸리니까 빼자!!\n\ndropna() 메소드나 preprocessing을 써도 되지만… 그건 나중에 해보자.\n\n\ndf_test.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n\n\n- Fare에 포함된 결측치도 걸린다 -&gt; 빼자! (평균으로 해주는 방법도 있는ㄷ ~나중에 하자고 좀~)\n\nB. 데이터 정리\n\n- 위에서 말한 조건들을 적용해서 데이터를 재가공한 뒤, 로지스틱 회귀를 해보자\n\ndf_train.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\n# step 1\nX = pd.get_dummies(df_train.drop(['Survived', 'Cabin', 'Name', 'Ticket', 'Age', 'Embarked', 'Fare'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(df_test.drop(['Cabin', 'Name', 'Ticket', 'Age', 'Embarked', 'Fare'], axis = 1))\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\nXX.assign(Survived = predictr.predict(XX))\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nSibSp\nParch\nSex_female\nSex_male\nSurvived\n\n\n\n\n0\n892\n3\n0\n0\nFalse\nTrue\n0\n\n\n1\n893\n3\n1\n0\nTrue\nFalse\n1\n\n\n2\n894\n2\n0\n0\nFalse\nTrue\n0\n\n\n3\n895\n3\n0\n0\nFalse\nTrue\n0\n\n\n4\n896\n3\n1\n1\nTrue\nFalse\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\n0\n0\nFalse\nTrue\n0\n\n\n414\n1306\n1\n0\n0\nTrue\nFalse\n1\n\n\n415\n1307\n3\n0\n0\nFalse\nTrue\n0\n\n\n416\n1308\n3\n0\n0\nFalse\nTrue\n0\n\n\n417\n1309\n3\n1\n1\nFalse\nTrue\n0\n\n\n\n\n418 rows × 7 columns\n\n\n\n\n정상적으로 잘 수행한 것 같다."
  },
  {
    "objectID": "2023_MP/practice/2. 로지스틱 회귀분석.html#c.-평가-1",
    "href": "2023_MP/practice/2. 로지스틱 회귀분석.html#c.-평가-1",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### C. 평가",
    "text": "### C. 평가\n\npredictr.score(X, y)\n\n0.8002244668911336\n\n\n\n생각보단 잘 한 것 같다.\n\n\nD. 제출\n\n\nyy = pd.DataFrame({'Survived' : predictr.predict(XX)})\nsubmit_df = pd.concat([df_test.PassengerId, yy], axis = 1)\nsubmit_df\n\n#submit_df.to_csv(directory, index = False)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\nkaggle에 제출하고 두근대는 결과는… 0.77511, 그리 높진 않다."
  },
  {
    "objectID": "2023_MP/practice/4. predictor의 이해.html",
    "href": "2023_MP/practice/4. predictor의 이해.html",
    "title": "sklearn.linear_model의 작동원리",
    "section": "",
    "text": "LogisticRegression의 작동원리와 sklearn.linear_model에 대해서 자세히 알아보자."
  },
  {
    "objectID": "2023_MP/practice/4. predictor의 이해.html#라이브러리-imports",
    "href": "2023_MP/practice/4. predictor의 이해.html#라이브러리-imports",
    "title": "sklearn.linear_model의 작동원리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport itertools"
  },
  {
    "objectID": "2023_MP/practice/4. predictor의 이해.html#로지스틱-회귀분석의-원리",
    "href": "2023_MP/practice/4. predictor의 이해.html#로지스틱-회귀분석의-원리",
    "title": "sklearn.linear_model의 작동원리",
    "section": "2. 로지스틱 회귀분석의 원리",
    "text": "2. 로지스틱 회귀분석의 원리\n\n저번에 봤었던 취업 자료를 가져와보자.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\nemployment를 예측하려면…\n\n## 1\nX = df.drop(['employment'], axis = 1)\ny = df.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## 3\npredictr.fit(X, y)\n\npredictr.predict(X)  ## yhat\n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], dtype=int64)\n\n\n\n\\(\\hat{y}\\)은 어떻게 나왔는가?\n- 아래 수식에 의하여 나왔음…\n\npredictr.coef_, predictr.intercept_  ## 로지스틱임에도 기울기와 절편이 있다.\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 - 8.45433334  ## yhat\nv = 1/(1+np.exp(-u))  # v : 확률같은 거\n\nv\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()  ## v가 0.5보다 클 경우 전부 True였음을 알 수 있음\n\n1.0\n\n\n해당 개체에 처리가 취해질 확률을 구하고, 그것이 0.5보다 크면 처리를 취한다.\n- 만약 적합된 v값을 알고 싶다면…\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3)\n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])\n\n\n\n우측의 값과 일치하는 것을 알 수 있다.(0번째 : 0일 확률, 1번째 : 1일 확률)"
  },
  {
    "objectID": "2023_MP/practice/4. predictor의 이해.html#predictor-파고들기",
    "href": "2023_MP/practice/4. predictor의 이해.html#predictor-파고들기",
    "title": "sklearn.linear_model의 작동원리",
    "section": "3. predictor 파고들기",
    "text": "3. predictor 파고들기\n\n아래와 같은 데이터를 가공해서 0~7까지는 train, 8~9까지는 test 셋으로 쓰도록 하자.\n\n\ndf = pd.DataFrame({'X':np.arange(20,30),'y':-np.arange(10)+1+np.random.randn(10)*0.1})\ndf\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n20\n0.992487\n\n\n1\n21\n-0.040013\n\n\n2\n22\n-0.984351\n\n\n3\n23\n-2.085536\n\n\n4\n24\n-3.023587\n\n\n5\n25\n-4.287162\n\n\n6\n26\n-5.085849\n\n\n7\n27\n-6.110568\n\n\n8\n28\n-6.798420\n\n\n9\n29\n-8.028488\n\n\n\n\n\n\n\n\ndf_train = df[:8]\ndf_test = df[8:]\n\ndf_train_X = df_train[['X']]\ndf_train_y = df_train[['y']]\ndf_test_X = df_test[['X']]\ndf_test_y = df_test[['y']]\n\n\n## predictor 두 개를 만들도록 리스트 컴프리헨션\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\npredictors[0].fit(df_train_X, df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n첫 번째 predictr에 적합하면 .score, .intercept_, .coef_가 해금된다. 두 번째 predictr에는 적용되지 않는다.~(당연한 거 아님?)~\n\n\nX, y에 들어갈 수 있는 형식\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr = sklearn.linear_model.LinearRegression()\n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'  ## 예외사항(error) 발생 시의 output\n\n\n가능한 형식들을 모두 모아놨다. 그럼 이것들을 가지고 어떤 녀석이 되는 지 딕셔너리 컴프리헨션을 해보자.\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n## itertools.product() : 원소들의 데카르트 곱을 리스트로 반환.\n## itertools.product('ABCD', repeat = 2)의 경우 크기가 2인 앞의 string 조합을 모두 반환\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론 | X에는 2차원 데이터만 들어올 수 있지만, y에는 1ㆍ2차원 데이터 모두가 들어올 수 있다.\n- 그리고 일반적으로, X에는 2차원 데이터 배열이 imput되기를 기대하고, y에는 1차원 데이터 배열이 imput되기를 기대한다."
  },
  {
    "objectID": "2023_MP/practice/4. predictor의 이해.html#첨언-데이터셋-이름-설정에-대하여",
    "href": "2023_MP/practice/4. predictor의 이해.html#첨언-데이터셋-이름-설정에-대하여",
    "title": "sklearn.linear_model의 작동원리",
    "section": "4. 첨언 | 데이터셋 이름 설정에 대하여",
    "text": "4. 첨언 | 데이터셋 이름 설정에 대하여\n\n설명변수와 반응변수, 테스트 셋과 트레인 셋을 부르는 변수 명을 어떻게 설정해야 할 지 나타내겠다.\n\nX : 설명변수 & Train\ny : 반응변수 & Train\nXX : 설명변수 & Test\nyy : 반응변수 & Test\nyhat : predictr.predict(X)\nyyhat : predictr.predict(XX)"
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html",
    "title": "오버피팅, 다중공선성",
    "section": "",
    "text": "오버피팅은 뭐고, 다중공선성은 왜 발생할까? 그리고 해결은 어떻게 할까?"
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html#라이브러리-imports",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html#라이브러리-imports",
    "title": "오버피팅, 다중공선성",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport sklearn"
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html#언더라잉과-오차항",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html#언더라잉과-오차항",
    "title": "오버피팅, 다중공선성",
    "section": "2. 언더라잉과 오차항",
    "text": "2. 언더라잉과 오차항\n- 만약 내가 원한다면, 관련이 있든 없든 무수히 많은 데이터를 모을 수 있다고 가정하자…\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\ndf_balance = pd.DataFrame((np.random.randn(500,5000)&gt;0.5).reshape(500,5000)*1,columns = ['X'+str(i) for i in range(5000)])\ndf_merged = pd.concat([df,df_balance],axis=1)\ndf_merged\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nX0\nX1\nX2\nX3\nX4\nX5\nX6\n...\nX4990\nX4991\nX4992\nX4993\nX4994\nX4995\nX4996\nX4997\nX4998\nX4999\n\n\n\n\n0\n135\n0.051535\n0\n1\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n1\n1\n1\n1\n1\n\n\n1\n935\n0.355496\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n485\n2.228435\n0\n1\n0\n1\n0\n0\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n1\n0\n1\n\n\n3\n65\n1.179701\n0\n1\n1\n0\n0\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n4\n445\n3.962356\n1\n0\n0\n0\n0\n0\n1\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0\n0\n0\n1\n1\n1\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n496\n310\n2.601212\n1\n0\n1\n1\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n497\n225\n0.042323\n0\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n498\n320\n1.041416\n0\n1\n0\n0\n0\n1\n0\n1\n...\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n499\n375\n3.626883\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n\n\n500 rows × 5003 columns\n\n\n\n\nemployment의 예측과 상관이 없을 개인의 선호, balance_game을 가져왔다. (5000종류)\n\n\n## df_train, df_test =  sklearn.model_selection.train_test_split(test_size = 0.2) 이걸로 해도 된다.\n## step 1\nX = df_merged.drop(['employment'], axis = 1)[:400]\nXX = df_merged.drop(['employment'], axis = 1)[400:]\ny = df_merged.employment[:400]\nyy = df_merged.employment[400:]\n\n## step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n(1.0, 0.79)\n\n\n\n쓸모없는 변수(y와 상관관계가 낮은 변수)를 사용해서 오버피팅되었다. train score가 상당히 높게 나왔다.(오차항까지 예측한 상황)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n\n# 1\nX = df.drop(['employment'], axis = 1)[:400]\nXX = df.drop(['employment'], axis = 1)[400:]\ny = df[['employment']][:400]\nyy = df[['employment']][400:]\n\n# 2\nprdtr = sklearn.linear_model.LogisticRegression()\n\n# 3\nprdtr.fit(X,y)\n\n# 4\nprdtr.score(X,y), prdtr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n(0.8925, 0.83)\n\n\n\ntest 데이터에서 스코어가 더 높았다."
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html#다중공선성",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html#다중공선성",
    "title": "오버피팅, 다중공선성",
    "section": "3. 다중공선성",
    "text": "3. 다중공선성\n- 아래와 같은 가짜뉴스를 읽어보자.(ChatGPT를 이용하여 생성한 가짜뉴스)\n헤드라인: “텝스와 다른 영어 인증 시험들, 결국은 토익과 비슷한 결과를 보여준다?”\n본문:\n최근 몇 년 동안, 토익의 신뢰성에 대한 논란이 계속되어 왔습니다. 이러한 배경 속에서 텝스(TEPS), 토플(TOEFL) 등 여러 새로운 영어 능력 평가 시험이 등장하였습니다. 많은 학생들과 직장인들은 이러한 새로운 시험들이 토익보다 더 신뢰성 있고 현실적인 능력을 평가할 것이라는 기대감을 가지고 있었습니다.\n그러나 최근에 발표된 연구결과에 따르면, 텝스와 다른 영어 인증 시험들도 결국에는 토익과 매우 비슷한 성적 분포와 결과를 보여주었다고 합니다. 연구 팀은 여러 시험들간의 점수 분포와 성적의 상관관계를 분석한 결과, 대부분의 시험들이 실제 영어 능력에 대해 유사한 평가를 제공한다는 결론을 내렸습니다.\n“많은 사람들이 새로운 시험들이 더 현실적이거나 다양한 영어 능력을 평가할 것이라 기대했지만, 실제로는 모든 시험들이 비슷한 결과를 보여주었습니다.” 라며 연구 팀의 대표는 이렇게 언급하였습니다.\n이러한 연구결과는 영어 능력 평가 시험의 표준화와 신뢰성에 대한 논의를 새롭게 불러일으킬 것으로 보입니다.\n- 뉴스에 근거하여 아래의 가짜 자료를 생성했다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\nNaN\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\nNaN\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\nNaN\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\nNaN\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\nNaN\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\nNaN\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\nNaN\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\nNaN\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\nNaN\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\nNaN\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ntoeic0 ~ toeic499는 유사토익을 의미\n\n- 모르는 정보 : 사내 고용 법칙\n\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic * 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n학점 1 증가는 토익 100점 증가와 비슷하다고 고려하고 있다.\n\n\nA. 이대로 분석 | 잘못됨\n\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop(['employment_score'], axis = 1)\ny = df_train.employment_score\nXX = df_test.drop(['employment_score'], axis = 1)\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.11705078212495712)\n\n\n\n두 점수가 큰 차이가 난다.(오차항까지 적합해버린 오버피팅의 상황)\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제로는 gpa는 1, toeic은 0.01, 나머지는 0이 되어야 하지만, 많이 다르다…"
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html#b.-제대로-분석했다면",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html#b.-제대로-분석했다면",
    "title": "오버피팅, 다중공선성",
    "section": "### B. 제대로 분석했다면?",
    "text": "### B. 제대로 분석했다면?\n- toeic과 gpa만이 유의미한 변수라는 걸 눈치챔. (아다리, 현실세계에선 일어날 수 없음)\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.loc[:, ['toeic', 'gpa']]\ny = df_train.employment_score\nXX = df_test.loc[:, ['toeic', 'gpa']]\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9133033622085311, 0.9127346436925985)\n\n\n\n스코어도 높음\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ntoeic    0.010063\ngpa      0.972163\ndtype: float64\n\n\n\n실제 계수값과 유사하도록 잘 추정됨\n\n\nC. 하다못해 toeic0와 gpa로 적합했다면???\n\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.loc[:, ['toeic0', 'gpa']]\ny = df_train.employment_score\nXX = df_test.loc[:, ['toeic0', 'gpa']]\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9120540945251211, 0.9115427614193155)\n\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ntoeic0    0.010101\ngpa       0.981302\ndtype: float64\n\n\n\n굉장히 합리적이다!"
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html#d.-고찰",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html#d.-고찰",
    "title": "오버피팅, 다중공선성",
    "section": "### D. 고찰",
    "text": "### D. 고찰\n- 의문 : 왜 변수를 더 많이 넣었는데, 정보를 더 많이 제공해줬는데, 이상한 결과가 나올까???\n\n규칙을 찾으면 안될 것 (반응변수와의 상관관계가 없는 것) 에서 규칙을 찾고 있으니까 (오차항을 적합) 잘 될리가 없지…\n\n- 쓸모없는 변수?\n\n\n진짜 쓰레기, 쓰잘데기 없는 것(X1 = 부먹/찍먹, X2 = 민초/반민초…) -&gt; 애초에 이딴걸 가지고 y를 맞출 생각도 들지 않음…\n\n\n실제론 쓸모 있는데, 대체제가 있는 경우 -&gt; 대체제를 보고 y를 맞출 것 같기도 한데, 둘은 너무 비슷함…\n\n\n- 1과 2 모두 과대적합(overfitting)을 야기하고, 2와 같은 상황에서 발생하는 문제를 다중공선성(multiple linearity)이라고 한다.\n\n1은 corr(x_1, y), corr(x_2, y)가 낮게 나온다 -&gt; y와의 관계가 없다. 2는 corr(x_1, y), corr(x_2, y)는 높게 나오는데, corr(x_1, x_2)도 높게 나온다."
  },
  {
    "objectID": "2023_MP/practice/6. 오버피팅, 다중공선성.html#다중공선성의-특징",
    "href": "2023_MP/practice/6. 오버피팅, 다중공선성.html#다중공선성의-특징",
    "title": "오버피팅, 다중공선성",
    "section": "4. 다중공선성의 특징",
    "text": "4. 다중공선성의 특징\n- 잘못된 분석을 재현하고, 계수를 해석해보자.\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n- 특이사항\n\ns.loc['toeic':].sum()\n\n0.010302732920633051\n\n\n\n비슷한 설명변수들의 회귀계수를 합하니까 0.01과 유사한 값이 나왔음…\n\n\nfig, ax = plt.subplots(3)\n\nfor i in range(3):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = i)\n    X = df_train.drop(['employment_score'], axis = 1)\n    XX = df_test.drop(['employment_score'], axis = 1)\n    y = df_train.employment_score\n    yy = df_test.employment_score\n\n    predictr = sklearn.linear_model.LinearRegression()\n\n    predictr.fit(X, y)\n\n    s = pd.Series(predictr.coef_)\n    ax[i].plot(s[1:], '-')\n    ax[i].set_title('sum of toeic coef = {}'.format(round(s[1:].sum(), 4)))\n\nfig.tight_layout()\n\n\n\n\n\n계수는 상당히 불안정하나, 그 합은 합리적인 값이 나온다.\n계수값의 해석이 용이하지 않다. 음의 계수값이 있다는 것은, 토익 유사한 시험의 점수를 올리면 취업이 오히려 안된다(…)라는 것과도 같다.\n\n\n이것의 해결은 직접 몇 개만 지우거나, 다중공선성을 해결하기 위해 패널티를 부여하는 모듈을 써서 해소 가능하다."
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html",
    "href": "2023_MP/practice/8. 선형모형의 적.html",
    "title": "선형모형의 적",
    "section": "",
    "text": "우리의 주적은 북한인데, 선형모형의 적은?"
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html#라이브러리-imports",
    "href": "2023_MP/practice/8. 선형모형의 적.html#라이브러리-imports",
    "title": "선형모형의 적",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing\nimport sklearn.impute\nimport seaborn as sns\nimport sklearn.tree"
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html#선형모형의-적",
    "href": "2023_MP/practice/8. 선형모형의 적.html#선형모형의-적",
    "title": "선형모형의 적",
    "section": "2. 선형모형의 적",
    "text": "2. 선형모형의 적\n\nA. 결측치의 존재\n\n문제 : 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않는다.\n- 해결방안\n1 : 결측치를 제거\n\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n둘을 혼합\n\n\n\n결측치를 impute\n\n\n\ntrain에서는 fit_transform, test에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값으로 일괄 impute\ninterploation(이미지 또는 시계열, 근처의 값과 자연스럽게 연동되도록 만들 수 있음)\n~train, test data를 합쳐서 fit_transform~ 이건 정보누수로 실격사유가 된다\n\n\n- 사용 가능한 코드나 모듈\n\nisna(), dropna(), sklearn.inpute의 하위 모듈 등."
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html#b.-다중공선성의-존재",
    "href": "2023_MP/practice/8. 선형모형의 적.html#b.-다중공선성의-존재",
    "title": "선형모형의 적",
    "section": "### B. 다중공선성의 존재",
    "text": "### B. 다중공선성의 존재\n문제 : 데이터의 설명변수가 역할이 겹칠 경우, 선형모형의 일반화 성능이 좋지 않음.\n- 해결방안\n\n\n변수 제거 &gt; 설명변수 간 corr을 파악하고, 느낌적으로 제거 &gt; &gt; PCA 등 차원축소기법을 이용한 제거\n공선성을 가지는 변수를 모아 새로운 변수로 변환 &gt; 느낌적으로 변환 &gt; &gt; PCA를 이용한 변환\nLasso, Ridge 등 패널티 계열을 사용 &gt; Lasso : l1 / liblinear &gt; &gt; Ridge : l2 &gt; &gt; Elastic net\n\n\n- corr파악 후 느낌적으로 제거의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\n\npandas의 데이터프레임에는 자체적으로 해당 메소드를 지원한다.\n\n\nsns.heatmap(X.corr(), annot = True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n- toeic과 유사 toeic끼리 상관성이 짙네?\n\n제거한다.\n\n\nC. 관련이 없는 변수의 존재\n\n문제 : 데이터에서 불필요한 설명변수가 너무 많을 경우, 선형모형의 일반화 성능이 좋지 않음.(overfitting)\n\n예시 : 고객이름, ID, Index 관련 변수(물론 얘네들도 어딘가 쓸모가 있을 수도 있다…)\n\n- 해결방법\n\n\n변수 제거 &gt; (y, X)의 corr을 파악하고 느낌적으로 제거(위에서와 달리 관련이 있어야 한다.) &gt; &gt; PCA를 이용한 제거 &gt; &gt; Lasso를 이용한 제거(여기서 Ridge는 사용하면 안된다. 해당 모듈은 유사한 것들의 계수 합이 일정하도록 조정하는 거니까…)\n더 많은 데이터를 확보 &gt; 하지만 이는 어렵다… 어떤 변수가 관련이 없다는 것을 파악하기 위해선 데이터를 많이 가져와야 하는데, Feature의 수가 많아질 때 필요한 데이터의 수는 지수적으로 증가한다.\n\n\n- 느낌적으로 제거 예시\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(), annot = True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nbalance0 ~ 2는 employment와의 상관성이 낮다. 따라서 제거하고 분석한다.\n\n\n## 1\nX = df_train.loc[:, :'gpa']\ny = df_train.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.882\n\n\n- Lasso를 이용한 제거 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n로지스틱 선형 회귀가 필요한 경우이다. 로지스틱 또한 penalty 계열 분석을 할 수 있다.\n\n\n## 1\nX = df_train.drop('employment', axis = 1)\ny = df_train.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(Cs = [0.1, 1, 10, 100], penalty = 'l1', solver = 'liblinear', random_state = 42)\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.876\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))  ## 시리즈의 경우 1차원의 입력값만 받는다.\ns.index = X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64\n\n\n\n위에서 쓸모없는 것을 제거하고 분석한 것에 비해 점수가 낮지만, 쓸모없는 것이라는 사실을 모르는 상황에서는 Lasso가 상당히 괜찮다."
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html#d.-이상치의-존재",
    "href": "2023_MP/practice/8. 선형모형의 적.html#d.-이상치의-존재",
    "title": "선형모형의 적",
    "section": "### D. 이상치의 존재",
    "text": "### D. 이상치의 존재\n문제 : 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 해결방법\n\n\n이상치를 제거하고 분석 &gt; 느낌적으로 제거 &gt; &gt; 이상치를 감지하는 지표를 사용하여 제거 &gt; &gt; 이상치를 자동으로 감지하는 모형 사용하여 이상치 제거 후 분석\n로버스트 선형회귀 계열을 이용 &gt; 이상치에 큰 영향을 받지 않음 &gt; sklearn.linear_model.HuberRegressor 등\n이상치를 완화시키는 변환을 사용 &gt; sklearn.preprocessing.PowerTransformer를 이용\n\n\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\n\ntransformr.fit_transform(df_train)\n\narray([[-1.40729341,  2.42405408],\n       [-1.31406689, -0.18677452],\n       [-1.13030154,  0.16485704],\n       [-0.50278108,  0.17667635],\n       [-0.02130412,  0.41617603],\n       [ 0.13926015,  0.55696978],\n       [ 0.81742569, -1.03040835],\n       [ 0.96759638, -0.62032873],\n       [ 0.96759638, -1.33362249],\n       [ 1.48386844, -0.56759919]])\n\n\n\nx, y = transformr.fit_transform(df_train).T\nx, y\n\n(array([-1.40729341, -1.31406689, -1.13030154, -0.50278108, -0.02130412,\n         0.13926015,  0.81742569,  0.96759638,  0.96759638,  1.48386844]),\n array([ 2.42405408, -0.18677452,  0.16485704,  0.17667635,  0.41617603,\n         0.55696978, -1.03040835, -0.62032873, -1.33362249, -0.56759919]))\n\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o', label = 'before')\nplt.plot(x, y, 'o', label = 'after')\nplt.xlabel('temp')\nplt.ylabel('ice sales')\nplt.legend()\nplt.show()\n\n\n\n\n\n강제로 정규화한 모습이다.\n\n\ntransformr.inverse_transform(transformr.fit_transform(df_train))\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n  warnings.warn(\n\n\narray([[-4.1       , 50.        ],\n       [-3.7       ,  9.2341745 ],\n       [-3.        ,  9.64277825],\n       [-1.3       ,  9.65789368],\n       [-0.5       ,  9.98778744],\n       [-0.3       , 10.20595116],\n       [ 0.3       ,  8.48692458],\n       [ 0.4       ,  8.81722682],\n       [ 0.4       ,  8.27315516],\n       [ 0.7       ,  8.8637837 ]])\n\n\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\n어차피 역변환 할 수 있는 것은 상관이 없다.\n\n\nE. 교호작용의 존재\n\n문제 : 설명 변수 간의 상호작용이 있는 경우, 이를 고려하지 않으면 데이터를 잘 설명하지 못할 수 있음.\n- 해결방안\n\n\n교호작용이 있는 열들의 값끼리 곱함\n교호작용에 영향을 받지 않는 모델 사용 &gt; sklearn.tree.DecisionTreeRegressor()\n\n\n- 교호작용이 있는 열을 곱함\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n둘 다 했을 때 가장 평균이 높고, 각각 하는 것만으로는 그렇게 큰 영향은 없는 것 같다.\n\n교호작용을 고려하지 않은 분석\n\n## 1\nX = df_train.drop('Weight_Loss', axis = 1)\ny = df_train.Weight_Loss\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.8208414124769222\n\n\n\npredictr.coef_\n\narray([5.21904037, 9.74766346])\n\n\n\n보충제는 5kg, 운동은 10kg의 감량효과가 있다고 추정하고 있다.\n\n\ndf_train.assign(Weight_Loss_hat = predictr.predict(X)).drop('Weight_Loss', axis = 1)\\\n.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss_hat', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n예측값과 실제 값의 차이가 크다.\n\n교호작용을 고려한 분석\n\ndf_train.assign(Interaction = df_train.Supplement * df_train.Exercise)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nInteraction\n\n\n\n\n0\nFalse\nFalse\n-0.877103\nFalse\n\n\n1\nTrue\nFalse\n1.604542\nFalse\n\n\n2\nTrue\nTrue\n13.824148\nTrue\n\n\n3\nTrue\nTrue\n13.004505\nTrue\n\n\n4\nTrue\nTrue\n13.701128\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\nFalse\n\n\n9996\nFalse\nFalse\n-0.217816\nFalse\n\n\n9997\nFalse\nTrue\n4.072701\nFalse\n\n\n9998\nTrue\nFalse\n-0.253796\nFalse\n\n\n9999\nFalse\nFalse\n-1.399092\nFalse\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\n## 1\n_df = df_train.assign(Interaction = df_train.Supplement * df_train.Exercise)\nX = _df.drop('Weight_Loss', axis = 1)\ny = _df.Weight_Loss\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9727754257714795\n\n\n\n정확도가 개선되었다.\n\n\ndf_train.assign(Weight_Loss_hat = predictr.predict(X)).drop('Weight_Loss', axis = 1)\\\n.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss_hat', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n평균을 보면(오차를 제거함) 표본과 동일한 것을 볼 수 있다."
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html#교호작용",
    "href": "2023_MP/practice/8. 선형모형의 적.html#교호작용",
    "title": "선형모형의 적",
    "section": "3. 교호작용",
    "text": "3. 교호작용"
  },
  {
    "objectID": "2023_MP/practice/8. 선형모형의 적.html#a.-아이스크림-타입-별-판매량",
    "href": "2023_MP/practice/8. 선형모형의 적.html#a.-아이스크림-타입-별-판매량",
    "title": "선형모형의 적",
    "section": "### A. 아이스크림 타입 별 판매량",
    "text": "### A. 아이스크림 타입 별 판매량\n- 왠지 익숙한 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nset(df_train.type)\n\n{'choco', 'vanilla'}\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n- 아이스크림의 종류에 따라 온도가 판매량에 미치는 정도가 다를 것으로 예상된다.\n\n아이스크림 종류와 온도간에 교호작용이 있다.\n\n교호작용을 고려하지 않은 경우\n\n## 1\nX = pd.get_dummies(df_train.drop('sales', axis = 1))\ny = df_train.sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9249530603100549\n\n\n\n이것만으로도 나름 높은 점수가 나오긴 했지만… 언제나 개선할 수 있는 건 개선해야 한다.\n\n\n_df = df_train.assign(sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, _df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, _df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n마음속의 언더라잉과 맞지 않는다 : 언더피팅된 상황이다.\n\n교호작용을 고려\n\nvan = pd.get_dummies(df_train.type, drop_first = True)*1\nvan\n\n\n\n\n\n\n\n\nvanilla\n\n\n\n\n0\n0\n\n\n1\n0\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n...\n...\n\n\n95\n1\n\n\n96\n1\n\n\n97\n1\n\n\n98\n1\n\n\n99\n1\n\n\n\n\n200 rows × 1 columns\n\n\n\n\n_df = df_train.assign(Interaction = df_train.temp * van.vanilla)\n_df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nInteraction\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n-0.0\n\n\n1\n-3.7\n35.852524\nchoco\n-0.0\n\n\n2\n-3.0\n37.428335\nchoco\n-0.0\n\n\n3\n-1.3\n38.323681\nchoco\n-0.0\n\n\n4\n-0.5\n39.713362\nchoco\n-0.0\n\n\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n12.4\n\n\n96\n13.4\n129.300464\nvanilla\n13.4\n\n\n97\n14.7\n136.596568\nvanilla\n14.7\n\n\n98\n15.0\n136.213140\nvanilla\n15.0\n\n\n99\n15.2\n135.595252\nvanilla\n15.2\n\n\n\n\n200 rows × 4 columns\n\n\n\n\n## 1\nX = pd.get_dummies(_df.drop('sales', axis = 1), drop_first = True)\ny = _df.sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9865793819066231\n\n\n\n점수가 훨씬 높게 나왔다.\n\n\n__df = _df.assign(sales_hat = predictr.predict(X))\n__df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nInteraction\nsales_hat\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n-0.0\n31.403121\n\n\n1\n-3.7\n35.852524\nchoco\n-0.0\n32.209366\n\n\n2\n-3.0\n37.428335\nchoco\n-0.0\n33.620295\n\n\n3\n-1.3\n38.323681\nchoco\n-0.0\n37.046835\n\n\n4\n-0.5\n39.713362\nchoco\n-0.0\n38.659325\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n12.4\n122.492017\n\n\n96\n13.4\n129.300464\nvanilla\n13.4\n127.521196\n\n\n97\n14.7\n136.596568\nvanilla\n14.7\n134.059129\n\n\n98\n15.0\n136.213140\nvanilla\n15.0\n135.567883\n\n\n99\n15.2\n135.595252\nvanilla\n15.2\n136.573719\n\n\n\n\n200 rows × 5 columns\n\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, __df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, __df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\npredictr.coef_\n\narray([ 2.01561216,  3.01356716, 20.46306209])\n\n\n\n모델이 언더라잉을 잘 따라가는 것을 볼 수 있다.(애초에 계수가 세개가 됨…)\n\n\nB. 교호작용, tree\n\nsklearn.tree.DecisionTreeRegressor()를 사용하면 교호작용을 손쉽게 적합할 수 있다.\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n아까와 동일한 자료를 tree로 분석해보자…\n\n\n## 1\nX = pd.get_dummies(df_train.drop('sales', axis = 1), drop_first = True)\ny = df_train.sales\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9963887702553287\n\n\n\n높은 스코어가 나온다.(오버피팅된 것은 아닐까?)\n\n\n_df = df_train.assign(sales_hat = predictr.predict(X))\n_df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nsales_hat\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n32.950261\n\n\n1\n-3.7\n35.852524\nchoco\n35.852524\n\n\n2\n-3.0\n37.428335\nchoco\n37.428335\n\n\n3\n-1.3\n38.323681\nchoco\n38.323681\n\n\n4\n-0.5\n39.713362\nchoco\n39.713362\n\n\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n119.708075\n\n\n96\n13.4\n129.300464\nvanilla\n129.300464\n\n\n97\n14.7\n136.596568\nvanilla\n136.596568\n\n\n98\n15.0\n136.213140\nvanilla\n136.213140\n\n\n99\n15.2\n135.595252\nvanilla\n135.595252\n\n\n\n\n200 rows × 4 columns\n\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco', alpha = 0.5)\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla', alpha = 0.5)\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, _df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, _df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n오차항까지 적합하고는 있으나… 처음 교호작용을 고려하지 않은 모델보다 성능은 좋은 것 같다. 따라서 이는 상당히 유용하다."
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html",
    "href": "2023_MP/Titanic/1. practice.html",
    "title": "Kaggle | 1st practice",
    "section": "",
    "text": "Kaggle의 competition에 대해 차근차근히 알아보고 첫 제출까지 해보도록 하자."
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html#라이브러리-imports",
    "href": "2023_MP/Titanic/1. practice.html#라이브러리-imports",
    "title": "Kaggle | 1st practice",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\n\n\n# 캐글에 있는 노트북을 이용하면 가상 컴퓨터에 세 개의 파일들이 직접 들어온다.\n\ntr = pd.read_csv(\"./data/train.csv\")\ntr.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntst = pd.read_csv('./data/test.csv')\ntst.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS"
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html#kaggle-competition",
    "href": "2023_MP/Titanic/1. practice.html#kaggle-competition",
    "title": "Kaggle | 1st practice",
    "section": "2. Kaggle Competition",
    "text": "2. Kaggle Competition\n\nA. 데이터 구경\n\n- 데이터의 설명을 빠르게 파악하는 방법\n1. 변수 위주로 kaggle 홈페이지에서 파악\n1. 구글 번역기 사용\n1. ChatGPT 이용\n\nChatGPT가 옳지 않은 소리를 할 때도 있지만, 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수 이름이 약어로 된 경우가 많은데, 이럴 경우 GPT가 유용하다."
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html#b.-메뉴",
    "href": "2023_MP/Titanic/1. practice.html#b.-메뉴",
    "title": "Kaggle | 1st practice",
    "section": "### B. 메뉴",
    "text": "### B. 메뉴\n\nOverview(개요)\n\n\n경진대회 주최자가 경진 대회의 배경, 목표, 데이터셋 설명 등을 기술.\n\n\nData(데이터)\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 포함됨.\n\n\nCode(코드)\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 포함됨.\n\n\nDiscussion(토론)\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이뤄짐.\n\n\nLeaderboard(리더보드)\n\n\n경진대회 참가자들의 모델에 대한 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRule(규칙)\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 작업, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들 * Overview : martic, prize, timeline * Rules : matric, 외부데이터 사용 가능 여부, 하루 최대 제출 수, 최종 선택 가능한 솔루션 수(limit)\n- 대회의 유형 * Getting Started : 상을 제공하지 않음. 튜토리얼 전용. * Featured : 가장 일반적인 유형, 스폰서 회사의 비즈니스 관련 문제이므로 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야 하고 발표할 것을 요구받을 수 있음. * Analytics : 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html#데이터-분석",
    "href": "2023_MP/Titanic/1. practice.html#데이터-분석",
    "title": "Kaggle | 1st practice",
    "section": "3. 데이터 분석",
    "text": "3. 데이터 분석\n\nA.test\n\n- 제출 결과는 리더보드에서 확인 가능\n- 답을 알 수 없고 제출해야 스코어만 확인할 수 있음"
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html#b.-train---스스로-풀어보고-채점할-수-있음",
    "href": "2023_MP/Titanic/1. practice.html#b.-train---스스로-풀어보고-채점할-수-있음",
    "title": "Kaggle | 1st practice",
    "section": "### B. train - 스스로 풀어보고 채점할 수 있음",
    "text": "### B. train - 스스로 풀어보고 채점할 수 있음\n- train 데이터를 채점해보자.\n# accuracy의 계산\n\ndf = pd.DataFrame({'surv' : [1,0,1,1,0], 'sex' : ['f','m','f','m','m']})\n\n- surv+열과 sex열에서 sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\ndf.surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: surv, dtype: int64\n\n\n\ndf.sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: sex, dtype: object\n\n\n\n(df.sex == 'f')*1  ## bool이 원소인 list에 1을 곱해준다. f이면 1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: sex, dtype: int32\n\n\n- 결과를 정리하면 아래와 같다.\n\npd.DataFrame({'real' : df.surv, 'estimate' : (df.sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n\nprint((df.surv == (df.sex == 'f')*1).sum()/5)\n##print((df.surv == (df.sex == 'f')*1).mean()) ## 동일한 코드\n\n0.8\n\n\n- 실제 train 자료에 접목해서 여성만 생존한다고 하여 accuracy를 구해보자.\n\n(tr.Survived == (tr.Sex == 'female')*1).mean()\n\n0.7867564534231201\n\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()  ## True or False는 0, 1로도 구분되나보다.\n\n0.7867564534231201\n\n\n- 그러면 예측한 데이터프레임을 파일로 만들어서 보내보자.\n\ntst[['PassengerId']].assign(Survived = (tst.Sex == 'female')*1)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\ntst[['PassengerId']].assign(Survived = (tst.Sex == 'female')*1).to_csv(\"gender_submission.csv\", index = False)\n\n\n해당 파일을 캐글에 업로드하면 submission이 완료된다.\n\n\nindex를 날려줘야 원하는 형식이 된다. (index = False를 하지 않으면 csv파일에 index의 숫자가 같이 저장된다…)"
  },
  {
    "objectID": "2023_MP/Titanic/1. practice.html#개념",
    "href": "2023_MP/Titanic/1. practice.html#개념",
    "title": "Kaggle | 1st practice",
    "section": "4. 개념",
    "text": "4. 개념\n- 캐글 대회는 시험과 비슷하다. * 캐글대회를 여는 사람은 보통 (1) 모의고사문제+답 (training set) (2) 실제시험문제 (test set)를 준다. * (1)의 자료에서는 문제(X,독립변수,설명변수)와 답(y,종속변수,반응변수)이 함께 주어진다. * (2)의 자료에서는 문제(X,독립변수,설명변수)만 주어진다. * 우리는 (1)을 이용하여 문제(X,독립변수,설명변수)와 답(y,종속변수,반응변수)사이의 관계를 찾아내는 훈련을 한다. * 그리고 그 훈련이 잘 되었는지를 평가하기 위해서 (2)를 풀어보고 그 결과를 제출한다."
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html",
    "href": "2023_MP/Titanic/3. autogluon.html",
    "title": "Kaggle | Autogluon",
    "section": "",
    "text": "자동 예측 프로그램인 Autogluon을 활용하여 titanic data를 적합해보자!"
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#라이브러리-imports",
    "href": "2023_MP/Titanic/3. autogluon.html#라이브러리-imports",
    "title": "Kaggle | Autogluon",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#pip install autogluon\n\n\nimport pandas as pd\nimport numpy as np\n\n## tabular(테이블) 형식의 데이터를 다루는 모듈을 다운로드한다.\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#분석",
    "href": "2023_MP/Titanic/3. autogluon.html#분석",
    "title": "Kaggle | Autogluon",
    "section": "2. 분석",
    "text": "2. 분석\n\nA. 데이터 입력\n\n\n문제를 받아오는 과정으로 비유할 수 있다.\n\n\ntr = TabularDataset('./data/train.csv')  ## 학습할 데이터\ntst = TabularDataset('./data/test.csv')\n\n## tr = TabularDataset('/kaggle/input/titanic/train.csv')  ## 학습할 데이터\n## tst = TabularDataset('/kaggle/input/titanic/test.csv')\n\n## tr = pd.read_csv('/kaggle/input/titanic/train.csv')\n## tst"
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#b.-predictor-생성",
    "href": "2023_MP/Titanic/3. autogluon.html#b.-predictor-생성",
    "title": "Kaggle | Autogluon",
    "section": "### B. Predictor 생성",
    "text": "### B. Predictor 생성\n\n문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\n\npredictr = TabularPredictor('Survived') ## target variable이 들어있는 데이터프레임, 변수 철자는 임의로 틀리게 설정\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_130536\"\n\n\n\npredictr는 뭔데?\n\n\ntype(predictr)\n\nautogluon.tabular.predictor.predictor.TabularPredictor\n\n\n\n대충 autogluon에서의 class인듯.\n\n\nC. 적합(fit)\n\n\n학습 과정에 해당한다.\n\n\npredictr.fit(tr) ## 학생(predictr)에게 문제(tr)를 주어 학습을 시킴(predictr.fit(tr))\n##tr 그 자체로 학습할 수 있는 건 다 시킨다. sklearn의 모델과는 차이가 있음\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_130536\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.71 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1930.98 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.3s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.36s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    1.83s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    1.08s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    7.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8324   = Validation score   (accuracy)\n    3.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    1.32s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8212   = Validation score   (accuracy)\n    7.74s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.93s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.67s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 28.23s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_130536\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20df4525d50&gt;\n\n\n학습 완료, 이에 따라 리더보드를 확인한다. (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.009000  0.928348                0.009000           0.928348            1       True         13\n1       NeuralNetFastAI   0.832402       0.028001  3.279027                0.028001           3.279027            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.029001  3.949707                0.001000           0.670681            2       True         14\n3              CatBoost   0.826816       0.006002  7.468165                0.006002           7.468165            1       True          7\n4              LightGBM   0.821229       0.006004  0.432719                0.006004           0.432719            1       True          4\n5        NeuralNetTorch   0.821229       0.031999  7.740229                0.031999           7.740229            1       True         12\n6            LightGBMXT   0.815642       0.005002  1.084200                0.005002           1.084200            1       True          3\n7        ExtraTreesGini   0.815642       0.061763  0.516426                0.061763           0.516426            1       True          8\n8      RandomForestEntr   0.815642       0.064586  0.538568                0.064586           0.538568            1       True          6\n9      RandomForestGini   0.815642       0.064718  0.637898                0.064718           0.637898            1       True          5\n10              XGBoost   0.810056       0.013002  1.323482                0.013002           1.323482            1       True         11\n11       ExtraTreesEntr   0.810056       0.062742  0.519159                0.062742           0.519159            1       True          9\n12       KNeighborsDist   0.653631       0.005996  0.012003                0.005996           0.012003            1       True          2\n13       KNeighborsUnif   0.653631       0.215770  1.826697                0.215770           1.826697            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.009000\n0.928348\n0.009000\n0.928348\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.028001\n3.279027\n0.028001\n3.279027\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.029001\n3.949707\n0.001000\n0.670681\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.006002\n7.468165\n0.006002\n7.468165\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.006004\n0.432719\n0.006004\n0.432719\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.031999\n7.740229\n0.031999\n7.740229\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.005002\n1.084200\n0.005002\n1.084200\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.061763\n0.516426\n0.061763\n0.516426\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.064586\n0.538568\n0.064586\n0.538568\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.064718\n0.637898\n0.064718\n0.637898\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.013002\n1.323482\n0.013002\n1.323482\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.062742\n0.519159\n0.062742\n0.519159\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.005996\n0.012003\n0.005996\n0.012003\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.215770\n1.826697\n0.215770\n1.826697\n1\nTrue\n1\n\n\n\n\n\n\n\nscore_val이 의미하는 것 * 실제로 predictr가 학습한 것은? &gt; predictor와 train set이 있고, train set에 데이터가 1000개 있다고 하면 해당 데이터를 전부 가용하지 않는다. &gt; * 800개를 사용한다고 하면 200개는 학습하지 않고 답을 맞춰 보는 식이다. &gt; &gt; * 200개는 왜 남겨두지? &gt; &gt; 문제에서 답을 찾는 규칙이 맞는지, 다른 데이터들에 대해서도 일반화시킬 수 있는 지 테스트 해보면 좋을 것 같다. 따라서 나머지 데이터셋에서 분석을 해본다. &gt; &gt; 실제 테스트에서 잘하기 위한 자체적 테스트셋에 해당, 200개의 나머지 테스트용 데이터셋을 validation set이라 일컫는다.\n\n\n\n\ntrain\nval\n\n\n\n\n학생1\n95%\n72%\n\n\n학생2\n80%\n80%\n\n\n…\n…\n…\n\n\n\ntrain(연습문제)만 계속 푼 것 보다, val(모의고사)에서 가장 높은 점수를 받은 것이 유의미할 것.\n\n그러니까 score_val는 모의고사 점수라고 보면 된다.\n\n- 따라서 가장 높은 점수를 받은 WeightedEnsemble_L2모델을 사용해보자.[1]\n\n[1] 처음 실습할 땐 분명 이게 제일 높았었는데…"
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#d.-예측predict",
    "href": "2023_MP/Titanic/3. autogluon.html#d.-예측predict",
    "title": "Kaggle | Autogluon",
    "section": "### D. 예측(predict)",
    "text": "### D. 예측(predict)\n\n학습 이후에 문제를 푸는 과정으로 비유.\n\n기존에 했던 분석들\n\n무조건 남자는 죽고, 여자는 사는 형식 0.7x / 0.76555\nRandomForestClassifier를 사용한 형식 0.8x / 0.77511\nRandomForestClassifier에서 하이퍼파라미터를 조정한 형식 0.8x / 0.76555 (트레인 셋에서의 분석에서는 더 높았는데 실제 결과는 오히려 더 낮았다.)\n\n4. WeightedEnsemble_L2모델 사용(알아서 사용하긴 함)\ntrain set을 일단 풀어보자(predict)\n\ntype(tr) ## 처음 보는 것으로 저장되는데 데이터프레임에서 쓸 수 있는 모든 기능들을 다 사용할 수 있다.\n\nautogluon.core.dataset.TabularDataset\n\n\n\ntr.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.8810325476992144\n\n\n\n정확도가 0.9349나 된다. 상당히 기대가 되는 부분\n\n\npredictr.predict(tst)\n\n0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n413    0\n414    1\n415    0\n416    0\n417    0\nName: Survived, Length: 418, dtype: int64\n\n\n\ntst.assign(Survived = predictr.predict(tst)).loc[:, ['PassengerId', 'Survived']]\\\n.to_csv('autogluon_submission.csv', index = False)\n\n\n제출 결과 정확도는 0.78947로 지금껏 가장 높은 수치가 나왔다."
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#개선",
    "href": "2023_MP/Titanic/3. autogluon.html#개선",
    "title": "Kaggle | Autogluon",
    "section": "3. 개선",
    "text": "3. 개선\n\n결과를 좀 더 개선할 수 있지 않을까?\n\n\nA. Fsize로 feature engeenering\n\n1) 데이터\n\ntr = TabularDataset('./data/train.csv')  ## 학습할 데이터\ntst = TabularDataset('./data/test.csv')\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./data/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n-피쳐 엔지니어링\n\ntr.assign(Fsize = tr.SibSp + tr.Parch)\ntst.assign(Fsize = tst.SibSp + tst.Parch)\n\n#tr.eval('Fsize = SibSp + Parch')\n#tst.eval('Fsize = SibSp + Parch')\n\ntr.head()  ## 원본 데이터를 손상시키지 않음, Fsize 열이 추가되지 않은 것을 알 수 있음\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n2) Predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132447\"\n\n\n3) 적합(fit)\n\npredictr.fit(tr.assign(Fsize = tr.SibSp + tr.Parch))  ## 새로운 데이터셋을 추가하여 학습\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132447\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.59 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1923.41 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.3s = Fit runtime\n    12 features in original data used to generate 25 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.37s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6425   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8268   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8492   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.7989   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    6.8s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8045   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    2.76s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.68s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    9.58s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.838    = Validation score   (accuracy)\n    0.83s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8492   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 25.25s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132447\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d8e852770&gt;\n\n\n-리더보드 확인\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0              LightGBM   0.849162       0.010999  0.554687                0.010999           0.554687            1       True          4\n1   WeightedEnsemble_L2   0.849162       0.012000  1.201853                0.001000           0.647166            2       True         14\n2         LightGBMLarge   0.837989       0.005001  0.827996                0.005001           0.827996            1       True         13\n3       NeuralNetFastAI   0.832402       0.024005  2.761039                0.024005           2.761039            1       True         10\n4        NeuralNetTorch   0.832402       0.034000  9.577353                0.034000           9.577353            1       True         12\n5            LightGBMXT   0.826816       0.004981  0.426716                0.004981           0.426716            1       True          3\n6              CatBoost   0.826816       0.005996  6.798872                0.005996           6.798872            1       True          7\n7               XGBoost   0.821229       0.008010  0.680577                0.008010           0.680577            1       True         11\n8      RandomForestEntr   0.815642       0.063459  0.504724                0.063459           0.504724            1       True          6\n9        ExtraTreesEntr   0.804469       0.062692  0.443597                0.062692           0.443597            1       True          9\n10       ExtraTreesGini   0.804469       0.065061  0.448723                0.065061           0.448723            1       True          8\n11     RandomForestGini   0.798883       0.064575  0.510397                0.064575           0.510397            1       True          5\n12       KNeighborsUnif   0.648045       0.007998  0.008998                0.007998           0.008998            1       True          1\n13       KNeighborsDist   0.642458       0.007999  0.011001                0.007999           0.011001            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBM\n0.849162\n0.010999\n0.554687\n0.010999\n0.554687\n1\nTrue\n4\n\n\n1\nWeightedEnsemble_L2\n0.849162\n0.012000\n1.201853\n0.001000\n0.647166\n2\nTrue\n14\n\n\n2\nLightGBMLarge\n0.837989\n0.005001\n0.827996\n0.005001\n0.827996\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.024005\n2.761039\n0.024005\n2.761039\n1\nTrue\n10\n\n\n4\nNeuralNetTorch\n0.832402\n0.034000\n9.577353\n0.034000\n9.577353\n1\nTrue\n12\n\n\n5\nLightGBMXT\n0.826816\n0.004981\n0.426716\n0.004981\n0.426716\n1\nTrue\n3\n\n\n6\nCatBoost\n0.826816\n0.005996\n6.798872\n0.005996\n6.798872\n1\nTrue\n7\n\n\n7\nXGBoost\n0.821229\n0.008010\n0.680577\n0.008010\n0.680577\n1\nTrue\n11\n\n\n8\nRandomForestEntr\n0.815642\n0.063459\n0.504724\n0.063459\n0.504724\n1\nTrue\n6\n\n\n9\nExtraTreesEntr\n0.804469\n0.062692\n0.443597\n0.062692\n0.443597\n1\nTrue\n9\n\n\n10\nExtraTreesGini\n0.804469\n0.065061\n0.448723\n0.065061\n0.448723\n1\nTrue\n8\n\n\n11\nRandomForestGini\n0.798883\n0.064575\n0.510397\n0.064575\n0.510397\n1\nTrue\n5\n\n\n12\nKNeighborsUnif\n0.648045\n0.007998\n0.008998\n0.007998\n0.008998\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.642458\n0.007999\n0.011001\n0.007999\n0.011001\n1\nTrue\n2\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(tr.Survived == predictr.predict(tr.assign(Fsize = tr.SibSp + tr.Parch))).mean()\n\n0.9696969696969697\n\n\n\ntst.assign(Survived = predictr.predict(tst.assign(Fsize = tst.SibSp + tst.Parch))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)\n\n\n제출 결과 : 점수가 오히려 더 낮아졌음\n\n더 개선해보자"
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#b.-fsize-drop",
    "href": "2023_MP/Titanic/3. autogluon.html#b.-fsize-drop",
    "title": "Kaggle | Autogluon",
    "section": "### B. Fsize + drop",
    "text": "### B. Fsize + drop\n1) data\n-피처 엔지니어링 (데이터 불러오는 건 위에서 했으니 일단 생략\n\n_tr = tr.assign(Fsize = lambda _df : _df.SibSp + _df.Parch).drop(['SibSp','Parch'],axis=1)\n_tst = tst.assign(Fsize = tst.SibSp + tst.Parch).drop(['SibSp','Parch'],axis=1)\n\n_tr.head()\n## df.drop(columns = [])\n## df.drop([], axis = 1) columns라고 지정해주지 않으면 디폴트로 행을 삭제하기 때문에\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\nA/5 21171\n7.2500\nNaN\nS\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n2) Predictor 생성\n\npredictr = TabularPredictor('Survived')\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132627\"\n\n\n3) 적합(fit)\n\npredictr.fit(_tr)\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132627\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.56 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1899.15 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.3s = Fit runtime\n    10 features in original data used to generate 23 features in processed data.\n    Train Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.36s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8212   = Validation score   (accuracy)\n    0.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.838    = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.8045   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8324   = Validation score   (accuracy)\n    7.6s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.7989   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8045   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.95s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8436   = Validation score   (accuracy)\n    10.87s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.82s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8492   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 26.66s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132627\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d858cd060&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.849162       0.043977  12.163954                0.000984           0.653803            2       True         14\n1        NeuralNetTorch   0.843575       0.032010  10.867509                0.032010          10.867509            1       True         12\n2              LightGBM   0.837989       0.010982   0.642641                0.010982           0.642641            1       True          4\n3         LightGBMLarge   0.832402       0.006009   0.821788                0.006009           0.821788            1       True         13\n4              CatBoost   0.832402       0.006051   7.597862                0.006051           7.597862            1       True          7\n5               XGBoost   0.826816       0.013022   0.450137                0.013022           0.450137            1       True         11\n6       NeuralNetFastAI   0.826816       0.017003   1.949074                0.017003           1.949074            1       True         10\n7            LightGBMXT   0.821229       0.006997   0.471555                0.006997           0.471555            1       True          3\n8      RandomForestEntr   0.810056       0.063482   0.526611                0.063482           0.526611            1       True          6\n9      RandomForestGini   0.804469       0.061717   0.544051                0.061717           0.544051            1       True          5\n10       ExtraTreesEntr   0.804469       0.064033   0.519959                0.064033           0.519959            1       True          9\n11       ExtraTreesGini   0.798883       0.064803   0.533057                0.064803           0.533057            1       True          8\n12       KNeighborsUnif   0.653631       0.006997   0.011003                0.006997           0.011003            1       True          1\n13       KNeighborsDist   0.648045       0.031997   0.016009                0.031997           0.016009            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.849162\n0.043977\n12.163954\n0.000984\n0.653803\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.843575\n0.032010\n10.867509\n0.032010\n10.867509\n1\nTrue\n12\n\n\n2\nLightGBM\n0.837989\n0.010982\n0.642641\n0.010982\n0.642641\n1\nTrue\n4\n\n\n3\nLightGBMLarge\n0.832402\n0.006009\n0.821788\n0.006009\n0.821788\n1\nTrue\n13\n\n\n4\nCatBoost\n0.832402\n0.006051\n7.597862\n0.006051\n7.597862\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.013022\n0.450137\n0.013022\n0.450137\n1\nTrue\n11\n\n\n6\nNeuralNetFastAI\n0.826816\n0.017003\n1.949074\n0.017003\n1.949074\n1\nTrue\n10\n\n\n7\nLightGBMXT\n0.821229\n0.006997\n0.471555\n0.006997\n0.471555\n1\nTrue\n3\n\n\n8\nRandomForestEntr\n0.810056\n0.063482\n0.526611\n0.063482\n0.526611\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.804469\n0.061717\n0.544051\n0.061717\n0.544051\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.804469\n0.064033\n0.519959\n0.064033\n0.519959\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.798883\n0.064803\n0.533057\n0.064803\n0.533057\n1\nTrue\n8\n\n\n12\nKNeighborsUnif\n0.653631\n0.006997\n0.011003\n0.006997\n0.011003\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.648045\n0.031997\n0.016009\n0.031997\n0.016009\n1\nTrue\n2\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(_tr.Survived == predictr.predict(_tr)).mean()\n\n0.9472502805836139\n\n\n\npredictr.predict(_tr)\n\n0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n886    0\n887    1\n888    0\n889    1\n890    0\nName: Survived, Length: 891, dtype: int64\n\n\n\n_tst.assign(Survived = predictr.predict(_tst)).loc[:, ['PassengerId', 'Survived']]\\\n.to_csv('autogluon(Fsize,Drop)_submission.csv', index = False)\n\n\n지금껏 가장 높은 결과가 나왔다!\n\n\n다중 공선성 문제를 개선한 결과라고 볼 수 있지… 음음.\n\n아니, 모자라. 더 개선해!!!"
  },
  {
    "objectID": "2023_MP/Titanic/3. autogluon.html#c.-best_quality",
    "href": "2023_MP/Titanic/3. autogluon.html#c.-best_quality",
    "title": "Kaggle | Autogluon",
    "section": "### C. best_quality",
    "text": "### C. best_quality\n1) data\n\ntr = TabularDataset(\"./data/train.csv\")\ntst = TabularDataset(\"./data/test.csv\")\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./data/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n2) predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132948\"\n\n\n3) 적합(fit)\n\n어떤 자원이 들어가든, 전부 지원해줄 테니 가장 좋은 퀄리티로 산출해!!\n\n\npredictr.fit(tr, presets = 'best_quality') \n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132948\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.53 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1996.57 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.4s = Fit runtime\n    11 features in original data used to generate 24 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.39s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6296   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6352   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    3.82s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8373   = Validation score   (accuracy)\n    5.36s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8339   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8305   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8552   = Validation score   (accuracy)\n    72.17s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8238   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8316   = Validation score   (accuracy)\n    0.49s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\nNo improvement since epoch 7: early stopping\nNo improvement since epoch 6: early stopping\nNo improvement since epoch 7: early stopping\n    0.853    = Validation score   (accuracy)\n    20.42s   = Training   runtime\n    0.13s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8373   = Validation score   (accuracy)\n    3.6s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8462   = Validation score   (accuracy)\n    68.5s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8429   = Validation score   (accuracy)\n    8.68s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8552   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 188.35s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132948\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d90435000&gt;\n\n\n\n대신 시간이 상당히 오래 걸린다…\n\n- 리더보드 확인\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.855219       0.036927  72.167391                0.036927          72.167391            1       True          7\n1       WeightedEnsemble_L2   0.855219       0.038929  73.009209                0.002002           0.841818            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.130997  20.415231                0.130997          20.415231            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.194014  68.497755                0.194014          68.497755            1       True         12\n4      LightGBMLarge_BAG_L1   0.842873       0.056998   8.680638                0.056998           8.680638            1       True         13\n5            XGBoost_BAG_L1   0.837262       0.055978   3.598592                0.055978           3.598592            1       True         11\n6           LightGBM_BAG_L1   0.837262       0.061885   5.357185                0.061885           5.357185            1       True          4\n7         LightGBMXT_BAG_L1   0.835017       0.049997   3.816595                0.049997           3.816595            1       True          3\n8   RandomForestGini_BAG_L1   0.833895       0.096996   0.553528                0.096996           0.553528            1       True          5\n9     ExtraTreesEntr_BAG_L1   0.831650       0.095051   0.494969                0.095051           0.494969            1       True          9\n10  RandomForestEntr_BAG_L1   0.830527       0.101071   0.535026                0.101071           0.535026            1       True          6\n11    ExtraTreesGini_BAG_L1   0.823793       0.111044   0.513969                0.111044           0.513969            1       True          8\n12    KNeighborsDist_BAG_L1   0.635241       0.004996   0.006006                0.004996           0.006006            1       True          2\n13    KNeighborsUnif_BAG_L1   0.629630       0.015998   0.005992                0.015998           0.005992            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.855219\n0.036927\n72.167391\n0.036927\n72.167391\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.855219\n0.038929\n73.009209\n0.002002\n0.841818\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.130997\n20.415231\n0.130997\n20.415231\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.194014\n68.497755\n0.194014\n68.497755\n1\nTrue\n12\n\n\n4\nLightGBMLarge_BAG_L1\n0.842873\n0.056998\n8.680638\n0.056998\n8.680638\n1\nTrue\n13\n\n\n5\nXGBoost_BAG_L1\n0.837262\n0.055978\n3.598592\n0.055978\n3.598592\n1\nTrue\n11\n\n\n6\nLightGBM_BAG_L1\n0.837262\n0.061885\n5.357185\n0.061885\n5.357185\n1\nTrue\n4\n\n\n7\nLightGBMXT_BAG_L1\n0.835017\n0.049997\n3.816595\n0.049997\n3.816595\n1\nTrue\n3\n\n\n8\nRandomForestGini_BAG_L1\n0.833895\n0.096996\n0.553528\n0.096996\n0.553528\n1\nTrue\n5\n\n\n9\nExtraTreesEntr_BAG_L1\n0.831650\n0.095051\n0.494969\n0.095051\n0.494969\n1\nTrue\n9\n\n\n10\nRandomForestEntr_BAG_L1\n0.830527\n0.101071\n0.535026\n0.101071\n0.535026\n1\nTrue\n6\n\n\n11\nExtraTreesGini_BAG_L1\n0.823793\n0.111044\n0.513969\n0.111044\n0.513969\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.635241\n0.004996\n0.006006\n0.004996\n0.006006\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.629630\n0.015998\n0.005992\n0.015998\n0.005992\n1\nTrue\n1\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9158249158249159\n\n\n\ntst[['PassengerId']].assign(Survived = predictr.predict(tst))\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)\n\n\n하지만 결과는 확실하다. 무려 0.813…"
  },
  {
    "objectID": "2023_MP.html",
    "href": "2023_MP.html",
    "title": "2023 MP",
    "section": "",
    "text": "전북대학교 2023년 2학기 통계학부 최규빈 교수님의 “기계학습활용(Machine Learning in Practice)” 강의를 듣고 그 내용을 나름대로 정리한 내용입니다.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치\n\n\n\n\n\n\n\npython\n\n\nMultipleLinearity\n\n\nOutlier\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n선형모형의 적\n\n\n\n\n\n\n\npython\n\n\nMultipleLinearity\n\n\nOutlier\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n오버피팅, 다중공선성\n\n\n\n\n\n\n\npython\n\n\nMultipleLinearity\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n다중공선성의 해소\n\n\n\n\n\n\n\npython\n\n\nMultipleLinearity\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n전처리 | 연속형 자료의 범위 조정\n\n\n\n\n\n\n\npython\n\n\nScaler\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nsklearn.linear_model의 작동원리\n\n\n\n\n\n\n\npython\n\n\nLogistic\n\n\nLinear\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | 결측치의 처리\n\n\n\n\n\n\n\npython\n\n\ntitanic\n\n\nimpute\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n선형회귀분석의 시작 | LinearRegression()\n\n\n\n\n\n\n\npython\n\n\nLinear\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n범주형 반응변수의 예측 | LogisticRegression()\n\n\n\n\n\n\n\npython\n\n\nLogistic\n\n\nkaggle\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n결측치의 처리\n\n\n\n\n\n\n\npython\n\n\nimpute\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | Autogluon\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\ntitanic\n\n\nautogluon\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | Alexis Cook의 코드\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | 1st practice\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n강신성\n\n\n\n\n\n\nNo matching items"
  }
]