[
  {
    "objectID": "2023_MP.html",
    "href": "2023_MP.html",
    "title": "2023 MP",
    "section": "",
    "text": "전북대학교 2023년 2학기 통계학부 최규빈 교수님의 “기계학습활용(Machine Learning in Practice)” 강의를 듣고 그 내용을 나름대로 정리한 내용입니다.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n의사결정나무의 옵션 이해\n\n\n\n\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nRandomForest | 의사결정나무\n\n\n\n\n\n\n\nRandomForest\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nBoosting | 의사결정나무\n\n\n\n\n\n\n\nBoosting\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nBagging | 의사결정나무\n\n\n\n\n\n\n\nBagging\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nplot_tree의 시각화\n\n\n\n\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n플랏 | 애니메이션\n\n\n\n\n\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n의사결정나무 | 작동원리\n\n\n\n\n\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치\n\n\n\n\n\n\n\nlinear_model\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n선형모형의 적\n\n\n\n\n\n\n\nlinear_model\n\n\npreprocessing\n\n\nimpute\n\n\ntree\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n오버피팅, 다중공선성\n\n\n\n\n\n\n\nlinear_model\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n다중공선성의 해소\n\n\n\n\n\n\n\nlinear_model\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n전처리 | 연속형 자료의 범위 조정\n\n\n\n\n\n\n\npreprocessing\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nsklearn.linear_model의 작동원리\n\n\n\n\n\n\n\nlinear_model\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | 결측치의 처리\n\n\n\n\n\n\n\npython\n\n\ntitanic\n\n\nimpute\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n선형회귀분석의 시작 | LinearRegression()\n\n\n\n\n\n\n\nlinear_model\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n범주형 반응변수의 예측 | LogisticRegression()\n\n\n\n\n\n\n\nlinear_model\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n결측치의 처리\n\n\n\n\n\n\n\nimpute\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | Autogluon\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\ntitanic\n\n\nautogluon\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | Alexis Cook의 코드\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nKaggle | 1st practice\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\n강신성\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html",
    "href": "unrefined file/r_practice/Regression Analysis.html",
    "title": "R Notebook",
    "section": "",
    "text": "options(repr.plot.width = 15, repr.plot.height = 8)  ## 플롯 크기 설정\n\n\nx &lt;- rnorm(100) ## rnorm() / dnorm() / pnorm() / qnorm()\nhist(x)\n\n\n\n\n\ndefault는 표준정규분포이다. (n, mean = 0, sd = 1)\n\n\nx &lt;- seq(-5, 5, 0.01)\nplot(x, dnorm(x), type = 'l', lwd = 2,\n     main = \"표준정규분포의 확률밀도함수\",\n     cex.main = 1,  ## character expansion.main : title\n     cex.lab = 1)   ## character expansion.lab : label\n\n\n\n\n\npnorm(q, mean, sd) | probability, cdf\n\n\nprint(pnorm(1.96))\n\n[1] 0.9750021\n\nprint(pnorm(4, 3, 1))\n\n[1] 0.8413447\n\nprint(pnorm(4, mean = 3, sd = 1))\n\n[1] 0.8413447\n\n\n\np_value를 구할 수 없다면 백분위수(quantile value)를 구해야 한다.\n\n\nprint(qnorm(0.95))\n\n[1] 1.644854\n\nprint(qnorm(0.975))\n\n[1] 1.959964\n\nprint(qnorm(0.995))\n\n[1] 2.575829\n\n\n\n\n\n\n자유도가 중요한 분포이다. &gt; 자유도 : t분포의 모양을 결정하는 것. parameter는 아니다. 그냥 아는 값.\n\n\n표본의 분포가 아닌 표본 통계량의 분포에 해당한다.\n\n\nx &lt;- rt(1000, 3)  ## 자유도가 3인 t분포에서의 샘플 1000개\nhist(x, cex.main = 1, cex.lab = 1)\n\n\n\n\n\n원리는 정규분포와 동일하다. rt(), pt(), dt(), qt()\n\n\nt분포의 확률밀도함수(probability Density function)\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dt(x, 3), type = 'l',\n     main = \"t분포의 확률밀도함수\")\n\n\n\n\n\n여러 개의 확률밀도함수를 같이 그려보자.\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dnorm(x), type = 'l', main = 't분포와 정규분포의 확률밀도함수'  ## , lty = 1\n     )\n\nlines(x, dt(x, 1), col = 'red', lty = 2)\nlines(x, dt(x, 3), col = 'blue', lty = 3)\nlines(x, dt(x, 30), col = 'green', lty = 4)\n\nlegend('topleft', c('N(0,1)', 't(1)', 't(3)', 't(30)'), ## query 문법처럼 사용가능한듯.\n       lty = 1:4, col = c('black', 'red', 'blue', 'green'))\n\n\n\n\n\n자유도 n이 커질 수록 표준정규분포의 확률밀도함수와 유사해진다.\n\n\\(T\\) ~ \\(t(df), ~~~ P(T≤t)\\)\n\\(T_0\\) ~ \\(t(df), ~~~ t_a(df) : P(T &gt; t_a(df)) = a\\)\n\nprint(pt(2, 3))  ## probability distribution function\n\n[1] 0.930337\n\nprint(qt(0.95, 3))  ## quantile\n\n[1] 2.353363\n\nprint(dt(0, 3))   ## probability density function\n\n[1] 0.3675526\n\n\n\n우측검정 시, 오른쪽 영역의 확률을 알고 싶을 경우\n\nStudent T distribution practice\n\npt(1,3, lower.tail = FALSE)   ## 낮은 값의 꼬리쪽으로 산출?\n\n[1] 0.1955011\n\nqt(0.05, 3, lower.tail = FALSE) ## 오른쪽에서의 a = 0.05인 quantile 값\n\n[1] 2.353363\n\nqt(0.95, 3)\n\n[1] 2.353363\n\n\n\n\n\n\nx &lt;- rchisq(1000, 4)\nhist(x)\n\n\n\n\n\\(x &gt; 0\\) 에서의 오른쪽으로 치우쳐진 분포가 나온다.\n\nx &lt;- seq(0, 20, 0.01)\nplot(x, dchisq(x, 5), type = 'l', lwd = 2,\n     main = '자유도가 5인 카이제곱분포의 확률밀도함수',\n     cex.main = 1, cex.lab = 1)\n\n\n\n\n\nx &lt;- seq(0, 200, 0.01)\nplot(x, dchisq(x, 100), type = 'l', lwd = 2,\n     main = '자유도가 100인 카이제곱분포의 확률밀도함수')\n\n\n\n\n자유도가 높을 수록 정규분포와 비슷한 모양을 띈다.\n- 이외 코드는 위와 유사하다…(t분포)\n\npchisq(100, 100)\n\n[1] 0.5188083\n\nqchisq(0.95, 100)\n\n[1] 124.3421\n\ndchisq(50, 50)\n\n[1] 0.03976148\n\n\n\n\n\n\nx &lt;- rf(1000, 4, 6)  ## n=100, 분자의 자유도(df1) = 4, 분모의 자유도(df2) = 6\nhist(x, breaks = 50)\n\n\n\n\n\nx &lt;- seq(0, 15, 0.01)\nplot(x, df(x, 4, 6), type = 'l', lwd = 2,\n     main = \"F분포의 확률밀도함수\")\nlines(x, df(x, 6, 4), col = 'red')\n\nlegend(\"topright\", c(\"F(4,6)\", \"F(6,4)\"), lty = 1:2, col = c('black', 'red'))\n\n\n\n\n\n활용 방법은 비슷하다.\n\n\nqf(0.95, 4, 6)\n\n[1] 4.533677\n\n\n\n\n\n\n\n예제) 비누공장데이터 : 9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6 (n=9)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\n\nbar_x = mean(x); bar_x\n\n[1] 9\n\nS_x = var(x); S_x\n\n[1] 0.0575\n\ns_x = sd(x); s_x\n\n[1] 0.2397916\n\n\n\nlower_x = bar_x - qt(0.975, 8)*s_x/sqrt(9)  ## 자료의 수가 9이므로 자유도 8인 t분포\nupper_x = bar_x + qt(0.975, 8)*s_x/sqrt(9)\n\nc(lower_x, upper_x)\n\n[1] 8.81568 9.18432\n\n\n\n95% 신뢰구간이다.\nquantile 값이 0.975면 우측 확률이 0.025인 t_a와 같으니까…\n\n\n\n\nls(t.test(x))\n\n [1] \"alternative\" \"conf.int\"    \"data.name\"   \"estimate\"    \"method\"     \n [6] \"null.value\"  \"p.value\"     \"parameter\"   \"statistic\"   \"stderr\"     \n\n\n\nt.test(x)$conf.int  ## 해당 모듈의 하위 모듈인 conf.int : 신뢰구간 산출\n\n[1] 8.81568 9.18432\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\nt.test(x, conf.level = 0.99)$conf.int ## 99% 신뢰구간, 디폴트는 95%\n\n[1] 8.731802 9.268198\nattr(,\"conf.level\")\n[1] 0.99\n\n\n\n\n\n\n\n\n\n예제) 공정온도에서의 제품의 강도(\\(N(73.7, 1)\\))\n\n두 가지 방법 : \\(p-value\\)와 \\(\\alpha\\)값 비교, 기각역과 관측값 비교\n\n가설 : \\(H_0 : \\mu = 73.7 ~ vs. ~ H_1 :\\mu &gt; 73.7\\)\n\n\\(\\bar{x} = 75.1, ~ n = 16, ~ \\alpha = 0.05\\)\n\nmu &lt;- 73.7; xbar &lt;- 75.1; sigma &lt;- 1; n &lt;- 16\n\n\n검정통계량의 관측값 : \\(z_0 = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{n}}\\)\n\n\nz_0 &lt;- (xbar - mu)/(sigma/sqrt(n)); z_0\n\n[1] 5.6\n\n\n\n기각역\n\n\nqnorm(0.95)\n\n[1] 1.644854\n\n\n\n기각역보다 검정통계량의 관측값이 휠씬 크므로 귀무가설 기각 가능\n\n\n\n\n\n예제 2\n\n\nmu &lt;- 2; x_bar &lt;- 1.96; s &lt;- 0.05; n &lt;- 50\n\n\nt_0 &lt;- (x_bar - mu)/(s/sqrt(50)); t_0\n\n[1] -5.656854\n\n\n\n기각역\n\n\n-qt(0.95, n - 1)\n\n[1] -1.676551\n\n\n\n유의확률(p-value)\n\n\npt(t_0, n-1)\n\n[1] 3.93525e-07\n\n\n\n\n\n\n예제) 일전의 비누공장 데이터, 모분산은 알리가 없음.\n\n\n가설 : \\(H_0 : \\mu = 9.2 ~ vs. ~ H_1 : \\mu ≠ 9.2\\)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\nxbar = mean(x)\nS_x = var(x)  ## r은 데이터 분석 툴이기 때문에 기본적으로 n-1로 나누어준다.\ns_x = sd(x)\n\n\n유의수준 \\(\\alpha = 0.05\\)\n검정통계량의 관측값 : \\(t_0 = \\frac{}{}, \\alpha = 0.05\\)\n\n\nt_0 &lt;- (xbar - 9.2)/(s_x/sqrt(9)); t_0\n\n[1] -2.502173\n\n\n\n기각역\n\n\nqt(0.975, 9-1)\n\n[1] 2.306004\n\n\n\n유의확률(p-balue)\n\n\npt(t_0, 8) + pt(-t_0, 8, lower.tail = FALSE)\n\n[1] 0.03681717\n\n\n\nt.test()\n\n\nt.test(x)\n\n\n    One Sample t-test\n\ndata:  x\nt = 112.6, df = 8, p-value = 4.325e-14\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n\n\ndefault는 \\(\\mu = 0\\)이기 때문에 귀무가설을 입력해줘햐 한다.\n\n6.1\n\nt.test(x, mu = 9.2, conf.level = 0.95, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.5022, df = 8, p-value = 0.03682\nalternative hypothesis: true mean is not equal to 9.2\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n## two.sided(de), less, greater"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html#분포",
    "href": "unrefined file/r_practice/Regression Analysis.html#분포",
    "title": "R Notebook",
    "section": "",
    "text": "options(repr.plot.width = 15, repr.plot.height = 8)  ## 플롯 크기 설정\n\n\nx &lt;- rnorm(100) ## rnorm() / dnorm() / pnorm() / qnorm()\nhist(x)\n\n\n\n\n\ndefault는 표준정규분포이다. (n, mean = 0, sd = 1)\n\n\nx &lt;- seq(-5, 5, 0.01)\nplot(x, dnorm(x), type = 'l', lwd = 2,\n     main = \"표준정규분포의 확률밀도함수\",\n     cex.main = 1,  ## character expansion.main : title\n     cex.lab = 1)   ## character expansion.lab : label\n\n\n\n\n\npnorm(q, mean, sd) | probability, cdf\n\n\nprint(pnorm(1.96))\n\n[1] 0.9750021\n\nprint(pnorm(4, 3, 1))\n\n[1] 0.8413447\n\nprint(pnorm(4, mean = 3, sd = 1))\n\n[1] 0.8413447\n\n\n\np_value를 구할 수 없다면 백분위수(quantile value)를 구해야 한다.\n\n\nprint(qnorm(0.95))\n\n[1] 1.644854\n\nprint(qnorm(0.975))\n\n[1] 1.959964\n\nprint(qnorm(0.995))\n\n[1] 2.575829\n\n\n\n\n\n\n자유도가 중요한 분포이다. &gt; 자유도 : t분포의 모양을 결정하는 것. parameter는 아니다. 그냥 아는 값.\n\n\n표본의 분포가 아닌 표본 통계량의 분포에 해당한다.\n\n\nx &lt;- rt(1000, 3)  ## 자유도가 3인 t분포에서의 샘플 1000개\nhist(x, cex.main = 1, cex.lab = 1)\n\n\n\n\n\n원리는 정규분포와 동일하다. rt(), pt(), dt(), qt()\n\n\nt분포의 확률밀도함수(probability Density function)\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dt(x, 3), type = 'l',\n     main = \"t분포의 확률밀도함수\")\n\n\n\n\n\n여러 개의 확률밀도함수를 같이 그려보자.\n\n\nx &lt;- seq(-5, 5, 0.01)\n\nplot(x, dnorm(x), type = 'l', main = 't분포와 정규분포의 확률밀도함수'  ## , lty = 1\n     )\n\nlines(x, dt(x, 1), col = 'red', lty = 2)\nlines(x, dt(x, 3), col = 'blue', lty = 3)\nlines(x, dt(x, 30), col = 'green', lty = 4)\n\nlegend('topleft', c('N(0,1)', 't(1)', 't(3)', 't(30)'), ## query 문법처럼 사용가능한듯.\n       lty = 1:4, col = c('black', 'red', 'blue', 'green'))\n\n\n\n\n\n자유도 n이 커질 수록 표준정규분포의 확률밀도함수와 유사해진다.\n\n\\(T\\) ~ \\(t(df), ~~~ P(T≤t)\\)\n\\(T_0\\) ~ \\(t(df), ~~~ t_a(df) : P(T &gt; t_a(df)) = a\\)\n\nprint(pt(2, 3))  ## probability distribution function\n\n[1] 0.930337\n\nprint(qt(0.95, 3))  ## quantile\n\n[1] 2.353363\n\nprint(dt(0, 3))   ## probability density function\n\n[1] 0.3675526\n\n\n\n우측검정 시, 오른쪽 영역의 확률을 알고 싶을 경우\n\nStudent T distribution practice\n\npt(1,3, lower.tail = FALSE)   ## 낮은 값의 꼬리쪽으로 산출?\n\n[1] 0.1955011\n\nqt(0.05, 3, lower.tail = FALSE) ## 오른쪽에서의 a = 0.05인 quantile 값\n\n[1] 2.353363\n\nqt(0.95, 3)\n\n[1] 2.353363\n\n\n\n\n\n\nx &lt;- rchisq(1000, 4)\nhist(x)\n\n\n\n\n\\(x &gt; 0\\) 에서의 오른쪽으로 치우쳐진 분포가 나온다.\n\nx &lt;- seq(0, 20, 0.01)\nplot(x, dchisq(x, 5), type = 'l', lwd = 2,\n     main = '자유도가 5인 카이제곱분포의 확률밀도함수',\n     cex.main = 1, cex.lab = 1)\n\n\n\n\n\nx &lt;- seq(0, 200, 0.01)\nplot(x, dchisq(x, 100), type = 'l', lwd = 2,\n     main = '자유도가 100인 카이제곱분포의 확률밀도함수')\n\n\n\n\n자유도가 높을 수록 정규분포와 비슷한 모양을 띈다.\n- 이외 코드는 위와 유사하다…(t분포)\n\npchisq(100, 100)\n\n[1] 0.5188083\n\nqchisq(0.95, 100)\n\n[1] 124.3421\n\ndchisq(50, 50)\n\n[1] 0.03976148\n\n\n\n\n\n\nx &lt;- rf(1000, 4, 6)  ## n=100, 분자의 자유도(df1) = 4, 분모의 자유도(df2) = 6\nhist(x, breaks = 50)\n\n\n\n\n\nx &lt;- seq(0, 15, 0.01)\nplot(x, df(x, 4, 6), type = 'l', lwd = 2,\n     main = \"F분포의 확률밀도함수\")\nlines(x, df(x, 6, 4), col = 'red')\n\nlegend(\"topright\", c(\"F(4,6)\", \"F(6,4)\"), lty = 1:2, col = c('black', 'red'))\n\n\n\n\n\n활용 방법은 비슷하다.\n\n\nqf(0.95, 4, 6)\n\n[1] 4.533677"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html#신뢰구간",
    "href": "unrefined file/r_practice/Regression Analysis.html#신뢰구간",
    "title": "R Notebook",
    "section": "",
    "text": "예제) 비누공장데이터 : 9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6 (n=9)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\n\nbar_x = mean(x); bar_x\n\n[1] 9\n\nS_x = var(x); S_x\n\n[1] 0.0575\n\ns_x = sd(x); s_x\n\n[1] 0.2397916\n\n\n\nlower_x = bar_x - qt(0.975, 8)*s_x/sqrt(9)  ## 자료의 수가 9이므로 자유도 8인 t분포\nupper_x = bar_x + qt(0.975, 8)*s_x/sqrt(9)\n\nc(lower_x, upper_x)\n\n[1] 8.81568 9.18432\n\n\n\n95% 신뢰구간이다.\nquantile 값이 0.975면 우측 확률이 0.025인 t_a와 같으니까…\n\n\n\n\nls(t.test(x))\n\n [1] \"alternative\" \"conf.int\"    \"data.name\"   \"estimate\"    \"method\"     \n [6] \"null.value\"  \"p.value\"     \"parameter\"   \"statistic\"   \"stderr\"     \n\n\n\nt.test(x)$conf.int  ## 해당 모듈의 하위 모듈인 conf.int : 신뢰구간 산출\n\n[1] 8.81568 9.18432\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\nt.test(x, conf.level = 0.99)$conf.int ## 99% 신뢰구간, 디폴트는 95%\n\n[1] 8.731802 9.268198\nattr(,\"conf.level\")\n[1] 0.99"
  },
  {
    "objectID": "unrefined file/r_practice/Regression Analysis.html#가설검정",
    "href": "unrefined file/r_practice/Regression Analysis.html#가설검정",
    "title": "R Notebook",
    "section": "",
    "text": "예제) 공정온도에서의 제품의 강도(\\(N(73.7, 1)\\))\n\n두 가지 방법 : \\(p-value\\)와 \\(\\alpha\\)값 비교, 기각역과 관측값 비교\n\n가설 : \\(H_0 : \\mu = 73.7 ~ vs. ~ H_1 :\\mu &gt; 73.7\\)\n\n\\(\\bar{x} = 75.1, ~ n = 16, ~ \\alpha = 0.05\\)\n\nmu &lt;- 73.7; xbar &lt;- 75.1; sigma &lt;- 1; n &lt;- 16\n\n\n검정통계량의 관측값 : \\(z_0 = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{n}}\\)\n\n\nz_0 &lt;- (xbar - mu)/(sigma/sqrt(n)); z_0\n\n[1] 5.6\n\n\n\n기각역\n\n\nqnorm(0.95)\n\n[1] 1.644854\n\n\n\n기각역보다 검정통계량의 관측값이 휠씬 크므로 귀무가설 기각 가능\n\n\n\n\n\n예제 2\n\n\nmu &lt;- 2; x_bar &lt;- 1.96; s &lt;- 0.05; n &lt;- 50\n\n\nt_0 &lt;- (x_bar - mu)/(s/sqrt(50)); t_0\n\n[1] -5.656854\n\n\n\n기각역\n\n\n-qt(0.95, n - 1)\n\n[1] -1.676551\n\n\n\n유의확률(p-value)\n\n\npt(t_0, n-1)\n\n[1] 3.93525e-07\n\n\n\n\n\n\n예제) 일전의 비누공장 데이터, 모분산은 알리가 없음.\n\n\n가설 : \\(H_0 : \\mu = 9.2 ~ vs. ~ H_1 : \\mu ≠ 9.2\\)\n\n\nx &lt;- c(9.0, 9.1, 8.8, 9.1, 9.0, 9.4, 9.2, 8.8, 8.6)\nxbar = mean(x)\nS_x = var(x)  ## r은 데이터 분석 툴이기 때문에 기본적으로 n-1로 나누어준다.\ns_x = sd(x)\n\n\n유의수준 \\(\\alpha = 0.05\\)\n검정통계량의 관측값 : \\(t_0 = \\frac{}{}, \\alpha = 0.05\\)\n\n\nt_0 &lt;- (xbar - 9.2)/(s_x/sqrt(9)); t_0\n\n[1] -2.502173\n\n\n\n기각역\n\n\nqt(0.975, 9-1)\n\n[1] 2.306004\n\n\n\n유의확률(p-balue)\n\n\npt(t_0, 8) + pt(-t_0, 8, lower.tail = FALSE)\n\n[1] 0.03681717\n\n\n\nt.test()\n\n\nt.test(x)\n\n\n    One Sample t-test\n\ndata:  x\nt = 112.6, df = 8, p-value = 4.325e-14\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n\n\ndefault는 \\(\\mu = 0\\)이기 때문에 귀무가설을 입력해줘햐 한다.\n\n6.1\n\nt.test(x, mu = 9.2, conf.level = 0.95, alternative = 'two.sided')\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.5022, df = 8, p-value = 0.03682\nalternative hypothesis: true mean is not equal to 9.2\n95 percent confidence interval:\n 8.81568 9.18432\nsample estimates:\nmean of x \n        9 \n\n## two.sided(de), less, greater"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html",
    "href": "2023_DV/Review/강신성_1123.html",
    "title": "1. 라이브러리 imports",
    "section": "",
    "text": "folium과 NYC Taxi 자료 분석\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\npd.options.plotting.backend = 'plotly'\npio.templates.default = 'plotly_white'"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#뉴욕",
    "href": "2023_DV/Review/강신성_1123.html#뉴욕",
    "title": "1. 라이브러리 imports",
    "section": "2. 뉴욕",
    "text": "2. 뉴욕\n\nA. 뉴욕의 주요 명소\n\n\n# 뉴욕의 주요 명소 및 위치를 데이터프레임으로 생성\nnyc_landmarks = {\n    \"Name\": [\"Wall Street\", \"Midtown Manhattan\", \"Times Square\", \n             \"Central Park\", \"Statue of Liberty\", \"Forest Park\", \"Citi Field\"],\n    \"Latitude\": [40.7074, 40.7549, 40.7580, 40.785091, 40.6892, 40.7028, 40.7571],\n    \"Longitude\": [-74.0113, -73.9840, -73.9855, -73.968285, -74.0445, -73.8495, -73.8458]\n}\n\ndf_nyc_landmarks = pd.DataFrame(nyc_landmarks)\ndf_nyc_landmarks\n\n\n\n\n\n\n\n\nName\nLatitude\nLongitude\n\n\n\n\n0\nWall Street\n40.707400\n-74.011300\n\n\n1\nMidtown Manhattan\n40.754900\n-73.984000\n\n\n2\nTimes Square\n40.758000\n-73.985500\n\n\n3\nCentral Park\n40.785091\n-73.968285\n\n\n4\nStatue of Liberty\n40.689200\n-74.044500\n\n\n5\nForest Park\n40.702800\n-73.849500\n\n\n6\nCiti Field\n40.757100\n-73.845800\n\n\n\n\n\n\n\n\n뉴욕 내부 지역에 대한 좌표값들이다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#b.-시각화",
    "href": "2023_DV/Review/강신성_1123.html#b.-시각화",
    "title": "1. 라이브러리 imports",
    "section": "### B. 시각화",
    "text": "### B. 시각화\n\n## 코로플레스가 아니라 점을 찍는거임\npx.scatter_mapbox(\n    data_frame = df_nyc_landmarks,\n    lat = 'Latitude',\n    lon = 'Longitude',\n    hover_data = 'Name',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\n\n                                                \n\n\n\n일단 만들어봤는데, 점들의 크기를 좀 키우고 싶다.(geom_point의 업데이트)\n\n\nfig = px.scatter_mapbox(\n    data_frame = df_nyc_landmarks,\n    lat = 'Latitude',\n    lon = 'Longitude',\n    hover_data = 'Name',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nfig.update_traces(\n    marker = {\n        'size' : 15,\n        'color' : 'red',\n        'opacity' : 0.5\n    }\n)\nfig.show(config={'scrollZoom':False})  ## folium에서 scrollWheelZoom = False와 동일\n\n\n                                                \n\n\n\nC. ChatGPT 지역설명\n\nChatGPT\n뉴욕은 세계에서 가장 중요한 금융 및 문화 중심지 중 하나로, 금융권 밀집 지역과 유명한 관광 명소가 많습니다. 다음은 뉴욕의 대표적인 금융권 밀집 지역과 주요 관광 명소 중 일부입니다:\n금융권 밀집 지역\n\n월스트리트 (Wall Street):\n\n월스트리트는 세계 금융의 상징이며, 뉴욕증권거래소(NYSE)와 많은 은행 및 금융 기관의 본사가 위치해 있습니다.\n이 지역은 글로벌 금융 및 경제의 중심지로 간주되며, ’월스트리트’는 종종 미국 금융 산업 전체를 지칭하는 용어로 사용됩니다.\n\n미드타운 (Midtown):\n\n미드타운 맨해튼은 많은 기업 본사, 유명 호텔, 쇼핑 지역 및 레스토랑이 밀집해 있는 지역입니다.\n이 지역에는 국제연합 본부, 메이시스 백화점, 록펠러 센터 등이 위치해 있습니다.\n\n\n주요 관광 명소\n\n타임스퀘어 (Times Square):\n\n타임스퀘어는 뉴욕의 상징적인 관광 명소 중 하나로, 번화한 광고판과 네온사인으로 유명합니다.\n이곳은 맨해튼의 중심부에 위치하며, 연극과 뮤지컬이 상연되는 브로드웨이 극장가로도 유명합니다.\n\n센트럴 파크 (Central Park):\n\n센트럴 파크는 뉴욕 시의 대표적인 공원으로, 도심 속 자연을 즐길 수 있는 아름다운 장소입니다.\n공원 내에는 호수, 산책로, 놀이터, 스포츠 시설 등이 마련되어 있으며, 다양한 문화 행사와 공연이 열립니다.\n\n자유의 여신상 (Statue of Liberty):\n\n자유의 여신상은 뉴욕 항구에 위치한 미국의 상징적인 조각상입니다.\n자유의 여신상은 미국의 자유와 민주주의를 상징하며, 세계적으로 유명한 관광 명소입니다.\n\n포레스트 공원 (Forest Park):\n\n이 공원은 뉴욕시 퀸즈 구역에 위치해 있습니다.\n포레스트 공원은 약 538 에이커의 면적을 가지고 있으며, 다양한 레크리에이션 활동 및 자연 트레일을 제공합니다.\n\n시티 필드 (Citi Field):\n\n시티 필드는 뉴욕시 퀸즈 구역에 위치한 야구 경기장입니다.\n이 경기장은 메이저 리그 야구의 뉴욕 메츠 팀의 홈 구장으로 사용됩니다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#nyctaxi-자료",
    "href": "2023_DV/Review/강신성_1123.html#nyctaxi-자료",
    "title": "1. 라이브러리 imports",
    "section": "3. NYCTaxi 자료",
    "text": "3. NYCTaxi 자료\n\nref: https://www.kaggle.com/competitions/nyc-taxi-trip-duration/overview"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#a.-데이터-불러오기",
    "href": "2023_DV/Review/강신성_1123.html#a.-데이터-불러오기",
    "title": "1. 라이브러리 imports",
    "section": "### A. 데이터 불러오기",
    "text": "### A. 데이터 불러오기\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/NYCTaxi.csv\")\ndf.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration'],\n      dtype='object')\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nid\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\nstore_and_fwd_flag\ntrip_duration\n\n\n\n\n0\nid2875421\n2\n2016-03-14 17:24:55\n2016-03-14 17:32:30\n1\n-73.982155\n40.767937\n-73.964630\n40.765602\nN\n455\n\n\n1\nid3194108\n1\n2016-06-01 11:48:41\n2016-06-01 12:19:07\n1\n-74.005028\n40.746452\n-73.972008\n40.745781\nN\n1826\n\n\n2\nid3564028\n1\n2016-01-02 01:16:42\n2016-01-02 01:19:56\n1\n-73.954132\n40.774784\n-73.947418\n40.779633\nN\n194\n\n\n3\nid1660823\n2\n2016-03-01 06:40:18\n2016-03-01 07:01:37\n5\n-73.982140\n40.775326\n-74.009850\n40.721699\nN\n1279\n\n\n4\nid1575277\n2\n2016-06-11 16:59:15\n2016-06-11 17:33:27\n1\n-73.999229\n40.722881\n-73.982880\n40.778297\nN\n2052\n\n\n\n\n\n\n\n\nB. 데이터 설명\n\nkaggle\n\nid: a unique identifier for each trip\nvendor_id: a code indicating the provider associated with the trip record\npickup_datetime: date and time when the meter was engaged\ndropoff_datetime: date and time when the meter was disengaged\npassenger_count: the number of passengers in the vehicle (driver entered value)\npickup_longitude: the longitude where the meter was engaged\npickup_latitude: the latitude where the meter was engaged\ndropoff_longitude: the longitude where the meter was disengaged\ndropoff_latitude: the latitude where the meter was disengaged\nstore_and_fwd_flag: This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\ntrip_duration: duration of the trip in seconds\n\nChatGPT\n\nid : 택시 고유번호\nvender_id : 택시 관련 서비스 제공업체(우버, 카카오 등 그런거)\npickup_datetime, dropoff_datetime : 승차시간, 하차시간\npassenger_cound : 탑승객 수\npickup_longitude, pickup_latitude : 승차 지역의 경도와 위도\ndropoff_longitude, dropoff_latitude : 하차지역의 위도와 경도\nstore_and_fwd_flag : 차량이 서버에 연결되어 있을 때 저장 후 전송되었음의 여부(N은 실시간 전송되었음)\ntrip_duration : 이동 총 소요 시간"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#c.-변수탐색",
    "href": "2023_DV/Review/강신성_1123.html#c.-변수탐색",
    "title": "1. 라이브러리 imports",
    "section": "### C. 변수탐색",
    "text": "### C. 변수탐색\n- 1단계 : df.info()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14587 entries, 0 to 14586\nData columns (total 11 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   id                  14587 non-null  object \n 1   vendor_id           14587 non-null  int64  \n 2   pickup_datetime     14587 non-null  object \n 3   dropoff_datetime    14587 non-null  object \n 4   passenger_count     14587 non-null  int64  \n 5   pickup_longitude    14587 non-null  float64\n 6   pickup_latitude     14587 non-null  float64\n 7   dropoff_longitude   14587 non-null  float64\n 8   dropoff_latitude    14587 non-null  float64\n 9   store_and_fwd_flag  14587 non-null  object \n 10  trip_duration       14587 non-null  int64  \ndtypes: float64(4), int64(3), object(4)\nmemory usage: 1.2+ MB\n\n\n\ndf.pickup_datetime[0]\n\n'2016-03-14 17:24:55'\n\n\n\nset(df.vendor_id)\n\n{1, 2}\n\n\n\n\n깔끔한 형태이며 결측치도 없음.\npickup_datetime 등 시간을 나타내는 것은 나중에 형태변환을 할 필요가 있음.\nvendor_id는 실제로는 범주형 자료를 의미함(더미변수)\n\n\n- 2단계 : 범주형자료의 빈도를 조사\n\ndf['vendor_id'].value_counts()  ## 개별 값들이 얼마나 많이 있는지 산출한다.\n## list(df['vendor_id']).count(1), list(df['vendor_id']).count(2)\n## {i:df['vendor_id'].count(i) for i in set(df['vendor_id'])}\n\nvendor_id\n2    7818\n1    6769\nName: count, dtype: int64\n\n\n\ndf['store_and_fwd_flag'].value_counts()\n\nstore_and_fwd_flag\nN    14506\nY       81\nName: count, dtype: int64\n\n\n\n실시간으로 보내진 자료는 훨씬 적음\n\n- 3단계 : 연속형변수의 분포를 조사\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14587 entries, 0 to 14586\nData columns (total 11 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   id                  14587 non-null  object \n 1   vendor_id           14587 non-null  int64  \n 2   pickup_datetime     14587 non-null  object \n 3   dropoff_datetime    14587 non-null  object \n 4   passenger_count     14587 non-null  int64  \n 5   pickup_longitude    14587 non-null  float64\n 6   pickup_latitude     14587 non-null  float64\n 7   dropoff_longitude   14587 non-null  float64\n 8   dropoff_latitude    14587 non-null  float64\n 9   store_and_fwd_flag  14587 non-null  object \n 10  trip_duration       14587 non-null  int64  \ndtypes: float64(4), int64(3), object(4)\nmemory usage: 1.2+ MB\n\n\n\n실질적으로 연속형 자료는 passenger_count와 trip_duration정도이다.(나머지는 좌표정보나 시간 정보이다.)\n\n\ndf.plot.hist(x = 'passenger_count')\n\n\n                                                \n\n\n\n1ㆍ2인 승객이 많고, 4인은 드물다. 5~6인은 4인보다 많다.\n\n\ndf.plot.hist(x = 'trip_duration')\n\n\n                                                \n\n\n\n거리가 짧은 쪽에 많이 몰려있지만, 이동거리가 매우 긴 승객들도 있다. (너무 큰 값들이 존재함)\n\n\nD. 데이터 변환\n\n# log변환\n- trip_duration의 스케일이 너무 크므로, log_trip_duration을 추가.\n\nnp.log(df.trip_duration).plot.hist()  ## numpy array에도 plot메소드가 있음...\n\n\n                                                \n\n\n\n범위가 줄어들어서 훨씬 시각화하기에도 좋아보임.\n\n# datetime 처리\n- pickup_datetime에서 시간(hour)만 추출하려면…\n\ndf.pickup_datetime\n\n0        2016-03-14 17:24:55\n1        2016-06-01 11:48:41\n2        2016-01-02 01:16:42\n3        2016-03-01 06:40:18\n4        2016-06-11 16:59:15\n                ...         \n14582    2016-05-16 22:12:09\n14583    2016-05-23 08:04:35\n14584    2016-05-31 16:56:13\n14585    2016-03-07 18:11:54\n14586    2016-05-09 17:26:56\nName: pickup_datetime, Length: 14587, dtype: object\n\n\n\ndf.pickup_datetime.str.split(' ').str[-1].str.split(':').str[0].astype('int')  ## 단순\n\n0        17\n1        11\n2         1\n3         6\n4        16\n         ..\n14582    22\n14583     8\n14584    16\n14585    18\n14586    17\nName: pickup_datetime, Length: 14587, dtype: int32\n\n\n\n애초에 잘 저장된 친구인데 해당 형태를 이용할 수는 없나?\n\n\ndf.pickup_datetime.apply(pd.to_datetime).dt.hour\n##pd.to_datetime(df.pickup_datetime)\n\n0        17\n1        11\n2         1\n3         6\n4        16\n         ..\n14582    22\n14583     8\n14584    16\n14585    18\n14586    17\nName: pickup_datetime, Length: 14587, dtype: int32\n\n\n- pickup_datetime에서 요일을 추출\n\npd.to_datetime(df.pickup_datetime).dt.dayofweek  ## 방법 1\n\n0        0\n1        2\n2        5\n3        1\n4        5\n        ..\n14582    0\n14583    0\n14584    1\n14585    0\n14586    0\nName: pickup_datetime, Length: 14587, dtype: int32\n\n\n\n0이 월요일, 6이 일요일\n\n\ndf.pickup_datetime.apply(pd.to_datetime).dt.strftime(\"%A\")  ## 방법 1(일단 외워야 할듯)\n\n0           Monday\n1        Wednesday\n2         Saturday\n3          Tuesday\n4         Saturday\n           ...    \n14582       Monday\n14583       Monday\n14584      Tuesday\n14585       Monday\n14586       Monday\nName: pickup_datetime, Length: 14587, dtype: object\n\n\n\n시각적으로 유용할 때가 있음…\n\n- 시간의 연산 / dropoff_datetime - pickup_datetime을 계산 & df.trip_duration과 비교\n\npd.to_datetime(df.dropoff_datetime)  - pd.to_datetime(df.pickup_datetime)\n\n0       0 days 00:07:35\n1       0 days 00:30:26\n2       0 days 00:03:14\n3       0 days 00:21:19\n4       0 days 00:34:12\n              ...      \n14582   0 days 00:15:37\n14583   0 days 00:14:45\n14584   0 days 00:42:31\n14585   0 days 00:17:15\n14586   0 days 01:03:41\nLength: 14587, dtype: timedelta64[ns]\n\n\n\ndtype이 datetime이라면, 판다스 내부에서 연산을 진행할 수 있다.\n\n\ndf.trip_duration  ## 택시를 탄 시간(초)\n\n0         455\n1        1826\n2         194\n3        1279\n4        2052\n         ... \n14582     937\n14583     885\n14584    2551\n14585    1035\n14586    3821\nName: trip_duration, Length: 14587, dtype: int64\n\n\n\n인덱스별로 시간이 동일함…\n\n# dist, speed 추가\n- 승차 위치와 하차 위치를 이용하여 거리를 계산\n\ndist = np.sqrt((df.pickup_latitude - df.dropoff_latitude)**2 + (df.pickup_longitude - df.dropoff_longitude)**2)\ndist\n\n0        0.017680\n1        0.033027\n2        0.008282\n3        0.060363\n4        0.057778\n           ...   \n14582    0.035054\n14583    0.023886\n14584    0.132513\n14585    0.023439\n14586    0.228013\nLength: 14587, dtype: float64\n\n\n\n사실 위와 같이 거리를 계산하면 잘못괴긴 함…\n\n\n실제로 저 직선경로로 차가 이동하지 않음\n곡면에 따른 직선 거리 왜곡…\n경도 표기상의 스케일 왜곡(위도가 높을수록 경도당 이동거리는 줄어듦…)\n\n\n그래도 대충 비슷하긴 하니까… 계산은 할거다.(이럴 경우 위처럼 한계점을 써줘야 함)\n\n\ndist.plot.hist()\n\n\n                                                \n\n\n\n굳이 로그변환 하지 않아도 상관없을 것 같음…(나중에 필요할 때 하면 된다. 색깔 넣을때)\n\n- 속력을 계산\n\n(dist / df.trip_duration).plot.hist()\n\n\n                                                \n\n\n\n이동거리를 시간(초)으로 나눔…(초속임)"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#e.-df_feature-생성",
    "href": "2023_DV/Review/강신성_1123.html#e.-df_feature-생성",
    "title": "1. 라이브러리 imports",
    "section": "### E. df_feature 생성",
    "text": "### E. df_feature 생성\n\ndf.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration'],\n      dtype='object')\n\n\n\ndf_feature = df.assign(\n    log_trip_duration = np.log(df.trip_duration),  ## 로그변환\n    pickup_datetime = pd.to_datetime(df.pickup_datetime),  ## datetime 형식으로 변환\n    dropoff_datetime = pd.to_datetime(df.dropoff_datetime),\n    dist = np.sqrt((df.pickup_latitude - df.dropoff_latitude)**2 + (df.pickup_longitude - df.dropoff_longitude)**2),\n    #---#\n    vendor_id = df.vendor_id.map({1:'A', 2:'B'})  ## 범주형으로 명시, 딕셔너리와 map()을 이용\n).assign(\n    speed = lambda _df : _df.dist / _df.trip_duration,\n    pickup_hour = lambda _df : _df.pickup_datetime.dt.hour,  ## 탑승 시간을 할당\n    dropoff_hour = lambda _df : _df.dropoff_datetime.dt.hour,\n    dayofweek = lambda _df : _df.pickup_datetime.dt.dayofweek  ## 탑승한 시점으로 요일을 잡음\n)\n\n\ndf_feature.head()\n\n\n\n\n\n\n\n\nid\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\nstore_and_fwd_flag\ntrip_duration\nlog_trip_duration\ndist\nspeed\npickup_hour\ndropoff_hour\ndayofweek\n\n\n\n\n0\nid2875421\nB\n2016-03-14 17:24:55\n2016-03-14 17:32:30\n1\n-73.982155\n40.767937\n-73.964630\n40.765602\nN\n455\n6.120297\n0.017680\n0.000039\n17\n17\n0\n\n\n1\nid3194108\nA\n2016-06-01 11:48:41\n2016-06-01 12:19:07\n1\n-74.005028\n40.746452\n-73.972008\n40.745781\nN\n1826\n7.509883\n0.033027\n0.000018\n11\n12\n2\n\n\n2\nid3564028\nA\n2016-01-02 01:16:42\n2016-01-02 01:19:56\n1\n-73.954132\n40.774784\n-73.947418\n40.779633\nN\n194\n5.267858\n0.008282\n0.000043\n1\n1\n5\n\n\n3\nid1660823\nB\n2016-03-01 06:40:18\n2016-03-01 07:01:37\n5\n-73.982140\n40.775326\n-74.009850\n40.721699\nN\n1279\n7.153834\n0.060363\n0.000047\n6\n7\n1\n\n\n4\nid1575277\nB\n2016-06-11 16:59:15\n2016-06-11 17:33:27\n1\n-73.999229\n40.722881\n-73.982880\n40.778297\nN\n2052\n7.626570\n0.057778\n0.000028\n16\n17\n5"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#시각화1-scatterdensity",
    "href": "2023_DV/Review/강신성_1123.html#시각화1-scatterdensity",
    "title": "1. 라이브러리 imports",
    "section": "4. 시각화1 : scatter/density",
    "text": "4. 시각화1 : scatter/density\n\nA. scatter(scatter_mapbox) 간단한 시각화\n\n\n## 위에서 썼던 코드 이용\nfig = px.scatter_mapbox(\n    data_frame = df_feature,  ## 피쳐 엔지니어링 된 자료\n    lat = 'pickup_latitude',  ## 어디서 타는지 시각화\n    lon = 'pickup_longitude',\n    center = {'lat':40.7322, 'lon':-73.9052},  ## 시작위치 지정해줄 수 있음, 자료가 있는 쪽으로 알아서 생성되긴 하지만...\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\nfig.update_traces(\n    marker = {\n        'size':2, \n    }\n)\nfig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\n해당 그림도 의미가 있음, 여기서 opacity를 작게, size를 크게 해서 밀도를 표현할수도 있지만… scattter_mapbox는 애초에 산점도 전문이다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#b.-densitydensity_mapbox",
    "href": "2023_DV/Review/강신성_1123.html#b.-densitydensity_mapbox",
    "title": "1. 라이브러리 imports",
    "section": "### B. density(density_mapbox)",
    "text": "### B. density(density_mapbox)\n\nfig = px.density_mapbox(\n    data_frame = df_feature,\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    radius = 1,  ## 각 점의 크기\n    center = {'lat':40.7322, 'lon':-73.9052},\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\n\nfig.show(config = {'scrollZoom' : False})"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#시각화-2-scatterdensity-alpha",
    "href": "2023_DV/Review/강신성_1123.html#시각화-2-scatterdensity-alpha",
    "title": "1. 라이브러리 imports",
    "section": "5. 시각화 2 : scatter/density + \\(\\alpha\\)",
    "text": "5. 시각화 2 : scatter/density + \\(\\alpha\\)\n\n맨날 각 데이터 눌러가면서 hover_data 확인하는 것 말고 한번에 볼 수 있는 방법은 없을까?\n\n\nA. density + passenger_count\n\n\ndf_feature.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'log_trip_duration', 'dist', 'speed', 'pickup_hour',\n       'dropoff_hour', 'dayofweek'],\n      dtype='object')\n\n\n\nfig = px.density_mapbox(\n    data_frame = df_feature,\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    radius = 2,  ## 줌 스케일과 무관하게 크기가 상대적으로 설정됨\n    center = {'lat' : 40.7322, 'lon' : -73.9052},\n    z = 'passenger_count',  ## z축, 다른 변수를 지정\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nfig.show(config = {'scrollZoom' : False})\n\n\n                                                \n\n\n\n\n\nimage.png\n\n\n\n딱히 density만을 plot한 것과 거의 차이가 없어보인다…\n\n따라서 특정 지역에 다수 손님이 타는 경향이 더 있다고 보기 어렵다.\n! 언더라잉(원자료)와 비교해서 차이가 있는지를 비교해야 의미가 있다!"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#b.-density-log_trip_duration",
    "href": "2023_DV/Review/강신성_1123.html#b.-density-log_trip_duration",
    "title": "1. 라이브러리 imports",
    "section": "### B. density + log_trip_duration",
    "text": "### B. density + log_trip_duration\n\ndf_feature.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'log_trip_duration', 'dist', 'speed', 'pickup_hour',\n       'dropoff_hour', 'dayofweek'],\n      dtype='object')\n\n\n\nfig = px.density_mapbox(\n    data_frame = df_feature,\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    radius = 1.5,\n    center = {'lat' : 40.7322, 'lon' : -73.9052},\n    z = 'log_trip_duration',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\n\nfig.show(config = {'scrollZoom':False})\n\n\n                                                \n\n\n\n이것도 density만을 plot한 것과 큰 차이가 없어보임… 따라서 이 그림상으론 특정 지역에서 주행 시간이 긴 손님이 많이 분포한다고 판단하기 어려워보임.\n\n시간은 교통 체증이나 이런 걸로도 오래 걸릴 수 있으니까…\n\nC. density + dist(duration과 일반적으로 비례하긴 함…)\n\n\nfig = px.density_mapbox(\n    data_frame = df_feature,\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    radius = 3,\n    center = {'lat':40.7322, 'lon':-73.9052},\n    z = 'dist',  ## 거리로 색상을 추가...\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\nfig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\n차이가 있어보임. 장거리 손님의 경우 멀리 떨어져 있는 지역의 색상이 짙게 나왔다.\n시티필드, 포레스트공원에는 확실히 장거리 손님이 많다고 해석됨"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#d.-density-speed얘는-duration이랑-dist와-엮여서-관련있을듯",
    "href": "2023_DV/Review/강신성_1123.html#d.-density-speed얘는-duration이랑-dist와-엮여서-관련있을듯",
    "title": "1. 라이브러리 imports",
    "section": "### D. density + speed(얘는 duration이랑 dist와 엮여서 관련있을듯)",
    "text": "### D. density + speed(얘는 duration이랑 dist와 엮여서 관련있을듯)\n\nfig = px.density_mapbox(\n    data_frame = df_feature,\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    radius=2.5,\n    center = {'lat':40.7322, 'lon':-73.9052},\n    z = 'speed',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\nfig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\n위에서와 유사하게 시티필드, 포레스트공원의 속도가 중심부보다 높은 것을 볼 수 있음.\n타임스퀘어, 미드타운의 속도가 density만을 plot한 것에 비해 옅은 것을 보아 낮음을 확인할 수 있음.\n\n\nE. scatter + vendor_id\n\n\nfig = px.scatter_mapbox(\n    data_frame = df_feature,\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    center = {'lat':40.7322, 'lon':-73.9052},\n    color = 'vendor_id',  ## density의 경우 애초에 밀도로 색상을 고려하니까 z이고, 여긴 color를 지정\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\nfig.update_traces(\n    marker = {\n        'size':2, \n    }\n)\nfig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\n어딘 A업체를 많이 타고, 어딘 B업체를 많이 타고… 뭐 그런 건 없고 고르게 분포한 것 같음."
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#f.-scatter-dayofweek",
    "href": "2023_DV/Review/강신성_1123.html#f.-scatter-dayofweek",
    "title": "1. 라이브러리 imports",
    "section": "### F. scatter + dayofweek",
    "text": "### F. scatter + dayofweek\n- dayofweek은 정수형으로 저장되어 있으나, 사실 범주형임…(가공이 필요)\n\nfig = px.scatter_mapbox(\n    data_frame = df_feature.assign(dayofweek = df_feature.dayofweek.astype(str)).sort_values('dayofweek'),  ## 요일을 범주형으로 추가하고, 순서대로 보이게 만듦\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    center = {'lat':40.7322, 'lon':-73.9052},\n    color = 'dayofweek',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=10,\n    width=750,\n    height=600\n)\nfig.update_traces(\n    marker = {\n        'size':3, \n    }\n)\nfig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\n요일별로 차이가 있을 줄 알았는데, 이 그림만으론 요일별 차이가 없어보임…"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#시각화-3-애니메이션",
    "href": "2023_DV/Review/강신성_1123.html#시각화-3-애니메이션",
    "title": "1. 라이브러리 imports",
    "section": "6. 시각화 3 : 애니메이션",
    "text": "6. 시각화 3 : 애니메이션\n\nA. scatter / (vendor_id, passenger_count, hour)\n\n\ndf_feature.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'log_trip_duration', 'dist', 'speed', 'pickup_hour',\n       'dropoff_hour', 'dayofweek'],\n      dtype='object')\n\n\n\nfig = px.scatter_mapbox(\n    data_frame = df_feature.sort_values('pickup_hour'), ## 인덱스 순서가 제대로 되도록...\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    color = 'vendor_id',\n    size = 'passenger_count', size_max = 5,\n    animation_frame = 'pickup_hour',\n    center = {'lat' : 40.7322, 'lon' : -73.9052},\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nfig.show(config = {'scrollZoom' : False})\n\n\n                                                \n\n\n\nB가 전체적으로 동그라미가 더 크다.(passenger_count) : 한 택시에 탑승하는 승객 수는 B업체가 더 많은듯)\n시간대별로 확실히 빈도수가 다르다.(오후 늦은 시간대가 제일 많은듯? 6시~9시)\n\n이렇게 시각화하여 느낌적인 느낌을 알아보고, 그것을 중심으로 시각화해보면 좋다.\n- 추가 시각화 1 : vendor_id별 평균 탑승객 수를 바플랏으로 시각화\n\ndf_feature.groupby('vendor_id').agg({'passenger_count' : 'mean'}).reset_index().plot.bar(x = 'passenger_count', y = 'vendor_id', color = 'vendor_id')\n\n\n                                                \n\n\n\n확실히 B사가 택시 당 평균 승객 수가 많다. 대형차량 위주로 운용하는 회사가 아닐까?\n\n- 추가 시각화 2 : vendor_id별 passenger_count를 boxplot으로 시각화\n\ndf_feature.sort_values('vendor_id').plot.box(x = 'vendor_id', y = 'passenger_count', color = 'vendor_id')\n\n\n                                                \n\n\n- 추가 시각화 3 : 히스토그램\n\ndf_feature.plot.hist(x = 'passenger_count', color = 'vendor_id', facet_col = 'vendor_id')\n\n\n                                                \n\n\n- 추가 시각화 4 : pickup_hour별 count를 바플랏으로 시각화(시간별로 얼마나 타는지 비교, 시간이 딱딱 끊어져 있으므로 히스토그램보다 이게 더 낫긴 할듯)\n\ndf_feature.pickup_hour.value_counts().sort_index().plot.bar()  ## 각 시간마다 행이 몇개인지 세는 게 되겠지.\n#df_feature.groupby('pickup_hour').agg({'pickup_hour' : 'count'})  ## count대신 len도 되고, size도 되고...\n#[len(df) for i, df in df_feature.groupby('pickup_hour')] 이렇게 해서 assign해도 되고... 등등\n\n\n                                                \n\n\n- 추가 시각화 5 : (pickup_hour, vendor_id) 별 count를 바플랏으로 시각화\n\ndf_feature.groupby(['pickup_hour', 'vendor_id']).agg('size').reset_index().rename({0 : 'count'}, axis = 1)\\\n.plot.bar(x = 'pickup_hour', y = 'count', color = 'vendor_id', facet_col = 'vendor_id')\n\n\n                                                \n\n\n- 추가 시각화 6 : areaplot으로 시각화\n\ndf_feature.groupby(['pickup_hour', 'vendor_id']).agg('size').reset_index().rename({0 : 'count'}, axis = 1)\\\n.plot.area(x = 'pickup_hour', y = 'count', color = 'vendor_id')\n\n\n                                                \n\n\n\nB의 경우는 5시 즈음에서, 16시 즈음에서 늘어나는 수준이 높다.\n\n- 추가 시각화 7 : 라인플랏\n\ndf_feature.groupby(['pickup_hour', 'vendor_id']).agg('size').reset_index().rename({0 : 'count'}, axis = 1)\\\n.plot.line(x = 'pickup_hour', y = 'count', color = 'vendor_id')\n\n\n                                                \n\n\n전체적인 포지션을 시각화하기엔 에리어 플랏도 좋다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#b.-scatter-vendor_id-dayofweek",
    "href": "2023_DV/Review/강신성_1123.html#b.-scatter-vendor_id-dayofweek",
    "title": "1. 라이브러리 imports",
    "section": "### B. scatter / (vendor_id, dayofweek)",
    "text": "### B. scatter / (vendor_id, dayofweek)\n\nfig = px.scatter_mapbox(\n    data_frame = df_feature.sort_values('dayofweek'),  ## 야매로 무조건 해줘야함\n    lat = 'pickup_latitude',\n    lon = 'pickup_longitude',\n    color = 'vendor_id',\n    size = 'passenger_count', size_max = 5,\n    animation_frame = 'dayofweek',\n    center = {'lat':40.7322, 'lon':-73.9052},\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nfig.show(config = {\"scrollZoom\" : False})\n\n\n                                                \n\n\n\n생각보다 요일별 특징은 그닥 뚜렷하지 않았음…"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#시각화-3---heatmap",
    "href": "2023_DV/Review/강신성_1123.html#시각화-3---heatmap",
    "title": "1. 라이브러리 imports",
    "section": "7. 시각화 3 - heatmap",
    "text": "7. 시각화 3 - heatmap\n\nA. (요일, 시간)에 따른 count 시각화\n\n\ntidydata = df_feature.pivot_table(\n    index = 'pickup_hour',\n    columns = 'dayofweek',\n    aggfunc = 'size'  ## size를 넣음으로써 value를 대체, value를 id로 넣고 함수를 len으로 적용해도 됨\n).stack().reset_index().rename({0 : 'count'}, axis = 1)\n\npx.density_heatmap(\n    data_frame = tidydata,\n    x = 'pickup_hour',\n    y = 'dayofweek',\n    z = 'count',  ## 밀도라 z축으로 색상을 택해야 함.\n    nbinsx = 24,  ## number of bins for x, 시간이니까 24개\n    nbinsy = 7,   ## 요일이니까 7개\n    height = 300\n)\n\n\n                                                \n\n\n\n1~4의 18~21시가 밝아보임.\n토요일, 일요일 새벽과 목금토 밤은 더 밝아보임 \\(\\to\\) 불금???"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#b.-요일-시간에-따른-dist-시각화",
    "href": "2023_DV/Review/강신성_1123.html#b.-요일-시간에-따른-dist-시각화",
    "title": "1. 라이브러리 imports",
    "section": "### B. (요일, 시간)에 따른 dist 시각화",
    "text": "### B. (요일, 시간)에 따른 dist 시각화\n\ntidydata = df_feature.pivot_table(\n    index = 'dayofweek',\n    columns = 'pickup_hour',\n    values = 'dist',\n    aggfunc = 'mean'\n).stack().reset_index().rename({0 : 'dist_mean'}, axis = 1)\n\npx.density_heatmap(\n    data_frame = tidydata,\n    x = 'pickup_hour',\n    y = 'dayofweek',\n    z = 'dist_mean',  ## 밀도라 z축으로 색상을 택해야 함.\n    nbinsx = 24,  ## number of bins for x, 시간이니까 24개\n    nbinsy = 7,   ## 요일이니까 7개\n    height = 300\n)\n\n\n                                                \n\n\n\n일요일 아침이 샛노랗네…?(여행을 끝내고 복귀하는 사람들이지 않을까?)\n\n\nC. (요일, 시간)에 따른 speed 시각화\n\n\ndf_feature.columns\n\nIndex(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n       'trip_duration', 'log_trip_duration', 'dist', 'speed', 'pickup_hour',\n       'dropoff_hour', 'dayofweek'],\n      dtype='object')\n\n\n\ntidydata = df_feature.pivot_table(\n    index = 'dayofweek',\n    columns = 'pickup_hour',\n    values = 'speed',\n    aggfunc = 'mean'\n).stack().reset_index().rename({0 : 'speed'}, axis = 1)\n\npx.density_heatmap(\n    data_frame = tidydata,\n    x = 'pickup_hour',\n    y = 'dayofweek',\n    z = 'speed',  ## 밀도라 z축으로 색상을 택해야 함.\n    nbinsx = 24,  ## number of bins for x, 시간이니까 24개\n    nbinsy = 7,   ## 요일이니까 7개\n    height = 300\n)\n\n\n                                                \n\n\n\n전체적으로 새벽시간때에는 속도가 빠름, 한산해서 교통체증이 심하지 않다."
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#시각화-5-경로-시각화",
    "href": "2023_DV/Review/강신성_1123.html#시각화-5-경로-시각화",
    "title": "1. 라이브러리 imports",
    "section": "8. 시각화 5 : 경로 시각화",
    "text": "8. 시각화 5 : 경로 시각화\n\n경로 시각화는 무거워서 좀 작은 데이터로 실습합니다…\n\n\ndf_feature_small = df_feature[::100].reset_index(drop=True)  ## 100칸 씩 건너뛰며 몇개만 추출\ndf_feature_small\n\n\n\n\n\n\n\n\nid\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\nstore_and_fwd_flag\ntrip_duration\nlog_trip_duration\ndist\nspeed\npickup_hour\ndropoff_hour\ndayofweek\n\n\n\n\n0\nid2875421\nB\n2016-03-14 17:24:55\n2016-03-14 17:32:30\n1\n-73.982155\n40.767937\n-73.964630\n40.765602\nN\n455\n6.120297\n0.017680\n0.000039\n17\n17\n0\n\n\n1\nid3667993\nB\n2016-01-03 04:18:57\n2016-01-03 04:27:03\n1\n-73.980522\n40.730530\n-73.997993\n40.746220\nN\n486\n6.186209\n0.023482\n0.000048\n4\n4\n6\n\n\n2\nid2002463\nB\n2016-01-14 12:28:56\n2016-01-14 12:37:17\n1\n-73.965652\n40.768398\n-73.960068\n40.779308\nN\n501\n6.216606\n0.012256\n0.000024\n12\n12\n3\n\n\n3\nid1635353\nB\n2016-03-04 23:20:58\n2016-03-04 23:49:29\n5\n-73.985092\n40.759190\n-73.962151\n40.709850\nN\n1711\n7.444833\n0.054412\n0.000032\n23\n23\n4\n\n\n4\nid1850636\nA\n2016-02-05 00:21:28\n2016-02-05 00:52:24\n1\n-73.994537\n40.750439\n-74.025719\n40.631100\nN\n1856\n7.526179\n0.123345\n0.000066\n0\n0\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n141\nid0621879\nA\n2016-04-23 09:31:33\n2016-04-23 09:51:33\n1\n-73.950783\n40.743614\n-74.006218\n40.722729\nN\n1200\n7.090077\n0.059239\n0.000049\n9\n9\n5\n\n\n142\nid2587483\nB\n2016-03-28 12:59:58\n2016-03-28 13:08:11\n2\n-73.953903\n40.787079\n-73.940842\n40.792461\nN\n493\n6.200509\n0.014127\n0.000029\n12\n13\n0\n\n\n143\nid1030598\nB\n2016-03-03 11:44:24\n2016-03-03 11:49:59\n1\n-74.005066\n40.719143\n-74.006065\n40.735134\nN\n335\n5.814131\n0.016022\n0.000048\n11\n11\n3\n\n\n144\nid3094934\nA\n2016-03-21 09:53:40\n2016-03-21 10:22:20\n1\n-73.986153\n40.722431\n-73.985977\n40.762669\nN\n1720\n7.450080\n0.040238\n0.000023\n9\n10\n0\n\n\n145\nid0503659\nB\n2016-04-19 18:06:09\n2016-04-19 18:23:09\n2\n-73.952209\n40.784500\n-73.966103\n40.804832\nN\n1020\n6.927558\n0.024626\n0.000024\n18\n18\n1\n\n\n\n\n146 rows × 17 columns"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#a.-경로-그리기px.line_mapbox",
    "href": "2023_DV/Review/강신성_1123.html#a.-경로-그리기px.line_mapbox",
    "title": "1. 라이브러리 imports",
    "section": "### A. 경로 그리기(px.line_mapbox())",
    "text": "### A. 경로 그리기(px.line_mapbox())\n- 경로를 그리는 것은 이미 함수로 나와있다.\n\ndf_sample = pd.DataFrame(\n    {'path':['A','A','B','B','B'],\n     'lon':[-73.986420,-73.995300,-73.975922,-73.988922,-73.962654],\n     'lat':[40.756569,40.740059,40.754192,40.762859,40.772449]}\n)\n\n\nfig = px.line_mapbox(\n    data_frame = df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    line_group = 'path',  ## 얜 넣으면 선분별로 그룹핑이 된다는데,,, 안해도 되네요. 왤까.\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600\n)\nfig.show(config = {'scrollZoom' : False})\n\n\n                                                \n\n\n\n뭔가 경유지가 점으로 표기되면 좋겠는데?\n\n- 산점도로 그리기\n\n_fig = px.scatter_mapbox(\n    data_frame = df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600    \n)\n_fig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n- 합치기\n\npx.scatter_mapbox(\n    data_frame=df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600    \n).data\n\n(Scattermapbox({\n     'hovertemplate': 'path=A&lt;br&gt;lat=%{lat}&lt;br&gt;lon=%{lon}&lt;extra&gt;&lt;/extra&gt;',\n     'lat': array([40.756569, 40.740059]),\n     'legendgroup': 'A',\n     'lon': array([-73.98642, -73.9953 ]),\n     'marker': {'color': '#636efa'},\n     'mode': 'markers',\n     'name': 'A',\n     'showlegend': True,\n     'subplot': 'mapbox'\n }),\n Scattermapbox({\n     'hovertemplate': 'path=B&lt;br&gt;lat=%{lat}&lt;br&gt;lon=%{lon}&lt;extra&gt;&lt;/extra&gt;',\n     'lat': array([40.754192, 40.762859, 40.772449]),\n     'legendgroup': 'B',\n     'lon': array([-73.975922, -73.988922, -73.962654]),\n     'marker': {'color': '#EF553B'},\n     'mode': 'markers',\n     'name': 'B',\n     'showlegend': True,\n     'subplot': 'mapbox'\n }))\n\n\n\n’mapbox에data를 붙이게 되면 튜플 형태의mapbox`들이 나온다.\n\n\nfig = px.line_mapbox(\n    data_frame=df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    line_group = 'path',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600    \n)\nscatter_data = px.scatter_mapbox(\n    data_frame=df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600    \n).data  ## geom_point 느낌으로 추가할 수 있는 \"데이터\"\n\nfig.add_trace(scatter_data[0])  ## 첫 번째 개체(빨간색)를 함침\nfig.add_trace(scatter_data[1])  ## 두 번째 개체(파란색)를 합침, 많으면 for문 이용해서 다 넣어야 하나봐...\nfig.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\nfig = px.line_mapbox(\n    data_frame=df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    line_group = 'path',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600    \n).data\nscatter_data = px.scatter_mapbox(\n    data_frame=df_sample,\n    lat = 'lat',\n    lon = 'lon',\n    color = 'path',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom=12,\n    width = 750,\n    height = 600    \n)\n\nscatter_data.add_trace(fig[0])\nscatter_data.add_trace(fig[1])\nscatter_data.show(config={'scrollZoom':False})\n\n\n                                                \n\n\n\nB. 전처리\n\n\n_df = df_feature_small.loc[[0], :]\n_df\n\n\n\n\n\n\n\n\nid\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\npickup_longitude\npickup_latitude\ndropoff_longitude\ndropoff_latitude\nstore_and_fwd_flag\ntrip_duration\nlog_trip_duration\ndist\nspeed\npickup_hour\ndropoff_hour\ndayofweek\n\n\n\n\n0\nid2875421\nB\n2016-03-14 17:24:55\n2016-03-14 17:32:30\n1\n-73.982155\n40.767937\n-73.96463\n40.765602\nN\n455\n6.120297\n0.01768\n0.000039\n17\n17\n0\n\n\n\n\n\n\n\n\n탑승과 하차를 더미화시키고, 그것에 따른 데이터들을 늘이고 싶다.(long data로)\n\n\ndf_pickup = df_feature_small.drop([col for col in _df.columns if 'dropoff' in col], axis = 1).assign(type = 'pickup')\\\n.rename({col:col.split('_')[-1] for col in _df.columns if 'pickup' in col}, axis = 1)\ndf_dropoff = df_feature_small.drop([col for col in _df.columns if 'pickup' in col], axis = 1).assign(type = 'dropoff')\\\n.rename({col:col.split('_')[-1] for col in _df.columns if 'dropoff' in col}, axis = 1)\n\n\npd.concat([df_pickup, df_dropoff], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nid\nvendor_id\ndatetime\npassenger_count\nlongitude\nlatitude\nstore_and_fwd_flag\ntrip_duration\nlog_trip_duration\ndist\nspeed\nhour\ndayofweek\ntype\n\n\n\n\n0\nid2875421\nB\n2016-03-14 17:24:55\n1\n-73.982155\n40.767937\nN\n455\n6.120297\n0.017680\n0.000039\n17\n0\npickup\n\n\n1\nid3667993\nB\n2016-01-03 04:18:57\n1\n-73.980522\n40.730530\nN\n486\n6.186209\n0.023482\n0.000048\n4\n6\npickup\n\n\n2\nid2002463\nB\n2016-01-14 12:28:56\n1\n-73.965652\n40.768398\nN\n501\n6.216606\n0.012256\n0.000024\n12\n3\npickup\n\n\n3\nid1635353\nB\n2016-03-04 23:20:58\n5\n-73.985092\n40.759190\nN\n1711\n7.444833\n0.054412\n0.000032\n23\n4\npickup\n\n\n4\nid1850636\nA\n2016-02-05 00:21:28\n1\n-73.994537\n40.750439\nN\n1856\n7.526179\n0.123345\n0.000066\n0\n4\npickup\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n287\nid0621879\nA\n2016-04-23 09:51:33\n1\n-74.006218\n40.722729\nN\n1200\n7.090077\n0.059239\n0.000049\n9\n5\ndropoff\n\n\n288\nid2587483\nB\n2016-03-28 13:08:11\n2\n-73.940842\n40.792461\nN\n493\n6.200509\n0.014127\n0.000029\n13\n0\ndropoff\n\n\n289\nid1030598\nB\n2016-03-03 11:49:59\n1\n-74.006065\n40.735134\nN\n335\n5.814131\n0.016022\n0.000048\n11\n3\ndropoff\n\n\n290\nid3094934\nA\n2016-03-21 10:22:20\n1\n-73.985977\n40.762669\nN\n1720\n7.450080\n0.040238\n0.000023\n10\n0\ndropoff\n\n\n291\nid0503659\nB\n2016-04-19 18:23:09\n2\n-73.966103\n40.804832\nN\n1020\n6.927558\n0.024626\n0.000024\n18\n1\ndropoff\n\n\n\n\n292 rows × 14 columns\n\n\n\n\n## 교수님 방법(이렇게까지 할건 아닌 것 같은데...)\n\npcol = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'pickup_hour']\ndcol = ['dropoff_datetime', 'dropoff_longitude', 'dropoff_latitude', 'dropoff_hour']\n\ndef transform(df) : \n    pickup = df.loc[:, ['id']+pcol].set_axis(['id', 'datetime', 'longitude', 'latitude', 'hour'], axis = 1).assign(type = 'pickup')\n    dropoff = df.loc[:,['id']+dcol].set_axis(['id', 'datetime', 'longitude', 'latitude', 'hour'], axis = 1).assign(type = 'dropoff')\n    return pd.concat([pickup, dropoff], axis = 0)\n\ndf_left = df_feature_small.drop(pcol + dcol, axis = 1)\ndf_right = pd.concat([transform(df) for i, df in df_feature_small.groupby('id')]).reset_index(drop = True)\ndf_feature_small2 = df_left.merge(df_right)\ndf_feature_small2.head()\n\n\n\n\n\n\n\n\nid\nvendor_id\npassenger_count\nstore_and_fwd_flag\ntrip_duration\nlog_trip_duration\ndist\nspeed\ndayofweek\ndatetime\nlongitude\nlatitude\nhour\ntype\n\n\n\n\n0\nid2875421\nB\n1\nN\n455\n6.120297\n0.017680\n0.000039\n0\n2016-03-14 17:24:55\n-73.982155\n40.767937\n17\npickup\n\n\n1\nid2875421\nB\n1\nN\n455\n6.120297\n0.017680\n0.000039\n0\n2016-03-14 17:32:30\n-73.964630\n40.765602\n17\ndropoff\n\n\n2\nid3667993\nB\n1\nN\n486\n6.186209\n0.023482\n0.000048\n6\n2016-01-03 04:18:57\n-73.980522\n40.730530\n4\npickup\n\n\n3\nid3667993\nB\n1\nN\n486\n6.186209\n0.023482\n0.000048\n6\n2016-01-03 04:27:03\n-73.997993\n40.746220\n4\ndropoff\n\n\n4\nid2002463\nB\n1\nN\n501\n6.216606\n0.012256\n0.000024\n3\n2016-01-14 12:28:56\n-73.965652\n40.768398\n12\npickup\n\n\n\n\n\n\n\n\n## 데이터프레임 각 열이 들어오면 반갈죽시킴\npd.concat([transform(df) for i, df in df_feature_small.groupby('id')]).reset_index(drop = True)\n\n\n\n\n\n\n\n\nid\ndatetime\nlongitude\nlatitude\nhour\ntype\n\n\n\n\n0\nid0037819\n2016-05-16 17:42:32\n-73.986420\n40.756569\n17\npickup\n\n\n1\nid0037819\n2016-05-16 17:47:05\n-73.995300\n40.740059\n17\ndropoff\n\n\n2\nid0049607\n2016-03-13 18:48:49\n-73.975922\n40.754192\n18\npickup\n\n\n3\nid0049607\n2016-03-13 18:56:08\n-73.988922\n40.762859\n18\ndropoff\n\n\n4\nid0051866\n2016-01-04 18:48:12\n-73.962654\n40.772449\n18\npickup\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n287\nid3825370\n2016-05-08 17:36:48\n-73.979195\n40.669765\n17\ndropoff\n\n\n288\nid3888107\n2016-06-21 18:30:05\n-73.969429\n40.757469\n18\npickup\n\n\n289\nid3888107\n2016-06-21 18:44:43\n-73.982742\n40.771969\n18\ndropoff\n\n\n290\nid3988208\n2016-03-01 21:40:13\n-73.948929\n40.797405\n21\npickup\n\n\n291\nid3988208\n2016-03-01 21:47:26\n-73.967438\n40.789543\n21\ndropoff\n\n\n\n\n292 rows × 6 columns"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#c.-vendor_id-passenger_count-시각화",
    "href": "2023_DV/Review/강신성_1123.html#c.-vendor_id-passenger_count-시각화",
    "title": "1. 라이브러리 imports",
    "section": "### C. vendor_id, passenger_count 시각화",
    "text": "### C. vendor_id, passenger_count 시각화\n\nfig = px.line_mapbox(\n    data_frame = df_feature_small2,\n    lat = 'latitude',\n    lon = 'longitude',\n    color = 'vendor_id',\n    line_group = 'id',  ## 개별 승객 당 묶어야 하니까...\n    center = {'lat' : 40.7322, 'lon' : -73.9052},\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nscatter_data = px.scatter_mapbox(\n    data_frame = df_feature_small2,\n    lat = 'latitude',\n    lon = 'longitude',\n    size = 'passenger_count',\n    size_max = 10,\n    color = 'vendor_id',\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n).data ## 튜플임\n\nfor sd in scatter_data :\n    fig.add_trace(sd)\n\n##fig.add_traces(scatter_data)\n\nfig.update_traces(\n    line = {'width':1},\n    opacity = 0.8    \n)\nfig.show(config = {'scrollZoom' : False})\n\n\n                                                \n\n\n\nB가 원이 크다는 게 확실히 보인다.(한번에 이용하는 사람 수가 많음)\n\n\nD. dayofweek별 시각화\n\n\ndf_feature_small2.dayofweek\n\n0      0\n1      0\n2      6\n3      6\n4      3\n      ..\n287    3\n288    0\n289    0\n290    1\n291    1\nName: dayofweek, Length: 292, dtype: int32\n\n\n\ntidydata = df_feature_small2.assign(dayofweek = lambda _df : _df.dayofweek.apply(str)).sort_values('dayofweek')\n\nfig = px.line_mapbox(\n    data_frame = tidydata,\n    lat = 'latitude',\n    lon = 'longitude',\n    color = 'dayofweek',  ## 여길 바꿔줘야 함\n    line_group = 'id',  ## 개별 승객 당 묶는 건 그대로 해야지...\n    center = {'lat' : 40.7322, 'lon' : -73.9052},\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nscatter_data = px.scatter_mapbox(\n    data_frame = tidydata,\n    lat = 'latitude',\n    lon = 'longitude',\n    size = 'passenger_count',\n    size_max = 10,\n    color = 'dayofweek',  ## 위랑 같은 색깔로 묶여아지...\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n).data ## 튜플임\n\nfor sd in scatter_data :\n    fig.add_trace(sd)\n\n##fig.add_traces(scatter_data)\n\nfig.update_traces(\n    line = {'width':1},\n    opacity = 0.8    \n)\nfig.show(config = {'scrollZoom' : False})\n\n\n                                                \n\n\n\n월요일이 특히 안쪽에서만 움직이는 인원이 많음"
  },
  {
    "objectID": "2023_DV/Review/강신성_1123.html#e.-speed별-시각화",
    "href": "2023_DV/Review/강신성_1123.html#e.-speed별-시각화",
    "title": "1. 라이브러리 imports",
    "section": "### E. speed별 시각화",
    "text": "### E. speed별 시각화\n\ndf_feature_small2.speed\n\n0      0.000039\n1      0.000039\n2      0.000048\n3      0.000048\n4      0.000024\n         ...   \n287    0.000048\n288    0.000023\n289    0.000023\n290    0.000024\n291    0.000024\nName: speed, Length: 292, dtype: float64\n\n\n\n이대로 넣으면 족된다…\n\n\ntidydata = df_feature_small2.assign(speed = lambda _df : pd.qcut(_df.speed, q = 4)).sort_values('speed', ascending = False)\n\nfig = px.line_mapbox(\n    data_frame = tidydata,\n    lat = 'latitude',\n    lon = 'longitude',\n    color = 'speed',  ## 여길 바꿔줘야 함\n    line_group = 'id',  ## 개별 승객 당 묶는 건 그대로 해야지...\n    center = {'lat' : 40.7322, 'lon' : -73.9052},\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n)\n\nscatter_data = px.scatter_mapbox(\n    data_frame = tidydata,\n    lat = 'latitude',\n    lon = 'longitude',\n    size = 'passenger_count',\n    size_max = 10,\n    color = 'speed',  ## 위랑 같은 색깔로 묶여아지...\n    #---#\n    mapbox_style = 'carto-positron',\n    zoom = 10,\n    width = 750,\n    height = 600\n).data ## 튜플임\n\nfor sd in scatter_data :\n    fig.add_trace(sd)\n\n##fig.add_traces(scatter_data)\n\nfig.update_traces(\n    line = {'width':1},\n    opacity = 0.8    \n)\nfig.show(config = {'scrollZoom' : False});\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotly\\express\\_core.py:2044: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotly\\express\\_core.py:2044: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n                                                \n\n\n\n멀리 가는 것은 평균 속도가 아무래도 빠르다.(가장 위의 것)"
  },
  {
    "objectID": "2023_DV/Review/강신성_1115.html",
    "href": "2023_DV/Review/강신성_1115.html",
    "title": "1. 라이브러리 imports",
    "section": "",
    "text": "이번엔 plotly로 그래픽스를 만들거다…\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport json\nimport requests"
  },
  {
    "objectID": "2023_DV/Review/강신성_1115.html#에너지사용량-시각화",
    "href": "2023_DV/Review/강신성_1115.html#에너지사용량-시각화",
    "title": "1. 라이브러리 imports",
    "section": "2. 에너지사용량 시각화",
    "text": "2. 에너지사용량 시각화\n\nA. 데이터 불러오기\n\n\nglobal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json').text)\nlocal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-municipalities-2018-geo.json').text)\n#--#\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0"
  },
  {
    "objectID": "2023_DV/Review/강신성_1115.html#b.-데이터-정리한글로-변환",
    "href": "2023_DV/Review/강신성_1115.html#b.-데이터-정리한글로-변환",
    "title": "1. 라이브러리 imports",
    "section": "### B. 데이터 정리(한글로 변환)",
    "text": "### B. 데이터 정리(한글로 변환)\n(1) global_dict 내의 영어이름과 df의 영어이름이 일치하는지 확인\n\nset(df.시도) == {l['properties']['name_eng'] for l in global_dict['features']}\n\nTrue\n\n\n\n{l['properties']['name_eng'] for l in global_dict['features']}\n\n{'Busan',\n 'Chungcheongbuk-do',\n 'Chungcheongnam-do',\n 'Daegu',\n 'Daejeon',\n 'Gangwon-do',\n 'Gwangju',\n 'Gyeonggi-do',\n 'Gyeongsangbuk-do',\n 'Gyeongsangnam-do',\n 'Incheon',\n 'Jeju-do',\n 'Jeollabuk-do',\n 'Jeollanam-do',\n 'Sejongsi',\n 'Seoul',\n 'Ulsan'}\n\n\n\n딕셔너리의 key만으로 딕셔너리 컴프리헨션 하면 set()을 한 것처럼 알파벳 순서대로 key값만 나온다.\n\n(2) global_dict 내의 영어이름과 한글이름을 이용해 변환을 위한 dictionary 생성\n\n_dct = {l['properties']['name_eng'] : l['properties']['name'] for l in global_dict['features']}\n_dct\n\n{'Seoul': '서울특별시',\n 'Busan': '부산광역시',\n 'Daegu': '대구광역시',\n 'Incheon': '인천광역시',\n 'Gwangju': '광주광역시',\n 'Daejeon': '대전광역시',\n 'Ulsan': '울산광역시',\n 'Sejongsi': '세종특별자치시',\n 'Gyeonggi-do': '경기도',\n 'Gangwon-do': '강원도',\n 'Chungcheongbuk-do': '충청북도',\n 'Chungcheongnam-do': '충청남도',\n 'Jeollabuk-do': '전라북도',\n 'Jeollanam-do': '전라남도',\n 'Gyeongsangbuk-do': '경상북도',\n 'Gyeongsangnam-do': '경상남도',\n 'Jeju-do': '제주특별자치도'}\n\n\n(3) df에 변환을 수행하여 영어지명을 한글지명으로 변환\n\ndf.assign(시도 = lambda _df : _df.시도.apply(lambda x : [v for k, v in _dct.items() if x == k].pop()))\n\n\n  \n    \n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\n서울특별시\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\n서울특별시\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\n서울특별시\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\n서울특별시\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\n서울특별시\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n2019\n제주특별자치도\n서귀포시\n34729\n7233931\n34641\n1306\n0\n\n\n996\n2020\n제주특별자치도\n제주시\n66504\n19819923\n99212\n22179\n0\n\n\n997\n2020\n제주특별자치도\n서귀포시\n34880\n7330040\n35510\n1639\n0\n\n\n998\n2021\n제주특별자치도\n제주시\n67053\n20275738\n103217\n25689\n0\n\n\n999\n2021\n제주특별자치도\n서귀포시\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n\n1000 rows × 8 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.시도.map(_dct)  ## 람다 이것저것 안쓰고 이렇게 하면 한번에 할 수도 있다.\n\n0        서울특별시\n1        서울특별시\n2        서울특별시\n3        서울특별시\n4        서울특별시\n        ...   \n995    제주특별자치도\n996    제주특별자치도\n997    제주특별자치도\n998    제주특별자치도\n999    제주특별자치도\nName: 시도, Length: 1000, dtype: object\n\n\n(4) local_dict와 global_dict의 지명정보를 정리하여 데이터프레임으로 만듦\n# 예비학습\n\npd.DataFrame(\n    [{'X':100,'y':0},\n     {'X':101,'y':1}]\n)    ## 딕셔너리를 리스트로 넣어버리면 행이 분리된 채로 넣어줄수도 있다.\n\n\n  \n    \n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n100\n0\n\n\n1\n101\n1\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n위와 동일한 원리로 코드와 이름에 해당하는 애들을 추출함('properties'에는 지역에 대한 코드 정보가 딕셔너리로 포함되어 있음)\n\n\ndf_local = pd.DataFrame([l['properties'] for l in local_dict['features']])\\\n.drop(['name_eng','base_year'],axis=1)\ndf_local\n\n\n  \n    \n\n\n\n\n\n\nname\ncode\n\n\n\n\n0\n종로구\n11010\n\n\n1\n중구\n11020\n\n\n2\n용산구\n11030\n\n\n3\n성동구\n11040\n\n\n4\n광진구\n11050\n\n\n...\n...\n...\n\n\n245\n함양군\n38380\n\n\n246\n거창군\n38390\n\n\n247\n합천군\n38400\n\n\n248\n제주시\n39010\n\n\n249\n서귀포시\n39020\n\n\n\n\n\n250 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf_global = pd.DataFrame([l['properties'] for l in global_dict['features']])\\\n.drop(['name_eng','base_year'],axis=1)\ndf_global\n\n\n  \n    \n\n\n\n\n\n\nname\ncode\n\n\n\n\n0\n서울특별시\n11\n\n\n1\n부산광역시\n21\n\n\n2\n대구광역시\n22\n\n\n3\n인천광역시\n23\n\n\n4\n광주광역시\n24\n\n\n5\n대전광역시\n25\n\n\n6\n울산광역시\n26\n\n\n7\n세종특별자치시\n29\n\n\n8\n경기도\n31\n\n\n9\n강원도\n32\n\n\n10\n충청북도\n33\n\n\n11\n충청남도\n34\n\n\n12\n전라북도\n35\n\n\n13\n전라남도\n36\n\n\n14\n경상북도\n37\n\n\n15\n경상남도\n38\n\n\n16\n제주특별자치도\n39\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n(5) loc에서 “전주시완산구”와 같이 정리된 지명들을 “완산구”로 변환\n\ndf_local.name\n\n0       종로구\n1        중구\n2       용산구\n3       성동구\n4       광진구\n       ... \n245     함양군\n246     거창군\n247     합천군\n248     제주시\n249    서귀포시\nName: name, Length: 250, dtype: object\n\n\n\n'시'가 들어간 이름들을 변환\n\n\n'12314ddd'.split('4')\n\n['1231', 'ddd']\n\n\n\n_dct = {l : l.split('시')[-1] for l in df_local.name if ('시' in l) and ('구' in l) and len(l) &gt; 3}\n_dct\n\n{'수원시장안구': '장안구',\n '수원시권선구': '권선구',\n '수원시팔달구': '팔달구',\n '수원시영통구': '영통구',\n '성남시수정구': '수정구',\n '성남시중원구': '중원구',\n '성남시분당구': '분당구',\n '안양시만안구': '만안구',\n '안양시동안구': '동안구',\n '안산시상록구': '상록구',\n '안산시단원구': '단원구',\n '고양시덕양구': '덕양구',\n '고양시일산동구': '일산동구',\n '고양시일산서구': '일산서구',\n '용인시처인구': '처인구',\n '용인시기흥구': '기흥구',\n '용인시수지구': '수지구',\n '청주시상당구': '상당구',\n '청주시서원구': '서원구',\n '청주시흥덕구': '흥덕구',\n '청주시청원구': '청원구',\n '천안시동남구': '동남구',\n '천안시서북구': '서북구',\n '전주시완산구': '완산구',\n '전주시덕진구': '덕진구',\n '포항시남구': '남구',\n '포항시북구': '북구',\n '창원시의창구': '의창구',\n '창원시성산구': '성산구',\n '창원시마산합포구': '마산합포구',\n '창원시마산회원구': '마산회원구',\n '창원시진해구': '진해구'}\n\n\n\ndf_local.set_index('name').rename(_dct).reset_index()\n\n\n  \n    \n\n\n\n\n\n\nname\ncode\n\n\n\n\n0\n종로구\n11010\n\n\n1\n중구\n11020\n\n\n2\n용산구\n11030\n\n\n3\n성동구\n11040\n\n\n4\n광진구\n11050\n\n\n...\n...\n...\n\n\n245\n함양군\n38380\n\n\n246\n거창군\n38390\n\n\n247\n합천군\n38400\n\n\n248\n제주시\n39010\n\n\n249\n서귀포시\n39020\n\n\n\n\n\n250 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n시도를 뺀 네임과 코드를 표기하였음\n\n(6) df_local과 df_global의 정보를 정리하여 merge, 합쳐진 정보를 df_json에 저장\n\ndf_local.set_index('name').rename(_dct).reset_index()\\\n.rename({'code':'code_local','name':'name_local'},axis=1)\\\n.assign(code = lambda df: df.code_local.str[:2])\n\n## 코드의 앞 두자리를 공통분모로 삼을 것이다. df_global에는 code 앞 두자리를 넣어놨음\n\n\n  \n    \n\n\n\n\n\n\nname_local\ncode_local\ncode\n\n\n\n\n0\n종로구\n11010\n11\n\n\n1\n중구\n11020\n11\n\n\n2\n용산구\n11030\n11\n\n\n3\n성동구\n11040\n11\n\n\n4\n광진구\n11050\n11\n\n\n...\n...\n...\n...\n\n\n245\n함양군\n38380\n38\n\n\n246\n거창군\n38390\n38\n\n\n247\n합천군\n38400\n38\n\n\n248\n제주시\n39010\n39\n\n\n249\n서귀포시\n39020\n39\n\n\n\n\n\n250 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf_json = df_local.set_index('name').rename(_dct).reset_index()\\\n.rename({'code':'code_local','name':'name_local'},axis=1)\\\n.assign(code = lambda df: df.code_local.str[:2])\\\n.merge(df_global)\ndf_json\n\n\n  \n    \n\n\n\n\n\n\nname_local\ncode_local\ncode\nname\n\n\n\n\n0\n종로구\n11010\n11\n서울특별시\n\n\n1\n중구\n11020\n11\n서울특별시\n\n\n2\n용산구\n11030\n11\n서울특별시\n\n\n3\n성동구\n11040\n11\n서울특별시\n\n\n4\n광진구\n11050\n11\n서울특별시\n\n\n...\n...\n...\n...\n...\n\n\n245\n함양군\n38380\n38\n경상남도\n\n\n246\n거창군\n38390\n38\n경상남도\n\n\n247\n합천군\n38400\n38\n경상남도\n\n\n248\n제주시\n39010\n39\n제주특별자치도\n\n\n249\n서귀포시\n39020\n39\n제주특별자치도\n\n\n\n\n\n250 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n(7) df_json과 df의 정보를 merge하기 위하여 ’서울특별시-종로구’와 같은 형식으로 공통열을 각각 생성, 생성된 공통열의 원소가 일치하는지 비교\n\ndf\n\n\n  \n    \n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n2019\nJeju-do\n서귀포시\n34729\n7233931\n34641\n1306\n0\n\n\n996\n2020\nJeju-do\n제주시\n66504\n19819923\n99212\n22179\n0\n\n\n997\n2020\nJeju-do\n서귀포시\n34880\n7330040\n35510\n1639\n0\n\n\n998\n2021\nJeju-do\n제주시\n67053\n20275738\n103217\n25689\n0\n\n\n999\n2021\nJeju-do\n서귀포시\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n\n1000 rows × 8 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ns1 = df_json.assign(on = lambda df: df.name + '-' + df.name_local)['on']\n\n\ns2 = df.assign(\n    시도 = df.시도.map({l['properties']['name_eng']:l['properties']['name'] for l in global_dict['features']})\n).assign(on = lambda df: df.시도 + '-' + df.지역)['on']\n\n\nset(s1)-set(s2), set(s2)-set(s1)\n\n({'인천광역시-남구'}, {'인천광역시-미추홀구'})\n\n\n\n똑같은 방식이었을 텐데 다른 값이 있다.\n\n\n인천 남구가 미추홀구로 행정명이 바뀜\n\n(8) 지역명을 적절히 변환\n\ndf_json\n\n\n  \n    \n\n\n\n\n\n\nname_local\ncode_local\ncode\nname\n\n\n\n\n0\n종로구\n11010\n11\n서울특별시\n\n\n1\n중구\n11020\n11\n서울특별시\n\n\n2\n용산구\n11030\n11\n서울특별시\n\n\n3\n성동구\n11040\n11\n서울특별시\n\n\n4\n광진구\n11050\n11\n서울특별시\n\n\n...\n...\n...\n...\n...\n\n\n245\n함양군\n38380\n38\n경상남도\n\n\n246\n거창군\n38390\n38\n경상남도\n\n\n247\n합천군\n38400\n38\n경상남도\n\n\n248\n제주시\n39010\n39\n제주특별자치도\n\n\n249\n서귀포시\n39020\n39\n제주특별자치도\n\n\n\n\n\n250 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf_json.assign(on = lambda df: df.name + '-' + df.name_local)\\\n.set_index('on').rename({'인천광역시-남구' : '인천광역시-미추홀구'}, axis = 0).reset_index().loc[:, ['on','code','code_local']]\n\n\n  \n    \n\n\n\n\n\n\non\ncode\ncode_local\n\n\n\n\n0\n서울특별시-종로구\n11\n11010\n\n\n1\n서울특별시-중구\n11\n11020\n\n\n2\n서울특별시-용산구\n11\n11030\n\n\n3\n서울특별시-성동구\n11\n11040\n\n\n4\n서울특별시-광진구\n11\n11050\n\n\n...\n...\n...\n...\n\n\n245\n경상남도-함양군\n38\n38380\n\n\n246\n경상남도-거창군\n38\n38390\n\n\n247\n경상남도-합천군\n38\n38400\n\n\n248\n제주특별자치도-제주시\n39\n39010\n\n\n249\n제주특별자치도-서귀포시\n39\n39020\n\n\n\n\n\n250 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n(9) 데이터프레임을 결합\n\ndf_left = df_json.assign(on = lambda df: df.name + '-' + df.name_local)\\\n.set_index('on').rename({'인천광역시-남구' : '인천광역시-미추홀구'}, axis = 0).reset_index().loc[:, ['on','code','code_local']]\n\ndf_right = df.assign(\n    시도 = df.시도.map({l['properties']['name_eng']:l['properties']['name'] for l in global_dict['features']})\n).assign(on = lambda df: df.시도 + '-' + df.지역)\n\ndf2 = pd.merge(df_left, df_right).drop('on', axis = 1).set_index(['시도','지역','년도']).reset_index()\ndf2\n\n\n  \n    \n\n\n\n\n\n\n시도\n지역\n년도\ncode\ncode_local\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n서울특별시\n종로구\n2018\n11\n11010\n17929\n9141777\n64818\n82015\n111\n\n\n1\n서울특별시\n종로구\n2019\n11\n11010\n17851\n9204140\n63492\n76653\n799\n\n\n2\n서울특별시\n종로구\n2020\n11\n11010\n17638\n9148895\n60123\n71263\n912\n\n\n3\n서울특별시\n종로구\n2021\n11\n11010\n22845\n18551145\n125179\n117061\n0\n\n\n4\n서울특별시\n중구\n2018\n11\n11020\n10598\n10056233\n81672\n75260\n563\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n제주특별자치도\n제주시\n2021\n39\n39010\n67053\n20275738\n103217\n25689\n0\n\n\n996\n제주특별자치도\n서귀포시\n2018\n39\n39020\n34154\n6914685\n34470\n1597\n0\n\n\n997\n제주특별자치도\n서귀포시\n2019\n39\n39020\n34729\n7233931\n34641\n1306\n0\n\n\n998\n제주특별자치도\n서귀포시\n2020\n39\n39020\n34880\n7330040\n35510\n1639\n0\n\n\n999\n제주특별자치도\n서귀포시\n2021\n39\n39020\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n\n1000 rows × 10 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\non 열은 단순히 merge를 위해 만들어둔 더미 열이므로 없애줌\n\n\nC. 시각화(2018년도 전기에너지 사용량)\n\n\nfor l in local_dict['features']\n\n\npx.choropleth_mapbox(\n    geojson = local_dict,  ## geo_data\n    featureidkey = 'properties.code',  ## key_on\n    data_frame = df2.query('년도 == 2018'),   ## data\n    locations = 'code_local',   ## columns 중 key(첫번째 열)\n    color = '에너지사용량(TOE)/전기',   ## columns 중 value(두번째 열)\n    hover_data = ['시도', '지역'],\n    #---#\n    mapbox_style=\"carto-positron\",\n    center={\"lat\": 36, \"lon\": 127.5},\n    zoom=6,\n    height=800,\n    width=800\n)\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "2023_DV/Review/강신성_1115.html#d.-시각화20182021년도-서울의-전기에너지-사용량",
    "href": "2023_DV/Review/강신성_1115.html#d.-시각화20182021년도-서울의-전기에너지-사용량",
    "title": "1. 라이브러리 imports",
    "section": "### D. 시각화(2018~2021년도 서울의 전기에너지 사용량)",
    "text": "### D. 시각화(2018~2021년도 서울의 전기에너지 사용량)\n\nseoul_dict = local_dict.copy()\nseoul_dict['features'] = [l for l in local_dict['features'] if l['properties']['code'][:2] == '11']\n\n\npx.choropleth_mapbox(\n    geojson = seoul_dict,\n    featureidkey = 'properties.code',\n    data_frame = df2,\n    locations = 'code_local',\n    color='에너지사용량(TOE)/전기',\n    hover_data=['시도','지역'],\n    animation_frame='년도',   ## 해당 옵션이 추가됨\n    #---#\n    mapbox_style=\"carto-positron\",\n    range_color=[0,400000],   ## 이건 부수적임\n    center={\"lat\": 37.5665, \"lon\": 126.9780},\n    zoom=9,\n    height=500,\n    width=700,\n)"
  },
  {
    "objectID": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html",
    "href": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "",
    "text": "심슨의 역설"
  },
  {
    "objectID": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#라이브러리-imports",
    "href": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#라이브러리-imports",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#필요한-코드-비교를-위한-시각화",
    "href": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#필요한-코드-비교를-위한-시각화",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "2. 필요한 코드 | 비교를 위한 시각화",
    "text": "2. 필요한 코드 | 비교를 위한 시각화\n\nA. geom_col()\n\n- 예시1 : geom_col() 기본적인 막대 그래프\n\ndf = pd.DataFrame({'x':[0,1],'y':[40,60]})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n40\n\n\n1\n1\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'x', y = 'y'))   ## geom_bar()는 그냥 없다고 생각하자.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시2 : \\(x\\)축이 범주형인 경우\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score'))   ## 설명변수가 문자열이어도 산출해준다.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시3 : fill = 'index'예시2에서 범주별 색깔로 구분하고 싶은 경우\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score', fill = 'sex'))   ## color 옵션도 있으나, 이것은 바깥의 테두리 색상만 바꾼다.\n\nfig + bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시4 : 예시3에서 scale_fill_manual()을 이용하여 색상 변경하기\n\ndf = pd.DataFrame({'sex':['male','female'],'score':[40,60]})\ndf\n\n\n  \n    \n\n\n\n\n\n\nsex\nscore\n\n\n\n\n0\nmale\n40\n\n\n1\nfemale\n60\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nfig = ggplot(df)\nbar = geom_col(aes(x = 'sex', y = 'score', fill = 'sex'))\n\nfig + bar + scale_fill_manual(['red','blue'])\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n색상 입력에는 이름이 정해진 색상들 뿐만 아니라 plt.plot(color = option)에서의 옵션과 같이 이미 설정된 C0, C1… 또는 hex code도 입력이 가능하다."
  },
  {
    "objectID": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#b.-facet_warp-한-면을-감싸다.",
    "href": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#b.-facet_warp-한-면을-감싸다.",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "### B. facet_warp() | 한 면을 감싸다.",
    "text": "### B. facet_warp() | 한 면을 감싸다.\n- 예시1 : facet_warp()를 이용한 면분할 – 반별로 면분할\n\ndf = pd.DataFrame({'sex':['male','female','male','female'],'score':[40,60,50,20],'class':['A','A','B','B']})\ndf\n\n\n\n\n\n\n\n\nsex\nscore\nclass\n\n\n\n\n0\nmale\n40\nA\n\n\n1\nfemale\n60\nA\n\n\n2\nmale\n50\nB\n\n\n3\nfemale\n20\nB\n\n\n\n\n\n\n\n\nggplot(df) + geom_col(aes(x='sex',y='score',fill='sex')) + scale_fill_manual(['red','blue']) + facet_wrap('class')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nclass별로 그래프의 면을 분할해서 표시\n\n\nggplot(df) + geom_col(aes(x = 'sex', y = 'score', fill = 'sex')) + scale_fill_manual(['red','blue'])  ## 지정해주지 않을 경우 기본적으로 합산하여 지정된다.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n- 예시2 : 성별로 면분할\n\ndf = pd.DataFrame({'sex':['male','female','male','female'],'score':[40,60,50,20],'class':['A','A','B','B']})\ndf\n\n\n  \n    \n\n\n\n\n\n\nsex\nscore\nclass\n\n\n\n\n0\nmale\n40\nA\n\n\n1\nfemale\n60\nA\n\n\n2\nmale\n50\nB\n\n\n3\nfemale\n20\nB\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nggplot(df) + geom_col(aes(x='class',y='score',fill='sex')) + facet_wrap('sex')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n이 경우 'sex'로 color를 구분했으므로 면마다 다른 색의 그래프만이 나온다."
  },
  {
    "objectID": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#심슨의-역설",
    "href": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#심슨의-역설",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "4. 심슨의 역설",
    "text": "4. 심슨의 역설\n- 버클리 대학교의 입학 데이터에서 gender bias가 존재한다는 주장이 있었다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\nA. 시각화 1 : 전체 합격률 시각화 – pandas 초보\n\n- 단순무식하게 query만 이용해서 그룹화\n\ndf.query('gender == \"female\" and result == \"pass\"')['count'].sum()\n\n772\n\n\n\n여성 지원자 중 합격한 사람의 수\n\n\ndf.query('gender == \"female\"')['count'].sum()\n\n1835\n\n\n\n총 여성 지원자 수\n\n\n(df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum(),\n df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum())\n\n(0.420708446866485, 0.5202526941657376)\n\n\n\ntidydata_ = pd.DataFrame({'female' : [df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum()],\n                          'male' : [df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum()]})\n\ntidydata_\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\n0\n0.420708\n0.520253\n\n\n\n\n\n\n\n\n이렇게 하면 시각화 못해요…\n\n\ntidydata = pd.DataFrame({'sex' : ['male', 'female'],\n                         'rate' : [df.query('gender == \"female\" and result == \"pass\"')['count'].sum()/df.query('gender == \"female\"')['count'].sum(),\n                                   df.query('gender == \"male\" and result == \"pass\"')['count'].sum()/df.query('gender == \"male\"')['count'].sum()]})\n\ntidydata\n\n\n\n\n\n\n\n\nsex\nrate\n\n\n\n\n0\nmale\n0.420708\n\n\n1\nfemale\n0.520253"
  },
  {
    "objectID": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#b.-시각화-1-전체-합격률-시각화-pandas-고수",
    "href": "2023_DV/Review/A8. 집단 간 비교_심슨의 역설.html#b.-시각화-1-전체-합격률-시각화-pandas-고수",
    "title": "집단 간 비교 : 심슨의 역설",
    "section": "### B. 시각화 1 : 전체 합격률 시각화 – pandas 고수",
    "text": "### B. 시각화 1 : 전체 합격률 시각화 – pandas 고수\ndf.pivot_table(index = row, columns = col, valuse = value_col, aggfunc = func)\n- 피벗 테이블을 만든다. (두 범주형 자료들을 나누는 것)\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n1063\n772\n\n\nmale\n1291\n1400\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count')    ## 집계함수의 디폴트 값이 mean이다\n\n\n  \n    \n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n177.166667\n128.666667\n\n\nmale\n215.166667\n233.333333\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n이 상태에서 reset_index()를 통해 자료를 쉽게 정리할 수 있다.\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\\\n.assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass']))\n## 비율까지 추가한 모습, lambda _df : _df를 통해 현재 데이터프레임까지 호출한 모습\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ngender\n\n\n\n\n\n\n\nfemale\n1063\n772\n0.420708\n\n\nmale\n1291\n1400\n0.520253\n\n\n\n\n\n\n\n\ndf.pivot_table(index='gender',columns='result',values='count',aggfunc=sum)\\\n.assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass'])).reset_index()\n\n\n\n\n\n\n\nresult\ngender\nfail\npass\nrate\n\n\n\n\n0\nfemale\n1063\n772\n0.420708\n\n\n1\nmale\n1291\n1400\n0.520253\n\n\n\n\n\n\n\n- 이제 가공된 데이터를 tidydata라고 하여 그래프를 그려보자.\n\ntidydata = df.pivot_table(index='gender',columns='result',values='count',aggfunc=sum).assign(rate = lambda _df : _df['pass']/(_df['fail'] + _df['pass'])).reset_index()\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x='gender',fill='gender',y='rate'))\nfig+bar\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n이 그래프를 보고 나온 주장 : 여성이 남성에 비해 합격률이 낮으니까 차별이 있다!!\n\n- 반박\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\n##df.pivot_table(index = ['department', 'gender'], columns = 'result', values = 'count', aggfunc = sum)\n## 위의 두 코드는 완전히 똑같은 동작을 한다. 아마도...\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ndepartment\ngender\n\n\n\n\n\n\nA\nfemale\n19\n89\n\n\nmale\n314\n511\n\n\nB\nfemale\n7\n18\n\n\nmale\n208\n352\n\n\nC\nfemale\n391\n202\n\n\nmale\n204\n121\n\n\nD\nfemale\n244\n131\n\n\nmale\n279\n138\n\n\nE\nfemale\n299\n94\n\n\nmale\n137\n54\n\n\nF\nfemale\n103\n238\n\n\nmale\n149\n224\n\n\n\n\n\n\n\n\ntemp.pass  ## pass 자체가 파이썬에서 특수하게 사용되는 조건어같은 거여서 안된다.\n\nSyntaxError: invalid syntax (2928908933.py, line 1)\n\n\n\ntemp = df.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\ntemp['fail'] + temp['pass']\n\ndepartment  gender\nA           female    108\n            male      825\nB           female     25\n            male      560\nC           female    593\n            male      325\nD           female    375\n            male      417\nE           female    393\n            male      191\nF           female    341\n            male      373\ndtype: int64\n\n\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\\\n.assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass']))\n##df.pivot_table(index = ['gender', 'department'], columns = 'result', values = 'count', aggfunc = sum).reset_index().assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass']))\n\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ndepartment\ngender\n\n\n\n\n\n\n\nA\nfemale\n19\n89\n0.824074\n\n\nmale\n314\n511\n0.619394\n\n\nB\nfemale\n7\n18\n0.720000\n\n\nmale\n208\n352\n0.628571\n\n\nC\nfemale\n391\n202\n0.340641\n\n\nmale\n204\n121\n0.372308\n\n\nD\nfemale\n244\n131\n0.349333\n\n\nmale\n279\n138\n0.330935\n\n\nE\nfemale\n299\n94\n0.239186\n\n\nmale\n137\n54\n0.282723\n\n\nF\nfemale\n103\n238\n0.697947\n\n\nmale\n149\n224\n0.600536\n\n\n\n\n\n\n\n- tidydata 완성\n\ndf.pivot_table(index=['department'],columns=['result','gender'],values='count',aggfunc=sum).stack()\\\n.assign(rate = lambda _df: _df['pass']/(_df.fail+_df['pass'])).reset_index().drop(['fail', 'pass'], axis = 1)\n\ntidydata = _\ntidydata\n\n\n\n\n\n\n\nresult\ndepartment\ngender\nrate\n\n\n\n\n0\nA\nfemale\n0.824074\n\n\n1\nA\nmale\n0.619394\n\n\n2\nB\nfemale\n0.720000\n\n\n3\nB\nmale\n0.628571\n\n\n4\nC\nfemale\n0.340641\n\n\n5\nC\nmale\n0.372308\n\n\n6\nD\nfemale\n0.349333\n\n\n7\nD\nmale\n0.330935\n\n\n8\nE\nfemale\n0.239186\n\n\n9\nE\nmale\n0.282723\n\n\n10\nF\nfemale\n0.697947\n\n\n11\nF\nmale\n0.600536\n\n\n\n\n\n\n\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x = 'gender', y = 'rate', fill = 'gender'))\n\nfig + bar + scale_fill_manual(['red', 'blue']) + facet_wrap(['department'])\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nC와 E를 제외하고는 오히려 여성의 합격 비율이 더 높은 것을 알 수 있다.\n\n\nD. 해석\n\n- 시각화 1 : 남자의 합격률이 더 높다 -&gt; 성차별이 있어보인다(?)\n- 시각화 2 : 학과별로 살펴보니 오히려 A, B, F, D의 경우 여성의 합격률이 높다.\n- 교재에서 설명한 이유 : 여성이 합격률이 낮은 학과에만 많이 지원하였기 때문.\n\ndf.pivot_table(index='department', columns='gender', values='count',aggfunc='sum')\\\n.stack().reset_index().rename({0:'count'},axis=1)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n\ntidydata = df.pivot_table(index='department', columns='gender', values='count',aggfunc='sum')\\\n.stack().reset_index().rename({0:'count'},axis=1)\n\n \nfig = ggplot(tidydata) \ncol = geom_col(aes(x='department',y='count',fill='gender'),position='dodge')\nfig+col\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\nA, B학과가 신체적 역량을 많이 요구로 하거나 하는 학과일 경우 아무래도 기피하겠지…\n은닉변수에 의해 왜곡이 될 수 있다"
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html",
    "href": "2023_DV/Review/A2. Seaborn.html",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "",
    "text": "seaborn을 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html#라이브러리-import",
    "href": "2023_DV/Review/A2. Seaborn.html#라이브러리-import",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html#seaborn과-matplotlib",
    "href": "2023_DV/Review/A2. Seaborn.html#seaborn과-matplotlib",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "2. Seaborn과 Matplotlib",
    "text": "2. Seaborn과 Matplotlib\n\nmatplotlib : 벡터 친화적\nseaborn : 데이터프레임 친화적\n\n\n분석할 데이터가 태뷸러데이터 형식인 경우가 많다.\nmatplotlib는 여전히 강력하지만, seaborn등 데이터프레임 친화적인 패키지가 우수한 경우가 많다.\n\n\nA. scatter plot\n\n\n## titanic data\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n\nsns.scatterplot(\n    df,\n    x = 'logFare',  ## 요금에 로그를 취한 값(너무 변동이 크니까)\n    y = 'Age',\n    hue = 'Sex',    ## 색상, 색조. 변수 별 색상을 나눠 표기한다.\n    style = 'Survived', style_order = [1,0],  ## Survived 여부로 마커 표시, style_order의 디폴트 값이 [0 =&gt; O,1 =&gt; X]이므로 그 순서를 변경\n    alpha = 0.6     ## 투명도 조절\n)\n\n&lt;Axes: xlabel='logFare', ylabel='Age'&gt;\n\n\n\n\n\n\nplt.hist(df.Age)\nplt.hist(df.Age[df.Survived == 1])  ## 그냥 그리면 겹쳐짐\n\nplt.show()\n\n\n\n\n- seaborn은 데이터과학에서 거의 표준적인 패키지. * 안하는 이유 * 간단한 시각화는 matplotlib가 유리 * seaborn에 대한 고급기능은 matplotlib에 대한 통찰이 있어야 가능 * plotline이 더 우수함(ggplot2) * plotly가 모든 면에서 seaborn을 압도하는 추세임\n\n\nB. seaborn의 고급기능 이해\n\nsns.scatterplot(\n    df,\n    x = 'logFare',\n    y = 'Age',\n    hue = 'Sex',\n    style = 'Survived', style_order = [1,0],\n    alpha = 0.8\n)\n\nfig = plt.gcf()\nax = plt.gca()\nax.set_title('Scattor Plot')\n\nfig.add_axes([0.6,0.2,0.25,0.25])\nax_mini = plt.gca()\n## ax_mini = fig.add_axes([0.6,0.2,0.25,0.25])과 동일\n\nax_mini.hist(df.Age)\nax_mini.hist(df.Age[df.Survived == 1])\nax_mini.set_title('Histogram')\nfig.suptitle('TITANIC')\n\nplt.show()\n\n\n\n\n\ntype(fig) ## seaborn으로 제작하였음에도 Figure의 형식을 지닌다.\n\nmatplotlib.figure.Figure"
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html#훌륭한-시각화",
    "href": "2023_DV/Review/A2. Seaborn.html#훌륭한-시각화",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "3. 훌륭한 시각화",
    "text": "3. 훌륭한 시각화"
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html#애드워드-터프티",
    "href": "2023_DV/Review/A2. Seaborn.html#애드워드-터프티",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "### 애드워드 터프티",
    "text": "### 애드워드 터프티\n- 데이터 시각화계의 거장\n\n엄격한 미니멀리즘\n최소한의 잉크로 많은 정보를 전달할 수 있다면 그것이 바로 좋은 그래프이다.\n\n\n너무 구시대적인 사고일 수도 있음. 적합할 수도 있고."
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html#찰스미나드의-도표",
    "href": "2023_DV/Review/A2. Seaborn.html#찰스미나드의-도표",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "### 찰스미나드의 도표",
    "text": "### 찰스미나드의 도표\n\n\n터프티도 극찬하고 ~중국이 놀라고, 일본이 경악하고…~\n\n\n군대의 크기, 2차원 평면상의 위치, 군대의 이동방향, 모스크바에서 퇴각하는 동안의 여러 날짜와 그 시점에서의 온도 -&gt; 6차원의 변수를 한 평면상에 표현\n\n미나드는 여러 그림을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함."
  },
  {
    "objectID": "2023_DV/Review/A2. Seaborn.html#미나드처럼-그리는-게-왜-어려운가",
    "href": "2023_DV/Review/A2. Seaborn.html#미나드처럼-그리는-게-왜-어려운가",
    "title": "Seaborn | 데이터프레임 친화적 패키지",
    "section": "4. 미나드처럼 그리는 게 왜 어려운가?",
    "text": "4. 미나드처럼 그리는 게 왜 어려운가?\n- 몸무게, 키, 성별, 국적을 나타내는 자료\n\ndf1=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male1.csv')   ## 남성의 키\ndf2=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male2.csv')   ## 남성의 몸무게\ndf3=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/female.csv')  ## 여성의 키와 몸무게\ndf4=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/foreign.csv') ## 외국인의 키와 몸무게, 성별, 국적\n\n\n_df = pd.concat([pd.concat([df1,df2],axis=1)\\\n                 .assign(g='m'),df3.assign(g='f')])\ndf = pd.concat([_df.assign(g2='korea'),df4.assign(g2='foreign')])\\\n.reset_index(drop=True)\ndf\n\n\n  \n    \n\n\n\n\n\n\nw\nh\ng\ng2\n\n\n\n\n0\n72.788217\n183.486773\nm\nkorea\n\n\n1\n66.606430\n173.599877\nm\nkorea\n\n\n2\n69.806324\n173.237903\nm\nkorea\n\n\n3\n67.449439\n173.223805\nm\nkorea\n\n\n4\n70.463183\n174.931946\nm\nkorea\n\n\n...\n...\n...\n...\n...\n\n\n1525\n78.154632\n188.324350\nm\nforeign\n\n\n1526\n74.754308\n183.017979\nf\nforeign\n\n\n1527\n91.196208\n190.100456\nm\nforeign\n\n\n1528\n87.770394\n187.987255\nm\nforeign\n\n\n1529\n88.021995\n193.456798\nm\nforeign\n\n\n\n\n\n1530 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nsns.scatterplot(\n    data=df,\n    x='w',\n    y='h',\n    hue='g',    ## group 1 : gender\n    style='g2',  ## group 2 : region\n    alpha=0.6\n)\n\n&lt;Axes: xlabel='w', ylabel='h'&gt;\n\n\n\n\n\n\n그래프를 이해하기 어려운 것은 아니지만, 아무래도 난잡한 것은 사실이다.\n\n-어려운 점 :\n\n센스 부족 : 센스가 없어서 그룹 구분할 생각을 못함\n개념 부족 : 타이디데이터( =tidy dataframe, long form dataframe) 형태로 데이터를 정리할 생각을 못함.\n코딩 못함 : 타이디테이터로 데이터를 변형하는 코드를 모름."
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html",
    "title": "Tidydata 심화 실습",
    "section": "",
    "text": "Tidydata를 만드는 방법의 모든(?) 것"
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html#라이브러리-imports",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html#라이브러리-imports",
    "title": "Tidydata 심화 실습",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html#사전학습",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html#사전학습",
    "title": "Tidydata 심화 실습",
    "section": "2. 사전학습",
    "text": "2. 사전학습\n\npd.concat()\n\n\ndf1 = pd.DataFrame({'A':[1,2,3],'B':[2,3,4]})\ndf2 = pd.DataFrame({'A':[-1,-2,-3],'B':[-2,-3,-4]})\n\n\ndisplay(\"df1\", df1)\ndisplay(\"df2\", df2)\n\n'df1'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n'df2'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n-1\n-2\n\n\n1\n-2\n-3\n\n\n2\n-3\n-4\n\n\n\n\n\n\n\n\n위 두 개의 데이터프레임을 합치고 싶다면?\n\n\ndisplay(pd.concat([df1, df2], axis = 1))\ndisplay(pd.concat([df1, df2], axis = 0).reset_index(drop = True))\n\n\n\n\n\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n1\n2\n-1\n-2\n\n\n1\n2\n3\n-2\n-3\n\n\n2\n3\n4\n-3\n-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n3\n-1\n-2\n\n\n4\n-2\n-3\n\n\n5\n-3\n-4"
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html#df.merge",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html#df.merge",
    "title": "Tidydata 심화 실습",
    "section": "### df.merge()",
    "text": "### df.merge()\n- 사이즈가 맞지 않는 두 데이터프레임의 정보를 결합\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'department':['A','B'], 'total':[3,4]})\n\ndisplay('big', big) ## title을 달아주고 아래 산출\ndisplay('small', small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ntotal\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n\ndisplay(big.merge(small))  ## 큰 거를 기준으로 작은거 병합\ndisplay(small.merge(big))  ## 작은거를 기준으로 큰거를 병합\n## 사실 둘 다 비슷하긴 함\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ntotal\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartment\ntotal\ngender\ncount\n\n\n\n\n0\nA\n3\nmale\n1\n\n\n1\nA\n3\nfemale\n2\n\n\n2\nB\n4\nmale\n3\n\n\n3\nB\n4\nfemale\n1\n\n\n\n\n\n\n\n\ndf.applymap()\n\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':np.random.rand(3), 'B':np.random.rand(3)})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.817682\n0.619777\n\n\n1\n0.049532\n0.122541\n\n\n2\n0.838686\n0.117128\n\n\n\n\n\n\n\n\n0.5보다 크면 yes, 0.5보다 작으면 no로 바꾸고 싶다면…\n\n\ndf.applymap(lambda x : 'yes' if x &gt; 0.5 else 'no')\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nyes\nyes\n\n\n1\nno\nno\n\n\n2\nyes\nno"
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html#d.-df.astype",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html#d.-df.astype",
    "title": "Tidydata 심화 실습",
    "section": "### D. df.astype()",
    "text": "### D. df.astype()\n- 데이터프레임이나 시리즈의 형식을 일괄적으로 변경\n\ndf = pd.DataFrame({'A':[0,1,2],'B':[4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0\n4\n\n\n1\n1\n5\n\n\n2\n2\n6\n\n\n\n\n\n\n\n\ndf.astype(float)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.0\n4.0\n\n\n1\n1.0\n5.0\n\n\n2\n2.0\n6.0\n\n\n\n\n\n\n\n\nE. 데이터프레임 열의 형식\n\n- info()에서의 형식, object는 일괄적으로 문자형이라는 것을 의미하는 게 아님.\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':['1','2','0','1',2], 'B':['2','3','0','0',0]})  ## integer가 포함되어 있다.\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   A       5 non-null      object\n 1   B       5 non-null      object\ndtypes: object(2)\nmemory usage: 208.0+ bytes\n\n\n- column의 이름이 이상하게 들어가 있는 경우도 있음.\n\ndf = pd.DataFrame({('A',''):[0,0,0], ('B',''):[1,1,1]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n\n\n\n\n\n0\n0\n1\n\n\n1\n0\n1\n\n\n2\n0\n1\n\n\n\n\n\n\n\n\ndf['A']\ndf[('A', '')]\n\n0    0\n1    0\n2    0\nName: (A, ), dtype: int64\n\n\n\n놀랍게도 둘은 같다. 인덱스를 다시 설정해주는 편이 정신건강에 이로움\n\n\ndf.columns ## 쓸모없는 멀티인덱스\n\nMultiIndex([('A', ''),\n            ('B', '')],\n           )"
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html#실습-에너지-사용량-시각화",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html#실습-에너지-사용량-시각화",
    "title": "Tidydata 심화 실습",
    "section": "3. 실습 : 에너지 사용량 시각화",
    "text": "3. 실습 : 에너지 사용량 시각화\n- 문제\n\npd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv')\n\n\n\n\n\n\n\n\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n종로구\n17,851\n9,204,140\n63,492\n76,653\n799\n\n\n1\n중구\n10,383\n10,078,848\n79,223\n68,210\n497\n\n\n2\n용산구\n17,138\n10,756,612\n51,229\n79,805\n11,128\n\n\n3\n성동구\n13,980\n11,804,313\n59,832\n99,986\n0\n\n\n4\n광진구\n21,556\n12,272,738\n68,756\n123,447\n0\n\n\n5\n동대문구\n21,794\n12,664,554\n65,913\n111,420\n0\n\n\n6\n중랑구\n23,950\n15,182,802\n59,370\n109,284\n7,442\n\n\n7\n성북구\n27,112\n15,938,807\n77,007\n148,376\n0\n\n\n8\n강북구\n23,334\n9,458,987\n47,731\n100,045\n0\n\n\n9\n도봉구\n13,168\n10,644,704\n44,985\n90,379\n5,268\n\n\n10\n노원구\n9,704\n17,197,086\n77,010\n94,340\n50,859\n\n\n11\n은평구\n25,200\n14,735,131\n75,914\n130,159\n14,370\n\n\n12\n서대문구\n17,651\n12,559,425\n65,164\n111,542\n6,330\n\n\n13\n마포구\n18,844\n15,024,186\n92,453\n114,931\n20,148\n\n\n14\n양천구\n14,690\n15,428,339\n70,721\n82,857\n49,258\n\n\n15\n강서구\n20,446\n20,641,866\n86,809\n128,786\n35,896\n\n\n16\n구로구\n17,204\n13,509,894\n59,916\n120,457\n2,963\n\n\n17\n금천구\n12,135\n7,420,441\n34,791\n69,814\n732\n\n\n18\n영등포구\n18,133\n14,914,027\n87,480\n114,238\n13,531\n\n\n19\n동작구\n20,102\n13,612,946\n66,811\n132,285\n899\n\n\n20\n관악구\n26,460\n14,997,859\n85,416\n158,543\n0\n\n\n21\n서초구\n12,856\n21,560,285\n135,491\n121,437\n38,866\n\n\n22\n강남구\n16,129\n29,961,585\n180,121\n149,045\n83,459\n\n\n23\n송파구\n19,331\n26,573,343\n139,117\n143,601\n71,954\n\n\n24\n강동구\n16,636\n15,048,315\n70,341\n121,931\n11,921\n\n\n\n\n\n\n\n에너지 사용량은 2018년부터 2021년까지의 기간 동안 서울, 부산 등 여러 지역에 대해 정리되어 있으며, 아래 주소 형식으로 저장되어 있다.\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2020.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2021.csv\n...\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2018.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2019.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2020.csv\nhttps://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2021.csv\n\n아래의 url, prov를 참고하여 모든 자료를 불러온 뒤 pd.concat()을 이용하여 하나의 df로 합쳐라.\n\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\n\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\n\n1. 풀이\n\nurl.format('Seoul2018')  ## 이런 식으로 하나하나 리스트로 지정해줘야 함\n\n'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv'\n\n\n\n[url.format(region + str(year)) for year in range(2018,2022) for region in prov]\n\n['https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2018.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2019.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2020.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Seoul2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Busan2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daegu2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Incheon2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gwangju2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Daejeon2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Ulsan2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Sejongsi2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeonggi-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gangwon-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongbuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Chungcheongnam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollabuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeollanam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangbuk-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Gyeongsangnam-do2021.csv',\n 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/Jeju-do2021.csv']\n\n\n\ndf = pd.concat([pd.read_csv(url.format(region + str(year))).assign(년도 = year, 시도 = region) for year in range(2018,2022) for region in prov], axis = 0)\n\n\ndf\n\n\n\n\n\n\n\n\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n년도\n시도\n\n\n\n\n0\n종로구\n17,929\n9,141,777\n64,818\n82,015\n111\n2018\nSeoul\n\n\n1\n중구\n10,598\n10,056,233\n81,672\n75,260\n563\n2018\nSeoul\n\n\n2\n용산구\n17,201\n10,639,652\n52,659\n85,220\n12,043\n2018\nSeoul\n\n\n3\n성동구\n14,180\n11,631,770\n60,559\n107,416\n0\n2018\nSeoul\n\n\n4\n광진구\n21,520\n12,054,796\n70,609\n130,308\n0\n2018\nSeoul\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19\n함양군\n12,505\n1,509,149\n6,328\n3,164\n0\n2021\nGyeongsangnam-do\n\n\n20\n거창군\n14,607\n2,322,093\n10,404\n8,850\n0\n2021\nGyeongsangnam-do\n\n\n21\n합천군\n16,039\n1,612,734\n7,587\n0\n0\n2021\nGyeongsangnam-do\n\n\n0\n제주시\n67,053\n20,275,738\n103,217\n25,689\n0\n2021\nJeju-do\n\n\n1\n서귀포시\n35,230\n7,512,206\n37,884\n2,641\n0\n2021\nJeju-do\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n문자열을 pd.read_csv()에 넣어준 후, 컴프리헨션 된 리스트를 행 방향으로 concat했다. 또한 연도와 시도의 정보를 유지시켰다.\n\n\n의미상 숫자형이지만, 문자형으로 입력이 된 자료를 모두 전처리하라.\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1000 entries, 0 to 1\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   지역                1000 non-null   object\n 1   건물동수              1000 non-null   object\n 2   연면적               1000 non-null   object\n 3   에너지사용량(TOE)/전기    1000 non-null   object\n 4   에너지사용량(TOE)/도시가스  1000 non-null   object\n 5   에너지사용량(TOE)/지역난방  1000 non-null   object\n 6   년도                1000 non-null   int64 \n 7   시도                1000 non-null   object\ndtypes: int64(1), object(7)\nmemory usage: 70.3+ KB\n\n\n\n지역, 시도의 경우 문자형으로 입력된 게 맞음. 하지만 나머지는 다 숫자형이 되어야 한다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : x.replace(',',''))\n\nAttributeError: 'int' object has no attribute 'replace'\n\n\n\n문자형이 아닌 숫자형인 녀석이 몇몇 있나보다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype \n---  ------            --------------  ----- \n 0   지역                1000 non-null   object\n 1   년도                1000 non-null   int64 \n 2   시도                1000 non-null   object\n 3   건물동수              1000 non-null   int32 \n 4   연면적               1000 non-null   int32 \n 5   에너지사용량(TOE)/전기    1000 non-null   int32 \n 6   에너지사용량(TOE)/도시가스  1000 non-null   int32 \n 7   에너지사용량(TOE)/지역난방  1000 non-null   int32 \ndtypes: int32(5), int64(1), object(2)\nmemory usage: 43.1+ KB\n\n\n\n자료의 형식이 알맞게 설정되었다.\n\n\n년도에는 쉼표가 없으므로 integer로 바꿈(여기선 애초에 숫자형으로 들어가긴 함)\n년도와 시도, 지역을 배제(문자형)\n혹시라도 integer인 녀석들을 string으로 변경 후 문자열 바꾸는 메소드를 통해 ,를 제거, 인덱스 초기화\n열의 이름을 아래와 같이 바꿔라.\n\n\nname_dict = {\n    '년도': 'Year',\n    '시도': 'Prov',\n    '지역': 'Reg',\n    '건물동수': 'BldgCount',\n    '연면적': 'Area',\n    '에너지사용량(TOE)/전기': 'Elec',\n    '에너지사용량(TOE)/도시가스': 'Gas',\n    '에너지사용량(TOE)/지역난방': 'Heat'\n}\n\n\n딕셔너리가 주어졌으므로 그냥 바꾸면 된다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)  ## axis를 꼭 지정해주자.\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n아래와 같은 그림을 시각화하라. \n\n\n가로축이 Year, 세로축이 LogEnergyUse(에너지 사용량에 log를 취한 것)이고, Region으로 면분할했으며, Type으로 라인의 색상을 구분했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n사용해야 할 것은 Prov, Year, Elec Gas Heat.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nlevel_2\n0\n\n\n\n\n0\nSeoul\n2018\nElec\n64818\n\n\n1\nSeoul\n2018\nGas\n82015\n\n\n2\nSeoul\n2018\nHeat\n111\n\n\n3\nSeoul\n2018\nElec\n81672\n\n\n4\nSeoul\n2018\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n2021\nGas\n25689\n\n\n2996\nJeju-do\n2021\nHeat\n0\n\n\n2997\nJeju-do\n2021\nElec\n37884\n\n\n2998\nJeju-do\n2021\nGas\n2641\n\n\n2999\nJeju-do\n2021\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n사용할 두 개의 열을 골라주고, 에너지 관련 세 개 열을 long data로 변환했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nSeoul\n2018\nElec\n64818\n\n\n1\nSeoul\n2018\nGas\n82015\n\n\n2\nSeoul\n2018\nHeat\n111\n\n\n3\nSeoul\n2018\nElec\n81672\n\n\n4\nSeoul\n2018\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n2021\nGas\n25689\n\n\n2996\nJeju-do\n2021\nHeat\n0\n\n\n2997\nJeju-do\n2021\nElec\n37884\n\n\n2998\nJeju-do\n2021\nGas\n2641\n\n\n2999\nJeju-do\n2021\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n이름을 바꾸고…\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n\n\n1\nBusan\n2018\nGas\n708240\n\n\n2\nBusan\n2018\nHeat\n23694\n\n\n3\nBusan\n2019\nElec\n602980\n\n\n4\nBusan\n2019\nGas\n675882\n\n\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n\n\n200\nUlsan\n2020\nHeat\n0\n\n\n201\nUlsan\n2021\nElec\n196412\n\n\n202\nUlsan\n2021\nGas\n312276\n\n\n203\nUlsan\n2021\nHeat\n0\n\n\n\n\n204 rows × 4 columns\n\n\n\n\n지역별로 중복되는 것들을 더했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.assign(LogEnergyUse = lambda _df : _df.EnergyUse.apply(np.log))\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\nLogEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n13.326971\n\n\n1\nBusan\n2018\nGas\n708240\n13.470538\n\n\n2\nBusan\n2018\nHeat\n23694\n10.072977\n\n\n3\nBusan\n2019\nElec\n602980\n13.309639\n\n\n4\nBusan\n2019\nGas\n675882\n13.423774\n\n\n...\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n12.634264\n\n\n200\nUlsan\n2020\nHeat\n0\n-inf\n\n\n201\nUlsan\n2021\nElec\n196412\n12.187970\n\n\n202\nUlsan\n2021\nGas\n312276\n12.651643\n\n\n203\nUlsan\n2021\nHeat\n0\n-inf\n\n\n\n\n204 rows × 5 columns\n\n\n\n\n그리고 로그를 취해준 것을 새로운 열로 할당해줬다. 이정도면 타이디데이터라 할 만 하다.\n\n\ntidydata = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.set_index(['Prov', 'Year']).loc[:, ['Elec', 'Gas', 'Heat']].stack().reset_index()\\\n.rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.assign(LogEnergyUse = lambda _df : _df.EnergyUse.apply(np.log))\n\n시각화\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = 'Year', y = 'LogEnergyUse', color = 'Type', linetype = 'Type'))\n\nfig + line + facet_wrap('Prov', scales = 'free')  ## 해당 옵션은 그래프마다 스케일을 따로 적용시킨다.\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n뭔가 시각화는 되었는데, 마음에 들지 않는다.\n\n\n## matplotlib로 해당 개체를 이전\nfig = (fig + line + facet_wrap('Prov', scales = 'free')).draw()\nfig\n\n\n\n\n\nfig.set_size_inches(10, 6)\nfig.set_dpi(150)\nfig\n\n\n\n\n\nmatplotlib에서의 메소드를 쉽게 적용시킬 수 있다.\n\n\nProv별로 총 에너지사용량이 많은 상위5개의 Reg을 찾고 아래와 같이 시각화 하라. \n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nReg\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\nSeoul\n64818\n82015\n111\n\n\n1\n중구\nSeoul\n81672\n75260\n563\n\n\n2\n용산구\nSeoul\n52659\n85220\n12043\n\n\n3\n성동구\nSeoul\n60559\n107416\n0\n\n\n4\n광진구\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n거창군\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n합천군\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n제주시\nJeju-do\n103217\n25689\n0\n\n\n999\n서귀포시\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n필요없는 열을 없앤다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nProv\nReg\nType\nEnergyUse\n\n\n\n\n0\nSeoul\n종로구\nElec\n64818\n\n\n1\nSeoul\n종로구\nGas\n82015\n\n\n2\nSeoul\n종로구\nHeat\n111\n\n\n3\nSeoul\n중구\nElec\n81672\n\n\n4\nSeoul\n중구\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\nJeju-do\n제주시\nGas\n25689\n\n\n2996\nJeju-do\n제주시\nHeat\n0\n\n\n2997\nJeju-do\n서귀포시\nElec\n37884\n\n\n2998\nJeju-do\n서귀포시\nGas\n2641\n\n\n2999\nJeju-do\n서귀포시\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\n지역과 구, 타입과 에너지를 표기했다. 이름도 적절히 설정해줬다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Reg'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nReg\nEnergyUse\n\n\n\n\n0\nBusan\n강서구\n200386\n\n\n1\nBusan\n금정구\n451212\n\n\n2\nBusan\n기장군\n287926\n\n\n3\nBusan\n남구\n491030\n\n\n4\nBusan\n동구\n156302\n\n\n...\n...\n...\n...\n\n\n245\nUlsan\n남구\n607820\n\n\n246\nUlsan\n동구\n281094\n\n\n247\nUlsan\n북구\n334844\n\n\n248\nUlsan\n울주군\n394217\n\n\n249\nUlsan\n중구\n395158\n\n\n\n\n250 rows × 3 columns\n\n\n\n\n구역별로 에너지 사용량을 합쳐버렸다.\n\n\ng = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Year', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Prov', 'Reg']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Reg'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\\\n.groupby(by = 'Prov')\n\npd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\n\n\n\n\n\n\n\n\nProv\nReg\nEnergyUse\n\n\n\n\n0\nBusan\n부산진구\n690344\n\n\n1\nBusan\n해운대구\n689901\n\n\n2\nBusan\n사하구\n522150\n\n\n3\nBusan\n북구\n493913\n\n\n4\nBusan\n남구\n491030\n\n\n...\n...\n...\n...\n\n\n0\nUlsan\n남구\n607820\n\n\n1\nUlsan\n중구\n395158\n\n\n2\nUlsan\n울주군\n394217\n\n\n3\nUlsan\n북구\n334844\n\n\n4\nUlsan\n동구\n281094\n\n\n\n\n78 rows × 3 columns\n\n\n\n\n구간마다 순위를 정해주기 위해 groupby 함수를 사용, sub-dataframe으로 쪼갠 후에 각각 sort_values() 해주었다.\n\n\npd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\\\n.reset_index().rename({'index' : 'rank'}, axis = 1)\n\n\n\n\n\n\n\n\nrank\nProv\nReg\nEnergyUse\n\n\n\n\n0\n0\nBusan\n부산진구\n690344\n\n\n1\n1\nBusan\n해운대구\n689901\n\n\n2\n2\nBusan\n사하구\n522150\n\n\n3\n3\nBusan\n북구\n493913\n\n\n4\n4\nBusan\n남구\n491030\n\n\n...\n...\n...\n...\n...\n\n\n73\n0\nUlsan\n남구\n607820\n\n\n74\n1\nUlsan\n중구\n395158\n\n\n75\n2\nUlsan\n울주군\n394217\n\n\n76\n3\nUlsan\n북구\n334844\n\n\n77\n4\nUlsan\n동구\n281094\n\n\n\n\n78 rows × 4 columns\n\n\n\n\n인덱스는 랭크와 동일하므로 따로 남겨둔다.\n\n\ntidydata = pd.concat([j.sort_values('EnergyUse', ascending = False).reset_index(drop = True).iloc[:5] for i, j in g], axis = 0)\\\n.reset_index().rename({'index' : 'rank'}, axis = 1)\n\nfig = ggplot(tidydata)\nbar = geom_col(aes(x = 'rank', y = 'EnergyUse', fill = 'Prov'))\n\nfig + bar + facet_wrap('Prov')\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n정보는 모두 포함하나, 짜임새가 없으므로 matplotlib로 이전\n\n\nfig = (fig + bar + facet_wrap('Prov')).draw()\n\n\nfig.set_size_inches(12, 6)\nfig.set_dpi(150)\nfig\n\n\n\n\n\n완료\n\n\n(Prov,Year)별 전기에너지 사용량 비율을 구하고 아래와 같이 시각화 하라. + 제주를 제외한 지역으로 한정하고 시각화하라.\n\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nBldgCount\nArea\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n17929\n9141777\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n10598\n10056233\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n14180\n11631770\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n21520\n12054796\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n12505\n1509149\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n14607\n2322093\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n16039\n1612734\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n67053\n20275738\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n35230\n7512206\n37884\n2641\n0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n사용해야 할 것\n\nx = ‘Year’, y = ‘ElecRate’, facet_wrap(’Prov”)\nElec, Gas, Heat\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nYear\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n2018\nSeoul\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n2021\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n2021\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n2021\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n2021\nJeju-do\n103217\n25689\n0\n\n\n999\n2021\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n필요없는 걸 없애고…\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\n\n\n\n\n\n\n\n\nYear\nProv\nType\nEnergyUse\n\n\n\n\n0\n2018\nSeoul\nElec\n64818\n\n\n1\n2018\nSeoul\nGas\n82015\n\n\n2\n2018\nSeoul\nHeat\n111\n\n\n3\n2018\nSeoul\nElec\n81672\n\n\n4\n2018\nSeoul\nGas\n75260\n\n\n...\n...\n...\n...\n...\n\n\n2995\n2021\nJeju-do\nGas\n25689\n\n\n2996\n2021\nJeju-do\nHeat\n0\n\n\n2997\n2021\nJeju-do\nElec\n37884\n\n\n2998\n2021\nJeju-do\nGas\n2641\n\n\n2999\n2021\nJeju-do\nHeat\n0\n\n\n\n\n3000 rows × 4 columns\n\n\n\n\nlong data로 변환했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\n\n\n\n\n\n\n\n\nType\nElec\nGas\nHeat\n\n\nYear\nProv\n\n\n\n\n\n\n\n2018\nBusan\n613522\n708240\n23694\n\n\nChungcheongbuk-do\n361490\n288927\n55002\n\n\nChungcheongnam-do\n456260\n420315\n24286\n\n\nDaegu\n457556\n599115\n77399\n\n\nDaejeon\n309660\n379571\n51341\n\n\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n403399\n4321\n\n\nJeollanam-do\n338032\n281895\n9012\n\n\nSejongsi\n70915\n30533\n61404\n\n\nSeoul\n3486022\n3617731\n546491\n\n\nUlsan\n196412\n312276\n0\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n지역(Prov)과 연도(Year)가 중복되는 값들을 각각 더해줘서 정리했다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\n\n\n\n\n\n\n\n\nType\nElec\nGas\nHeat\nEnergyUse\n\n\nYear\nProv\n\n\n\n\n\n\n\n\n2018\nBusan\n613522\n708240\n23694\n1345456\n\n\nChungcheongbuk-do\n361490\n288927\n55002\n705419\n\n\nChungcheongnam-do\n456260\n420315\n24286\n900861\n\n\nDaegu\n457556\n599115\n77399\n1134070\n\n\nDaejeon\n309660\n379571\n51341\n740572\n\n\n...\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n403399\n4321\n764778\n\n\nJeollanam-do\n338032\n281895\n9012\n628939\n\n\nSejongsi\n70915\n30533\n61404\n162852\n\n\nSeoul\n3486022\n3617731\n546491\n7650244\n\n\nUlsan\n196412\n312276\n0\n508688\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n총 에너지 사용량 중 전기 에너지만을 구해야 하니 먼저 총 에너지를 구해준다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\n\n\n\n\n\n\n\n\nType\nElec\nEnergyUse\nElecRate\n\n\nYear\nProv\n\n\n\n\n\n\n\n2018\nBusan\n613522\n1345456\n0.455996\n\n\nChungcheongbuk-do\n361490\n705419\n0.512447\n\n\nChungcheongnam-do\n456260\n900861\n0.506471\n\n\nDaegu\n457556\n1134070\n0.403464\n\n\nDaejeon\n309660\n740572\n0.418136\n\n\n...\n...\n...\n...\n...\n\n\n2021\nJeollabuk-do\n357058\n764778\n0.466878\n\n\nJeollanam-do\n338032\n628939\n0.537464\n\n\nSejongsi\n70915\n162852\n0.435457\n\n\nSeoul\n3486022\n7650244\n0.455675\n\n\nUlsan\n196412\n508688\n0.386115\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n필요없는 것을 없애고 비율을 넣어줬다. 이제 필요한 것은 비율 뿐이다.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\\\n.drop(['Elec', 'EnergyUse'], axis = 1).reset_index()\n\n\n\n\n\n\n\nType\nYear\nProv\nElecRate\n\n\n\n\n0\n2018\nBusan\n0.455996\n\n\n1\n2018\nChungcheongbuk-do\n0.512447\n\n\n2\n2018\nChungcheongnam-do\n0.506471\n\n\n3\n2018\nDaegu\n0.403464\n\n\n4\n2018\nDaejeon\n0.418136\n\n\n...\n...\n...\n...\n\n\n63\n2021\nJeollabuk-do\n0.466878\n\n\n64\n2021\nJeollanam-do\n0.537464\n\n\n65\n2021\nSejongsi\n0.435457\n\n\n66\n2021\nSeoul\n0.455675\n\n\n67\n2021\nUlsan\n0.386115\n\n\n\n\n68 rows × 3 columns\n\n\n\n\n타이디데이터 같다.\n\n\ntidydata = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1)\\\n.drop(['Reg', 'BldgCount', 'Area'], axis = 1)\\\n.set_index(['Year', 'Prov']).stack().reset_index().rename({'level_2' : 'Type', 0 : 'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Year', 'Prov'], columns = 'Type', values = 'EnergyUse', aggfunc = 'sum')\\\n.assign(EnergyUse = lambda _df : _df.Elec + _df.Gas + _df.Heat)\\\n.drop(['Gas', 'Heat'], axis = 1).assign(ElecRate = lambda _df : _df.Elec / _df.EnergyUse)\\\n.drop(['Elec', 'EnergyUse'], axis = 1).reset_index()\n\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = 'Year', y = 'ElecRate', color = 'Prov'), linetype = 'dashed')\npoint = geom_point(aes(x = 'Year', y = 'ElecRate', color = 'Prov', shape = 'Prov'))\n\nfig + line + point\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 16 rows containing missing values.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\guides\\guides.py:259: PlotnineWarning: geom_point legend : Removed 4 rows containing missing values.\n\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n나름 괜찮지만 그래도 matplotlib에 데려와보자.\n\n\nfig_ = (fig + line + point).draw()\nfig_.set_dpi(150)\nfig_\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\layer.py:364: PlotnineWarning: geom_point : Removed 16 rows containing missing values.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\mizani\\palettes.py:706: UserWarning: Palette can return a maximum of 13 values. 17 values requested.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\guides\\guides.py:259: PlotnineWarning: geom_point legend : Removed 4 rows containing missing values.\n\n\n\n\n\n\n~와 나 너무 잘하는 거 아님?~"
  },
  {
    "objectID": "2023_DV/Review/B0. tidydata 심화실습.html#pd.merge의-이용",
    "href": "2023_DV/Review/B0. tidydata 심화실습.html#pd.merge의-이용",
    "title": "Tidydata 심화 실습",
    "section": "4. pd.merge()의 이용",
    "text": "4. pd.merge()의 이용\n\n그냥 뇌정지 올 것 같아도 일단 tidydata로 변환하고 시작하자!!\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\n\n\n\n\n\n\n\n\nReg\nYear\nProv\nElec\nGas\nHeat\n\n\n\n\n0\n종로구\n2018\nSeoul\n64818\n82015\n111\n\n\n1\n중구\n2018\nSeoul\n81672\n75260\n563\n\n\n2\n용산구\n2018\nSeoul\n52659\n85220\n12043\n\n\n3\n성동구\n2018\nSeoul\n60559\n107416\n0\n\n\n4\n광진구\n2018\nSeoul\n70609\n130308\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n함양군\n2021\nGyeongsangnam-do\n6328\n3164\n0\n\n\n996\n거창군\n2021\nGyeongsangnam-do\n10404\n8850\n0\n\n\n997\n합천군\n2021\nGyeongsangnam-do\n7587\n0\n0\n\n\n998\n제주시\n2021\nJeju-do\n103217\n25689\n0\n\n\n999\n서귀포시\n2021\nJeju-do\n37884\n2641\n0\n\n\n\n\n1000 rows × 6 columns\n\n\n\n\n이런 데이터가 있으면… 일단 value 세 개인 Elec, Gas, Heat를 녹여야 함.\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nEnergyUse\n\n\n\n\n0\nBusan\n2018\n1345456\n\n\n1\nBusan\n2019\n1301422\n\n\n2\nBusan\n2020\n1314100\n\n\n3\nBusan\n2021\n1951909\n\n\n4\nChungcheongbuk-do\n2018\n705419\n\n\n...\n...\n...\n...\n\n\n63\nSeoul\n2021\n7650244\n\n\n64\nUlsan\n2018\n512512\n\n\n65\nUlsan\n2019\n491191\n\n\n66\nUlsan\n2020\n500742\n\n\n67\nUlsan\n2021\n508688\n\n\n\n\n68 rows × 3 columns\n\n\n\n\nwidedata로 만들었음. 일단 해!\n\n\ndf.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\n\n\n\n\n\n\nProv\nYear\nType\nEnergyUse\n\n\n\n\n0\nBusan\n2018\nElec\n613522\n\n\n1\nBusan\n2018\nGas\n708240\n\n\n2\nBusan\n2018\nHeat\n23694\n\n\n3\nBusan\n2019\nElec\n602980\n\n\n4\nBusan\n2019\nGas\n675882\n\n\n...\n...\n...\n...\n...\n\n\n199\nUlsan\n2020\nGas\n306896\n\n\n200\nUlsan\n2020\nHeat\n0\n\n\n201\nUlsan\n2021\nElec\n196412\n\n\n202\nUlsan\n2021\nGas\n312276\n\n\n203\nUlsan\n2021\nHeat\n0\n\n\n\n\n204 rows × 4 columns\n\n\n\n\n큰 데이터와 작은 데이터가 만들어졌다.\n\n\nbig = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year', 'Type'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\nsmall = df.set_index(['지역', '년도', '시도']).applymap(lambda x : str(x).replace(',','')).astype(int).reset_index()\\\n.rename(name_dict, axis = 1).drop(['BldgCount', 'Area'], axis = 1)\\\n.set_index(['Reg', 'Year', 'Prov']).stack().reset_index().rename({'level_3':'Type', 0:'EnergyUse'}, axis = 1)\\\n.pivot_table(index = ['Prov', 'Year'], values = 'EnergyUse', aggfunc = 'sum').reset_index()\n\n\npd.merge(big, small, on = ['Year', 'Prov']).loc[lambda _df : _df['Type'] == 'Elec'].reset_index(drop = True)\\\n.assign(ElecRate = lambda _df : _df.EnergyUse_x / _df.EnergyUse_y)\\\n.drop(['EnergyUse_x', 'EnergyUse_y'], axis = 1)\n\n\n\n\n\n\n\n\nProv\nYear\nType\nElecRate\n\n\n\n\n0\nBusan\n2018\nElec\n0.455996\n\n\n1\nBusan\n2019\nElec\n0.463324\n\n\n2\nBusan\n2020\nElec\n0.457401\n\n\n3\nBusan\n2021\nElec\n0.534566\n\n\n4\nChungcheongbuk-do\n2018\nElec\n0.512447\n\n\n...\n...\n...\n...\n...\n\n\n63\nSeoul\n2021\nElec\n0.455675\n\n\n64\nUlsan\n2018\nElec\n0.384385\n\n\n65\nUlsan\n2019\nElec\n0.392067\n\n\n66\nUlsan\n2020\nElec\n0.387118\n\n\n67\nUlsan\n2021\nElec\n0.386115\n\n\n\n\n68 rows × 4 columns\n\n\n\n\n타이디데이터가 됐다."
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html",
    "title": "Tidydata 만들기",
    "section": "",
    "text": "여러가지 방법들을 사용해서 tidydata를 만들어보자!"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#라이브러리-imports",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#라이브러리-imports",
    "title": "Tidydata 만들기",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#pandas---lambda-_df-의-활용",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#pandas---lambda-_df-의-활용",
    "title": "Tidydata 만들기",
    "section": "2. Pandas - lambda _df :의 활용",
    "text": "2. Pandas - lambda _df :의 활용\n\nA. lambda _df : with indexer\n\n- 예시 1 : 아래와 같은 데이터프레임이 있다고 할 때, 표현 1, 2, 3은 모두 같은 문법이다.\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n표현 1\n\ndf[df.A.isna()]  ## 행 슬라이싱, [False, True, False, False]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n표현 2\n\ndf[(lambda _df : _df.A.isna())(df)]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n표현 3\n\ndf[lambda _df : _df.A.isna()]  ## 괄호로 함수를 묶어줘도 되고(그럼 구분이 쉬워진다), 안해도 됨\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n- 예시 2 : .loc, .iloc\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.loc[lambda _df : _df.A.isna()]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\ndf.iloc[lambda _df : list(_df.A.isna())]  ## iloc은 튜플로 입력이 안된다.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\niloc의 경우 범용성이 떨어지긴 한다… 그러니까 왠만해선 .loc을 사용하거나 시리즈를 list로 묶어주자…\n\n근데 왜 이런 문법이 있을까? 연속적으로 DataFrame을 변화시켜야 할 경우 유용하기 때문이다.\n- 예시 3\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.assign(D = df.A + df.B + df.C)  ## 결측치가 있을 경우 합은 NaN이 됨.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\n해당 데이터프레임에서 결측치의 수가 50%가 넘는 열만 고르고 싶다면?\n\n\ndf.assign(D = df.A + df.B + df.C).loc[:, lambda _df : _df.isna().mean() &gt; 0.5]\n\n\n\n\n\n\n\n\nD\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n3\n11.0"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#b.-lambda-df-with-assign",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#b.-lambda-df-with-assign",
    "title": "Tidydata 만들기",
    "section": "### B. lambda df: with assign",
    "text": "### B. lambda df: with assign\n예시 1\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.assign(D = df.A + df.B + df.C)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\n여기에서 결측치의 값을 count하여 새로운 열 E에 할당하고 싶다면?\n\n\ndf.assign(D = df.A + df.B + df.C).assign(E = lambda _df : _df.isna().sum(axis = 1))\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n2\n\n\n1\nNaN\n3.0\n4.0\nNaN\n2\n\n\n2\n1.0\nNaN\n5.0\nNaN\n2\n\n\n3\n1.0\n4.0\n6.0\n11.0\n0\n\n\n\n\n\n\n\n예시 2 : 원본 데이터를 손상시키지 않으며 데이터를 변형하고 싶을 때\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':[12,234,3456,12345,654222]})\ndf\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n\ndf2 = df\ndf2['B'] = np.log(df2.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n???\n\n~이성적으로 이해하기 어려운 결과~ 왜 이렇게 될까? 왜냐면 df2는 df와 같은 녀석을 의미하기 때문이다.(id가 같음)\n\ndf2 = df.copy()\ndf2['B'] = np.log(df2.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n이러면 원본 데이터를 손상시키지 않는다. 또는…\n\n\ndf.assign(B = lambda _df : np.log(df.A)).assign(C = lambda _df : (_df.B - _df.B.mean())/_df.B.std())\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#pandas---multi_index의-이해",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#pandas---multi_index의-이해",
    "title": "Tidydata 만들기",
    "section": "3. Pandas - Multi_index의 이해",
    "text": "3. Pandas - Multi_index의 이해\n\nA. 원래 df, s는 딕셔너리 계열임\n\n- 예시 1 : df는 dict에서 만들 수 있음\n\ndct = {'A': [1,2,3],'B': [2,3,4]}  ## 애초에 데이터프레임에 입력되는 값과 동일...\ndf = pd.DataFrame(dct)\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 예시 2 : s도 dict에서 만들 수 있음.\n\ndct = {'43052': 80, '43053': 90, '43054': 50}   ## key가 index가 된다.\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n- 예시 3 : dict의 키로 올 수 있는 것들\n\n튜플로 dict를 만든다면?\n\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns\n\n43052  4    80\n43053  1    90\n43054  2    50\ndtype: int64\n\n\n\ns.index\n\nMultiIndex([('43052', 4),\n            ('43053', 1),\n            ('43054', 2)],\n           )\n\n\n\n멀쩡하게 잘 작동한다. 그리고 MultiIndex가 나온다."
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#b.-.index혹은-.columns에-name이-있는-경우",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#b.-.index혹은-.columns에-name이-있는-경우",
    "title": "Tidydata 만들기",
    "section": "### B. .index혹은 .columns에 name이 있는 경우",
    "text": "### B. .index혹은 .columns에 name이 있는 경우\n예시 1 : index에 이름이 있는 경우\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns.rename_axis(['id'])  ## set_axis()가 인덱스와 컬럼의 이름을 조정하는 것이라면 이건 그것의 이름을 조정한다.\n\nid\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns.index, s.rename_axis(['id']).index\n\n(Index(['43052', '43053', '43054'], dtype='object'),\n Index(['43052', '43053', '43054'], dtype='object', name='id'))\n\n\n예시 2 : index에 이름이 있는 경우(멀티 인덱스)\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns.rename_axis(['id','year'])\n\nid     year\n43052  4       80\n43053  1       90\n43054  2       50\ndtype: int64\n\n\n\nMultiIndex에서 인덱스의 이름을 각각 지정해준 경우이다.\n\n\n예시 2가 데이터프레임이라면 이렇게 보인다.\n\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ndf = pd.DataFrame(s.rename_axis(['id','year']))  ## index의 이름을 지정해줌\ndf\n\n\n\n\n\n\n\n\n\n0\n\n\nid\nyear\n\n\n\n\n\n43052\n4\n80\n\n\n43053\n1\n90\n\n\n43054\n2\n50\n\n\n\n\n\n\n\n\n만약 여기서 각 원소를 호출하고 싶다면…\n\n\ndf.loc[('43052', 4)]\n\n0    80\nName: (43052, 4), dtype: int64\n\n\n\n그냥 멀티인덱스, 튜플을 입력하여 호출하면 된다."
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#tidydata",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#tidydata",
    "title": "Tidydata 만들기",
    "section": "4. Tidydata",
    "text": "4. Tidydata\n\nA. tidydata의 개념\n\n- 아래의 자료는 불리하다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\n만약 A학과에 해당하는 결과만 뽑고 싶다면? -&gt; department가 column으로 있어야 함…\npass인 사람만 bar plot을 그리고 싶다면? result가 column으로 있어야 함…\n\n원하는 정보를 쉽게 뽑아낼 수 있는 데이터를 tidydata라고 한다.\n\ntidydata = df['pass'].reset_index()\n#---#\nfig = ggplot(tidydata)\ncol = geom_col(aes(x='department',y='pass',fill='gender'),position='dodge')   ## dodge 설정으로 누적으로 표기하지 않고 옆에 늘여서 표시\nfig + col\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#b.-tidydata가-아닌-예시",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#b.-tidydata가-아닌-예시",
    "title": "Tidydata 만들기",
    "section": "### B. tidydata가 아닌 예시",
    "text": "### B. tidydata가 아닌 예시\n\nMultiIndex 구조를 가지면 무조건 tidydata가 아님\n열의 값이 여러 개의 정보를 가지고 있다면 tidydata가 아님\nwide data는 tidydata가 아님 -&gt; melt나 pivot_table등을 활용하여 조정해줘야…\n\n- wide df 예시\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n- 여러 방법을 통해 tidydata로 변환\n\ndf.set_index(['Date']).stack().reset_index().rename({'level_1' : 'Brand', 0 : 'Sales'}, axis = 1)\n\n\n\n\n\n\n\n\nDate\nBrand\nSales\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-10\nApple\n324\n\n\n2\n2019-10\nHuawei\n136\n\n\n3\n2019-10\nXiaomi\n109\n\n\n4\n2019-10\nOppo\n76\n\n\n...\n...\n...\n...\n\n\n203\n2020-10\nNokia\n20\n\n\n204\n2020-10\nLenovo\n22\n\n\n205\n2020-10\nOnePlus\n9\n\n\n206\n2020-10\nSony\n22\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n- melt를 통해 변환\n\ndf.melt(id_vars = ['Date'])  ## 이럼 한번에 되서 편하긴 하다. wide data 한정\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-11\nSamsung\n461\n\n\n2\n2019-12\nSamsung\n426\n\n\n3\n2020-01\nSamsung\n677\n\n\n4\n2020-02\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06\nAsus\n16\n\n\n204\n2020-07\nAsus\n12\n\n\n205\n2020-08\nAsus\n20\n\n\n206\n2020-09\nAsus\n15\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\ndf.melt(id_vars = [])는 index로 지정한 열외의 모든 열들을 한 행에 엮어버린다.\ndf.stack()의 경우 set_index()나 reset_index()등 사용해야 할 게 많을 수 있다. 하지만 직관적이고 사용에 용이하다.\ndf.stack()의 반대로 df.unstack()을 사용하여 인덱스를 컬럼으로 올릴수도 있다."
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#pivot_table-groupby-aggregate",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#pivot_table-groupby-aggregate",
    "title": "Tidydata 만들기",
    "section": "5. pivot_table, groupby + aggregate",
    "text": "5. pivot_table, groupby + aggregate\n\n대부분은 pivot_table로 해결이 된다.\n\n\nA. intro\n\n- tidydata 만드는 개념 : 그룹화 -&gt; 집계\n예제 1 : 아래의 데이터프레임에서 학과, 성별로 count의 합계를 구하라.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\ndf.pivot_table(index = ['department', 'gender'], values = 'count', aggfunc = sum).reset_index()\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n\n인덱스에만 몰아주든 둘다 넣어주든 똑같이 tidydata를 만들어준다.\n\n- 예시에서 본 작업은 아래의 작업들로 세분화할 수 있다.\n\n그룹화(쿼리) : 하나의 DataFrame을 sub-dataframe으로 나누는 과정, 전체 자료를 (학과, 성별)로 묶어 총 12개의 sub-dataframe을 만든다.\n각각집계 : 나눠진 sub-dataframe에서 어떠한 계산을 각각 수행함, 나눠진 sub-dataframe에서 지원자 수의 합계를 각각 구함\n\n- 위와 같은 작업을 하려면 아래와 같은 요소들이 필요하다.\n\n그룹변수~(없는 용어임)~ : 그룹화를 위해 필요한 변수 : DataFrame을 sub-dataframe으로 나누는 역할. &gt;&gt; index and columns &gt; 범주형이거나 범주형으로 바꿀 수 있는 데이터\n집계변수~(이것도 없는 용어임)~ : 집계함수의 대상이 되는 변수 &gt;&gt; values\n집계함수 : 그룹화된 데이터프레임에 수행하는 계산을 정의하는 함수 &gt;&gt; aggfunc"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#b.-pivot_table의-문법",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#b.-pivot_table의-문법",
    "title": "Tidydata 만들기",
    "section": "### B. pivot_table의 문법",
    "text": "### B. pivot_table의 문법\n- pivot_table의 문법\ndf.pivot_table(\n    index = 그룹변수\n    columns = 그룹변수\n    values = 집계변수\n    aggfunc = 집계함수\n)\nindex & columns에 그룹변수를 적절히 나누어 입력한다.\n예시 : 집계함수 전달방법\n\ndf = pd.DataFrame({'category':['A']*5+['B']*5, 'value':np.concatenate([np.random.randn(5), np.random.randn(5)+10])})\ndf\n\n\n\n\n\n\n\n\ncategory\nvalue\n\n\n\n\n0\nA\n0.383420\n\n\n1\nA\n1.084175\n\n\n2\nA\n1.142778\n\n\n3\nA\n0.307894\n\n\n4\nA\n0.237787\n\n\n5\nB\n10.355951\n\n\n6\nB\n8.336925\n\n\n7\nB\n8.617227\n\n\n8\nB\n8.073155\n\n\n9\nB\n8.513784\n\n\n\n\n\n\n\n\ndf.pivot_table(index = 'category', values = 'value', aggfunc = np.sum)\ndf.pivot_table(index = 'category', values = 'value', aggfunc = 'sum')\n## 동일한 코드\n\n\n\n\n\n\n\n\nvalue\n\n\ncategory\n\n\n\n\n\nA\n3.156054\n\n\nB\n43.897041\n\n\n\n\n\n\n\n\ndf.pivot_table(index = 'category', values = 'value', aggfunc = ['sum', 'count'])\n\n\n\n\n\n\n\n\nsum\ncount\n\n\n\nvalue\nvalue\n\n\ncategory\n\n\n\n\n\n\nA\n3.156054\n5\n\n\nB\n43.897041\n5\n\n\n\n\n\n\n\n\n집계함수들의 리스트를 넣는다면 각각의 집계치를 따로 알아서 구해준다.\n\n\nC. groupby + aggregate의 문법\n\n- groupby + aggregate\n\ndf.groupby(그룹변수).aggregate({집계변수:집계함수})\n\n그룹화를 한 후(index, columns), 무엇을 집계할 것인지 dictionary로 지정해준다.(values, aggfunc)\n\n딕셔너리로 지정해주는 것은 pivot_table도 가능하긴 하다…"
  },
  {
    "objectID": "2023_DV/Review/A9. tidydata, 시각화.html#연습---airline-data",
    "href": "2023_DV/Review/A9. tidydata, 시각화.html#연습---airline-data",
    "title": "Tidydata 만들기",
    "section": "6. 연습 - Airline Data",
    "text": "6. 연습 - Airline Data\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58492 entries, 0 to 58491\nData columns (total 14 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   MONTH      58492 non-null  int64  \n 1   DAY        58492 non-null  int64  \n 2   WEEKDAY    58492 non-null  int64  \n 3   AIRLINE    58492 non-null  object \n 4   ORG_AIR    58492 non-null  object \n 5   DEST_AIR   58492 non-null  object \n 6   SCHED_DEP  58492 non-null  int64  \n 7   DEP_DELAY  57659 non-null  float64\n 8   AIR_TIME   57474 non-null  float64\n 9   DIST       58492 non-null  int64  \n 10  SCHED_ARR  58492 non-null  int64  \n 11  ARR_DELAY  57474 non-null  float64\n 12  DIVERTED   58492 non-null  int64  \n 13  CANCELLED  58492 non-null  int64  \ndtypes: float64(3), int64(8), object(3)\nmemory usage: 6.2+ MB\n\n\n- ChatGPT의 도움을 받아 해당 인포를 정리\n\nMONTH: 비행이 이루어진 월을 나타냄. 1에서 12 사이의 값을 갖음.\nDAY: 비행이 이루어진 일자를 나타냄. 월에 따라 1~28/29/30/31 사이의 값을 1. 가질 수 있음.\nWEEKDAY: 비행이 이루어진 요일을 나타냄. 일반적으로 1(일요일)부터 7(토요일1)까지의 값을 갖음.\nAIRLINE: 해당 항공편을 운영하는 항공사의 약어나 코드를 나타냄.\nORG_AIR: 비행기가 출발하는 공항의 약어나 코드를 나타냄.\nDEST_AIR: 비행기가 도착하는 공항의 약어나 코드를 나타냄.\nSCHED_DEP: 원래의 예정된 출발 시간을 나타냄. 시간은 일반적으로 HHMM 형식으로 표시될 수 있음.\nDEP_DELAY: 출발 지연 시간을 나타냄. 음수 값은 조기 출발, 양수 값은 지연을 의미함.\nAIR_TIME: 실제 공중에서 비행한 시간을 분 단위로 나타냄.\nDIST: 비행 거리를 나타냄. 일반적으로 마일 또는 킬로미터로 표시됨.\nSCHED_ARR: 원래의 예정된 도착 시간을 나타냄. SCHED_DEP와 같은 형식으로 표시될 수 있음.\nARR_DELAY: 도착 지연 시간을 나타냄. 음수는 조기 도착, 양수는 지연을 의미함.\nDIVERTED: 항공편이 다른 곳으로 우회되었는지를 나타냄. 1은 우회, 0은 정상 경로를 의미함.\nCANCELLED: 항공편이 취소되었는지 여부를 나타냄. 1은 취소, 0은 취소되지 않음을 의미함.\n\n# 예제1 : 항공사별로 도착지연시간의 평균을 구하라.\n\ndf.pivot_table(index = 'AIRLINE', values = 'ARR_DELAY', aggfunc = 'mean')\ndf.groupby(by = 'AIRLINE').aggregate({'ARR_DELAY' : 'mean'})\n## 동일한 코드\n\n\n\n\n\n\n\n\nARR_DELAY\n\n\nAIRLINE\n\n\n\n\n\nAA\n5.542661\n\n\nAS\n-0.833333\n\n\nB6\n8.692593\n\n\nDL\n0.339691\n\n\nEV\n7.034580\n\n\nF9\n13.630651\n\n\nHA\n4.972973\n\n\nMQ\n6.860591\n\n\nNK\n18.436070\n\n\nOO\n7.593463\n\n\nUA\n7.765755\n\n\nUS\n1.681105\n\n\nVX\n5.348884\n\n\nWN\n6.397353\n\n\n\n\n\n\n\n# 예제2 : 항공사 별로 비행취소건수의 합계를 구하라. 취소건수가 높은 항공사 순으로 정렬하라.\n\ndf.pivot_table(index = 'AIRLINE', values = 'CANCELLED', aggfunc = 'sum').sort_values('CANCELLED', ascending = False)\ndf.groupby(by = 'AIRLINE').aggregate({'CANCELLED' : 'sum'}).sort_values('CANCELLED', ascending = False)\n## 역시 동일한 코드\n\n\n\n\n\n\n\n\nCANCELLED\n\n\nAIRLINE\n\n\n\n\n\nAA\n154\n\n\nMQ\n152\n\n\nEV\n146\n\n\nOO\n142\n\n\nUA\n93\n\n\nWN\n93\n\n\nDL\n38\n\n\nNK\n25\n\n\nUS\n21\n\n\nF9\n10\n\n\nVX\n6\n\n\nB6\n1\n\n\nAS\n0\n\n\nHA\n0\n\n\n\n\n\n\n\n# 예제3 : 항공사별로 비행취소율을 구하라. 비행취소율이 가장 높은 항공사 순으로 정렬하라\n\ndf.pivot_table(index = 'AIRLINE', values = 'CANCELLED', aggfunc = 'mean').sort_values('CANCELLED', ascending = False)\ndf.groupby(by = 'AIRLINE').aggregate({'CANCELLED' : 'mean'}).sort_values('CANCELLED', ascending = False)\n## 동일한 코드\n\n\n\n\n\n\n\n\nCANCELLED\n\n\nAIRLINE\n\n\n\n\n\nMQ\n0.043791\n\n\nEV\n0.024923\n\n\nOO\n0.021554\n\n\nAA\n0.017303\n\n\nNK\n0.016491\n\n\nUS\n0.013003\n\n\nUA\n0.011935\n\n\nWN\n0.011048\n\n\nF9\n0.007593\n\n\nVX\n0.006042\n\n\nDL\n0.003585\n\n\nB6\n0.001842\n\n\nAS\n0.000000\n\n\nHA\n0.000000\n\n\n\n\n\n\n\n# 예제4 : (항공사, 요일)별 비행취소건수와 비행취소율을 조사하라.\n\ndf.pivot_table(index = ['AIRLINE', 'WEEKDAY'], values = 'CANCELLED', aggfunc = ['sum', 'mean'])\ndf.groupby(by = ['AIRLINE', 'WEEKDAY']).aggregate({'CANCELLED' : ['sum','mean']})\n## 동일\n\n\n\n\n\n\n\n\n\nCANCELLED\n\n\n\n\nsum\nmean\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\nAA\n1\n41\n0.032106\n\n\n2\n9\n0.007341\n\n\n3\n16\n0.011949\n\n\n4\n20\n0.015004\n\n\n5\n18\n0.014151\n\n\n...\n...\n...\n...\n\n\nWN\n3\n18\n0.014118\n\n\n4\n10\n0.007911\n\n\n5\n7\n0.005828\n\n\n6\n10\n0.010132\n\n\n7\n7\n0.006066\n\n\n\n\n98 rows × 2 columns\n\n\n\n# 예제4 : (항공사, 요일)별로 CANCELLED는 평균과 합계를 구하고, AIR_TIME은 평균과 표준편차를 구하여라.\n\ndf.pivot_table(index = ['AIRLINE', 'WEEKDAY'], values = ['CANCELLED', 'AIR_TIME'], aggfunc = {'CANCELLED' : ['sum', 'mean'], 'AIR_TIME' : ['mean', 'std']})\ndf.groupby(by = ['AIRLINE', 'WEEKDAY']).aggregate({'CANCELLED' : ['mean', 'sum'], 'AIR_TIME' : ['mean', 'std']})\n## 거의 유사한 코드임\n\n\n\n\n\n\n\n\n\nCANCELLED\nAIR_TIME\n\n\n\n\nmean\nsum\nmean\nstd\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n41\n147.610569\n73.442540\n\n\n2\n0.007341\n9\n143.851852\n73.211275\n\n\n3\n0.011949\n16\n144.514005\n73.340675\n\n\n4\n0.015004\n20\n141.124618\n69.220840\n\n\n5\n0.014151\n18\n145.430966\n76.711095\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n18\n104.219920\n53.869040\n\n\n4\n0.007911\n10\n107.200800\n54.466218\n\n\n5\n0.005828\n7\n107.893635\n57.172695\n\n\n6\n0.010132\n10\n109.247433\n56.149388\n\n\n7\n0.006066\n7\n107.602273\n56.419207\n\n\n\n\n98 rows × 4 columns\n\n\n\n# 예제5 : 운행구간(거리의 구간)을 그룹화하고, 운행구간 별 비행취소건수와 취소율을 구하여라.\n\npd.qcut(df.DIST, q = 4)  ## 어레이나 시리즈를 넣어주면 해당 시리즈를 분위수로 컷한다.\n\n0          (391.0, 690.0]\n1        (1199.0, 4502.0]\n2          (391.0, 690.0]\n3         (690.0, 1199.0]\n4        (1199.0, 4502.0]\n               ...       \n58487    (1199.0, 4502.0]\n58488      (391.0, 690.0]\n58489     (66.999, 391.0]\n58490     (690.0, 1199.0]\n58491      (391.0, 690.0]\nName: DIST, Length: 58492, dtype: category\nCategories (4, interval[float64, right]): [(66.999, 391.0] &lt; (391.0, 690.0] &lt; (690.0, 1199.0] &lt; (1199.0, 4502.0]]\n\n\n\ndf.assign(DIST_CUT = pd.qcut(df.DIST, q = 4))\\\n.pivot_table(index = 'DIST_CUT', values = 'CANCELLED', aggfunc = ['sum', 'mean'])\n\n\n\n\n\n\n\n\nsum\nmean\n\n\n\nCANCELLED\nCANCELLED\n\n\nDIST_CUT\n\n\n\n\n\n\n(66.999, 391.0]\n334\n0.022659\n\n\n(391.0, 690.0]\n196\n0.013503\n\n\n(690.0, 1199.0]\n203\n0.013637\n\n\n(1199.0, 4502.0]\n148\n0.010313\n\n\n\n\n\n\n\n\n긴 구간일 수록 취소율이 낮은 것 같다."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html",
    "title": "훌륭한 시각화 2",
    "section": "",
    "text": "훌륭한 시각화란 무엇일까…"
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#presentation-vs-exploration",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#presentation-vs-exploration",
    "title": "훌륭한 시각화 2",
    "section": "1. Presentation vs Exploration",
    "text": "1. Presentation vs Exploration\n\n두 방식의 차이와, 장단점은 무엇일까?\n\n\nA. Presentation\n\n\n- 프리젠테이션 방식의 시각화는 화자가 다듬은 이야기를 전달하기에 좋은 시각화이다. 즉, 잘 정리된 메시지를 전달하기에 좋다."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#b.-exploration",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#b.-exploration",
    "title": "훌륭한 시각화 2",
    "section": "### B. Exploration",
    "text": "### B. Exploration\n\n\n문학적유기체라는 작품으로 어떤 소설책을 시각화한 것이다.\n수형도와 색깔로 구성되고, 수형도는 단원 \\(\\leftarrow\\) 문단 \\(\\leftarrow\\) 문장 \\(\\leftarrow\\) 단어 순으로 뻗어진다. 색깔은 소설에서 등장하는 소재를 구분(범주형 변수 표현에 적합)한다.\n\n- 익스플로래이션 방식은 독자가 스스로 그림에서 메시지를 찾아내도록 한다. 소설을 읽어보지 않은 사람은 해당 그래픽으로 소설책의 전체 주제를 미리 파악할 수 있고, 읽어본 사람은 분석 및 탐구를 할 수 있을 것이다.\n\nC. 절충\n\n- 카이로 : 사실 프리젠테이션과 익스플로래이션은 절충 가능하다.\n\n## 이 그림을 보면 아래의 코드가 생각나야 함\nfig = ggplot(tidydata)\nline = geom_line(aes(x = '소득', y = '불평등', color = '정부'))\npoint = geom_point(aes(x = '소득', y = '불평등'))\ntext = geom_text(aes(x = '소득', y = '불평등', label = '연도'))\n\nfig + line + point + text\n\n초록색 정부 : 소득이 증가 & 불평등이 훨씬 더 증가\n갈색 정부 : 매우 빠른 경제성장 & 불평등의 해소\n포인트 간의 간격이 조밀하다 = 변화가 더디다 // 포인트 간의 간격이 넓다 = 변화가 빠르다.\n\n- 언뜻 보기에는 익숙한 라인플랏처럼 보이지만, 해석할 만한 정보가 많다.\n\n익스플로레이션 형태의 그래프는 그릴 줄도 알아야 하지만, 남이 그린 그래프를 해석할 수도 있어야 한다."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#이성적-낙관주의",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#이성적-낙관주의",
    "title": "훌륭한 시각화 2",
    "section": "2. 이성적 낙관주의",
    "text": "2. 이성적 낙관주의"
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#a.-인구문제",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#a.-인구문제",
    "title": "훌륭한 시각화 2",
    "section": "### A. 인구문제",
    "text": "### A. 인구문제\n- 주장 1 : 가난한 나라에서 애를 너무 많이 낳음 \\(\\leftarrow\\) 세계인구가 90억까지 증가할 것이다.\n- 주장 2 : 잘사는 나라에서 애를 적게 낳음 \\(\\leftarrow\\) 극심한 고령화 문제가 발생할 것이다.\n\nB. 리들리의 메시지\n\n둘다 틀렸는뎁쇼???\n- 가난한 나라의 출산율은 점점 감소\n- 잘 사는 사라의 출산율은 점점 증가\n\n따라서 세계의 인구는 안정화되고, 고령화 문제도 오지 않을 것이다."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#c.-카이로",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#c.-카이로",
    "title": "훌륭한 시각화 2",
    "section": "### C. 카이로",
    "text": "### C. 카이로\n- 리들러의 메시지는 아래의 그림들이 더 잘 전달한다.\n\nfig = ggplot(tidydata)\nline = geom_line(aes(x = '연도', y = '인구증가율', color = '국가'))\nfig + line\n\n노르웨이, 영국, 스웨덴, 스페인, 이탈리아의 경우 출산율이 반등함(일본과 독일의 경우는 감소세가 아주 누그러듬)\n인도, 브라질, 중국과 같은 나라는 출산율이 대폭 감소\n\n\nD. 교수님 소감\n\n- 어떠한 현상을 살펴볼 때, 그것의 부분집합들이 역시 그러한 지 살펴보는 것은 기본임.\n- 중요한 선을 제외한 나머지는 일러레를 통해 회색으로 처리한 것이 시각적으로 우수하다.\n- 과학적인 논문작업에 들어갈 그림이라면 임의로 회색처리한 것이 다소 비판을 받을 수 있음."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#시각화-예시",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#시각화-예시",
    "title": "훌륭한 시각화 2",
    "section": "3. 시각화 예시",
    "text": "3. 시각화 예시"
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#a.-상관관계의-해석",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#a.-상관관계의-해석",
    "title": "훌륭한 시각화 2",
    "section": "### A. 상관관계의 해석",
    "text": "### A. 상관관계의 해석\n\n중졸 여성의 비율과 출산율을 나타냄\n- 해설 : 당신이 더 교육받고 부유할수록, 가질 아이들의 수는 적어집니다.\n\n??? 여중을 다 때려부수면 출산율이 올라가나요?\n\n\nB. 남미 국가들의 국방력\n\n\n- 교수님께선 이를 쓸모없는 그래픽이라고 말씀하셨지…(뭐 기억나는 게 있나요?)\n- 아래가 더 우수한 그림이다.(바 차트)\n\n- 그리고 이게 더 우수한 시각화이다.(분야별 바 차트)\n\n\n분야별 비교가 유용하다. 브라질의 국방력은 모든 지표에서 1등인 것 같다.\n브라질을 하이라이팅한 것도 우아함…(라고 하셨음)\n\n근데 흑막을 제거하면 어떻게 될까.\n\n- 흑막을 제거\n\n\n인구라는 흑막을 제거하고 보니 인구당 군인수도, 국방비 지출도, 군인 당 교육 투자비도 높지 않다.\n즉, 내실이 없다.\n\n- 최종적으로 제안하는 그래프\n\n\n우측하단 : 관심있는 그래프가 아님(지역적 특성을 나타냈을뿐…)\n좌측하단 : 산점도??\n\nfig = ggplot(tidydata)\npoint = geom_point(aes(x = '인구', y = '군인 수', size = '예산'), alpha = 0.5)\npoint2 = geom_point(aes(x = '인구', y = '군인 수'))\nfig + point + point2\n\n교수님 : 사실 저는 아래의 그래프가 좋은 시각화라고 생각 안해요.\n\n- x축과 y축에 비슷한 정보가 들어가있다. 모든 점들이 직선에 몰려있다면 왜 2차원으로 표현해야 할까???\n\n산점도에서 데이터를 한눈에 파악하고 특징을 요약하기 위해서는 \\(x\\)축과 \\(y\\)축을 너무 비슷한 성질의 변수로 설정하지 마라.\n\nfig1 = fig + geom_point(aes(x = '토익', y = '텝스', color = '합격여부', shape = '회사명'))\nfig2 = fig + geom_point(aes(x = '토익', y = 'GPA', color = '합격여부', shape = '회사명'))\nfig2가 더 합리적이다.(토익과 텝스가 유사한 항목이라면…)\n\n\n\nC. 스페인의 실업률\n\n * 명암으로 왜 크기비교를 하는 것인가??? 게다가 명암으로 한 크기비교마저 이상한데???\n\n비교를 위해서는 바플랏이 더 우수하다."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#d.-은행들의-시가총액",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#d.-은행들의-시가총액",
    "title": "훌륭한 시각화 2",
    "section": "### D. 은행들의 시가총액",
    "text": "### D. 은행들의 시가총액\n- 카이로 교수햄의 강의자료에 등장하는 그림(bubble chart)\n- 회색이 before, 검은색이 after임\n\n\n우리 눈은 부피의 비교를 잘 못하기 때문에, 해당 차트는 결과를 왜곡한다… 버블 차트의 경우 크기를 왜곡시키므로 사용을 지양해야 한다.\n\n\nE. 분열된 유권자들\n\n- 하지만 아래의 버블차트는 우수하다.(왜? 크기비교 자체가 목적이 아니므로)\n\n\n선거 지도의 경우 수치 비교에 별로 관심이 없다.(미국의 경우 간접선거라 그럼…)\n민주당표와 공화당표가 어떤 지역에 몰렸는지 파악하는 것이 중요하므로, aes중 가장 중요한 \\(x\\)와 \\(y\\)를 모두 지역정보를 표현하기 위해 투자함(좌표)"
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#f.-좋은-aes-속성들일반적으로",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#f.-좋은-aes-속성들일반적으로",
    "title": "훌륭한 시각화 2",
    "section": "### F. 좋은 aes 속성들(일반적으로…)",
    "text": "### F. 좋은 aes 속성들(일반적으로…)\n\n아래로 갈수록 뭔가 난잡해지고 있음…\n- 근데 스티븐 잡스는 하위에 해당하는 Volume 방식으로 시각화를 했음.\n\n\n왜???\n\n\n자사의 점유율이 높아보이도록 트릭을 쓴 것이라고 본다."
  },
  {
    "objectID": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#시각화의-정석",
    "href": "2023_DV/Review/B2. 훌륭한 시각화(after midterm).html#시각화의-정석",
    "title": "훌륭한 시각화 2",
    "section": "4. 시각화의 정석",
    "text": "4. 시각화의 정석\n그래서, 어떻게 하라는 건데요???\n\n시간 경과에 따른 변화를 보여주고 싶다!(시계열 자료) &gt; 라인플랏\n집단 간 비교를 하고 싶다! &gt; 바플랏\n변수 간 관계를 알고 싶다! &gt; 산점도"
  },
  {
    "objectID": "2023_DV/Review/A6. Pandas 팁.html",
    "href": "2023_DV/Review/A6. Pandas 팁.html",
    "title": "Pandas 사용 팁",
    "section": "",
    "text": "Pandas에서 유용하게 사용할 수 있는 여러가지 메소드들을 알아보자!"
  },
  {
    "objectID": "2023_DV/Review/A6. Pandas 팁.html#라이브러리-imports",
    "href": "2023_DV/Review/A6. Pandas 팁.html#라이브러리-imports",
    "title": "Pandas 사용 팁",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/A6. Pandas 팁.html#pabdas-transform-column",
    "href": "2023_DV/Review/A6. Pandas 팁.html#pabdas-transform-column",
    "title": "Pandas 사용 팁",
    "section": "2. pabdas : transform column",
    "text": "2. pabdas : transform column\nA. lambda\nB. map\n\nC. s.apply(변환함수) | 원소들을 각각 변환\n\n\n변환함수 : 원래 형식을 보존하면서 원소들을 바꾸는 함수\n집계함수 : 벡터 -&gt; 스칼라 (평균을 불러주는 함수 : [1,2,3,4,5] -&gt; 3)\n\n\n라고 하자.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')   ## DataFrame\ns = df.Height   ## Series\n\n\ns\n\n0        189cm\n1        179cm\n2        172cm\n3        181cm\n4        172cm\n         ...  \n17655    190cm\n17656    195cm\n17657    190cm\n17658    187cm\n17659    186cm\nName: Height, Length: 17660, dtype: object\n\n\n\n뒤에 cm가 붙어있는 범주형 자료로 저장되어있음.\n\n\ns.apply(lambda x : int(x[:3])) ## 집계함수가 아닌 변환함수만 적용할 수 있음. 각 원소에 함수 적용.\n##s.apply(lambda x : x[:3]).apply(int)    ## 연쇄적으로\n##s.apply(lambda x : x[:3]).astype('int64')   ## astype() 이용\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64\n\n\n\ncm를 제거하고 포맷을 정수형으로 변경하였다."
  },
  {
    "objectID": "2023_DV/Review/A6. Pandas 팁.html#d.-s.str-idx.str-string-오브젝트에만-사용할-수-있는-함수를-사용",
    "href": "2023_DV/Review/A6. Pandas 팁.html#d.-s.str-idx.str-string-오브젝트에만-사용할-수-있는-함수를-사용",
    "title": "Pandas 사용 팁",
    "section": "### D. s.str, idx.str | string 오브젝트에만 사용할 수 있는 함수를 사용",
    "text": "### D. s.str, idx.str | string 오브젝트에만 사용할 수 있는 함수를 사용\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\n\"180cm\"[:3]\n\n'180'\n\n\n\n'180cm'.replace('cm','')\n\n'180'\n\n\n\n위와 같은 연산을 시리즈에 적용시키고 싶다.\n\n\ns.str[:3]\n##s.str.replace('cm', '')   ## 개별 문자열과 동일하게 메소드를 적용시켜도 된다.\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n\n문자열의 메소드를 그대로 적용 가능\n\n- 예시2 : 원소별로 isupper를 수행(대문자인지 판별)\n\n_s = pd.Series(['A','B','C','d','e','F'])\n_s\n\n0    A\n1    B\n2    C\n3    d\n4    e\n5    F\ndtype: object\n\n\n\n_s.str.isupper()\n\n0     True\n1     True\n2     True\n3    False\n4    False\n5     True\ndtype: bool\n\n\n- 예시3 : 원소별로 공백 제거(pd.Serise 뿐만 아니라 pd.index 자료형에도 사용가능)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\nidx = df.columns\n\n\nidx.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'Joined', 'LoanedFrom',\n       'ContractValidUntil', 'Height', 'Weight', 'ReleaseClause', 'KitNumber',\n       'BestOverallRating'],\n      dtype='object')\n\n\n- 쉽게 말해서 string데이터를 지닌 개체에 string에 사용할 수 있는 메소드를 사용할 수 있도록 하는 게 pandas의 str이라고 보면 된다.\n\nE. s.astype() | 조건을 충족한 시리즈의 타입을 변경\n\n- 예시1 : 원소의 타입을 변경\n\ns = pd.Series(list('12345'))\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: object\n\n\n\ns.astype(int)\n##s.apply(int)\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n- 예시2 : 원소의 타입을 변환한 이후 브로드캐스팅\n\ns1 = pd.Series(list('12345'))\ns2 = pd.Series([-1,-2,-3,-4,-5])\n\n\ns1+s2\n\nTypeError: ignored\n\n\n\nError : 형식이 달라 불가능\n\n\ns1.astype(int) + s2\n\n0    0\n1    0\n2    0\n3    0\n4    0\ndtype: int64\n\n\n\ns2.astype(str) + s1\n\n0    -11\n1    -22\n2    -33\n3    -44\n4    -55\ndtype: object\n\n\n- 예시3 : 원소의 타입을 변환한 이후 브로드캐스팅(str)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Embarked열과 Pclass열을 사용하여 아래와 같은 New Feature를 만들어라.\n\n\n\nEmbarked\nPclass\nNew Feature\n\n\n\n\n‘S’\n3\n‘S3’\n\n\n‘C’\n1\n‘C1’\n\n\n‘S’\n3\n‘S3’\n\n\n‘S’\n1\n‘S1’\n\n\n‘S’\n3\n‘S3’\n\n\n\n둘다 문자열이면 단순히 +를 이용해 브로드캐스팅하면 되지만, 타입이 달라 불가하다.\n\ndf.Embarked + df.Pclass.apply(str)\n##df.Embarked + df.Pclass.astype(str)\n##df.Embarked + pd.Series(list(map(lambda x : str(x)), df.Pclass))\n\n0    S3\n1    C1\n2    S3\n3    S1\n4    S3\ndtype: object"
  },
  {
    "objectID": "2023_DV/Review/A6. Pandas 팁.html#f.-컴프리헨션-lambda-map을-무시하지-말-것",
    "href": "2023_DV/Review/A6. Pandas 팁.html#f.-컴프리헨션-lambda-map을-무시하지-말-것",
    "title": "Pandas 사용 팁",
    "section": "### F. 컴프리헨션, lambda + map을 무시하지 말 것",
    "text": "### F. 컴프리헨션, lambda + map을 무시하지 말 것\n- 예시1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n위 자료에서 아래와 같은 변환을 하고 싶다면 apply만으로 사용하기에 부담이 된다.\n\\[\nf(\\text{sex}, \\text{sibsp}) =\n\\begin{cases}\n0.7 + 0.25 \\times \\text{sibsp} & \\text{if } \\text{sex} = \\text{'female'} \\\\\n0.2 + 0.15 \\times \\text{sibsp} & \\text{otherwise}\n\\end{cases}\n\\]\n\nlist(map(lambda sex, sibsp : 0.7+0.25*sibsp if sex == 'female' else 0.2+0.15*sibsp, df.Sex, df.SibSp))\n\n[0.35, 0.95, 0.7, 0.95, 0.2]\n\n\n\ndf.assign(Probablity = list(map(lambda sex, sibsp : 0.7+0.25*sibsp if sex == 'female' else 0.2+0.15*sibsp, df.Sex, df.SibSp)))\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\nProbablity\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n0.35\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n0.95\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n0.70\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n0.95\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n0.20\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 예시2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Name열을 아래와 같이 분리하는 작업을 수행하라.\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen\n\n\n\n- 풀이 1\n\ndf.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')\n\n0                            [Braund, Mr, Owen Harris]\n1    [Cumings, Mrs, John Bradley (Florence Briggs T...\n2                             [Heikkinen, Miss, Laina]\n3       [Futrelle, Mrs, Jacques Heath (Lily May Peel)]\n4                           [Allen, Mr, William Henry]\nName: Name, dtype: object\n\n\n\n[[title, name + ' ' + f_name] for f_name, title, name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n- 풀이 2 : 이중 컴프리헨션이 될까 해서 해봤는데… 되네?~(솔직히 안될 이유가 없긴 함, 리스트를 반환하는 거니까…)~\n\n[[names[0], names[1] + ' ' + f_name] for f_name, names in [[f_name, names.split('. ')] for f_name, names in df.Name.str.split(', ')]]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n\nlists = [[names[0], names[1] + ' ' + f_name] for f_name, names in [[f_name, names.split('. ')] for f_name, names in df.Name.str.split(', ')]]\npd.DataFrame({'title' : np.array(lists)[:,0], 'Name' : np.array(lists)[:,1]})\n\n\n\n\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "",
    "text": "matplotlib를 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#사전작업",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#사전작업",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 import\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (3, 2)\nmatplotlib.rcParams['figure.dpi'] = 150"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#간단한-꺾은선-그래프",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#간단한-꺾은선-그래프",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "2. 간단한 꺾은선 그래프",
    "text": "2. 간단한 꺾은선 그래프\nplt.plot()을 사용하여 간단하게 그래프를 그릴 수 있다.\n\ny값만 지정한 경우\n\n\nplt.plot([1,2,4,3])\nplt.show()\n\n\n\n\n\nx값과 y값 같이 지정한 경우\n\n\nplt.plot([1,2,3,4],[1,2,4,3])\nplt.show()\n\n\n\n\n\nx값과 y값에 변수를 지정하여 넣어주는 경우\n\n\nx = [1,2,3,4]\ny = [1,2,4,3]\n\nplt.plot(x,y)\nplt.show()\n\n\n\n\n- 이외에도 다양한 옵션을 사용하여 그래프를 다채롭게 그릴 수 있는데, 지금부터 그것들을 알아보도록 하자.\n\nplt.plot의 옵션\nplt.plot()에서 괄호 안에 문자열을 넣음으로서 세 가지 옵션을 간단하게 적용할 수 있다.\nplt.plot(x,y,'--')  ## 파선 그래프\nplt.plot(x,y,':')   ## 점선 그래프\nplt.plot(x,y,'r')   ## 선의 색상이 빨간색\nplt.plot(x,y,'r--') ## 빨간색의 파선 그래프\n...\n- 게다가 세 옵션을 순서 상관없이 집어넣어 적용 가능하다!\n\nLine StylesColorsMarkers\n\n\n\n\n\ncharacter\ndescription\n\n\n\n\n‘-’\nsolid line style\n\n\n‘–’\ndashed line style\n\n\n‘-.’\ndash-dot line style\n\n\n‘:’\ndotted line style\n\n\n\n\n\n\n\n\ncharacter\ncolor\n\n\n\n\n‘b’\nblue\n\n\n‘g’\ngreen\n\n\n‘r’\nred\n\n\n‘c’\ncyan\n\n\n‘m’\nmagenta\n\n\n‘y’\nyellow\n\n\n‘k’\nblack\n\n\n‘w’\nwhite\n\n\n\n\n\n\n\n\ncharacter\ndescription\n\n\n\n\n‘.’\npoint marker\n\n\n‘,’\npixel marker\n\n\n‘o’\ncircle marker\n\n\n‘v’\ntriangle_down marker\n\n\n‘^’\ntriangle_up marker\n\n\n‘&lt;’\ntriangle_left marker\n\n\n‘&gt;’\ntriangle_right marker\n\n\n‘1’\ntri_down marker\n\n\n‘2’\ntri_up marker\n\n\n‘3’\ntri_left marker\n\n\n‘4’\ntri_right marker\n\n\n‘8’\noctagon marker\n\n\n‘s’\nsquare marker\n\n\n‘p’\npentagon marker\n\n\n‘P’\nplus (filled) marker\n\n\n’*’\nstar marker\n\n\n‘h’\nhexagon1 marker\n\n\n‘H’\nhexagon2 marker\n\n\n‘+’\nplus marker\n\n\n‘x’\nx marker\n\n\n‘X’\nx (filled) marker\n\n\n‘D’\ndiamond marker\n\n\n‘d’\nthin_diamond marker\n\n\n‘|’\nvline marker\n\n\n’_’\nhline marker\n\n\n\n\n\n\n그 외에 다른 옵션을 보고 싶다면 아래를 참조하라.\n\nother options or colors\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\nhttps://matplotlib.org/2.0.2/examples/color/named_colors.html\nhex code\nhttps://htmlcolorcodes.com/\nother linestyles\nhttps://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html\n\n- preset에 있는 색상 외 다른 색상을 적용\n\nplt.plot(x,y,'--',color = 'lime')\n\n\n\n\n\nusing color name\n\n\nplt.plot(x,y,color = '#751F9B')\n\n\n\n\n\nusing hex code\n\n- 선의 형태를 다양하게 변경\n\nplt.plot(x,y,linestyle = 'dashed')\nplt.show()\n\n\n\n\n\n문자열로 직접 지정\n\n\nplt.plot(x,y,linestyle = (0, (1,1)))\n\n\n\n\n\n파선의 길이를 직접 지정\n\n\n\nplt.plot()에서 scatter plot을 생성\nmarker 옵션을 변경하여 scatter plot을 손쉽게 그릴 수도 있다.\n\nplt.plot(x,y,'db')  ## diamonds, blue\n\n\n\n\n\n\ndot connected plot\n\nplt.plot(x,y,':or')  ## dotline(:), circle(o), red\n\n\n\n\n\n\npile up\nplt.show()를 입력하기 전 계속해서 그래프를 그리면 중첩된다.\n\nplt.plot([1,2,3,2], '--o', color = 'orange')\nplt.plot([2,3,1,4], '--o', color = 'skyblue')\n\nplt.show()\n\nplt.plot([4,4,2,1], '--o', color = 'cyan')\n\nplt.show()\n\n\n\n\n\n\n\n\nplt.plot([1,2,3,2], '--o', color = 'C1')\nplt.plot([2,3,1,4], '--o', color = 'C0')\n\nplt.show()\n\n\n\n\n\n위와 같은 경우에는 color를 지정하지 않을 경우 먼저 입력한 그래프에 C0가 지정된다.\n\n\n\n응용 : scatter plot and line plot\n- 유사 단순선형회귀\n설명변수와 오차, 반응변수를 지정해주자.\n\nx = np.arange(-5,5,0.1)\neps = np.random.randn(100)\ny = 2*x + eps ## 벗어나도록 겹치게\n\n\nplt.plot(x,y,'.b')     ## 실제 데이터\nplt.plot(x,2*x,'--r')  ## 회귀선\nplt.show()\n\n\n\n\n\n\n적합한 그래프를 그릴 때\n- summary: boxplot, histogram, lineplot, scatterplot\n\n라인플랏: 추세\n☆★☆ 스캐터플랏: 두 변수의 관계\n박스플랏: 분포(일상용어)의 비교, 이상치\n히스토그램: 분포(통계용어)파악\n바플랏: 크기비교"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#객체지향적-시각화",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#객체지향적-시각화",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "3. 객체지향적 시각화",
    "text": "3. 객체지향적 시각화\n\nA. 배경지식\n- 그림을 저장해둔 뒤 나중에 꺼내보고 싶다면? | plt.gcf() : Get Current Figure.\n\nplt.plot([1, 2, 3, 2],'--o')\nfig = plt.gcf() ## plt.show()를 하기 전, 현재 표기되는 figure를 얻는다.\n\n\n\n\n\nfig\n\n\n\n\n\n위와 같이 변수에 저장된 것을 알 수 있다.\n\n\n\nB. fig의 해체\nfig\nfig.axes\n\nax = fig.axes[0]\nax.yaxis\nax.xaxis\n\nlines = ax.get_lines()[0]\nlines[0]\n\nfig &gt; 그래프 그 자체\naxes &gt; 그래프의 구역\naxis &gt; x축, y축\nline &gt; 직선형 그래프\n\n등등등…\n아무튼 여러 개체가 나뉘어있다.\n\n개념(비유) : * Figure(fig) : 도화지 * Axes(ax) : 도화지에 존재하는 그림틀 * Axis, Lines : 그림틀 위에 올려지는 물체(object)\n\n\n\nC. plt.plot()없이 그래프 그리기\n\nplt.plot([1,2,4,3], '--o')\nplt.show()\n\n\n\n\n위와 같은 그래프를 plt.plot()없이 만들어보자!\n- 아래의 코드를 하나하나 뜯어보자.\n\nfig = plt.Figure()\n\nax = fig.add_axes([0.125,0.11,0.775,0.77])\nax.set_xlim([-0.15, 3.15])  # setting x axis limit\nax.set_ylim([0.9, 3.1])     # setting y axis limit\nline = matplotlib.lines.Line2D(\n    xdata = [0,1,2,3],\n    ydata = [1,2,3,2],\n    linestyle = '--',\n    marker = 'o'\n)\nax.add_line(line)\n\nfig\n\n\n\n\n1. 최상위 하이라이트(figure) 생성\n\nfig = plt.figure(); fig   ## 최상위 하이라이트인 그림만 만들어냄.\n\n&lt;Figure size 450x300 with 0 Axes&gt;\n\n\n&lt;Figure size 450x300 with 0 Axes&gt;\n\n\n2. 그래프가 들어갈 공간(axes) 생성\n\nax = fig.add_axes([0.125,0.11,0.775,0.77]); fig  ## 가로시작, 세로시작, 종횡비\n\n\n\n\n3. 직선을 지정 후 추가\n\nline = matplotlib.lines.Line2D(\n    xdata = [0,1,2,3],\n    ydata = [1,2,3,2],\n    linestyle = '--',\n    marker = 'o'\n)\n\n\nmatplotlib에서 라인을 만드는 함수가 따로 있었다.\n\n\nax.add_line(line)\n\n&lt;matplotlib.lines.Line2D at 0x1c3c591e380&gt;\n\n\n\nfig\n\n\n\n\n4. 직선이 제대로 표기되지 않는 것 같으니 x축과 y축의 한계를 설정\n\nax.set_xlim([-0.15, 3.15])\nax.set_ylim([0.9, 3.1])\n\nfig\n\n\n\n\n\n\nD. 또 코드의 대체\n1. line2D 오브젝트를 쓰지 않는 방법\n\n## genarally\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.plot([1,2,3,2], '--o')\nfig\n\n\n\n\n\nax.plot()을 사용\n\n2. add_axes()를 쓰지 않는 방법(중요!)\n\nfig = plt.Figure()\nax = fig.subplots(1)\nax.plot([1,2,3,2], '--o')\nfig\n\n\n\n\n\nax = fig.subplots()을 사용\n\n3. fig와 ax들을 한번에 지정(중요!)\n\nfig, ax = plt.subplots(1) ## 중요함\nax.plot([1,2,3,2], '--o')\nplt.show()\n\n\n\n\n\n\nE. 정리 (\\(\\star\\star\\star\\))\n아래의 코드는 모두 같은 애들이었다.\n\nplt.plot([1,2,3,2], '--o')\n\n\nfig, ax = plt.subplots()\nax.plot([1,2,3,2], '--o')\n\n\nfig = plt.Figure()\nax = fig.subplots()\nax.plot([1,2,3,2], '--o')\nfig\n\n\nfig = plt.Figure()\nax = fig.add_axes([0.125, 0.11, 0.775, 0.77])\nax.plot([1,2,3,2], '--o')\nfig\n\nplt.subplots()과 ax.plot()의 경우 상당히 유용한 코드이니 꼭 숙지할 것!"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#미니맵과-서브플롯",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#미니맵과-서브플롯",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "4. 미니맵과 서브플롯",
    "text": "4. 미니맵과 서브플롯\n\nA. 미니맵\nfig.add_axes()를 사용한다.\n\nfig = plt.Figure()\nax = fig.add_axes([0,0,2,2]); fig\n\n\n\n\n\nax_mini = fig.add_axes([1.4,0.2,0.5,0.5])  ## 가로 세로 위치(중심위치), 종횡비\nax.plot([1,5,3,4], '--o')\nax_mini.plot([1,2,3,1], '--or')\n\nfig\n\n\n\n\n\n생성된 fig에 axes를 하나 더 추가하여 만들어냈다.\n\n\n\nB. 서브플롯\nplt.subplots(), fig.subplots()을 이용해보자.\n\nfig, axs = plt.subplots(2)  ## 2행\n\n\n\n\n\naxs\n\narray([&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;], dtype=object)\n\n\n\naxs에 ax들이 array형태로 저장되어 있다.\n\n\naxs[0].plot([1,2,3,2], '--r')\naxs[1].plot([1,2,4,3], '--o')\n\nfig\n\n\n\n\n\n뭔가 레이아웃이 가려져있고 이상하다.\n\n\nfig.tight_layout(); fig\n\n\n\n\n\n왠만해선 fig.tight_layout()을 해주도록 하자.\n\n\n차피 axs가 array 형태로 저장되므로 그것을 따로 지정해주고 싶다면 아래와 같이 사용하는 것을 권장한다.\n\n\nfig, (ax1, ax2) = plt.subplots(2)\nax1.plot([1,2,3,2], '--r')\nax2.plot([1,2,4,3], '--o')\nfig.tight_layout()\n\n\n\n\n\n\nC. 서브플롯 스케일 조정 및 다중화\n- 스케일 변경\n\nfig, (ax1, ax2) = plt.subplots(2, figsize = (3,3))  ## 종횡비\nax1.plot([1,2,3,2], '--r')\nax2.plot([1,2,4,3], '--o')\nfig.tight_layout()\n\n\n\n\n\n미리 설정해줬던 dpi에 의거하여 종횡비가 배수로 적용된다.\n\n- 더 많은 서브플롯 생성\n\nfig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2,2, figsize = (3,3))\nax1.plot([1,2,4,3], 'o', color = 'C0')\nax2.plot([1,2,4,3], 'o', color = 'C1')\nax3.plot([1,2,4,3], 'o', color = 'C2')\nax4.plot([1,2,4,3], 'o', color = 'C3')\nfig.tight_layout()\n\n\n\n\n- 사용자 정의 서브플롯 생성\nplt.subplot() ## s가 없는 subplot(), 즉, 하나만 만들어진다.\n\nplt.figure(figsize=(3,3))\nplt.subplot(2,2,1)  ## 2×2의 1\nplt.plot([1,2,4,3],'o', color='C0')\nplt.subplot(1,2,2)\nplt.plot([1,2,4,3],'o', color='C1')\nplt.subplot(2,2,3)\nplt.plot([1,2,4,3],'o', color='C2')\nplt.tight_layout()\n\nfig = plt.gcf()\n\n\n\n\n\n이미 생성된 figure의 크기를 조정\n\n\nfig.set_size_inches(2,2); fig"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#title",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#title",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "5. title",
    "text": "5. title\n\ntitle을 만드는 함수는 어떤 오브젝트에 소속되는 게 좋을까? 1. plt -&gt; subplot의 제목을 설정 가능 2. fig -&gt; 전체제목(super title)을 설정할 수 있음 3. ax -&gt; subplot들의 제목을 설정할 수 있음\n\n\nA. plt.title()\nfigure를 생성하지 않은 기본적인 환경에서 타이틀을 달아준다.\n\n## 가장 평범한 플롯\nplt.plot([1,2,3,2])\nplt.title('asdf')\nplt.show()\n\n\n\n\n\n\nB. ax.set_title()\nfigure와 axes를 생성했을 경우, 각 ax마다 타이틀을 달아줄 수 있다.\n\n## title이 axes에 존재\nfig, ax = plt.subplots()\nax.set_title('asdf')\nax.plot([1,2,3,2])\n\nplt.show()\n\n\n\n\n\n\nC. fig.suptitle() | 권장하지 않는 방법\n원래 figure 자체에 타이틀을 붙이는 것은 불가능하다.\n\n##--------fig : 원래는 불가능--------\nplt.plot([1,2,3,2])\nfig = plt.gcf()\nfig.suptitle('asdf')\n\nplt.show()\n\n\n\n\n\n\nD. 응용\n\nplt.subplots()과 set_title()을 이용\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (4,2))\nax1.set_title('asdf')\nax2.set_title('1234')\nax1.plot([1,2,3,2])\nax2.plot([1,2,3,2])\nfig.tight_layout()\n\n\n\n\n\nfigure를 생성하지 않고 plt.subplot()과 plt.title()을 이용하여 손수 지정\n\n\nplt.subplot(1,2,1)\nplt.plot([1,2,3])\nplt.title('asdf')\nplt.subplot(1,2,2)\nplt.plot([1,2,3])\nplt.title('1234')\nplt.tight_layout()\n\n\n\n\n\nfig.suptitle()을 이용한 방법\n\n\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.set_title('asdf')\nax2.set_title('1234')\nfig.suptitle('asdf1234')\nfig.tight_layout()\n\n\n\n\n\n\nE. plt.gca()\nplt.gca()를 통해 ax개체를 다룰 수도 있다.\n\nplt.plot([1,2,3,2])\nax = plt.gca()\nax.set_title('asdf')  ## 현재의 axis에 바로 타이틀을 설정해준다.\n\nText(0.5, 1.0, 'asdf')"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#산점도의-응용-표본상관계수",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#산점도의-응용-표본상관계수",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "6. 산점도의 응용 | 표본상관계수",
    "text": "6. 산점도의 응용 | 표본상관계수\n\nA. 산점도와 표본상관계수\n아래처럼 두 연속형 자료가 주어질 경우 산점도로 나타낼 수 있다.\n\nweight = [44,48,49,58,62,68,69,70,76,79]\nheight = [159,160,162,165,167,162,165,175,165,172]\n\nplt.plot(weight,height,'.')  ## option : '.' marker가 .인 산점도 산출\nplt.show()\n\n\n\n\n아래 표본상관계수의 정의에 따라 데이터에서의 표본상관계수를 구해보자.\n- (표본)상관계수의 정의\n\\[r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2 }}=\\sum_{i=1}^{n}\\tilde{x}_i\\tilde{y}_i \\]\n\\[단,~\\tilde{x}_i=\\frac{(x_i-\\bar{x})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}},~ \\tilde{y}_i=\\frac{(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}}\\]\n\n위 식에서 \\(\\tilde{x}_i\\)와 \\(\\tilde{y}_i\\)는 \\(x_i\\)와 \\(y_i\\)를 표준화한 것이다.\n\n(데이터를 불러오자)\n\nx=[44,48,49,58,62,68,69,70,76,79]\ny=[159,160,162,165,167,162,165,175,165,172]\n\n(평균을 0으로)\n\nxx = x - np.mean(x); print(xx)\nyy = y - np.mean(y); print(yy)\n\n[-18.3 -14.3 -13.3  -4.3  -0.3   5.7   6.7   7.7  13.7  16.7]\n[-6.2 -5.2 -3.2 -0.2  1.8 -3.2 -0.2  9.8 -0.2  6.8]\n\n\n(퍼진 정도를 표준화)\n\nx_standard = xx/np.sqrt(np.sum(xx**2))\ny_standard = yy/np.sqrt(np.sum(yy**2))\n\n(표본상관계수 산출)\n\nnp.sum(x_standard*y_standard)\n\n0.7138620583559141\n\n\n\n이미 정의된 코드를 통해 해당 결과가 맞는지 확인해보자.\n\n\nnp.corrcoef(x,y)\n\narray([[1.        , 0.71386206],\n       [0.71386206, 1.        ]])"
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#b.-산점도를-보고-상관계수의-부호를-해석",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#b.-산점도를-보고-상관계수의-부호를-해석",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "### B. 산점도를 보고 상관계수의 부호를 해석",
    "text": "### B. 산점도를 보고 상관계수의 부호를 해석\n- 아래의 그림은 상관계수 r의 값이 양수인가 음수인가?\n\nx=[44,48,49,58,62,68,69,70,76,79]\ny=[159,160,162,165,167,162,165,175,165,172]\n\nplt.plot(x, y, 'o')\nplt.show()\n\n\n\n\n\nxx = x-np.mean(x)\nyy = y-np.mean(y) \nxxx = xx/np.sqrt(np.sum(xx**2))\nyyy = yy/np.sqrt(np.sum(yy**2))\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (10,3))\nax1.plot(x,y, 'o')\nax1.set_title(r'$(x_i,y_i)$')\nax2.plot(xx,yy,'o') ## mean to 0\nax2.set_title(r'$(x_i-\\bar{x}, y_i-\\bar{y})$')\nax3.plot(xxx,yyy,'o') ## standarized\nax3.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\n\nplt.show()\n\n\n\n\n\n마지막 \\(\\tilde{x}_i\\), \\(\\tilde{y}_i\\)를 곱한 값이 양수인 것과 음수인 것을 체크해보자.\n\n\n1,3사분면에 점들이 많으므로 상관계수의 부호는 양수일 것이다."
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#d.-산점도를-보고-상관계수의-절대값을-해석",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#d.-산점도를-보고-상관계수의-절대값을-해석",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "### D. 산점도를 보고 상관계수의 절대값을 해석",
    "text": "### D. 산점도를 보고 상관계수의 절대값을 해석\n- 기울기가 동일하지만 직선 근처의 퍼짐이 다른 두 개의 자료\n\nx=np.arange(0,10,0.1)\ny1=x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## N(0,1)\ny2=x+np.random.normal(loc=0,scale=7.0,size=len(x))  ## N(0,7)\n\nplt.plot(x,y1,'.')\nplt.plot(x,y2,'x')\nplt.show()\n\n\n\n\n\n표준화하는 함수 tilde() 정의\n\n\ndef tilde(x):\n    xx = x-np.mean(x)\n    xxx = xx / np.sqrt(np.sum(xx**2))\n    return xxx\n\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (4,2))\nax1.plot(x,y1,'.'); ax1.plot(x,y2,'x'); ax1.set_title(r'$(x_i,y_i)$')\nax2.plot(tilde(x), tilde(y1),'.'); ax2.plot(tilde(x), tilde(y2), 'x'); ax2.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\nfig.tight_layout()\n\n\n\n\n- 직선 근처의 퍼짐은 동일하지만, 직선의 기울기가 다른 경우\n\nx=np.arange(0,10,0.1)\ny1=x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## 기울기가 1\ny2=0.2*x+np.random.normal(loc=0,scale=1.0,size=len(x))  ## 기울기가 0.2\n\nplt.plot(x,y1,'.')\nplt.plot(x,y2,'x')\n\nplt.show()\n\n\n\n\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(4,2))\nax1.plot(x,y1,'.'); ax1.plot(x,y2,'x'); ax1.set_title(r'$(x_i,y_i)$')\nax2.plot(tilde(x),tilde(y1),'.'); ax2.plot(tilde(x),tilde(y2),'x'); ax2.set_title(r'$(\\tilde{x}_i,\\tilde{y}_i)$')\nfig.tight_layout()\n\n\n\n\n기울기가 클수록, 퍼짐 정도가 작을수록 상관계수의 절댓값이 높다."
  },
  {
    "objectID": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#산점도-응용예제2---앤스콤의-4분할",
    "href": "2023_DV/Review/A1. 꺾은선, 산점도, 객체지향화.html#산점도-응용예제2---앤스콤의-4분할",
    "title": "Plot | 꺾은선, 산점도, 객체지향화",
    "section": "7. 산점도 응용예제2 - 앤스콤의 4분할",
    "text": "7. 산점도 응용예제2 - 앤스콤의 4분할\n- 표본상관계수가 모두 동일한 네 자료를 보라.\n\nx1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n\nx2 = x1\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n\nx3 = x1\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\nnp.corrcoef(x1,y1),np.corrcoef(x2,y2),np.corrcoef(x3,y3),np.corrcoef(x4,y4)\n\n(array([[1.        , 0.81642052],\n        [0.81642052, 1.        ]]),\n array([[1.        , 0.81623651],\n        [0.81623651, 1.        ]]),\n array([[1.        , 0.81628674],\n        [0.81628674, 1.        ]]),\n array([[1.        , 0.81652144],\n        [0.81652144, 1.        ]]))\n\n\n\n음, 다 비슷한 자료겠구나… 양의 상관관계를 띄겠네?\n\n라고 속단하긴 이르다.\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(6,4))\nax1.plot(x1,y1,'o'); ax1.set_title(f'corrcoef = {np.corrcoef(x1,y1)[0,1] : .6f}')\nax2.plot(x2,y2,'o'); ax2.set_title(f'corrcoef = {np.corrcoef(x2,y2)[0,1] : .6f}')\nax3.plot(x3,y3,'o'); ax3.set_title(f'corrcoef = {np.corrcoef(x3,y3)[0,1] : .6f}')\nax4.plot(x4,y4,'o'); ax4.set_title(f'corrcoef = {np.corrcoef(x4,y4)[0,1] : .6f}')\nfig.tight_layout()\n\n\n\n\n4개의 그림은 모두 같은 상관계수를 가지나, 그 느낌이 전혀 다르다.\n- 앤스콤플랏의 4개의 그림은 모두 같은 상관계수를 가진다. 하지만, 4개의 그림은 느낌이 전혀 다르다.\n- 같은 표본상관계수를 가진다고 하여 같은 관계성을 가지는 것은 아니다. 표본상관계수는 x,y의 비례정도를 측정하는데 그 값이 1에 가깝다고 하여 꼭 정비례의 관계가 있음을 의미하는 건 아니다.\n\\((x_i,y_i)\\)의 산점도가 선형성을 보일 때만 “표본상관계수가 1에 가까우므로 정비례의 관계에 있다”라는 논리전개가 성립한다.\n\n앤스콤의 첫번째 플랏 : 산점도가 선형 -&gt; 표본상관계수가 0.816 = 정비례의 관계가 0.816 정도\n앤스콤의 두번째 플랏 : 산점도가 선형이 아님 -&gt; 표본상관계수가 크게 의미없음.\n앤스콤의 세번째 플랏 : 산점도가 선형인듯 보이나 하나의 이상치가 있음 -&gt; 하나의 이상치가 표본상관계수의 값을 무너뜨릴 수 있으므로 표본상관계수 값을 신뢰할 수 없음.\n앤스콤의 네번째 플랏 : 산점도를 그려보니 이상한 그림 -&gt; 표본상관계수를 계산할 수는 있으나, 그게 무슨 의미가 있을까?\n\n산점도가 선형성을 보일 때만 표본상관계수가 1에 가까우므로 정비례의 관계에 있다라는 논리전개가 성립한다.\n\n1번만 의미가 있음. 3번의 경우 이상치가 존재하여 신뢰할 수 없음.\n\n\n교훈\n상관계수를 해석하기에 앞서서 산점도가 선형성을 보이는 지 체크할 것! 항상 통계량은 적절한 가정하에서만 말이 된다는 사실을 기억할 것!"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "",
    "text": "데이터프레임 df의 열이름에 actor라는 단어가 포함된 column만을 선택하는 코드를 작성하라"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#사전작업",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#사전작업",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 설치\n\n\nimport pandas as pd\nimport numpy as np\n\n\n데이터 불러오기 및 확인\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\n...\nnum_user_for_reviews\nlanguage\ncountry\ncontent_rating\nbudget\ntitle_year\nactor_2_facebook_likes\nimdb_score\naspect_ratio\nmovie_facebook_likes\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n...\n3054.0\nEnglish\nUSA\nPG-13\n237000000.0\n2009.0\n936.0\n7.9\n1.78\n33000\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n...\n1238.0\nEnglish\nUSA\nPG-13\n300000000.0\n2007.0\n5000.0\n7.1\n2.35\n0\n\n\n2\nColor\nSam Mendes\n602.0\n148.0\n0.0\n161.0\nRory Kinnear\n11000.0\n200074175.0\nAction|Adventure|Thriller\n...\n994.0\nEnglish\nUK\nPG-13\n245000000.0\n2015.0\n393.0\n6.8\n2.35\n85000\n\n\n3\nColor\nChristopher Nolan\n813.0\n164.0\n22000.0\n23000.0\nChristian Bale\n27000.0\n448130642.0\nAction|Thriller\n...\n2701.0\nEnglish\nUSA\nPG-13\n250000000.0\n2012.0\n23000.0\n8.5\n2.35\n164000\n\n\n4\nNaN\nDoug Walker\nNaN\nNaN\n131.0\nNaN\nRob Walker\n131.0\nNaN\nDocumentary\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n7.1\nNaN\n0\n\n\n\n\n5 rows × 28 columns\n\n\n\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n열 이름에서 단어의 구분이 모두 '_'로 되어있으므로, 열이름에 split()함수를 적용시킬 수 있을 것 같다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#풀이",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#풀이",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "2. 풀이",
    "text": "2. 풀이\n\nfor 문을 이용하여 풀이해보자.\n\n\n['actor' in i.split('_') for i in df.columns]\n\n[False,\n False,\n False,\n False,\n False,\n True,\n True,\n True,\n False,\n False,\n True,\n False,\n False,\n False,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n False,\n False,\n False]"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#결과",
    "href": "2023_DV/Solution Assemble/특정 단어를 포함하는 열 선택.html#결과",
    "title": "[문제 풀이] 특정 단어를 포함하는 열 선택",
    "section": "3. 결과",
    "text": "3. 결과\n\ndf.loc[:, ['actor' in i.split('_') for i in df.columns]]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n완료"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html",
    "href": "2023_MP/Titanic/A2. autogluon.html",
    "title": "Kaggle | Autogluon",
    "section": "",
    "text": "자동 예측 프로그램인 Autogluon을 활용하여 titanic data를 적합해보자!"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#라이브러리-imports",
    "href": "2023_MP/Titanic/A2. autogluon.html#라이브러리-imports",
    "title": "Kaggle | Autogluon",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#pip install autogluon\n\n\nimport pandas as pd\nimport numpy as np\n\n## tabular(테이블) 형식의 데이터를 다루는 모듈을 다운로드한다.\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#분석",
    "href": "2023_MP/Titanic/A2. autogluon.html#분석",
    "title": "Kaggle | Autogluon",
    "section": "2. 분석",
    "text": "2. 분석\n\nA. 데이터 입력\n\n\n문제를 받아오는 과정으로 비유할 수 있다.\n\n\ntr = TabularDataset('./data/train.csv')  ## 학습할 데이터\ntst = TabularDataset('./data/test.csv')\n\n## tr = TabularDataset('/kaggle/input/titanic/train.csv')  ## 학습할 데이터\n## tst = TabularDataset('/kaggle/input/titanic/test.csv')\n\n## tr = pd.read_csv('/kaggle/input/titanic/train.csv')\n## tst"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#b.-predictor-생성",
    "href": "2023_MP/Titanic/A2. autogluon.html#b.-predictor-생성",
    "title": "Kaggle | Autogluon",
    "section": "### B. Predictor 생성",
    "text": "### B. Predictor 생성\n\n문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\n\npredictr = TabularPredictor('Survived') ## target variable이 들어있는 데이터프레임, 변수 철자는 임의로 틀리게 설정\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_130536\"\n\n\n\npredictr는 뭔데?\n\n\ntype(predictr)\n\nautogluon.tabular.predictor.predictor.TabularPredictor\n\n\n\n대충 autogluon에서의 class인듯.\n\n\nC. 적합(fit)\n\n\n학습 과정에 해당한다.\n\n\npredictr.fit(tr) ## 학생(predictr)에게 문제(tr)를 주어 학습을 시킴(predictr.fit(tr))\n##tr 그 자체로 학습할 수 있는 건 다 시킨다. sklearn의 모델과는 차이가 있음\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_130536\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.71 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1930.98 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.3s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.36s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    1.83s    = Training   runtime\n    0.22s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    1.08s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    7.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8324   = Validation score   (accuracy)\n    3.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    1.32s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8212   = Validation score   (accuracy)\n    7.74s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.93s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.67s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 28.23s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_130536\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20df4525d50&gt;\n\n\n학습 완료, 이에 따라 리더보드를 확인한다. (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.009000  0.928348                0.009000           0.928348            1       True         13\n1       NeuralNetFastAI   0.832402       0.028001  3.279027                0.028001           3.279027            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.029001  3.949707                0.001000           0.670681            2       True         14\n3              CatBoost   0.826816       0.006002  7.468165                0.006002           7.468165            1       True          7\n4              LightGBM   0.821229       0.006004  0.432719                0.006004           0.432719            1       True          4\n5        NeuralNetTorch   0.821229       0.031999  7.740229                0.031999           7.740229            1       True         12\n6            LightGBMXT   0.815642       0.005002  1.084200                0.005002           1.084200            1       True          3\n7        ExtraTreesGini   0.815642       0.061763  0.516426                0.061763           0.516426            1       True          8\n8      RandomForestEntr   0.815642       0.064586  0.538568                0.064586           0.538568            1       True          6\n9      RandomForestGini   0.815642       0.064718  0.637898                0.064718           0.637898            1       True          5\n10              XGBoost   0.810056       0.013002  1.323482                0.013002           1.323482            1       True         11\n11       ExtraTreesEntr   0.810056       0.062742  0.519159                0.062742           0.519159            1       True          9\n12       KNeighborsDist   0.653631       0.005996  0.012003                0.005996           0.012003            1       True          2\n13       KNeighborsUnif   0.653631       0.215770  1.826697                0.215770           1.826697            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.009000\n0.928348\n0.009000\n0.928348\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.028001\n3.279027\n0.028001\n3.279027\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.029001\n3.949707\n0.001000\n0.670681\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.006002\n7.468165\n0.006002\n7.468165\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.006004\n0.432719\n0.006004\n0.432719\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.031999\n7.740229\n0.031999\n7.740229\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.005002\n1.084200\n0.005002\n1.084200\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.061763\n0.516426\n0.061763\n0.516426\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.064586\n0.538568\n0.064586\n0.538568\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.064718\n0.637898\n0.064718\n0.637898\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.013002\n1.323482\n0.013002\n1.323482\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.062742\n0.519159\n0.062742\n0.519159\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.005996\n0.012003\n0.005996\n0.012003\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.215770\n1.826697\n0.215770\n1.826697\n1\nTrue\n1\n\n\n\n\n\n\n\nscore_val이 의미하는 것 * 실제로 predictr가 학습한 것은? &gt; predictor와 train set이 있고, train set에 데이터가 1000개 있다고 하면 해당 데이터를 전부 가용하지 않는다. &gt; * 800개를 사용한다고 하면 200개는 학습하지 않고 답을 맞춰 보는 식이다. &gt; &gt; * 200개는 왜 남겨두지? &gt; &gt; 문제에서 답을 찾는 규칙이 맞는지, 다른 데이터들에 대해서도 일반화시킬 수 있는 지 테스트 해보면 좋을 것 같다. 따라서 나머지 데이터셋에서 분석을 해본다. &gt; &gt; 실제 테스트에서 잘하기 위한 자체적 테스트셋에 해당, 200개의 나머지 테스트용 데이터셋을 validation set이라 일컫는다.\n\n\n\n\ntrain\nval\n\n\n\n\n학생1\n95%\n72%\n\n\n학생2\n80%\n80%\n\n\n…\n…\n…\n\n\n\ntrain(연습문제)만 계속 푼 것 보다, val(모의고사)에서 가장 높은 점수를 받은 것이 유의미할 것.\n\n그러니까 score_val는 모의고사 점수라고 보면 된다.\n\n- 따라서 가장 높은 점수를 받은 WeightedEnsemble_L2모델을 사용해보자.[1]\n\n[1] 처음 실습할 땐 분명 이게 제일 높았었는데…"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#d.-예측predict",
    "href": "2023_MP/Titanic/A2. autogluon.html#d.-예측predict",
    "title": "Kaggle | Autogluon",
    "section": "### D. 예측(predict)",
    "text": "### D. 예측(predict)\n\n학습 이후에 문제를 푸는 과정으로 비유.\n\n기존에 했던 분석들\n\n무조건 남자는 죽고, 여자는 사는 형식 0.7x / 0.76555\nRandomForestClassifier를 사용한 형식 0.8x / 0.77511\nRandomForestClassifier에서 하이퍼파라미터를 조정한 형식 0.8x / 0.76555 (트레인 셋에서의 분석에서는 더 높았는데 실제 결과는 오히려 더 낮았다.)\n\n4. WeightedEnsemble_L2모델 사용(알아서 사용하긴 함)\ntrain set을 일단 풀어보자(predict)\n\ntype(tr) ## 처음 보는 것으로 저장되는데 데이터프레임에서 쓸 수 있는 모든 기능들을 다 사용할 수 있다.\n\nautogluon.core.dataset.TabularDataset\n\n\n\ntr.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.8810325476992144\n\n\n\n정확도가 0.9349나 된다. 상당히 기대가 되는 부분\n\n\npredictr.predict(tst)\n\n0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n413    0\n414    1\n415    0\n416    0\n417    0\nName: Survived, Length: 418, dtype: int64\n\n\n\ntst.assign(Survived = predictr.predict(tst)).loc[:, ['PassengerId', 'Survived']]\\\n.to_csv('autogluon_submission.csv', index = False)\n\n\n제출 결과 정확도는 0.78947로 지금껏 가장 높은 수치가 나왔다."
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#개선",
    "href": "2023_MP/Titanic/A2. autogluon.html#개선",
    "title": "Kaggle | Autogluon",
    "section": "3. 개선",
    "text": "3. 개선\n\n결과를 좀 더 개선할 수 있지 않을까?\n\n\nA. Fsize로 feature engeenering\n\n1) 데이터\n\ntr = TabularDataset('./data/train.csv')  ## 학습할 데이터\ntst = TabularDataset('./data/test.csv')\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./data/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n-피쳐 엔지니어링\n\ntr.assign(Fsize = tr.SibSp + tr.Parch)\ntst.assign(Fsize = tst.SibSp + tst.Parch)\n\n#tr.eval('Fsize = SibSp + Parch')\n#tst.eval('Fsize = SibSp + Parch')\n\ntr.head()  ## 원본 데이터를 손상시키지 않음, Fsize 열이 추가되지 않은 것을 알 수 있음\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n2) Predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132447\"\n\n\n3) 적합(fit)\n\npredictr.fit(tr.assign(Fsize = tr.SibSp + tr.Parch))  ## 새로운 데이터셋을 추가하여 학습\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132447\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.59 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1923.41 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.3s = Fit runtime\n    12 features in original data used to generate 25 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.37s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6425   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8268   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8492   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.7989   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    6.8s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8045   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    2.76s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.68s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    9.58s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.838    = Validation score   (accuracy)\n    0.83s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8492   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 25.25s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132447\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d8e852770&gt;\n\n\n-리더보드 확인\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0              LightGBM   0.849162       0.010999  0.554687                0.010999           0.554687            1       True          4\n1   WeightedEnsemble_L2   0.849162       0.012000  1.201853                0.001000           0.647166            2       True         14\n2         LightGBMLarge   0.837989       0.005001  0.827996                0.005001           0.827996            1       True         13\n3       NeuralNetFastAI   0.832402       0.024005  2.761039                0.024005           2.761039            1       True         10\n4        NeuralNetTorch   0.832402       0.034000  9.577353                0.034000           9.577353            1       True         12\n5            LightGBMXT   0.826816       0.004981  0.426716                0.004981           0.426716            1       True          3\n6              CatBoost   0.826816       0.005996  6.798872                0.005996           6.798872            1       True          7\n7               XGBoost   0.821229       0.008010  0.680577                0.008010           0.680577            1       True         11\n8      RandomForestEntr   0.815642       0.063459  0.504724                0.063459           0.504724            1       True          6\n9        ExtraTreesEntr   0.804469       0.062692  0.443597                0.062692           0.443597            1       True          9\n10       ExtraTreesGini   0.804469       0.065061  0.448723                0.065061           0.448723            1       True          8\n11     RandomForestGini   0.798883       0.064575  0.510397                0.064575           0.510397            1       True          5\n12       KNeighborsUnif   0.648045       0.007998  0.008998                0.007998           0.008998            1       True          1\n13       KNeighborsDist   0.642458       0.007999  0.011001                0.007999           0.011001            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBM\n0.849162\n0.010999\n0.554687\n0.010999\n0.554687\n1\nTrue\n4\n\n\n1\nWeightedEnsemble_L2\n0.849162\n0.012000\n1.201853\n0.001000\n0.647166\n2\nTrue\n14\n\n\n2\nLightGBMLarge\n0.837989\n0.005001\n0.827996\n0.005001\n0.827996\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.024005\n2.761039\n0.024005\n2.761039\n1\nTrue\n10\n\n\n4\nNeuralNetTorch\n0.832402\n0.034000\n9.577353\n0.034000\n9.577353\n1\nTrue\n12\n\n\n5\nLightGBMXT\n0.826816\n0.004981\n0.426716\n0.004981\n0.426716\n1\nTrue\n3\n\n\n6\nCatBoost\n0.826816\n0.005996\n6.798872\n0.005996\n6.798872\n1\nTrue\n7\n\n\n7\nXGBoost\n0.821229\n0.008010\n0.680577\n0.008010\n0.680577\n1\nTrue\n11\n\n\n8\nRandomForestEntr\n0.815642\n0.063459\n0.504724\n0.063459\n0.504724\n1\nTrue\n6\n\n\n9\nExtraTreesEntr\n0.804469\n0.062692\n0.443597\n0.062692\n0.443597\n1\nTrue\n9\n\n\n10\nExtraTreesGini\n0.804469\n0.065061\n0.448723\n0.065061\n0.448723\n1\nTrue\n8\n\n\n11\nRandomForestGini\n0.798883\n0.064575\n0.510397\n0.064575\n0.510397\n1\nTrue\n5\n\n\n12\nKNeighborsUnif\n0.648045\n0.007998\n0.008998\n0.007998\n0.008998\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.642458\n0.007999\n0.011001\n0.007999\n0.011001\n1\nTrue\n2\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(tr.Survived == predictr.predict(tr.assign(Fsize = tr.SibSp + tr.Parch))).mean()\n\n0.9696969696969697\n\n\n\ntst.assign(Survived = predictr.predict(tst.assign(Fsize = tst.SibSp + tst.Parch))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)\n\n\n제출 결과 : 점수가 오히려 더 낮아졌음\n\n더 개선해보자"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#b.-fsize-drop",
    "href": "2023_MP/Titanic/A2. autogluon.html#b.-fsize-drop",
    "title": "Kaggle | Autogluon",
    "section": "### B. Fsize + drop",
    "text": "### B. Fsize + drop\n1) data\n-피처 엔지니어링 (데이터 불러오는 건 위에서 했으니 일단 생략\n\n_tr = tr.assign(Fsize = lambda _df : _df.SibSp + _df.Parch).drop(['SibSp','Parch'],axis=1)\n_tst = tst.assign(Fsize = tst.SibSp + tst.Parch).drop(['SibSp','Parch'],axis=1)\n\n_tr.head()\n## df.drop(columns = [])\n## df.drop([], axis = 1) columns라고 지정해주지 않으면 디폴트로 행을 삭제하기 때문에\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\nA/5 21171\n7.2500\nNaN\nS\n1\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n0\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n2) Predictor 생성\n\npredictr = TabularPredictor('Survived')\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132627\"\n\n\n3) 적합(fit)\n\npredictr.fit(_tr)\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132627\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.56 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1899.15 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.3s = Fit runtime\n    10 features in original data used to generate 23 features in processed data.\n    Train Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.36s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMXT ...\n    0.8212   = Validation score   (accuracy)\n    0.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.838    = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini ...\n    0.8045   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost ...\n    0.8324   = Validation score   (accuracy)\n    7.6s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.7989   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8045   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.95s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.45s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8436   = Validation score   (accuracy)\n    10.87s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.82s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8492   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 26.66s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132627\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d858cd060&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.849162       0.043977  12.163954                0.000984           0.653803            2       True         14\n1        NeuralNetTorch   0.843575       0.032010  10.867509                0.032010          10.867509            1       True         12\n2              LightGBM   0.837989       0.010982   0.642641                0.010982           0.642641            1       True          4\n3         LightGBMLarge   0.832402       0.006009   0.821788                0.006009           0.821788            1       True         13\n4              CatBoost   0.832402       0.006051   7.597862                0.006051           7.597862            1       True          7\n5               XGBoost   0.826816       0.013022   0.450137                0.013022           0.450137            1       True         11\n6       NeuralNetFastAI   0.826816       0.017003   1.949074                0.017003           1.949074            1       True         10\n7            LightGBMXT   0.821229       0.006997   0.471555                0.006997           0.471555            1       True          3\n8      RandomForestEntr   0.810056       0.063482   0.526611                0.063482           0.526611            1       True          6\n9      RandomForestGini   0.804469       0.061717   0.544051                0.061717           0.544051            1       True          5\n10       ExtraTreesEntr   0.804469       0.064033   0.519959                0.064033           0.519959            1       True          9\n11       ExtraTreesGini   0.798883       0.064803   0.533057                0.064803           0.533057            1       True          8\n12       KNeighborsUnif   0.653631       0.006997   0.011003                0.006997           0.011003            1       True          1\n13       KNeighborsDist   0.648045       0.031997   0.016009                0.031997           0.016009            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.849162\n0.043977\n12.163954\n0.000984\n0.653803\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.843575\n0.032010\n10.867509\n0.032010\n10.867509\n1\nTrue\n12\n\n\n2\nLightGBM\n0.837989\n0.010982\n0.642641\n0.010982\n0.642641\n1\nTrue\n4\n\n\n3\nLightGBMLarge\n0.832402\n0.006009\n0.821788\n0.006009\n0.821788\n1\nTrue\n13\n\n\n4\nCatBoost\n0.832402\n0.006051\n7.597862\n0.006051\n7.597862\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.013022\n0.450137\n0.013022\n0.450137\n1\nTrue\n11\n\n\n6\nNeuralNetFastAI\n0.826816\n0.017003\n1.949074\n0.017003\n1.949074\n1\nTrue\n10\n\n\n7\nLightGBMXT\n0.821229\n0.006997\n0.471555\n0.006997\n0.471555\n1\nTrue\n3\n\n\n8\nRandomForestEntr\n0.810056\n0.063482\n0.526611\n0.063482\n0.526611\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.804469\n0.061717\n0.544051\n0.061717\n0.544051\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.804469\n0.064033\n0.519959\n0.064033\n0.519959\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.798883\n0.064803\n0.533057\n0.064803\n0.533057\n1\nTrue\n8\n\n\n12\nKNeighborsUnif\n0.653631\n0.006997\n0.011003\n0.006997\n0.011003\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.648045\n0.031997\n0.016009\n0.031997\n0.016009\n1\nTrue\n2\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(_tr.Survived == predictr.predict(_tr)).mean()\n\n0.9472502805836139\n\n\n\npredictr.predict(_tr)\n\n0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n886    0\n887    1\n888    0\n889    1\n890    0\nName: Survived, Length: 891, dtype: int64\n\n\n\n_tst.assign(Survived = predictr.predict(_tst)).loc[:, ['PassengerId', 'Survived']]\\\n.to_csv('autogluon(Fsize,Drop)_submission.csv', index = False)\n\n\n지금껏 가장 높은 결과가 나왔다!\n\n\n다중 공선성 문제를 개선한 결과라고 볼 수 있지… 음음.\n\n아니, 모자라. 더 개선해!!!"
  },
  {
    "objectID": "2023_MP/Titanic/A2. autogluon.html#c.-best_quality",
    "href": "2023_MP/Titanic/A2. autogluon.html#c.-best_quality",
    "title": "Kaggle | Autogluon",
    "section": "### C. best_quality",
    "text": "### C. best_quality\n1) data\n\ntr = TabularDataset(\"./data/train.csv\")\ntst = TabularDataset(\"./data/test.csv\")\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./data/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n2) predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231017_132948\"\n\n\n3) 적합(fit)\n\n어떤 자원이 들어가든, 전부 지원해줄 테니 가장 좋은 퀄리티로 산출해!!\n\n\npredictr.fit(tr, presets = 'best_quality') \n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels\\ag-20231017_132948\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.13\nOperating System:   Windows\nPlatform Machine:   AMD64\nPlatform Version:   10.0.19045\nDisk Space Avail:   57.53 GB / 255.01 GB (22.6%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    1996.57 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n        Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n        Reducing Vectorizer vocab size from 8 to 4 to avoid OOM error\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 5 | ['__nlp__.miss', '__nlp__.mr', '__nlp__.mrs', '__nlp__.william', '__nlp__._total_']\n    0.4s = Fit runtime\n    11 features in original data used to generate 24 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.39s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6296   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6352   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install ray==2.6.3`\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    3.82s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8373   = Validation score   (accuracy)\n    5.36s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8339   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8305   = Validation score   (accuracy)\n    0.54s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8552   = Validation score   (accuracy)\n    72.17s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8238   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8316   = Validation score   (accuracy)\n    0.49s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\nNo improvement since epoch 7: early stopping\nNo improvement since epoch 6: early stopping\nNo improvement since epoch 7: early stopping\n    0.853    = Validation score   (accuracy)\n    20.42s   = Training   runtime\n    0.13s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8373   = Validation score   (accuracy)\n    3.6s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8462   = Validation score   (accuracy)\n    68.5s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.8429   = Validation score   (accuracy)\n    8.68s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8552   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 188.35s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231017_132948\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x20d90435000&gt;\n\n\n\n대신 시간이 상당히 오래 걸린다…\n\n- 리더보드 확인\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.855219       0.036927  72.167391                0.036927          72.167391            1       True          7\n1       WeightedEnsemble_L2   0.855219       0.038929  73.009209                0.002002           0.841818            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.130997  20.415231                0.130997          20.415231            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.194014  68.497755                0.194014          68.497755            1       True         12\n4      LightGBMLarge_BAG_L1   0.842873       0.056998   8.680638                0.056998           8.680638            1       True         13\n5            XGBoost_BAG_L1   0.837262       0.055978   3.598592                0.055978           3.598592            1       True         11\n6           LightGBM_BAG_L1   0.837262       0.061885   5.357185                0.061885           5.357185            1       True          4\n7         LightGBMXT_BAG_L1   0.835017       0.049997   3.816595                0.049997           3.816595            1       True          3\n8   RandomForestGini_BAG_L1   0.833895       0.096996   0.553528                0.096996           0.553528            1       True          5\n9     ExtraTreesEntr_BAG_L1   0.831650       0.095051   0.494969                0.095051           0.494969            1       True          9\n10  RandomForestEntr_BAG_L1   0.830527       0.101071   0.535026                0.101071           0.535026            1       True          6\n11    ExtraTreesGini_BAG_L1   0.823793       0.111044   0.513969                0.111044           0.513969            1       True          8\n12    KNeighborsDist_BAG_L1   0.635241       0.004996   0.006006                0.004996           0.006006            1       True          2\n13    KNeighborsUnif_BAG_L1   0.629630       0.015998   0.005992                0.015998           0.005992            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.855219\n0.036927\n72.167391\n0.036927\n72.167391\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.855219\n0.038929\n73.009209\n0.002002\n0.841818\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.130997\n20.415231\n0.130997\n20.415231\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.194014\n68.497755\n0.194014\n68.497755\n1\nTrue\n12\n\n\n4\nLightGBMLarge_BAG_L1\n0.842873\n0.056998\n8.680638\n0.056998\n8.680638\n1\nTrue\n13\n\n\n5\nXGBoost_BAG_L1\n0.837262\n0.055978\n3.598592\n0.055978\n3.598592\n1\nTrue\n11\n\n\n6\nLightGBM_BAG_L1\n0.837262\n0.061885\n5.357185\n0.061885\n5.357185\n1\nTrue\n4\n\n\n7\nLightGBMXT_BAG_L1\n0.835017\n0.049997\n3.816595\n0.049997\n3.816595\n1\nTrue\n3\n\n\n8\nRandomForestGini_BAG_L1\n0.833895\n0.096996\n0.553528\n0.096996\n0.553528\n1\nTrue\n5\n\n\n9\nExtraTreesEntr_BAG_L1\n0.831650\n0.095051\n0.494969\n0.095051\n0.494969\n1\nTrue\n9\n\n\n10\nRandomForestEntr_BAG_L1\n0.830527\n0.101071\n0.535026\n0.101071\n0.535026\n1\nTrue\n6\n\n\n11\nExtraTreesGini_BAG_L1\n0.823793\n0.111044\n0.513969\n0.111044\n0.513969\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.635241\n0.004996\n0.006006\n0.004996\n0.006006\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.629630\n0.015998\n0.005992\n0.015998\n0.005992\n1\nTrue\n1\n\n\n\n\n\n\n\n4) 예측(predict)\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9158249158249159\n\n\n\ntst[['PassengerId']].assign(Survived = predictr.predict(tst))\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)\n\n\n하지만 결과는 확실하다. 무려 0.813…"
  },
  {
    "objectID": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html",
    "href": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html",
    "title": "Kaggle | 결측치의 처리",
    "section": "",
    "text": "Titanic 데이터에는 결측치가 상당히 많았는데, 그것을 처리해서 분석해보자."
  },
  {
    "objectID": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#라이브러리-imports",
    "href": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#라이브러리-imports",
    "title": "Kaggle | 결측치의 처리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#!pip install missingno\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.impute\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\nimport missingno as msno"
  },
  {
    "objectID": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#데이터-불러오기",
    "href": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#데이터-불러오기",
    "title": "Kaggle | 결측치의 처리",
    "section": "2. 데이터 불러오기",
    "text": "2. 데이터 불러오기\n\n#!kaggle competitions download -c titanic\n#!unzip titanic.zip -d ./titanic\n#df_train = pd.read_csv('titanic/train.csv')\n#df_test = pd.read_csv('titanic/test.csv')\n#!rm titanic.zip\n#!rm -rf titanic/\n\n## 리눅스 서버가 구축되어 있다면 데이터를 바로 불러오기가 편리하다.\n\n\ndf_test = pd.read_csv('./data/test.csv')\ndf_train = pd.read_csv('./data/train.csv')"
  },
  {
    "objectID": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#결측치-확인-및-처리",
    "href": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#결측치-확인-및-처리",
    "title": "Kaggle | 결측치의 처리",
    "section": "3. 결측치 확인 및 처리",
    "text": "3. 결측치 확인 및 처리\n결측치 확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n시각화\n\nmsno.matrix(df_train)\n\n&lt;Axes: &gt;\n\n\n\n\n\n결측치 처리\n\n수치형은 수치형끼리, 범주형은 범주형끼리 처리하자.\n\n\ndf_imputed = df_train.copy()\n\ntrain_num = df_train.select_dtypes(include = 'number')\ntrain_obj = df_train.select_dtypes(exclude = 'number')\n\ndf_imputed[train_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(train_num)\ndf_imputed[train_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\n\ndf_imputed.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    float64\n 1   Survived     891 non-null    float64\n 2   Pclass       891 non-null    float64\n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          891 non-null    float64\n 6   SibSp        891 non-null    float64\n 7   Parch        891 non-null    float64\n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        891 non-null    object \n 11  Embarked     891 non-null    object \ndtypes: float64(7), object(5)\nmemory usage: 83.7+ KB\n\n\n\n결측치가 완전히 메꿔진 것을 확인할 수 있다."
  },
  {
    "objectID": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#분석",
    "href": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#분석",
    "title": "Kaggle | 결측치의 처리",
    "section": "4. 분석(?)",
    "text": "4. 분석(?)\n늘 해왔던 것처럼…\n\nset(df_train.columns) - set(df_test.columns)\n\n{'Survived'}\n\n\n\nSurvived : 반응변수\n\n- 근데 몇 번 결측치 처리를 반복해야 하므로 위에서의 과정을 함수로 만들어버리자.\n\ndef impute_missing(df):\n    \"\"\"\n    imputing missing and output whole dataframe\n    \n    df : DataFrame include NaN value\n    \"\"\"\n    df_ = df.copy()  ## 데이터를 복사\n    \n    df_num = df_.select_dtypes(include = 'number')  ## 해당하는 데이터 타입만 선택\n    df_obj = df_.select_dtypes(exclude = 'number')\n    \n    df_[df_num.columns] = sklearn.impute.SimpleImputer(strategy = 'mean').fit_transform(df_num)\n    df_[df_obj.columns] = sklearn.impute.SimpleImputer(strategy = 'most_frequent').fit_transform(df_obj)\n    \n    return df_\n\n\npd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1)))\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nAge\nSibSp\nParch\nFare\nName_Abbing, Mr. Anthony\nName_Abbott, Mr. Rossmore Edward\nName_Abbott, Mrs. Stanton (Rosa Hunt)\nName_Abelson, Mr. Samuel\n...\nCabin_F G73\nCabin_F2\nCabin_F33\nCabin_F38\nCabin_F4\nCabin_G6\nCabin_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1.0\n3.0\n22.000000\n1.0\n0.0\n7.2500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\n2.0\n1.0\n38.000000\n1.0\n0.0\n71.2833\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n3.0\n3.0\n26.000000\n0.0\n0.0\n7.9250\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\n4.0\n1.0\n35.000000\n1.0\n0.0\n53.1000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\n5.0\n3.0\n35.000000\n0.0\n0.0\n8.0500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887.0\n2.0\n27.000000\n0.0\n0.0\n13.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n887\n888.0\n1.0\n19.000000\n0.0\n0.0\n30.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n888\n889.0\n3.0\n29.699118\n1.0\n2.0\n23.4500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n889\n890.0\n1.0\n26.000000\n0.0\n0.0\n30.0000\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n890\n891.0\n3.0\n32.000000\n0.0\n0.0\n7.7500\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n891 rows × 1730 columns\n\n\n\n\n늘 그랬던 것처럼 get_dummies를 해줬는데… 뭔가 이상하다.\n\n\npd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1))).shape\n\n(891, 1730)\n\n\n행이 1730개??? &gt; 이러한 상황에서는 선형 모델이 제대로 작동하지 않는다…!\n\n# step 1\nX = pd.get_dummies(impute_missing(df_train.drop(['Survived'], axis = 1)))\ny = df_train.Survived\nXX = pd.get_dummies(impute_missing(df_test))\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\npredictr.predict(XX)\n\nC:\\Users\\hollyriver\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nValueError: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Cabin_A11\n- Cabin_A18\n- Cabin_A21\n- Cabin_A29\n- Cabin_A9\n- ...\nFeature names seen at fit time, yet now missing:\n- Cabin_A10\n- Cabin_A14\n- Cabin_A16\n- Cabin_A19\n- Cabin_A20\n- ...\n\n\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\n형식이 object인 것들이 가지고 있는 유니크한 값들이 몇개인지를 딕셔너리 컴프리헨션 해봤다.\n\n\n사실상 해당 수가 엄청나게 많은 Name, Ticket, Cabin의 경우 없애는 편이 더 좋아보인다."
  },
  {
    "objectID": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#진짜-분석",
    "href": "2023_MP/Titanic/A3. 로지스틱_결측치 처리.html#진짜-분석",
    "title": "Kaggle | 결측치의 처리",
    "section": "5. 진짜 분석",
    "text": "5. 진짜 분석\n\n## step 1\nX = pd.get_dummies(impute_missing(df_train).drop(['Name', 'Ticket', 'Cabin', 'Survived'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(impute_missing(df_test).drop(['Name', 'Ticket', 'Cabin'], axis = 1))\n\n## step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\ndf_test[['PassengerId']].assign(Survived = predictr.predict(XX))\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\ndf_test[['PassengerId']].assign(Survived = predictr.predict(XX)).to_csv(\"submission\", index = False)\n\n\n이렇게 하면 된다."
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html",
    "title": "Boosting | 의사결정나무",
    "section": "",
    "text": "Boosting, 의사결정나무의 초전설 최종진화형(테이블데이터 분석의 정수)"
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#라이브러리-imports",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#라이브러리-imports",
    "title": "Boosting | 의사결정나무",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport sklearn.ensemble\n\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n#---#\nimport matplotlib.animation\nimport IPython"
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#사용할-데이터",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#사용할-데이터",
    "title": "Boosting | 의사결정나무",
    "section": "2. 사용할 데이터",
    "text": "2. 사용할 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:80]\ntemp.sort()\neps = np.random.randn(80)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n0\n-4.1\n10.900261\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n...\n...\n...\n\n\n75\n9.7\n50.813741\n\n\n76\n10.3\n42.304739\n\n\n77\n10.6\n45.662019\n\n\n78\n12.1\n48.739157\n\n\n79\n12.4\n46.007937\n\n\n\n\n80 rows × 2 columns\n\n\n\n\n아이스크림 그거 맞다."
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#기존의-분석법-체크",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#기존의-분석법-체크",
    "title": "Boosting | 의사결정나무",
    "section": "3. 기존의 분석법 체크",
    "text": "3. 기존의 분석법 체크\n- 선형\n\n선형회귀\n로지스틱회귀\nLasso, Ridge\n\n\n- 의사결정나무\n\n파라미터 위주 모형의 한계를 극복\n\n\n배깅 : 부스트랩, 다양성\n랜덤포레스트 : 부스트랩, 약한 트리ㆍ더 큰 다양성\n부스팅??? : 성장.\n\n\n부스팅은 약한 트리가 점점 강한 트리로 성장하는 느낌이다."
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#부스팅으로-일단-적합",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#부스팅으로-일단-적합",
    "title": "Boosting | 의사결정나무",
    "section": "4. 부스팅으로 일단 적합",
    "text": "4. 부스팅으로 일단 적합\n\n## step 1\nX = df_train[['temp']]\ny = df_train['sales']\n\n## step 2\npredictr = sklearn.ensemble.GradientBoostingRegressor()  ## 기울기 부스팅 회귀...?\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\nyhat = predictr.predict(X)\n\n\nplt.plot(X, y, 'o')\nplt.plot(X, yhat, '--')\nplt.show()\n\n\n\n\n\n먼가 기존 트리보다 상당히 완만함."
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#yhat을-얻는-과정-어려움",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#yhat을-얻는-과정-어려움",
    "title": "Boosting | 의사결정나무",
    "section": "5. yhat을 얻는 과정 : 어려움…",
    "text": "5. yhat을 얻는 과정 : 어려움…\n- my_trees 생성\n\npredictr.estimators_[0][0]  ## 얘네는 이중 리스트를 써서 두번 인덱스를 지정해줘야 똑같음... 뭐임...\n\nDecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n                      random_state=RandomState(MT19937) at 0x13753387440)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n                      random_state=RandomState(MT19937) at 0x13753387440)\n\n\n\ntrees = [t[0] for t in predictr.estimators_]\ntrees[0]\n\nDecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n                      random_state=RandomState(MT19937) at 0x13753387440)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n                      random_state=RandomState(MT19937) at 0x13753387440)\n\n\n- 단순시도 (여태껏 그랬듯이 나무들의 평균으로) – 실패\n\n_yhat = np.array([tree.predict(X) for tree in trees]).mean(axis = 0)\n_yhat\n\narray([-1.98529568, -1.68873716, -1.50105449, -1.32748757, -1.17877814,\n       -1.12461928, -1.48130074, -1.47726246, -1.47726246, -1.14576839,\n       -1.14576839, -0.93324255, -0.93324255, -0.82550938, -1.14740524,\n       -0.63621774, -0.63621774, -0.99552612, -0.99552612, -0.56894418,\n       -0.56894418, -0.56894418, -0.05104935, -0.39699577, -0.39699577,\n       -0.39699577, -0.38296042, -0.38296042, -0.16511506, -0.13001062,\n       -0.18975132, -0.18975132, -0.09772732, -0.09772732, -0.02478889,\n       -0.02478889, -0.25650094, -0.02421372, -0.02421372, -0.03653755,\n       -0.08676362, -0.08676362, -0.08676362, -0.05359582, -0.05359582,\n        0.43424239,  0.43353213,  0.19572773,  0.19572773,  0.54183429,\n        0.54183429,  0.2947621 ,  0.2947621 ,  0.2947621 ,  0.21585817,\n        0.21585817,  0.7964528 ,  0.59531418,  0.49763334,  0.49763334,\n        0.78455561,  0.78455561,  0.55907873,  0.48574479,  1.16729012,\n        0.89795288,  0.92295305,  1.11059435,  1.09672204,  0.98224138,\n        0.9138514 ,  1.05038217,  0.86805699,  1.04227523,  1.49564195,\n        1.89066912,  1.19912714,  1.46250802,  1.70344969,  1.51603631])\n\n\n\nplt.plot(X, y, 'o')\nplt.plot(X, yhat, '--')\nplt.plot(X, _yhat, '--')\n\n\n\n\n\n예?! 이건 아니잖아요!\n\n\nplt.plot(X, y, 'o')\nplt.plot(X, yhat, '--')\nplt.plot(X, _yhat+y.mean(), '--')\n\n\n\n\n\n똑같잖아요!\n\n\nplt.plot(X, y, 'o')\nplt.plot(X, yhat, '--')\nplt.plot(X, _yhat*100, '--')\n\n\n\n\n\n~어 근데 뭔가 윤곽선이 익숙하긴 한데…~\n\n- 처음 3개의 의사결정나무의 예측\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[0][0],\n    max_depth = 1,\n    feature_names = X.columns.to_list()\n);\n\n\n\n\n\n1번째 트리.\n\n\n중심 노드를 0으로 두고, 그 양옆의 값들을 중심값에서의 차이로 설정하였음.\n\n\nplt.plot(X, y, 'o')\nplt.plot(X, trees[0].predict(X)+y.mean(), '--', label = 'yhat with 1st tree')\nplt.plot(X, trees[1].predict(X)+y.mean(), '--', label = 'yhat with 2nd tree')\nplt.plot(X, trees[2].predict(X)+y.mean(), '--', label = 'yhat with 3rd tree')\nplt.plot(X, trees[-1].predict(X)+y.mean(), '--', label = 'yhat with last tree')\nplt.legend()\nplt.show()\n\n\n\n\n\n초기 값들은 어느정도 경사가 있는데, 계속 나아갈수록 직선에 가까워진다… 이걸 어떻게 합쳐야 부스팅의 예측값이 나오는 걸까???\n\n- 시각화로 확인\n\n태초 : yhat = y.mean()으로 적합\n첫번째 나무 반영 : 현재까지의 적합값 + 첫번째 나무의 적합값 × 0.1(predictr.learning_rate) – ver 0.01\n두번째 나무 반영 : 현재까지의 적합값 + 두번째 나무의 적합값 × 0.1 – ver 0.02\n\n\\(\\dots\\)\n\n100번째 나무 반영 : 현재까지의 적합값 + 100번째 나무의 적합값 × 0.1 &gt; 최종 yhat – ver 1.00\n\n\ntrees[0].predict(X)+trees[1].predict(X)  ## 최초 관측한 자료\n\narray([-28.12857773, -28.12857773, -28.12857773, -28.12857773,\n       -28.12857773, -28.12857773, -28.12857773, -28.12857773,\n       -28.12857773, -17.76744106, -17.76744106, -17.76744106,\n       -17.76744106, -17.76744106, -17.76744106, -17.76744106,\n       -17.76744106, -17.76744106, -17.76744106,  -8.07411483,\n        -8.07411483,  -8.07411483,  -8.07411483,  -8.07411483,\n        -8.07411483,  -8.07411483,  -8.07411483,  -8.07411483,\n        -1.82748008,  -1.82748008,  -1.82748008,  -1.82748008,\n        -1.82748008,  -1.82748008,  -1.82748008,  -1.82748008,\n        -1.82748008,  -1.82748008,  -1.82748008,  -1.82748008,\n        -1.82748008,  -1.82748008,  -1.82748008,  -1.82748008,\n        -1.82748008,   6.48143774,   6.48143774,   6.48143774,\n         6.48143774,   6.48143774,   6.48143774,   6.48143774,\n         6.48143774,   6.48143774,   6.48143774,   6.48143774,\n        11.74216296,  11.74216296,  11.74216296,  11.74216296,\n        11.74216296,  11.74216296,  11.74216296,  11.74216296,\n        19.21655361,  19.21655361,  19.21655361,  19.21655361,\n        19.21655361,  19.21655361,  19.21655361,  19.21655361,\n        19.21655361,  19.21655361,  29.52785838,  29.52785838,\n        29.52785838,  29.52785838,  29.52785838,  29.52785838])\n\n\n\npredictions = [tree.predict(X) for tree in trees]  ## 각 예측값 할당\n\nplt.plot(X, y, 'o')\nplt.plot(X, np.repeat(y.mean(), len(X)), '--', label = 'WeakPredictor (ver 0.00)')\nplt.plot(X,\n         np.array(predictions[:1]).sum(axis = 0)*0.1 + y.mean(),\n         '--', label = 'WeakPredictor (ver 0.01)')\nplt.plot(X,\n         np.array(predictions[:2]).sum(axis = 0)*0.1 + y.mean(),\n         '--', label = 'WeakPredictor (ver 0.02)')\nplt.plot(X,\n         np.array(predictions[:3]).sum(axis = 0)*0.1 + y.mean(),\n         '--', label = 'WeakPredictor (ver 0.03)')\nplt.plot(X,\n         np.array(predictions).sum(axis = 0)*0.1 + y.mean(),\n         '--', label = 'WeakPredictor (ver 1.00)')\nplt.legend()\nplt.show()\n\n\n\n\n\n##이걸로 해도 되기는 하는데, 이건 항상 predict()를 해줘야하니까 연산이 반복되서 리소스 할당에 좋지 않을듯\n##np.array([trees[i].predict(X) for i in range(2)]).mean(axis = 0)\n\n- 해당 과정을 애니메이션으로 표현\n\n## 어셈블리 코드\ndef ensemble(trees, i = None) :\n    if i is None :\n        i = len(trees)\n    else :\n        i = i+1  ## loc하게 되면 그것 다음것까지로 넣어야되니까.\n    yhat = np.array(predictions[:i]).sum(axis = 0)*0.1 + y.mean()  ## 축의 값들을 가중치를 반영하여 더해야 함.\n    return yhat\n\n\nfig, ax = plt.subplots(1)\nplt.close()  ## 쌩 피규어가 콘솔에 안뜨도록 함.\n\ndef func(frame) :\n    ax.clear()\n    ax.plot(X, y, 'o', label = 'RawData')  ## 원자료\n    ax.plot(X, ensemble(trees, frame), '--', label = 'WeakPredictor (ver ({})'.format(round((frame+1)*0.01, 3)))\n    ax.legend()\n\nani = matplotlib.animation.FuncAnimation(fig, func, frames = 100)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#재현",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#재현",
    "title": "Boosting | 의사결정나무",
    "section": "6. 재현",
    "text": "6. 재현\n\n애초에 그러면 각각의 트리를 어떻게 적합한거지?\n\n\nA. 재현의 확인\n\n- 아이디어 :\n\n처음부터 yhat을 강하게 학습하지 않고 약하게 조금씩 학습하다.\n부족한 공부는 (학습이 덜 되어있는 부분 : SSE = y - yhat)은 조금씩 강화하면서 보완하자.\n\n- 구현 : my_trees, my_residuals를 직접 구현\n\ntrees[0]\n\nDecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n                      random_state=RandomState(MT19937) at 0x13753387440)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(criterion='friedman_mse', max_depth=3,\n                      random_state=RandomState(MT19937) at 0x13753387440)\n\n\n\nmy_trees = []\nmy_residuals = []\n\n\nres = y - y.mean()  ## 잔차(태초의)\n\n## 100번 공부\nfor i in range(100) :\n    tree = sklearn.tree.DecisionTreeRegressor(max_depth = 3, criterion = 'friedman_mse')\n    tree.fit(X, res)  ## 아직 적합되지 못한 잔차에 대해서 적합을 함(오버피팅???)\n    yhat = tree.predict(X)\n    res = res - yhat*0.1  ## 학습한 것을 다 반영하지 말고 learning_rate만큼만 반영하자.\n    my_trees.append(tree)\n    my_residuals.append(res)\n\n## 덜 깊게 적합하는 선에서 잔차를 적합하고, 또 적합하고... 이 과정을 100번 반복한다.\n\n- 비교 : my_trees와 trees의 비교(고정된 i)\n\ni = 10\nfig = plt.figure()\nax = fig.subplots(2,2)\nplt.close()\n\nax[0,0].plot(X, y, 'o', alpha = 0.5)\nax[0,0].plot(X, ensemble(trees, i), '--')\nax[0,0].set_title('Boosting yhat')\n\nax[0,1].plot(X, y, 'o', alpha = 0.5)\nax[0,1].plot(X, ensemble(my_trees, i), '--')\nax[0,1].set_title('Handmade yhat')\n\nsklearn.tree.plot_tree(trees[i], max_depth = 0, ax = ax[1,0], feature_names = X.columns.to_list())\nsklearn.tree.plot_tree(my_trees[i], max_depth = 0, ax = ax[1,1], feature_names = X.columns.to_list())\n\nfig\n\n\n\n\n\n모두 일치하는 것을 알 수 있다. 잘 구현함\n\n- 비교 : 애니메이션\n\n## ax가 2차원 개체가 for문을 직접 걸어줄 수 음슴.\n\n\nfig, ax = plt.subplots(2,2)\nplt.close()\n\ndef func(frame) :\n    for i in range(2) :\n        for j in range(2) :\n            ax[i,j].clear()\n            ax[i,j].plot(X, y, 'o', alpha = 0.5)\n\n    ax[0,0].plot(X, ensemble(trees, frame), '--')\n    ax[0,1].plot(X, ensemble(my_trees, frame), '--')\n\n    sklearn.tree.plot_tree(trees[frame], max_depth = 0, feature_names = X.columns.to_list(), ax = ax[1,0])\n    sklearn.tree.plot_tree(my_trees[frame], max_depth = 0, feature_names = X.columns.to_list(), ax = ax[1,1])\n\nani = matplotlib.animation.FuncAnimation(fig, func, frames = 100)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n두 결과가 완전히 동일하다. 따라서 랜덤포레스트의 트리별 동작은 위의 코드와 같다고 볼 수 있다."
  },
  {
    "objectID": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#b.-step별-분석",
    "href": "2023_MP/practice/26. 의사결정나무 - 부스팅.html#b.-step별-분석",
    "title": "Boosting | 의사결정나무",
    "section": "### B. Step별 분석",
    "text": "### B. Step별 분석\n\nfig, ax = plt.subplots(1,4, figsize = (10, 3))\nplt.close()\n\n\ndef func(frame) :\n    for a in ax :\n        a.clear()\n\n    ax[0].set_title('Step 0')\n    ax[0].plot(X, y, 'o', alpha = 0.5)\n    ax[0].plot(X, ensemble(my_trees, frame), '--')\n    \n    ax[1].set_title('Step 1 : Residuals')  ## 적합하고 남은 잔차를 계산\n    ax[1].plot(X, my_residuals[frame], 'o', alpha = 0.5)  ## 잔차 시각화\n    ax[1].set_ylim(-20,20)\n    \n    ax[2].set_title('Step 2 : Fitting')  ## 잔차에 대해 트리로 적합\n    ax[2].plot(X, my_residuals[frame], 'o', alpha = 0.5)\n    ax[2].plot(X, my_trees[frame].predict(X), '--')  ## 잔차에 대한 적합선\n    ax[2].set_ylim(-20,20)\n    \n    ax[3].set_title('Step 3 : Update')\n    ax[3].plot(X, y, 'o', alpha = 0.5)\n    ax[3].plot(X, ensemble(my_trees, frame), '--', color = 'C1')\n    ax[3].plot(X, ensemble(my_trees, frame+1), '--', color = 'C3')\n\n\nani = matplotlib.animation.FuncAnimation(fig, func, frames = 100)\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- Step 0\n\n파란점 y : 원래 공부할 양\n주황색 선분 yhat : 현재까지 공부한 양\n\n- Step 1\n\ni번째 남아있는 공부량(residuals)\n\n- Step 2\n\ni번째 남아있는 공부량에 대해 트리로 적합(덜 빡빡하게 적합함. max_depth = 3)\n적합을 진행할 수록 분기점이 달라짐(나중에 되면 바깥 영역으로도 적합함)\n\n- Step 3\n\n적합한 뒤 새로 갱신된 내용, 이 크기는 업데이트가 될 수록 줄어듦.\n\n#\n\n관찰1 : “Step1 : Residuals”은 점점 단순오차처럼 변화한다.(잔차에 선형성이 있었다가 정규분포처럼 모임)\n관찰2 : “Step2 : Fitting”의 분기점들은 고정된 값이 아니다.(계속 변화한다.)\n관찰3 : “Step3 : Update” 업데이트 되는 양은 반복이 진행될 수록 점점 작아진다.\n\n- 느낌 : 조금씩 데이터를 학습한다.(남아있는 공부량에 대해서…) 학습할 자료가 오차항처럼 보인다면? 그때는 적합을 멈춘다.(오버피팅 방지, 피팅하면 직선이 되니까 알고리즘이 자동으로 멈춥니당. 물론 샘플을 뽑는 과정에서 계속해서 오버피팅을 시도하긴 함.)\n\n트리의 수를 굳이 100개 꽉 채우지 말고 중간에서 끊어도 되잖아?"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "",
    "text": "sklearn의 linear_mode.LinearRegression()을 사용하여 선형회귀분석을 해보자!"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#라이브러리-imports",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#라이브러리-imports",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#data",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#data",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "2. Data",
    "text": "2. Data\n\n전주시의 기온 자료\n\n\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()   ## 자료를 크기 순서대로 정렬, sort_values()와 비슷하달까...\n\n\ntemp\n\narray([-4.1, -3.7, -3. , -1.3, -0.5, -0.3,  0.3,  0.4,  0.4,  0.7,  0.7,\n        0.9,  0.9,  1. ,  1.2,  1.4,  1.4,  1.5,  1.5,  2. ,  2. ,  2. ,\n        2.3,  2.5,  2.5,  2.5,  2.6,  2.6,  2.9,  3.2,  3.5,  3.5,  3.6,\n        3.7,  3.8,  4.2,  4.4,  4.5,  4.5,  4.6,  4.9,  4.9,  4.9,  5. ,\n        5. ,  5.1,  5.6,  5.9,  5.9,  6. ,  6. ,  6.1,  6.1,  6.3,  6.3,\n        6.4,  6.4,  6.5,  6.7,  6.8,  6.8,  7. ,  7. ,  7.1,  7.2,  7.4,\n        7.7,  8. ,  8.1,  8.1,  8.3,  8.4,  8.4,  8.4,  8.5,  8.8,  8.9,\n        9.1,  9.2,  9.3,  9.4,  9.4,  9.5,  9.6,  9.6,  9.7,  9.8,  9.9,\n       10.2, 10.3, 10.6, 10.6, 10.8, 11.2, 12.1, 12.4, 13.4, 14.7, 15. ,\n       15.2])\n\n\n- 아래와 같은 모형을 가정하자. \\[\\textup{아이스크림 판매량}= 20 ＋ \\textup{온도} × 2.5 × \\textup{오차(운)}\\]\n\n더미 모형 생성\n\n\nnp.random.seed(43052)\neps = np.random.randn(100)*3  ## 오차\nicecream_sales = 20 + temp * 2.5 + eps\n\n\nplt.plot(temp, icecream_sales, 'o')\nplt.show()\n\n\n\n\n\n상기 결과를 관측했다고 생각합시다.\n\n\ndf = pd.DataFrame({'temp' : temp, 'sales' : icecream_sales})\ndf\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n0\n-4.1\n10.900261\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n...\n...\n...\n\n\n95\n12.4\n54.926065\n\n\n96\n13.4\n54.716129\n\n\n97\n14.7\n56.194791\n\n\n98\n15.0\n60.666163\n\n\n99\n15.2\n61.561043\n\n\n\n\n100 rows × 2 columns"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#게임세팅",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#게임세팅",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "3. 게임세팅",
    "text": "3. 게임세팅\n- 편의상 아래와 같은 기호를 도입하자.\n\n(df.temp[0], df.temp[1], … , df.temp[99]) = \\((x_1,x_2,\\dots,x_{100})=(-4.1,-3.7,\\dots,15.2)\\)\n(df.sales[0], df.sales[1], … , df.sales[99]) = \\((y_1,y_2,\\dots,y_{100})=(10.90,14.00, \\dots,61.56)\\)\n\n\n이 자료 \\(\\big\\{(x_i,y_i)\\big\\}_{i=1}^{100}\\)를 바탕으로 어떠한 패턴을 발견하여 새로운 \\(x\\)에 대한 예측값을 알고 싶다 : \\(\\hat{y}\\)\n\nA. 질문\n- 기온이 \\(x = -2.0\\)일 때, 아이스크림을 얼마정도 판다고 보는 게 타당할까?\nB. 답 1\n- \\(x = -2.0\\) 근처의 데이터를 살펴보자.\n\ndf[(-4.0 &lt; df.temp) & (0.0 &gt; df.temp)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n5\n-0.3\n20.317853\n\n\n\n\n\n\n\n\n\\(-1.3\\)이 제일 가까운데, 대충 \\(17.67\\) 언저리 아닐까…?\n\n\nA. 산점도와 추세선\n\n- 자료를 바탕으로 그림을 그려보자\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot([-2.0],[17.67],'x')     # 이미 들어가있는 플롯에 점을 하나 찍는다. 마커는 X\n\nplt.show()\n\n\n\n\n\n예상한 것(17.67)보다 못팔 것 같은데…?"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-아이디어",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-아이디어",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 아이디어",
    "text": "### B. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자!~(사실 모형을 우리가 만들었으니 이미 추세선을 알고 있긴 함)~\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.temp, 20+df.temp*2.5, '--')  ## 위에서 직접 설정했던 자료의 관계, 절편이 20이고 기울기가 2.5\n\nplt.show()\n\n\n\n\n- 사실 \\(y = 20 + 2.5x\\)라는 추세선을 이미 알고 있었음.\n- 그래서 \\(x = -2\\)라면 \\(y = 20 - 2.5 × 2 = 15\\)라고 보는 게 합리적임(오차를 고려 안하면)\n\n허나, 실제 상황에서 우리는 \\(20, 2.5\\)라는 숫자를 모른다.\n\n- 게임셋팅 * 원래 게임 : 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임 * 변형된 게임 : \\(20, 2.5\\)라는 숫자를 잘 찾는 게임. 즉, 데이처를 보고 최대한 \\(y_i \\approx ax_i+b\\)가 되도록 \\(a, b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#분석",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "4. 분석",
    "text": "4. 분석\n\n그렇다면 늘 했던 것처럼 네 단계로 분석을 해보자.\n\n\nA. 데이터\n\n\n# step 1 -- data\ntrain = pd.DataFrame({'temp' : temp, 'sales' : icecream_sales})\n\nX = train[['temp']]\ny = train['sales']\n\n\n데이터를 학습해서 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄 수 있는 아이(predictor)를 만들자.~(근데 이정도면 학생이 아니라 노예…)~"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-predictor",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-predictor",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. predictor",
    "text": "### B. predictor\n\n# step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n\nsklearn의 linear_model.LinearRegression()을 사용했다. 이러면 가장 기본적인 선형회귀를 진행한다.(LSE를 쓰는 그거 있잖아…)\n\n\nC. 학습\n\n\n# step 3\npredictr.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n학생이 train을 완료했다."
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#d.-예측predict",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#d.-예측predict",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### D. 예측(predict)",
    "text": "### D. 예측(predict)\n- 학생(predictr) : 데이터를 살펴보니 True는 이럴 것 같아요.\n\ny_hat = predictr.predict(X)  ## X값에 해당하는 y_hat값을 예측하여 산출.\n\n\nplt.plot(X, y, 'o', alpha = 0.5)\nplt.plot(X, y_hat, 'o--', alpha = 0.5)\n\nplt.show()\n\n\n\n\n- 그럼 기울기와 절편은 어디에 저장된 걸까?\n- predictr : 여깄음.\n\n(predictr.coef_, predictr.intercept_)\n\n(array([2.51561216]), 19.66713126947925)\n\n\n- 새로운 데이터 \\(x = -2\\)에 대한 예측\n\nfloat(predictr.coef_)*(-2) + float(predictr.intercept_)\n\n14.63590694951262\n\n\n\n해당 결과값을 그래프에 나타내면…\n\n\nX_input = pd.DataFrame({'temp' : [-2.0]})\n\n\nplt.plot(X, y, 'o', alpha = 0.5)\nplt.plot(X, y_hat, '--', alpha = 0.5)\nplt.plot(X_input, predictr.predict(X_input), 'xr')  ## 원래는 리스트나 어레이로 넣어주는 게 정배긴 함\n\nplt.show()\n\n\n\n\n\n예측값이 직선상에 위치함을 알 수 있다."
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#두-타입의-아이스크림초코-바닐라에-대한-회귀분석",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#두-타입의-아이스크림초코-바닐라에-대한-회귀분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "5. 두 타입의 아이스크림(초코 / 바닐라)에 대한 회귀분석",
    "text": "5. 두 타입의 아이스크림(초코 / 바닐라)에 대한 회귀분석\n\n이전의 기온 자료를 바꿔 아래와 같은 모형을 가정해보자.\n\n\nA. Data\n\n\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()   ## 자료를 크기 순서대로 정렬\ntemp  ## 전주시의 기온 100개 자료\n\narray([-4.1, -3.7, -3. , -1.3, -0.5, -0.3,  0.3,  0.4,  0.4,  0.7,  0.7,\n        0.9,  0.9,  1. ,  1.2,  1.4,  1.4,  1.5,  1.5,  2. ,  2. ,  2. ,\n        2.3,  2.5,  2.5,  2.5,  2.6,  2.6,  2.9,  3.2,  3.5,  3.5,  3.6,\n        3.7,  3.8,  4.2,  4.4,  4.5,  4.5,  4.6,  4.9,  4.9,  4.9,  5. ,\n        5. ,  5.1,  5.6,  5.9,  5.9,  6. ,  6. ,  6.1,  6.1,  6.3,  6.3,\n        6.4,  6.4,  6.5,  6.7,  6.8,  6.8,  7. ,  7. ,  7.1,  7.2,  7.4,\n        7.7,  8. ,  8.1,  8.1,  8.3,  8.4,  8.4,  8.4,  8.5,  8.8,  8.9,\n        9.1,  9.2,  9.3,  9.4,  9.4,  9.5,  9.6,  9.6,  9.7,  9.8,  9.9,\n       10.2, 10.3, 10.6, 10.6, 10.8, 11.2, 12.1, 12.4, 13.4, 14.7, 15. ,\n       15.2])\n\n\n- 아래와 같은 모형을 가정하자.\n\\[\\textup{초코 아이스크림 판매량} = 20 + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\\[\\textup{바닐라 아이스크림 판매량} = 40 + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\nnp.random.seed(43052)\nchoco = 20 + temp*2.5 + np.random.randn(100)*3  ## random normal distribution\nvanilla = 40 + temp*2.5 + np.random.randn(100)*3\n\n\nplt.plot(temp, choco, 'o', label = 'choco')\nplt.plot(temp, vanilla, 'o', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n\n우리는 위와 같은 정보를 관측했다고 가정하자.\n\n\ndf1 = pd.DataFrame({'temp' : temp, 'type' : ['choco' for i in range(100)], 'sales' : choco})\ndf2 = pd.DataFrame({'temp' : temp, 'type' : ['vanilla' for i in range(100)], 'sales' : vanilla})\n\ndf = pd.concat([df1, df2], axis = 0).reset_index(drop = True)\ndf\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\n\n\n\n\n0\n-4.1\nchoco\n10.900261\n\n\n1\n-3.7\nchoco\n14.002524\n\n\n2\n-3.0\nchoco\n15.928335\n\n\n3\n-1.3\nchoco\n17.673681\n\n\n4\n-0.5\nchoco\n19.463362\n\n\n...\n...\n...\n...\n\n\n195\n12.4\nvanilla\n68.708075\n\n\n196\n13.4\nvanilla\n75.800464\n\n\n197\n14.7\nvanilla\n79.846568\n\n\n198\n15.0\nvanilla\n78.713140\n\n\n199\n15.2\nvanilla\n77.595252\n\n\n\n\n200 rows × 3 columns"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-분석",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-분석",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 분석",
    "text": "### B. 분석\n- 언제처럼 늘 그랬던 것처럼…\n\n# step 1\n## X = pd.get_dummies(df).drop(['sales'], axis = 1) ## 이게 제일 범용적이긴 함\nX = df.loc[:, ['temp', 'type']].assign(type = (df.type == 'choco')) ## 직관적으로 쓴 코드, 범주형은 인식을 못한다.\ny = df['sales']\n\n# step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\ndf = df.assign(sales_hat = predictr.predict(X));df\n\n\n\n\n\n\n\n\ntemp\ntype\nsales\nsales_hat\n\n\n\n\n0\n-4.1\nchoco\n10.900261\n9.286731\n\n\n1\n-3.7\nchoco\n14.002524\n10.295689\n\n\n2\n-3.0\nchoco\n15.928335\n12.061366\n\n\n3\n-1.3\nchoco\n17.673681\n16.349439\n\n\n4\n-0.5\nchoco\n19.463362\n18.367355\n\n\n...\n...\n...\n...\n...\n\n\n195\n12.4\nvanilla\n68.708075\n71.446479\n\n\n196\n13.4\nvanilla\n75.800464\n73.968875\n\n\n197\n14.7\nvanilla\n79.846568\n77.247989\n\n\n198\n15.0\nvanilla\n78.713140\n78.004708\n\n\n199\n15.2\nvanilla\n77.595252\n78.509187\n\n\n\n\n200 rows × 4 columns\n\n\n\n- 가장 중요한 시각화까지…\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.loc[df.type == 'choco'].temp, df.loc[df.type == 'choco'].sales_hat, '--', color = 'brown', label = 'choco')\nplt.plot(df.loc[df.type == 'vanilla'].temp, df.loc[df.type == 'vanilla'].sales_hat, '--', color = 'yellow', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n\n별다른 뜻 없이 (초코, 바닐라)에 (1, 0)을 넣었는데, 어떻게 뭐가 나오긴 했다.\n\n\n어케했음???\n\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([  2.52239574, -20.54021854]), 40.16877158069265)\n\n\n\ncoef_(기울기)가 2개지요.\n\n온도와 범주형 자료인 아이스크림 종류에 따라 기울기가 다르다. 온도 1도가 변할때마다 판매량은 2.52239574가 변하고, 아이스크림 종류가 1 변할때마다(0에서 1이니까 바닐라에서 초코로 바뀜) -20.54를 곱한 수를 더하여 수식을 설명하였다.\n예측\n- 온도가 \\(-2\\)이고, type이 vanilla(0)라면 예측값은?\n\nXnew = pd.DataFrame({'temp' : [-2], 'type' : [0]})\n\npredictr.predict(Xnew)\n\narray([35.1239801])\n\n\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.loc[df.type == 'choco'].temp, df.loc[df.type == 'choco'].sales_hat, '--', color = 'brown', label = 'choco')\nplt.plot(df.loc[df.type == 'vanilla'].temp, df.loc[df.type == 'vanilla'].sales_hat, '--', color = 'yellow', label = 'vanilla')\nplt.plot(Xnew.temp, predictr.predict(Xnew), 'or', label = 'prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nC. 데이터 전처리\n\n- 아까 pd.get_dummies()를 잠시 본 것 같은데, 이걸 어떻게, 왜 써야 하는 지 알아보자.\n\nX = df[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = pd.get_dummies(X);X\n\n\n\n\n\n\n\n\ntemp\ntype_choco\ntype_vanilla\n\n\n\n\n0\n-4.1\nTrue\nFalse\n\n\n1\n-3.7\nTrue\nFalse\n\n\n2\n-3.0\nTrue\nFalse\n\n\n3\n-1.3\nTrue\nFalse\n\n\n4\n-0.5\nTrue\nFalse\n\n\n...\n...\n...\n...\n\n\n195\n12.4\nFalse\nTrue\n\n\n196\n13.4\nFalse\nTrue\n\n\n197\n14.7\nFalse\nTrue\n\n\n198\n15.0\nFalse\nTrue\n\n\n199\n15.2\nFalse\nTrue\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n원-핫 인코딩 : 표현하고 싶은 단어에는 1을, 그것이 아닌 것에는 0을 부여\n\n- LinearRegression 모델의 경우 범주형 자료를 자동으로 인식하지 못한다. 따라서 구분할 범주형 변수가 많다면, pd.get_dummies()를 통해 범주를 나눠주어야 한다."
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#d.-모형의-평가",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#d.-모형의-평가",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### D. 모형의 평가",
    "text": "### D. 모형의 평가\n- 단순선형회귀분석의 경우 모형을 \\(R^2\\)(결정계수)로 평가한다.\n\n다만 이것이 높다고 해서 무조건적으로 좋은 건 아니고, 명확한 기준도 없다. 모형 간 상대적인 좋음을 비교하는 것 뿐이다.\n\n- LogisticRegression에서는 적중률로 딱 떨어지게 점수를 내줄 수 있겠지만, 이건 그렇게 해버리면 0점이 나와버리겠지…"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#설명변수가-많을-때",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#설명변수가-많을-때",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "6. 설명변수가 많을 때",
    "text": "6. 설명변수가 많을 때\n- kaggle에서 “Medical Cose Personal Datasets”을 다운로드\n\nhttps://www.kaggle.com/datasets/mirichoi0218/insurance\n\n\ndf = pd.read_csv(\".\\data\\insurance.csv\")\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\nA. 분석\n\n\n열 이름을 먼저 알아보자.\n\n\nset(df.columns)\n\n{'age', 'bmi', 'charges', 'children', 'region', 'sex', 'smoker'}\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\n- 대충 여러가지 범주형ㆍ연속형 설명변수들과 보험료의 관계를 요약하고 싶다고 하자.\n\n먼저 범주형 자료(sex, smoker, region)들을 원-핫 인코딩 해주자.\n\n\nX = pd.get_dummies(df.drop(['charges'], axis = 1))\ny = df.charges\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n- 그럼 뭐 늘 하던대로…\n\n# 2\npredictr = sklearn.linear_model.LinearRegression()\n\n# 3\npredictr.fit(X, y)\n\n# 4\ndf = df.assign(y_hat = predictr.predict(X))\n\n\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\ny_hat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns\n\n\n\n- charge와 y_hat이 잘 안맞는 것 같은데…?"
  },
  {
    "objectID": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-평가",
    "href": "2023_MP/practice/A0. 회귀분석_아이스크림.html#b.-평가",
    "title": "선형회귀분석의 시작 | LinearRegression()",
    "section": "### B. 평가",
    "text": "### B. 평가\n\npredictr.score(X, y)\n\n0.7509130345985205\n\n\n\n\\(R^2 = \\frac{SSR}{SST} = 0.7509130345985205\\)\n0.7 이상이면 망한 모형까진 아니지만…\n\n계수 해석\n- 상수항\n\npredictr.intercept_\n\n-666.9377199366372\n\n\n\n기본적인 보험료(다른 모든 것이 0일 때)는 -666이다.~(딱봐도 이상하죠? 그래서 별로 의미는 없다.)~\n\n- 계수\n\npd.DataFrame({'columns' : X.columns, 'coef' : predictr.coef_})\n\n\n\n\n\n\n\n\ncolumns\ncoef\n\n\n\n\n0\nage\n256.856353\n\n\n1\nbmi\n339.193454\n\n\n2\nchildren\n475.500545\n\n\n3\nsex_female\n65.657180\n\n\n4\nsex_male\n-65.657180\n\n\n5\nsmoker_no\n-11924.267271\n\n\n6\nsmoker_yes\n11924.267271\n\n\n7\nregion_northeast\n587.009235\n\n\n8\nregion_northwest\n234.045336\n\n\n9\nregion_southeast\n-448.012814\n\n\n10\nregion_southwest\n-373.041756\n\n\n\n\n\n\n\n\n연속형 : 나이, bmi, 자녀의 수가 많을수록 보험료는 올라갔다.\n범주형 : 여성, 흡연자의 경우 보험료가 더 비쌌다.\n지역은 잘 모르겠으나, 나머지는 꽤 그럴듯해 보인다.(지역에 대한 정보는 알기 어려움…)"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html",
    "title": "RandomForest | 의사결정나무",
    "section": "",
    "text": "랜덤포레스트를 이용해 적합을 해보자!"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#라이브러리-imports",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#라이브러리-imports",
    "title": "RandomForest | 의사결정나무",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport sklearn.ensemble\n\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#사용할-데이터",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#사용할-데이터",
    "title": "RandomForest | 의사결정나무",
    "section": "2. 사용할 데이터",
    "text": "2. 사용할 데이터\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/insurance.csv')\ndf_train\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#bagging의-단점",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#bagging의-단점",
    "title": "RandomForest | 의사결정나무",
    "section": "3. Bagging의 단점",
    "text": "3. Bagging의 단점\n\nA. 일단 Bagging으로 적합\n\n\n## step 1\nX = pd.get_dummies(df_train.drop('charges', axis = 1), drop_first = True)\ny = df_train.charges\n\n## step 2\npredictr = sklearn.ensemble.BaggingRegressor()\n\n## step 3\npredictr.fit(X, y)\n\nBaggingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingRegressorBaggingRegressor()"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#b.-결과-시각화",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#b.-결과-시각화",
    "title": "RandomForest | 의사결정나무",
    "section": "### B. 결과 시각화",
    "text": "### B. 결과 시각화\n- 관찰 : 트리들이 다양하지 않다.\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[4],  ## 4번째 tree\n    feature_names = X.columns.to_list(),  ## 설명변수 이름\n    max_depth = 1\n);\n\nfig = plt.gcf()\nfig.set_dpi(200)\n\n\n\n\n\n리샘플링을 하더라도, 개별 노드의 피쳐는 바뀌지 않을 것임.\n\n트리가 다양하지 않은 게 약점이다. (왜?)\n\nC. 우수성 vs 다양성\n\n- 다양성이 없는게 왜 문제인가?\n\n기존의 의사결정나무 : 데이터에 최적화된 똑똑한 하나의 트리만 발견\nBagging : “데이터의 최적화”를 희생하고 “다양성”을 확보. 즉, 똑똑한 하나의 트리(원자료를 전부 사용한 트리)대신에 모자란 여러 개의 트리를 생성하고 힘을 합침.\n\n- 비슷한 생각을 가진 10명의 천재 vs 다양한 의견을 가진 10명의 범재\n\n뭐가 좋을까요?~(솔직히 전자가 더 좋은 것 같긴 한ㄷ)~\n\nref:김용대 교수님\n\n앙상블 : 같은 데이터를 여러 개의 기계학습 알고리즘들이 분석하여 각자 지식을 습득한 후 이를 결합하여 새롭고 유용한 지식을 창출하는 방법이다.\n앙상블의 성능을 높이기 위해서는 개별 알고리즘들의 성능보다는 알고리즘들의 다양성이 훨씬 중요하다. 즉, 주어진 문제에 대해서 모두 비슷한 답을 주는 성능이 우수한 10개의 알고리즘보다는 성능은 좀 떨어지지만 다양한 답을 제공하는 10개의 알고리즘이 앙상블에는 더 효율적이다.\n\n- 요약\n\n통찰 : Bagging은 의사결정나무보다 다양성을 추구하는 알고리즘이다.\n문제점 : 하지만 \\(\\textbf{X}\\)가 고차원인 상황에서 배깅만으로는 그렇게 다양한 트리가 나오지 않는다.(모든 트리가 일괄적으로 흡연여부가 보험료에 미치는 영향을 우선적으로 연구함…)\n소망 : 흡연여부만 연구하지 않고 다른 변수들을 최우선적으로 연구하는 사람이 있었으면 좋겠을 텐데…(그러면 트리가 더 다양해질 텐데…)"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#랜덤포레스트",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#랜덤포레스트",
    "title": "RandomForest | 의사결정나무",
    "section": "5. 랜덤포레스트",
    "text": "5. 랜덤포레스트"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#a.-개념",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#a.-개념",
    "title": "RandomForest | 의사결정나무",
    "section": "### A. 개념",
    "text": "### A. 개념\n- RandomForest = Bagging + max_feafure = int\n\n요런 느낌…(강제로 안해본 것들에 대해서도 적합하도록 만듦)\n\n- 알고리즘의 비교\n\n## Bagging\ntrees = []\nfor i in range(100) :\n    tree = sklearn.tree.DecisionTreeRegressor()\n    X_sample, y_sample = boostrap(X, y)  ## 실제론 없는 코드임, 함수지정 해줘야함... 아마도.\n    tree.fit(X_sample, y_sample)  ## 붓스트랩 된 샘플로만 적합\n    trees.append(tree)  ## 학습한 나무를 저장\n    \nyhat = ensemble(trees)  ## 이것도 실제론 없는 코드임... 그냥 종합한다고. 여러 개의 나무를 종합하여 하나의 예측값 산출\n\n## RandomForest\nforest = []\nfor i in range(100) :\n    tree = sklearn.tree.DecisionTreeRegressor(max_features = 1)  ## 매순간 설명변수가 바뀜\n    X_sample, y_sample = bootstrap(X, y)  ## 동일하게 매순간 샘플 바낌\n    tree.fit(X_sample, y_sample)\n    forest.append(tree)  ## 학습한 나무를 숲에 저장\n\nyhat = ensemble(forest)  ## 배깅과 동일하게 평균으로 종합하여 하나의 예측값으로 산출\n\n\nB. 일단 적합\n\n\n## step 1 -- 이미 함\n## step 2\npredictr = sklearn.ensemble.RandomForestRegressor(\n    max_depth = 1,\n    max_features = 1/3  ## parameter를 float으로 지정해줄 경우 비율로 지정됨\n)\n\n## step 3\npredictr.fit(X, y)\n\nRandomForestRegressor(max_depth=1, max_features=0.3333333333333333)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=1, max_features=0.3333333333333333)\n\n\n\n주의1 : max_feature = 1은 1개의 feature를 고려한다는 의미리고, max_feature = 1.0(default)은 100% feature를 고려한다는 의미이다.\n주의2 : max_features = 1.0이면 “RandomForest = Bagging”이다.(물론 적합하는 수가 랜덤포레스트는 100개긴 함… 아무튼 따라서 반드시 따로 지정해줄 것.)"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#c.-시각화",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#c.-시각화",
    "title": "RandomForest | 의사결정나무",
    "section": "### C. 시각화",
    "text": "### C. 시각화\n\nfig.set_size_inches\n\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[5],\n    feature_names = X.columns.to_list()\n);"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#랜덤포레스트-재현",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#랜덤포레스트-재현",
    "title": "RandomForest | 의사결정나무",
    "section": "5. 랜덤포레스트 재현",
    "text": "5. 랜덤포레스트 재현\n\nA. random_state 추출\n\n\npredictr.estimators_\n\n[DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1031990468),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1429725713),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=350207566),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1100997744),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=510567808),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1755990883),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1996922071),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=674517068),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=595230725),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1012915459),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1428147165),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1395867295),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=169573435),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=127953600),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1926193238),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1376984195),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=2082790252),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=2057838983),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1053173062),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1578708662),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=472956975),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1333396687),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1821740983),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=783319209),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=36941881),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=799253070),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=635449342),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=132352020),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1264208795),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=350471677),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1095074218),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1593268467),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1404619332),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1608943847),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1236305187),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=961677438),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1166412997),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=650745900),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=2052279443),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=117113583),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1527210463),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1678776161),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1988471961),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1217358045),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=2088952586),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1865914230),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1078690409),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=982590624),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1057491803),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1673372238),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1270235607),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1796340065),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=901389640),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1043083946),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=769852308),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1182321155),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1842203459),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1880960863),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=898421758),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=934179744),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=452818797),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1305469509),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1213378674),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1961139761),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1891314091),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=367224874),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1959981631),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=369686042),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1645132117),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1253064599),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1639414708),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=543064642),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1822056120),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1524730351),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1968911770),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=164462170),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1247270882),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=2146910087),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1747823265),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1367209261),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=2035701061),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=158299833),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=263663306),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=636743074),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1080424218),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1350662801),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1024104977),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=387973769),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1853942881),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=78023758),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=479660592),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=475053108),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=307992757),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=175199539),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1958141260),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1358125826),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1050156658),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=793812352),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=27296515),\n DecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                       random_state=1256409330)]\n\n\n\n트리 오브젝트 각자 고유의 random_state를 가지고 있다.\n\n- 첫번째 트리 : random_state확인\n\npredictr.estimators_[0].random_state\n\n1031990468\n\n\n- 각 나무들의 random_state 추출\n\nrs = [tree.random_state for tree in predictr.estimators_]"
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#b.-forest-생성",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#b.-forest-생성",
    "title": "RandomForest | 의사결정나무",
    "section": "### B. forest 생성",
    "text": "### B. forest 생성\n\nsklearn.tree.DecisionTreeRegressor(\n    max_depth = 1, max_features = 1/3,\n    random_state = predictr.estimators_[0].random_state\n)\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1031990468)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1031990468)\n\n\n\nmy_forest = [sklearn.tree.DecisionTreeRegressor(max_depth = 1, max_features = 1/3, random_state = r) for r in rs]\n##한번에\n##my_forest = [sklearn.tree.DecisionTreeRegressor(max_depth = 1, max_features = 1/3, random_state = tree.random_state) for tree in predictr.estimators_]\n\n\nmy_forest[-1]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1256409330)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1256409330)\n\n\n\npredictr.estimators_[-1]\n\nDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1256409330)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1, max_features=0.3333333333333333,\n                      random_state=1256409330)\n\n\n\n똑같은 개체임니당.\n\n\nC. 붓스트랩 샘플 생성\n\n- 저장된 부스트랩 샘플을 확보하자.\n\npredictr.estimators_samples_  ## Bagging에서 썼던 코드...\n\nAttributeError: 'RandomForestRegressor' object has no attribute 'estimators_samples_'\n\n\n\n샘플들을 재현하기 귀찮게 되어있다…(그런 메소드는 없다네요…)\n\n- 그냥 새로 만들어보자!(코드 뜯기)\n\nsklearn.ensemble._forest._generate_sample_indices?\n\n\nSignature:\nsklearn.ensemble._forest._generate_sample_indices(\n    random_state,\n    n_samples,\n    n_samples_bootstrap,\n)\nDocstring: Private function used to _parallel_build_trees function.\nFile:      c:\\users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\nType:      function\n\n\n\n\n트리에 사용할 샘플을 만드는 코드.\n\nrandom_state와 샘플의 크기, 붓스트랩 할 샘플의 크기를 지정하면 샘플의 인덱스를 내놓는 코드.\n\nsample = sklearn.ensemble._forest._generate_sample_indices\nsample(1,10,10)\n\narray([5, 8, 9, 5, 0, 0, 1, 7, 6, 9])\n\n\n- 원자료\n\ndf_train.shape\n\n(1338, 7)\n\n\n\nmy_index = [sample(random_state = r, n_samples = 1338, n_samples_bootstrap = 1338) for r in rs]\n\n\n샘플을 뽑는 random_state가 r인 인덱스가 나온다."
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#d.-적합",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#d.-적합",
    "title": "RandomForest | 의사결정나무",
    "section": "### D. 적합",
    "text": "### D. 적합\n\nfor idx, tree in zip(my_index, my_forest) :\n    X_sample, y_sample = X.loc[idx], y.loc[idx]\n    ## or np.array(X)[idx], np.array(y)[idx]\n    tree.fit(X_sample, y_sample)\n\n## or\n#for i in range(len(my_index)):\n#    X_sample, y_sample = X.loc[my_index[i]], y.loc[my_index[i]]\n#    my_forest[i].fit(X_sample, y_sample)\n\n\nE. 앙상블\n\n\nnp.array([tree.predict(X) for tree in my_forest]).mean(axis = 0).shape\n\n(1338,)\n\n\n\ndef ensemble(forest) :\n    return np.stack([tree.predict(X) for tree in my_forest]).mean(axis = 0)\n\n\nensemble(my_forest)\n\narray([16637.15797026, 11985.8217605 , 12109.06436558, ...,\n       11966.44532935, 10499.17514846, 18168.48871085])\n\n\n\npredictr.predict(X)\n\narray([16637.15797026, 11985.8217605 , 12109.06436558, ...,\n       11966.44532935, 10499.17514846, 18168.48871085])\n\n\n\n최종 결과가 나온다."
  },
  {
    "objectID": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#f.-주의",
    "href": "2023_MP/practice/25. 의사결정나무 - 랜덤포레스트.html#f.-주의",
    "title": "RandomForest | 의사결정나무",
    "section": "### F. 주의",
    "text": "### F. 주의\n- max_depth를 1로 제한한 이유\n\nmax_depth가 깊을 경우 두 결과가 일치하지 않을 수 있다.\n\n\n실제 돌아가는 코드와의 괴리\n\n\nsklearn.tree.plot_tree(predictr.estimators_[0]);\n\n\n\n\n\nsklearn.tree.plot_tree(my_forest[0]);\n\n\n\n\n\nsamples의 값이 다름. 배깅도 그렇고, 랜덤포레스트도 그렇고 적합 중 가중치(predictr.fit(sample_weight = [...])를 취하는 방식으로 적합하기 때문.\n\nmax_depth가 깊을 경우 샘플이 하나여서 더이상 노드를 나누지 못할 수 있음. 가중치를 지정해줄 수도 있지만, 그건 너무 과잉투자인 것 같아 이정도만 설명하고 넘어갑니당…"
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html",
    "href": "2023_MP/practice/A7. 선형모형의 적.html",
    "title": "선형모형의 적",
    "section": "",
    "text": "우리의 주적은 북한인데, 선형모형의 적은?"
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html#라이브러리-imports",
    "href": "2023_MP/practice/A7. 선형모형의 적.html#라이브러리-imports",
    "title": "선형모형의 적",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing\nimport sklearn.impute\nimport seaborn as sns\nimport sklearn.tree"
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html#선형모형의-적",
    "href": "2023_MP/practice/A7. 선형모형의 적.html#선형모형의-적",
    "title": "선형모형의 적",
    "section": "2. 선형모형의 적",
    "text": "2. 선형모형의 적\n\nA. 결측치의 존재\n\n문제 : 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않는다.\n- 해결방안\n1 : 결측치를 제거\n\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n둘을 혼합\n\n\n\n결측치를 impute\n\n\n\ntrain에서는 fit_transform, test에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값으로 일괄 impute\ninterploation(이미지 또는 시계열, 근처의 값과 자연스럽게 연동되도록 만들 수 있음)\n~train, test data를 합쳐서 fit_transform~ 이건 정보누수로 실격사유가 된다\n\n\n- 사용 가능한 코드나 모듈\n\nisna(), dropna(), sklearn.inpute의 하위 모듈 등."
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html#b.-다중공선성의-존재",
    "href": "2023_MP/practice/A7. 선형모형의 적.html#b.-다중공선성의-존재",
    "title": "선형모형의 적",
    "section": "### B. 다중공선성의 존재",
    "text": "### B. 다중공선성의 존재\n문제 : 데이터의 설명변수가 역할이 겹칠 경우, 선형모형의 일반화 성능이 좋지 않음.\n- 해결방안\n\n\n변수 제거 &gt; 설명변수 간 corr을 파악하고, 느낌적으로 제거 &gt; &gt; PCA 등 차원축소기법을 이용한 제거\n공선성을 가지는 변수를 모아 새로운 변수로 변환 &gt; 느낌적으로 변환 &gt; &gt; PCA를 이용한 변환\nLasso, Ridge 등 패널티 계열을 사용 &gt; Lasso : l1 / liblinear &gt; &gt; Ridge : l2 &gt; &gt; Elastic net\n\n\n- corr파악 후 느낌적으로 제거의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\n\npandas의 데이터프레임에는 자체적으로 해당 메소드를 지원한다.\n\n\nsns.heatmap(X.corr(), annot = True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n- toeic과 유사 toeic끼리 상관성이 짙네?\n\n제거한다.\n\n\nC. 관련이 없는 변수의 존재\n\n문제 : 데이터에서 불필요한 설명변수가 너무 많을 경우, 선형모형의 일반화 성능이 좋지 않음.(overfitting)\n\n예시 : 고객이름, ID, Index 관련 변수(물론 얘네들도 어딘가 쓸모가 있을 수도 있다…)\n\n- 해결방법\n\n\n변수 제거 &gt; (y, X)의 corr을 파악하고 느낌적으로 제거(위에서와 달리 관련이 있어야 한다.) &gt; &gt; PCA를 이용한 제거 &gt; &gt; Lasso를 이용한 제거(여기서 Ridge는 사용하면 안된다. 해당 모듈은 유사한 것들의 계수 합이 일정하도록 조정하는 거니까…)\n더 많은 데이터를 확보 &gt; 하지만 이는 어렵다… 어떤 변수가 관련이 없다는 것을 파악하기 위해선 데이터를 많이 가져와야 하는데, Feature의 수가 많아질 때 필요한 데이터의 수는 지수적으로 증가한다.\n\n\n- 느낌적으로 제거 예시\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(), annot = True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nbalance0 ~ 2는 employment와의 상관성이 낮다. 따라서 제거하고 분석한다.\n\n\n## 1\nX = df_train.loc[:, :'gpa']\ny = df_train.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.882\n\n\n- Lasso를 이용한 제거 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n로지스틱 선형 회귀가 필요한 경우이다. 로지스틱 또한 penalty 계열 분석을 할 수 있다.\n\n\n## 1\nX = df_train.drop('employment', axis = 1)\ny = df_train.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(Cs = [0.1, 1, 10, 100], penalty = 'l1', solver = 'liblinear', random_state = 42)\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.876\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))  ## 시리즈의 경우 1차원의 입력값만 받는다.\ns.index = X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64\n\n\n\n위에서 쓸모없는 것을 제거하고 분석한 것에 비해 점수가 낮지만, 쓸모없는 것이라는 사실을 모르는 상황에서는 Lasso가 상당히 괜찮다."
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html#d.-이상치의-존재",
    "href": "2023_MP/practice/A7. 선형모형의 적.html#d.-이상치의-존재",
    "title": "선형모형의 적",
    "section": "### D. 이상치의 존재",
    "text": "### D. 이상치의 존재\n문제 : 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 해결방법\n\n\n이상치를 제거하고 분석 &gt; 느낌적으로 제거 &gt; &gt; 이상치를 감지하는 지표를 사용하여 제거 &gt; &gt; 이상치를 자동으로 감지하는 모형 사용하여 이상치 제거 후 분석\n로버스트 선형회귀 계열을 이용 &gt; 이상치에 큰 영향을 받지 않음 &gt; sklearn.linear_model.HuberRegressor 등\n이상치를 완화시키는 변환을 사용 &gt; sklearn.preprocessing.PowerTransformer를 이용\n\n\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\n\ntransformr.fit_transform(df_train)\n\narray([[-1.40729341,  2.42405408],\n       [-1.31406689, -0.18677452],\n       [-1.13030154,  0.16485704],\n       [-0.50278108,  0.17667635],\n       [-0.02130412,  0.41617603],\n       [ 0.13926015,  0.55696978],\n       [ 0.81742569, -1.03040835],\n       [ 0.96759638, -0.62032873],\n       [ 0.96759638, -1.33362249],\n       [ 1.48386844, -0.56759919]])\n\n\n\nx, y = transformr.fit_transform(df_train).T\nx, y\n\n(array([-1.40729341, -1.31406689, -1.13030154, -0.50278108, -0.02130412,\n         0.13926015,  0.81742569,  0.96759638,  0.96759638,  1.48386844]),\n array([ 2.42405408, -0.18677452,  0.16485704,  0.17667635,  0.41617603,\n         0.55696978, -1.03040835, -0.62032873, -1.33362249, -0.56759919]))\n\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o', label = 'before')\nplt.plot(x, y, 'o', label = 'after')\nplt.xlabel('temp')\nplt.ylabel('ice sales')\nplt.legend()\nplt.show()\n\n\n\n\n\n강제로 정규화한 모습이다.\n\n\ntransformr.inverse_transform(transformr.fit_transform(df_train))\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but PowerTransformer was fitted with feature names\n  warnings.warn(\n\n\narray([[-4.1       , 50.        ],\n       [-3.7       ,  9.2341745 ],\n       [-3.        ,  9.64277825],\n       [-1.3       ,  9.65789368],\n       [-0.5       ,  9.98778744],\n       [-0.3       , 10.20595116],\n       [ 0.3       ,  8.48692458],\n       [ 0.4       ,  8.81722682],\n       [ 0.4       ,  8.27315516],\n       [ 0.7       ,  8.8637837 ]])\n\n\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\n어차피 역변환 할 수 있는 것은 상관이 없다.\n\n\nE. 교호작용의 존재\n\n문제 : 설명 변수 간의 상호작용이 있는 경우, 이를 고려하지 않으면 데이터를 잘 설명하지 못할 수 있음.\n- 해결방안\n\n\n교호작용이 있는 열들의 값끼리 곱함\n교호작용에 영향을 받지 않는 모델 사용 &gt; sklearn.tree.DecisionTreeRegressor()\n\n\n- 교호작용이 있는 열을 곱함\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n둘 다 했을 때 가장 평균이 높고, 각각 하는 것만으로는 그렇게 큰 영향은 없는 것 같다.\n\n교호작용을 고려하지 않은 분석\n\n## 1\nX = df_train.drop('Weight_Loss', axis = 1)\ny = df_train.Weight_Loss\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.8208414124769222\n\n\n\npredictr.coef_\n\narray([5.21904037, 9.74766346])\n\n\n\n보충제는 5kg, 운동은 10kg의 감량효과가 있다고 추정하고 있다.\n\n\ndf_train.assign(Weight_Loss_hat = predictr.predict(X)).drop('Weight_Loss', axis = 1)\\\n.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss_hat', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n예측값과 실제 값의 차이가 크다.\n\n교호작용을 고려한 분석\n\ndf_train.assign(Interaction = df_train.Supplement * df_train.Exercise)\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nInteraction\n\n\n\n\n0\nFalse\nFalse\n-0.877103\nFalse\n\n\n1\nTrue\nFalse\n1.604542\nFalse\n\n\n2\nTrue\nTrue\n13.824148\nTrue\n\n\n3\nTrue\nTrue\n13.004505\nTrue\n\n\n4\nTrue\nTrue\n13.701128\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\nFalse\n\n\n9996\nFalse\nFalse\n-0.217816\nFalse\n\n\n9997\nFalse\nTrue\n4.072701\nFalse\n\n\n9998\nTrue\nFalse\n-0.253796\nFalse\n\n\n9999\nFalse\nFalse\n-1.399092\nFalse\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\n## 1\n_df = df_train.assign(Interaction = df_train.Supplement * df_train.Exercise)\nX = _df.drop('Weight_Loss', axis = 1)\ny = _df.Weight_Loss\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9727754257714795\n\n\n\n정확도가 개선되었다.\n\n\ndf_train.assign(Weight_Loss_hat = predictr.predict(X)).drop('Weight_Loss', axis = 1)\\\n.pivot_table(index = 'Supplement', columns = 'Exercise', values = 'Weight_Loss_hat', aggfunc = 'mean')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n평균을 보면(오차를 제거함) 표본과 동일한 것을 볼 수 있다."
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html#교호작용",
    "href": "2023_MP/practice/A7. 선형모형의 적.html#교호작용",
    "title": "선형모형의 적",
    "section": "3. 교호작용",
    "text": "3. 교호작용"
  },
  {
    "objectID": "2023_MP/practice/A7. 선형모형의 적.html#a.-아이스크림-타입-별-판매량",
    "href": "2023_MP/practice/A7. 선형모형의 적.html#a.-아이스크림-타입-별-판매량",
    "title": "선형모형의 적",
    "section": "### A. 아이스크림 타입 별 판매량",
    "text": "### A. 아이스크림 타입 별 판매량\n- 왠지 익숙한 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nset(df_train.type)\n\n{'choco', 'vanilla'}\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\nplt.legend()\nplt.show()\n\n\n\n\n- 아이스크림의 종류에 따라 온도가 판매량에 미치는 정도가 다를 것으로 예상된다.\n\n아이스크림 종류와 온도간에 교호작용이 있다.\n\n교호작용을 고려하지 않은 경우\n\n## 1\nX = pd.get_dummies(df_train.drop('sales', axis = 1))\ny = df_train.sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9249530603100549\n\n\n\n이것만으로도 나름 높은 점수가 나오긴 했지만… 언제나 개선할 수 있는 건 개선해야 한다.\n\n\n_df = df_train.assign(sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, _df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, _df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n마음속의 언더라잉과 맞지 않는다 : 언더피팅된 상황이다.\n\n교호작용을 고려\n\nvan = pd.get_dummies(df_train.type, drop_first = True)*1\nvan\n\n\n\n\n\n\n\n\nvanilla\n\n\n\n\n0\n0\n\n\n1\n0\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n...\n...\n\n\n95\n1\n\n\n96\n1\n\n\n97\n1\n\n\n98\n1\n\n\n99\n1\n\n\n\n\n200 rows × 1 columns\n\n\n\n\n_df = df_train.assign(Interaction = df_train.temp * van.vanilla)\n_df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nInteraction\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n-0.0\n\n\n1\n-3.7\n35.852524\nchoco\n-0.0\n\n\n2\n-3.0\n37.428335\nchoco\n-0.0\n\n\n3\n-1.3\n38.323681\nchoco\n-0.0\n\n\n4\n-0.5\n39.713362\nchoco\n-0.0\n\n\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n12.4\n\n\n96\n13.4\n129.300464\nvanilla\n13.4\n\n\n97\n14.7\n136.596568\nvanilla\n14.7\n\n\n98\n15.0\n136.213140\nvanilla\n15.0\n\n\n99\n15.2\n135.595252\nvanilla\n15.2\n\n\n\n\n200 rows × 4 columns\n\n\n\n\n## 1\nX = pd.get_dummies(_df.drop('sales', axis = 1), drop_first = True)\ny = _df.sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9865793819066231\n\n\n\n점수가 훨씬 높게 나왔다.\n\n\n__df = _df.assign(sales_hat = predictr.predict(X))\n__df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nInteraction\nsales_hat\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n-0.0\n31.403121\n\n\n1\n-3.7\n35.852524\nchoco\n-0.0\n32.209366\n\n\n2\n-3.0\n37.428335\nchoco\n-0.0\n33.620295\n\n\n3\n-1.3\n38.323681\nchoco\n-0.0\n37.046835\n\n\n4\n-0.5\n39.713362\nchoco\n-0.0\n38.659325\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n12.4\n122.492017\n\n\n96\n13.4\n129.300464\nvanilla\n13.4\n127.521196\n\n\n97\n14.7\n136.596568\nvanilla\n14.7\n134.059129\n\n\n98\n15.0\n136.213140\nvanilla\n15.0\n135.567883\n\n\n99\n15.2\n135.595252\nvanilla\n15.2\n136.573719\n\n\n\n\n200 rows × 5 columns\n\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla')\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, __df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, __df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\npredictr.coef_\n\narray([ 2.01561216,  3.01356716, 20.46306209])\n\n\n\n모델이 언더라잉을 잘 따라가는 것을 볼 수 있다.(애초에 계수가 세개가 됨…)\n\n\nB. 교호작용, tree\n\nsklearn.tree.DecisionTreeRegressor()를 사용하면 교호작용을 손쉽게 적합할 수 있다.\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n아까와 동일한 자료를 tree로 분석해보자…\n\n\n## 1\nX = pd.get_dummies(df_train.drop('sales', axis = 1), drop_first = True)\ny = df_train.sales\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\n0.9963887702553287\n\n\n\n높은 스코어가 나온다.(오버피팅된 것은 아닐까?)\n\n\n_df = df_train.assign(sales_hat = predictr.predict(X))\n_df\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\nsales_hat\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n32.950261\n\n\n1\n-3.7\n35.852524\nchoco\n35.852524\n\n\n2\n-3.0\n37.428335\nchoco\n37.428335\n\n\n3\n-1.3\n38.323681\nchoco\n38.323681\n\n\n4\n-0.5\n39.713362\nchoco\n39.713362\n\n\n...\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n119.708075\n\n\n96\n13.4\n129.300464\nvanilla\n129.300464\n\n\n97\n14.7\n136.596568\nvanilla\n136.596568\n\n\n98\n15.0\n136.213140\nvanilla\n136.213140\n\n\n99\n15.2\n135.595252\nvanilla\n135.595252\n\n\n\n\n200 rows × 4 columns\n\n\n\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, df_train.loc[df_train.type == 'choco'].sales, 'o', label = 'choco', alpha = 0.5)\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, df_train.loc[df_train.type != 'choco'].sales, 'o', label = 'vanilla', alpha = 0.5)\n\nplt.plot(df_train.loc[df_train.type == 'choco'].temp, _df.loc[df_train.type == 'choco'].sales_hat, '--', color = 'C0')\nplt.plot(df_train.loc[df_train.type != 'choco'].temp, _df.loc[df_train.type != 'choco'].sales_hat, '--', color = 'C1')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n오차항까지 적합하고는 있으나… 처음 교호작용을 고려하지 않은 모델보다 성능은 좋은 것 같다. 따라서 이는 상당히 유용하다."
  },
  {
    "objectID": "2023_MP/practice/A3. predictor의 이해.html",
    "href": "2023_MP/practice/A3. predictor의 이해.html",
    "title": "sklearn.linear_model의 작동원리",
    "section": "",
    "text": "LogisticRegression의 작동원리와 sklearn.linear_model에 대해서 자세히 알아보자."
  },
  {
    "objectID": "2023_MP/practice/A3. predictor의 이해.html#라이브러리-imports",
    "href": "2023_MP/practice/A3. predictor의 이해.html#라이브러리-imports",
    "title": "sklearn.linear_model의 작동원리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport itertools"
  },
  {
    "objectID": "2023_MP/practice/A3. predictor의 이해.html#로지스틱-회귀분석의-원리",
    "href": "2023_MP/practice/A3. predictor의 이해.html#로지스틱-회귀분석의-원리",
    "title": "sklearn.linear_model의 작동원리",
    "section": "2. 로지스틱 회귀분석의 원리",
    "text": "2. 로지스틱 회귀분석의 원리\n\n저번에 봤었던 취업 자료를 가져와보자.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\nemployment를 예측하려면…\n\n## 1\nX = df.drop(['employment'], axis = 1)\ny = df.employment\n\n## 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## 3\npredictr.fit(X, y)\n\npredictr.predict(X)  ## yhat\n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], dtype=int64)\n\n\n\n\\(\\hat{y}\\)은 어떻게 나왔는가?\n- 아래 수식에 의하여 나왔음…\n\npredictr.coef_, predictr.intercept_  ## 로지스틱임에도 기울기와 절편이 있다.\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 - 8.45433334  ## yhat\nv = 1/(1+np.exp(-u))  # v : 확률같은 거\n\nv\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()  ## v가 0.5보다 클 경우 전부 True였음을 알 수 있음\n\n1.0\n\n\n해당 개체에 처리가 취해질 확률을 구하고, 그것이 0.5보다 크면 처리를 취한다.\n- 만약 적합된 v값을 알고 싶다면…\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3)\n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])\n\n\n\n우측의 값과 일치하는 것을 알 수 있다.(0번째 : 0일 확률, 1번째 : 1일 확률)"
  },
  {
    "objectID": "2023_MP/practice/A3. predictor의 이해.html#predictor-파고들기",
    "href": "2023_MP/practice/A3. predictor의 이해.html#predictor-파고들기",
    "title": "sklearn.linear_model의 작동원리",
    "section": "3. predictor 파고들기",
    "text": "3. predictor 파고들기\n\n아래와 같은 데이터를 가공해서 0~7까지는 train, 8~9까지는 test 셋으로 쓰도록 하자.\n\n\ndf = pd.DataFrame({'X':np.arange(20,30),'y':-np.arange(10)+1+np.random.randn(10)*0.1})\ndf\n\n\n\n\n\n\n\n\nX\ny\n\n\n\n\n0\n20\n0.992487\n\n\n1\n21\n-0.040013\n\n\n2\n22\n-0.984351\n\n\n3\n23\n-2.085536\n\n\n4\n24\n-3.023587\n\n\n5\n25\n-4.287162\n\n\n6\n26\n-5.085849\n\n\n7\n27\n-6.110568\n\n\n8\n28\n-6.798420\n\n\n9\n29\n-8.028488\n\n\n\n\n\n\n\n\ndf_train = df[:8]\ndf_test = df[8:]\n\ndf_train_X = df_train[['X']]\ndf_train_y = df_train[['y']]\ndf_test_X = df_test[['X']]\ndf_test_y = df_test[['y']]\n\n\n## predictor 두 개를 만들도록 리스트 컴프리헨션\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\npredictors[0].fit(df_train_X, df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n첫 번째 predictr에 적합하면 .score, .intercept_, .coef_가 해금된다. 두 번째 predictr에는 적용되지 않는다.~(당연한 거 아님?)~\n\n\nX, y에 들어갈 수 있는 형식\n\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr = sklearn.linear_model.LinearRegression()\n        predictr.fit(X,y)\n        return 'no error'\n    except:\n        return 'error'  ## 예외사항(error) 발생 시의 output\n\n\n가능한 형식들을 모두 모아놨다. 그럼 이것들을 가지고 어떤 녀석이 되는 지 딕셔너리 컴프리헨션을 해보자.\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n## itertools.product() : 원소들의 데카르트 곱을 리스트로 반환.\n## itertools.product('ABCD', repeat = 2)의 경우 크기가 2인 앞의 string 조합을 모두 반환\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론 | X에는 2차원 데이터만 들어올 수 있지만, y에는 1ㆍ2차원 데이터 모두가 들어올 수 있다.\n- 그리고 일반적으로, X에는 2차원 데이터 배열이 imput되기를 기대하고, y에는 1차원 데이터 배열이 imput되기를 기대한다."
  },
  {
    "objectID": "2023_MP/practice/A3. predictor의 이해.html#첨언-데이터셋-이름-설정에-대하여",
    "href": "2023_MP/practice/A3. predictor의 이해.html#첨언-데이터셋-이름-설정에-대하여",
    "title": "sklearn.linear_model의 작동원리",
    "section": "4. 첨언 | 데이터셋 이름 설정에 대하여",
    "text": "4. 첨언 | 데이터셋 이름 설정에 대하여\n\n설명변수와 반응변수, 테스트 셋과 트레인 셋을 부르는 변수 명을 어떻게 설정해야 할 지 나타내겠다.\n\nX : 설명변수 & Train\ny : 반응변수 & Train\nXX : 설명변수 & Test\nyy : 반응변수 & Test\nyhat : predictr.predict(X)\nyyhat : predictr.predict(XX)"
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html",
    "title": "다중공선성의 해소",
    "section": "",
    "text": "Ridge와 Lasso를 통해 다중공선성을 극복해보자."
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#라이브러리-imports",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#라이브러리-imports",
    "title": "다중공선성의 해소",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#ridge-l2-penalty",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#ridge-l2-penalty",
    "title": "다중공선성의 해소",
    "section": "2. Ridge : L2-penalty",
    "text": "2. Ridge : L2-penalty\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n위와 같은 데이터에서 toeic0~toeic499는 설명변수 간 상관관계가 높은 녀석들이다.\n\n\nA. True World\n\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic']\nyy = df_test[['employment_score']]\n## step2 \npredictr = sklearn.linear_model.LinearRegression()\n## step3\npredictr.fit(X,y)\n## step4 : pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(f'train_score:\\t{predictr.score(X,y):.4f}')\nprint(f'test_score:\\t{predictr.score(XX,yy):.4f}')\n\ntrain_score:    0.9133\ntest_score: 0.9127\n\n\n- 언더라잉만 잘 적합한 결과, 오차항 때문에 1.0은 나오기 힘듦\n\n이 점수는 현실적으로 달성하기 어려워…"
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#b.-무지성",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#b.-무지성",
    "title": "다중공선성의 해소",
    "section": "### B. 무지성…",
    "text": "### B. 무지성…\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.drop(['employment_score'], axis = 1)\ny = df_train[['employment_score']]\nXX = df_test.drop(['employment_score'], axis = 1)\nyy = df_test[['employment_score']]\n## step2 \npredictr = sklearn.linear_model.LinearRegression()\n## step3\npredictr.fit(X,y)\n## step4 : pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\n명백한 오버피팅…\n\n\nC. Ridge\n\n- 통계학자 : 이럴경우 Ridge를 사용하면 됩니다…\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge()  ## 로지스틱의 경우 LogisticRegressionCV(penalty = 'l2')를 사용 가능\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1173\n\n\n\n??? 안되는데요?\n\n- 하이퍼 파라미터를 튜닝하면 됩니다…\n\n## step1 --- 다넣음\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)  ## alpha = 500000000.\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\n오라클에 비할 바는 아니긴 한데, 공선성이 있는 경우라도 적절한 alpha를 고른다면 망하지는 않음."
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#d.-ridge의-작동원리",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#d.-ridge의-작동원리",
    "title": "다중공선성의 해소",
    "section": "### D. Ridge의 작동원리",
    "text": "### D. Ridge의 작동원리\n- 정확한 설명…\nSVD를 이용하여 이론적으로 계산하면, sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 가 항상 존재함을 증명할 수 있음…\n\n그렇다네요.\n\n- 직관적 설명(엄밀하지 않은 설명)\n\nLinearRegression은 왜 망했지???\n\n\n취업 자료의 예제를 보면 토익 성적의 계수는 실제로 0.01이다. 적당히… * toeic_coef+toeic0_coef+…+toeic499_coef \\(\\approx\\) 0.01이라면 대충 맞는 답이다.\n\n\n근데 사실 이 0.01이라는 값은 몇 개의 계수만 있어도 만들 순 있을거임… -&gt; 나머지 설명변수가 모두 불필요한 특징이 됨.\n\n\n그래가지고 불필요한 특징은 다중공선성의 문제 때문에 오버피팅을 유발한다.\n\n그래서 Ridge는 몇 개의 계수만 빼고 나머지들이 쓸모없는 게 되지 않도록, 다 유의미하도록 계수에 패널티를 부여한다.\n\nE. \\(\\alpha\\)에 따른 계수값 변화\n\n- 여러 개의 predictor를 alpha의 값을 달리하며 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## 아래에서 배울 RidgeCV에서 이 값들 중 어느 값이 가장 좋을 지 결정하게 할 수 있음\n\n## step3 \nfor predictr in predictrs:  ## 이건 리스트로 만드는 게 아니니까...\n    predictr.fit(X,y)\n## step4 -- pass \n\n\nplt.plot(predictrs[0].coef_[1:], label = r'$\\alpha$ = {}'.format(predictrs[0].alpha))\nplt.plot(predictrs[2].coef_[1:], label = r'$\\alpha$ = {}'.format(predictrs[2].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\nalpha의 값이 작을수록, 그 변동 폭이 줄어듦을 알 수 있다.\n- 마지막 predictor의 계수값을 살펴보면…\n\n\ns = pd.Series(predictrs[-1].coef_)\ns.set_axis(X.columns, axis = 0)\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음(한두개로 계수 0.01을 만들 수 없음)\n모든 변수는 대량 2e-5(\\(\\approx\\frac{1}{100}\\frac{1}{501}\\))정도 똑같이 중요하다고 생각된다.\n살짝 (\\(\\frac{1}{100}\\frac{1}{501}\\))보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아니다.\n\n\n[predictr.coef_[1:].sum() for predictr in predictrs]\n\n[0.010274546089787007,\n 0.010157633994689774,\n 0.009948779293105905,\n 0.009866050921714562,\n 0.009854882844936588,\n 0.009820059959693872,\n 0.00949099901512329]\n\n\n\n갈수록 합의 크기가 작아짐…\n\n\n1/100*1/501\n\n1.9960079840319362e-05\n\n\n\n게대가 본래 기대될 회귀계수의 값보다 전체적으로 조금씩 낮은 편"
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#f.-alpha-정리",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#f.-alpha-정리",
    "title": "다중공선성의 해소",
    "section": "### F. \\(\\alpha\\) 정리",
    "text": "### F. \\(\\alpha\\) 정리\n- L2-penalty는 대충 분산같은 것…\n\nx = np.random.randn(5)\nL2_penalty = (x**2).sum()  ## 제곱합, 평균에서 멀어진...\n(L2_penalty, 5*(x.var() + (x.mean()**2)))  ## 2차 적률인듯. E(X**2)\n\n(10.591975556137934, 10.591975556137934)\n\n\n\nfor predictr in predictrs :\n    print(\n        f'alpha={predictr.alpha:.0e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}')\n\nalpha=5e+02 l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5e+03 l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5e+04 l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5e+05 l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5e+06 l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5e+07 l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5e+08 l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n\nalpha의 값이 늘어날수록, penalty의 값이 규모가 작아진다. 그에따라 계수들의총합도 점점 낮아진다…\n게다가 test_score도 어느순간부터 낮아지기 시작한다…"
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#ridgecv",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#ridgecv",
    "title": "다중공선성의 해소",
    "section": "3. RidgeCV",
    "text": "3. RidgeCV\n- 입력한 alpha값들 중에서 가장 적절한 alpha값을 제시해준다.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()  ## 일단 alpha를 지정해주지 않는 모습...\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\nprint(f'df_train score : {predictr.score(X, y):.5f}')\nprint(f'df_test score : {predictr.score(XX, yy):.5f}')\n\ndf_train score : 1.00000\ndf_test score : 0.11915\n\n\n\n아직 overfitting된 모습…\n\n왜냐! alphas의 후보는 0.1, 1.0, 10.0이 디폴트니까…\n- 따라서 이 후보를 직접 넣어주자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 -- 여기서 alpha의 후보들을 alphas에 리스트로 지정해준다.\npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8])\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\n(predictr.score(X, y), predictr.score(XX, yy))\n\n(0.7521268560159359, 0.7450309251010893)\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n\nalpha를 5,000,000로 설정했더니 가장 좋은 결과가 나왔다는 것을 알 수 있다."
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#lasso",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#lasso",
    "title": "다중공선성의 해소",
    "section": "4. Lasso",
    "text": "4. Lasso\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\nA. Lasso를 이용한 분석\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\npredictr = sklearn.linear_model.Lasso()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n(0.8600312387900632, 0.8306176063318933)\n\n\n\nprint(f'train_score:\\t {predictr.score(X,y):.4f}')\nprint(f'test_score:\\t {predictr.score(XX,yy):.4f}')\n\ntrain_score:     0.8600\ntest_score:  0.8306\n\n\n\nalpha를 default로 두었음에도 굉장히 우수한 결과가 나왔다."
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#b.-lasso의-원리",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#b.-lasso의-원리",
    "title": "다중공선성의 해소",
    "section": "### B. Lasso의 원리",
    "text": "### B. Lasso의 원리\n- 정확한 설명\n\n지금 이해하기엔 어려움…\n\n- 상관성이 짙은 설명변수 몇개로만 그 합의 계수를 만들게 해서는 안된다.\n\n아주 적은 숫자의 coef만 살려두고, 나머지는 0으로 강제한다.\n계수가 0이라는 것은 해당 변수를 제거한 것과 같은 효과를 가진다.\n\n\nplt.plot(predictr.coef_[1:])\n\n\n\n\n\n실제로 계수값이 0인 녀석이 많음을 알 수 있다.\n\n\nC. \\(\\alpha\\)의 값에 따른 변화\n\n- 여러 개의 predictor를 학습시켜 계수값들의 변화를 관찰해보자.\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\nalphas = np.linspace(0.1, 2, 20)\npredictrs = [sklearn.linear_model.Lasso(alpha = alpha) for alpha in alphas]\n\n## 3\nfor predictr in predictrs:\n    predictr.fit(X, y)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.115e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.991e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.375e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.588e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.730e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.671e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.117e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.875e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.086e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.192e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+00, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n\nplt.plot(predictrs[0].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[9].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[9].alpha.round(5)))\nplt.plot(predictrs[-1].coef_[1:], label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\nplt.show()\n\n\n\n\n\n계수값들의 분산이 갈수록 작아지는 것을 느낄 수 있다.\n\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[9].alpha:.4f}\\tsum(toeic_coef)={predictrs[9].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.1000    sum(toeic_coef)=0.010169320378140704\nalpha=1.0000    sum(toeic_coef)=0.009987870459109604\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n\npredictor들의 toeic 계수 합은 여전히 0.01 근처….\n\n\nplt.plot([(predictr.coef_ != 0).sum() for predictr in predictrs])\n\n\n\n\n\nalpha값이 커질수록 0이 아닌 계수의 갯수가 줄어드는 것을 볼 수 있다."
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#d.-lassocvcross-validation",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#d.-lassocvcross-validation",
    "title": "다중공선성의 해소",
    "section": "### D. LassoCV(Cross Validation)",
    "text": "### D. LassoCV(Cross Validation)\n- 가장 적합한 \\(\\alpha\\)값을 자동으로 찾아준다.\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score\n\n## 2\npredictr = sklearn.linear_model.LassoCV(alphas = np.linspace(0.1, 2, 20))\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.989e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.860e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.633e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.252e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.352e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.306e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.440e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.798e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.242e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.872e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.992e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.436e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.353e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.627e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.897e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.514e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.461e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.812e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.234e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.637e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.876e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.340e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.293e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.578e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.930e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.021e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.646e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.295e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.779e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.310e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.075e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.837e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.093e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.277e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.170e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.617e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.724e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.145e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.746e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.480e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.328e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.987e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.132e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.659e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.356e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.658e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.074e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.203e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.698e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.031e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.921e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.384e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.669e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.082e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.384e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.782e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.134e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.001e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.107e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.057e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.464e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.606e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.704e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.481e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.384e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.910e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.173e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.835e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.949e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.883e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.713e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.017e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+02, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\n0.9555099850022306\n\n\n\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9555099850022306, 0.8756348559919926)\n\n\n\n살짝 과적합된 면이 있으나, 그래도 상당히 높은 수치이다."
  },
  {
    "objectID": "2023_MP/practice/A6. 다중공선성의 해소.html#coef를-0으로-만드는-수학적-장치",
    "href": "2023_MP/practice/A6. 다중공선성의 해소.html#coef를-0으로-만드는-수학적-장치",
    "title": "다중공선성의 해소",
    "section": "5. coef를 0으로 만드는 수학적 장치",
    "text": "5. coef를 0으로 만드는 수학적 장치\n\nRidge : L2-penalty\n\ncoef의 값들을 가중치에 따라 분할하는 수학적 장치.\n\n\n\n패널티 : 상관성이 짙은 설명변수들의 계수값을 제곱한 뒤 합치고(L2-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여.\n\n\nLasso : L1-penalty\n\n다수의 coef 값들을 0으로 만드는 수학적 장치\n\n\n\n패널티 : 상관성이 짙은 설명변수들의 계수값의 절대값을 구한 뒤에 합치고(L1-norm을 구하고), 그 값이 0에서 떨어져 있을수록 패널티 부여."
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html",
    "title": "plot_tree의 시각화",
    "section": "",
    "text": "의사결정나무의 plot_tree를 시각화해보자"
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html#라이브러리-imports",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html#라이브러리-imports",
    "title": "plot_tree의 시각화",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport graphviz\n\n#-#\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html#데이터-적합",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html#데이터-적합",
    "title": "plot_tree의 시각화",
    "section": "2. 데이터 적합",
    "text": "2. 데이터 적합\n\n먼저 데이터를 트리로 적합해놓은 뒤 해당 데이터를 통해서 시각화를 해보자.\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/insurance.csv')\ndf_train\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\n## step 1\nX = pd.get_dummies(df_train.drop('charges', axis = 1))\ny = df_train['charges']\n\n## step 2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 3)\n\n## step 3\npredictr.fit(X, y)\n\n## step 4 -- pass\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)"
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html#matplotlib-기반-시각화",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html#matplotlib-기반-시각화",
    "title": "plot_tree의 시각화",
    "section": "3. matplotlib 기반 시각화",
    "text": "3. matplotlib 기반 시각화\n\nA. plot_tree 기본 시각화\n\n\nsklearn.tree.plot_tree(predictr);  ## ;을 통해 계산과정 제거 가능\n\n\n\n\n잘 안보여…"
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html#b.-max_depth-조정",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html#b.-max_depth-조정",
    "title": "plot_tree의 시각화",
    "section": "### B. max_depth 조정",
    "text": "### B. max_depth 조정\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth = 0\n);\n\n\n\n\n\n일단 보이기는 하는데, 위에 하나만 보이겠지…\n\n\nC. 변수이름 추가 | feature_names = list\n\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth = 0,\n    feature_names = X.columns.to_list()  ## 교수님은 잘 되시던데 나는 왜 리스트로 넣어야만 할까...\n);\n\n\n\n\n\nx[5]같은 식으로 순서만 표시되던 게, 이름이 표기되었다."
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html#d.-fig-오브젝트",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html#d.-fig-오브젝트",
    "title": "plot_tree의 시각화",
    "section": "### D. fig 오브젝트",
    "text": "### D. fig 오브젝트\n- plt.gcf()를 통해 fig오브젝트로 추출\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth = 1,\n    feature_names = X.columns.to_list()\n);\n\nfig = plt.gcf()\n\n\n\n\n\n이제 이녀석은 matplotlib에서 다룰 수 있다.\n\n\nfig.suptitle(\"Can we setting title?\")\n\nText(0.5, 0.98, 'Can we setting title?')\n\n\n\nfig\n\n\n\n\n- dpi(해상도) 조정\n\nfig.set_dpi(250)\nfig\n\n\n\n\n\n아마 해상도를 무쟈게 올리면 되기야 하겠지… 근데 그럼 사진이 엄청나게 커지겠지…\n\n\nE. matplotlib의 ax에 그리기\n\n- tree로 적합한 값의 차이 정도와 plot_tree를 위아래로 표기하기\n\nfig = plt.figure()\nax = fig.subplots(2,1)\nax[0].plot(y, y, '--')\nax[0].plot(y, predictr.predict(X), 'o', alpha = 0.1)\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth = 1,\n    feature_names = X.columns.to_list(),\n    ax = ax[1]  ## 해당 옵션으로 ax에 삽입이 가능하다.\n);"
  },
  {
    "objectID": "2023_MP/practice/B1. Tree Plot 시각화.html#graphviz",
    "href": "2023_MP/practice/B1. Tree Plot 시각화.html#graphviz",
    "title": "plot_tree의 시각화",
    "section": "4. GraphViz",
    "text": "4. GraphViz\n딱봐도 보기 불편한데, 뭔가 개선을 해놨지 않았을까???\n그래서 준비했습니다!!\n\ng = sklearn.tree.export_graphviz(\n    predictr,\n    feature_names = X.columns.to_list()\n)\n\n\ngraphviz.Source(g)\n\n\n\n\n\n위아래로 타이트하게 나오면서 스크롤로 볼 수 있게 된 모습, 애초에 sklearn에서 해당 사항을 우려해서 이렇게 만들어놓았다.\n\n- 파일로 추출해서 저장하려면?\n\ngraphviz.Source(g).render('tree', format = 'pdf')  ## 파일명, 포맷\n\n'tree.pdf'\n\n\n\n작업공간에 파일이 추가된 것을 볼 수 있다."
  },
  {
    "objectID": "2023_MP/practice/A9. 플랏_애니메이션.html",
    "href": "2023_MP/practice/A9. 플랏_애니메이션.html",
    "title": "플랏 | 애니메이션",
    "section": "",
    "text": "matplotlib의 animation 모듈을 통해 플랏 애니메이션을 만들어보자!"
  },
  {
    "objectID": "2023_MP/practice/A9. 플랏_애니메이션.html#라이브러리-imports",
    "href": "2023_MP/practice/A9. 플랏_애니메이션.html#라이브러리-imports",
    "title": "플랏 | 애니메이션",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython"
  },
  {
    "objectID": "2023_MP/practice/A9. 플랏_애니메이션.html#funcanimation",
    "href": "2023_MP/practice/A9. 플랏_애니메이션.html#funcanimation",
    "title": "플랏 | 애니메이션",
    "section": "2. FuncAnimation",
    "text": "2. FuncAnimation\n\nA. 모티브\n\n\nk = 6\nx = np.linspace(0, 10, 100)\ny = np.sin(0.1*k*x)\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\\(k = 1, 2, 3, \\dots\\)로 바꾸면서 변화하는 그림을 연속으로 출력되게 하여 애니메이션으로 보고 싶다. 따라서…\n\n하나의 고정된 그림을 정의\n그림 안의 내용물을 frame에 따라 바꾸는 동작을 정의\n\n위 두 과정을 결합하는 전략을 생각해보자.\n- 위 두 과정을 코드로 나타내면 아래와 같다.\n\nk = 4  ## frame에 따라 달라지는 값\nx = np.linspace(0, 10, 100)\ny = np.sin(0.1*k*x)\n\nfig = plt.figure()  ## 하나의 고정된 그림을 정의하는 코드\nax = fig.gca()  ## axes를 꺼내옴\nax.plot(x, y)  ## 고정된 그림에서 내용물을 frame에 따라 바꾸는 동작을 정의하는 코드"
  },
  {
    "objectID": "2023_MP/practice/A9. 플랏_애니메이션.html#b.-animation",
    "href": "2023_MP/practice/A9. 플랏_애니메이션.html#b.-animation",
    "title": "플랏 | 애니메이션",
    "section": "### B. Animation",
    "text": "### B. Animation\n- 하나의 고정된 그림을 정의하는 코드\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n- 고정된 그림에서 내용물을 frame에 따라서 바꾸는 함수\n\ndef func(frame):\n    ax = fig.gca()  ## axes를 가져옴\n    ax.clear()  ## 사이클을 반복하기 전에 기존에 입력해뒀던 것을 지움. 아직 그려진 게 없으면 변화 X\n    x = np.linspace(0, 10, 100)\n    y = np.sin(0.1*frame*x)  ## frame에 따라서 달라지는 값\n    ax.plot(x, y)  ## 고정된 그림에서 그 내용물을 frame에 따라서 바꾸는 코드\n\n## 해당 함수에는 return이 필요없다.\n\n- 위 두개를 결합하여 애니메이션으로 재생\n\nani = matplotlib.animation.FuncAnimation(\n    fig, ## 하나의 고정된 그림\n    func,  ## 내용물을 frame에 따라 바꾸는 동작\n    frames = 50  ## frame의 갯수\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))  ## 앞의 display는 생략해도 돌아감\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nani를 javascript HTML 형식으로 바꾼 뒤, IPython을 통해 읽어왔음."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RiverFlow",
    "section": "",
    "text": "Python을 이용한 문제 풀이, Kaggle competition, R 프로그래밍 연습 등 학습 내역과 작업물들을 기록합니다.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "",
    "text": "예측해야 할 유형이 범주형일 때 사용할 수 있는 분석 기법 중 하나인 로지스틱 회귀분석을 해보자!"
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html#라이브러리-imports",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html#라이브러리-imports",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model"
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html#로지스틱-회귀분석",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html#로지스틱-회귀분석",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "2. 로지스틱 회귀분석",
    "text": "2. 로지스틱 회귀분석\n- 연속형 설명변수와 범주형 반응변수와의 관계\n\nA. 성적과 취업 여부 데이터\n\n\n학점과 토익 성적, 그리고 취업 여부를 나타낸 데이터가 있다.(교수님이 만드신 페이크 데이터이다.)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf  ## 페이크 데이터입니다.\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nplt.plot(df.toeic[df.employment == 0], df.gpa[df.employment == 0], 'o')\nplt.plot(df.toeic[df.employment == 1], df.gpa[df.employment == 1], 'o')\nplt.show()\n\n\n\n\n- 뭔가 관련성을 찾을 수 있을 것 같지 않은가?~(당연하지 그렇게 만드셨으니까)~\n\n그래서 토익 성적ㆍ학점과 취업여부의 관계를 구하고 싶다."
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html#b.-분석",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html#b.-분석",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### B. 분석",
    "text": "### B. 분석\n\n# step 1\nX = pd.get_dummies(df[['toeic', 'gpa']])\ny = df.employment\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\ndf = df.assign(employment_hat = predictr.predict(X))\n\n\nsklearn.linear_model.LogisticRegression()\n\n\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemployment_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n로지스틱 회귀분석으로 적합 및 예측이 완료되었다.\n\n\nC. 평가\n\n\npredictr.score(X, y)\n\n0.882\n\n\n\n이건 y와 y_hat이 동일한 정도를 나타낸다, 나름 잘 맞춘 것 같지 않은가?\n\n- 시각화를 해야 정확히 알 수 있겠지? 현재 예측치와 기존 예측치를 비교해보자.\n\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemployment_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\ndf_filtered = df[predictr.predict(X) == 1]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,5))\n\nfig.suptitle('Constrat Pradiction and Real')\n\nax1.plot(df.toeic, df.gpa, 'o', color = 'C0', label = 'employed')\nax1.plot(df.loc[df.employment == 1].toeic, df.loc[df.employment == 1].gpa, 'o', color = 'C1', label = 'not yet employed')\nax1.set_title('Real Data')\n\nax2.plot(df.toeic, df.gpa, 'o', color = 'C0', label = 'employed')\nax2.plot(df_filtered.toeic, df_filtered.gpa, 'o', color = 'C1', label = 'not yet employed')\nax2.set_title('Estimated Data')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n어때요, 나름 합리적이지 않나요?"
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html#로지스틱-회귀분석의-실적용",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html#로지스틱-회귀분석의-실적용",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "3. 로지스틱 회귀분석의 실적용",
    "text": "3. 로지스틱 회귀분석의 실적용\n\n그럼 타이타닉 데이터에서 로지스틱 회귀분석을 통해 결과를 잘 예측할 수 있지 않을까요?\n\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/HollyRiver/Machine_learning_in_practice/main/kaggle/titanic/data/train.csv')\ndf_test = pd.read_csv('https://raw.githubusercontent.com/HollyRiver/Machine_learning_in_practice/main/kaggle/titanic/data/test.csv')\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nkaggle 입문하기에서 보았던 타이타닉 데이터이다.\n- 여기서 반응변수를 쉽게 구하려면…\n\nset(df_train.columns) - set(df_test.columns)\n\n{'Survived'}\n\n\n- 하나만 남는 것을 볼 수 있다.\n\n아! 테스트 셋에 없는 열이니까 저게 y겠구나!"
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html#a.-늘-해왔던-것처럼-분석하면-안된다",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html#a.-늘-해왔던-것처럼-분석하면-안된다",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### A. 늘 해왔던 것처럼 분석…~(하면 안된다)~",
    "text": "### A. 늘 해왔던 것처럼 분석…~(하면 안된다)~\n\n# step 1\nX = pd.get_dummies(df_train.drop(['Survived'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(df_test)\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n\n\n오류가 나온다.\n\nInput X contains NaN.\n선형 회귀에서 설명변수의 input값에는 결측치가 있으면 안된다!!!\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\n결측치가 있는 열을 제거, 행을 제거, 둘 다. 또는 결측치를 impute해야 하는데…\n\n- 일단 Cabin 열은 결측치가 너무 많으니까 빼자!\n- Name이나 Ticket과 같은 변수는 이성적으로 봤을 때 바로 one-hot 인코딩 하기에는 어색하니 빼자!\n\nlen(set(df_train.Name)), len(set(df_train.Ticket))  ## 다 다름, 거의 다 다름\n\n(891, 681)\n\n\n- Age, Embarked에 포함된 약간의 결측치가 마음에 걸리니까 빼자!!\n\ndropna() 메소드나 preprocessing을 써도 되지만… 그건 나중에 해보자.\n\n\ndf_test.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n\n\n- Fare에 포함된 결측치도 걸린다 -&gt; 빼자! (평균으로 해주는 방법도 있는ㄷ ~나중에 하자고 좀~)\n\nB. 데이터 정리\n\n- 위에서 말한 조건들을 적용해서 데이터를 재가공한 뒤, 로지스틱 회귀를 해보자\n\ndf_train.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n\n\n# step 1\nX = pd.get_dummies(df_train.drop(['Survived', 'Cabin', 'Name', 'Ticket', 'Age', 'Embarked', 'Fare'], axis = 1))\ny = df_train.Survived\nXX = pd.get_dummies(df_test.drop(['Cabin', 'Name', 'Ticket', 'Age', 'Embarked', 'Fare'], axis = 1))\n\n# step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n# step 3\npredictr.fit(X, y)\n\n# step 4\nXX.assign(Survived = predictr.predict(XX))\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nSibSp\nParch\nSex_female\nSex_male\nSurvived\n\n\n\n\n0\n892\n3\n0\n0\nFalse\nTrue\n0\n\n\n1\n893\n3\n1\n0\nTrue\nFalse\n1\n\n\n2\n894\n2\n0\n0\nFalse\nTrue\n0\n\n\n3\n895\n3\n0\n0\nFalse\nTrue\n0\n\n\n4\n896\n3\n1\n1\nTrue\nFalse\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\n0\n0\nFalse\nTrue\n0\n\n\n414\n1306\n1\n0\n0\nTrue\nFalse\n1\n\n\n415\n1307\n3\n0\n0\nFalse\nTrue\n0\n\n\n416\n1308\n3\n0\n0\nFalse\nTrue\n0\n\n\n417\n1309\n3\n1\n1\nFalse\nTrue\n0\n\n\n\n\n418 rows × 7 columns\n\n\n\n\n정상적으로 잘 수행한 것 같다."
  },
  {
    "objectID": "2023_MP/practice/A1. 로지스틱 회귀분석.html#c.-평가-1",
    "href": "2023_MP/practice/A1. 로지스틱 회귀분석.html#c.-평가-1",
    "title": "범주형 반응변수의 예측 | LogisticRegression()",
    "section": "### C. 평가",
    "text": "### C. 평가\n\npredictr.score(X, y)\n\n0.8002244668911336\n\n\n\n생각보단 잘 한 것 같다.\n\n\nD. 제출\n\n\nyy = pd.DataFrame({'Survived' : predictr.predict(XX)})\nsubmit_df = pd.concat([df_test.PassengerId, yy], axis = 1)\nsubmit_df\n\n#submit_df.to_csv(directory, index = False)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\nkaggle에 제출하고 두근대는 결과는… 0.77511, 그리 높진 않다."
  },
  {
    "objectID": "2023_MP/practice/B0. 의사결정나무의 작동원리.html",
    "href": "2023_MP/practice/B0. 의사결정나무의 작동원리.html",
    "title": "의사결정나무 | 작동원리",
    "section": "",
    "text": "의사결정나무가 작동하는 원리에 대해서 알아보자!"
  },
  {
    "objectID": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#라이브러리-imports",
    "href": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#라이브러리-imports",
    "title": "의사결정나무 | 작동원리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\nimport sklearn.tree\n\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#원리",
    "href": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#원리",
    "title": "의사결정나무 | 작동원리",
    "section": "2. 원리",
    "text": "2. 원리\n\nA. max_depth\n\n\n얼만큼 깊게 나눌 것이냐???를 결정한다. 이 말을 이해해보자.\n\n- Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\neps = np.random.randn(100)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n0\n-4.1\n10.900261\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n...\n...\n...\n\n\n95\n12.4\n54.926065\n\n\n96\n13.4\n54.716129\n\n\n97\n14.7\n56.194791\n\n\n98\n15.0\n60.666163\n\n\n99\n15.2\n61.561043\n\n\n\n\n100 rows × 2 columns\n\n\n\n\n## 일단 늘 했던 것처럼...\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) ## NEW!\n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- 결과를 시각화\n\nplt.plot(X, y, 'o', alpha = 0.5, label = 'True')\nplt.plot(X, predictr.predict(X), '--', label = 'Predicted')\nplt.legend()\nplt.show()\n\n\n\n\n\n5.0정도를 기준으로 계단이 뚝 끊긴 모습이다.\n\n기존에는 막 이래저래 왔다갔다 하면서 오버피팅이 되는 모습이었는데, max_depth = 1옵션을 추가하니 뚝 끊겨서 값이 산정된다. 어떤 원리인지 알겠는가…?\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]\n\n\n\n\n\n\n의사결정나무는 이런 방식으로 적합을 한다!!\n\n적당한 \\(x\\)값을 기준으로 데이터를 나눈 뒤, 각 구간에 해당하는 녀석들의 \\(y\\)값 평균으로 예측해버린다!\n\n## 그럼 max_depth가 2라면???\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nplt.plot(X, y, 'o', label = 'true', alpha = 0.5)\nplt.plot(X, predictr.predict(X), '--', label = 'predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x1822a965e40&gt;\n\n\n\n\n\n\n방금 전의 tree_plot을 기억하는가? 거기서 한 번 더 내려온 것(한 단계 더 깊게 적합)이 max_depth = 2이다!"
  },
  {
    "objectID": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#b.-애니메이션",
    "href": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#b.-애니메이션",
    "title": "의사결정나무 | 작동원리",
    "section": "### B. 애니메이션",
    "text": "### B. 애니메이션\n\n아직도 잘 감이 오지 않는다면, animation을 이용해 max_depth에 따른 예측값의 변화를 시각화해보자.\n\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    predictr = sklearn.tree.DecisionTreeRegressor(max_depth = frame+1)\n    predictr.fit(X, y)\n    ax.plot(X, y, 'o', alpha = 0.5, label = 'True')\n    ax.plot(X, predictr.predict(X), '--', label = 'Predicted')\n    ax.set_title('max_depth = {}'.format(str(frame+1)))\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = 10\n)\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nmax_depth의 값이 커질수록 계단의 구간이 많아지더니 점을 따라가기 시작했다…\n\n\n코드를 쉽게 쓰기 위해서 func에다 fitting하는 것까지 넣어줬지만, 원래는 외부에서 fitting된 녀석을 넣어주는 게 리소스 절감에 용이하긴 하다…\n\n\nC. 분할을 결정하는 기준?\n\n그럼 그 계단의 경계는 어떻게 결정되는 걸까?\n\npredictr.score(X, y)\n\n0.8561424389856722\n\n\n\ny_hat = predictr.predict(X)\nsklearn.metrics.r2_score(y, y_hat)  ## 딱히 라이브러리를 들여오지 않아도 나오네...\n\n0.8561424389856722\n\n\n\n회귀모형이 전체 변동 중 얼마만큼을 설명했는지를 나타내는 \\(R^2\\)를 좋은 분할을 판단하는 기준으로 사용한다.\n\n즉, 가능한 계단의 경계값들 중 \\(R^2\\)이 가장 높은 값이 분할로 결정된다!!\n- 이 과정을 구현해보자.\n\ndef fit_predict(X, y, c) :\n    \"\"\"\n    X와 y, 분할할 구간을 넣어주면 max_depth = 1일 때의 구간에 따른 예측값을 반환하는 함수.\n    \"\"\"\n    X = np.array(X).reshape(-1)  ## 1차원으로 깨줌\n    y = np.array(y)\n    yhat = y*0  ## 초기값 배정\n    yhat[X&lt;=c] = y[X&lt;=c].mean()  ## bool list로 슬라이싱\n    yhat[X&gt;c] = y[X&gt;c].mean()\n    \n    return yhat\n\n\nyhat_bad = fit_predict(X, y, 1)\nyhat_good = fit_predict(X, y, 5)\n\nfig, ax = plt.subplots(1,2, figsize = (7,3))\nax[0].plot(X, y, 'o', alpha = 0.5)\nax[0].plot(X, yhat_bad, '--')\nax[0].set_title('R-squared = {}'.format(round(sklearn.metrics.r2_score(y, yhat_bad), 4)))\nax[1].plot(X, y, 'o', alpha = 0.5)\nax[1].plot(X, yhat_good, '--')\nax[1].set_title('R-quared = {}'.format(round(sklearn.metrics.r2_score(y, yhat_good), 4)))\n\nText(0.5, 1.0, 'R-quared = 0.6167')\n\n\n\n\n\n\nc의 값이 5일 때, \\(R^2\\)의 값이 더 클 뿐만 아니라, 예측한 값이 더 직관적으로 좋아보인다.\n\n- 그래서 트리가 max_depth = 1일 경우 분할을 결정하는 방법! ~= 노가다…~\n\n적당한 \\(c\\)를 고른다.\n분할 \\((-\\infty, c), [c, \\infty)\\)를 생성하고 yhat을 계산한다.\nr2_score(y, yhat)을 계산하고 기록한다.\n상기 과정을 무한반복한 뒤, r2_score(y, yhat)의 값을 가장 작게 만드는 \\(c\\)를 구간으로 택한다.\n\n\nfit_predict??\n\n\nSignature: fit_predict(X, y, c)\nSource:   \ndef fit_predict(X, y, c) :\n    \"\"\"\n    X와 y, 분할할 구간을 넣어주면 max_depth = 1일 때의 구간에 따른 예측값을 반환하는 함수.\n    \"\"\"\n    X = np.array(X).reshape(-1)  ## 1차원으로 깨줌\n    y = np.array(y)\n    yhat = y*0  ## 초기값 배정\n    yhat[X&lt;=c] = y[X&lt;=c].mean()  ## bool list로 슬라이싱\n    yhat[X&gt;c] = y[X&gt;c].mean()\n    \n    return yhat\nFile:      c:\\users\\hollyriver\\appdata\\local\\temp\\ipykernel_8144\\486783211.py\nType:      function\n\n\n\n\ncuts = np.arange(-5, 15)\nfig = plt.figure()\ndef func(frame) :\n    ax = fig.gca()\n    ax.clear()\n    yhat = fit_predict(X, y, cuts[frame])\n    \n    ax.plot(X, y, 'o', alpha = 0.5, label = 'True')\n    ax.plot(X, yhat, '--', label = 'Predicted')\n    ax.set_title(f'c = {cuts[frame]},  R-squared = {round(sklearn.metrics.r2_score(y, yhat), 4)}')\n    ax.legend()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = len(cuts)\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n요런 느낌으로다가…\n\n- 그럼 tree가 찾은 값 5.05를 우리가 직접 찾아보자.\n\ncuts = np.arange(-5, 15, 0.001).round(5)\nscores = np.array([sklearn.metrics.r2_score(y, fit_predict(X, y, c)) for c in cuts])\npd.DataFrame({'cut':cuts, 'score':scores}).plot.line(x = 'cut', y = 'score', backend = 'plotly')\n\n\n                                                \n\n\n\ncuts[scores.argmax()]  ## R에서의 which()와 비슷한 느낌의 코드\n\n5.0\n\n\n\nmax_depth = 2인 경우는 max_depth = 1의 결과로 발생한 2개의 조각을 각각 전체 자료로 생각하고, max_depth = 1일 때의 분할방법을 반복적용한다.\nX.columns == ['temp', 'type']와 같은 경우라면, 설명변수를 하나씩 고정하여 각각 최적의 분할을 생성하고, r2_score관점에서 가장 우수한 설명변수를 선택한다.(즉, 가장 중요한 설명변수가 최우선적으로 고려된다.)"
  },
  {
    "objectID": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#토론",
    "href": "2023_MP/practice/B0. 의사결정나무의 작동원리.html#토론",
    "title": "의사결정나무 | 작동원리",
    "section": "3. 토론",
    "text": "3. 토론\n- 의사결정나무 vs 선형모형 !!!\n\n의사결정나무의 장점\n\n\n시각화가 유리하다, 설명력이 좋다.\n특성(feature)의 중요도를 파악하기 용이하다(그야 가장 우수한 설명변수가 depth에 따라 최우선적으로 고려되니까.)\n\\(\\textbf{y} \\sim \\textbf{X}\\) 사이에 존재하는 비선형성을 간단히 모델링할 수 있다.\n모형에 대한 가정들이 필요없다.(Nonparametric Model)\n\n\n의사결정나무의 단점 : 오버피팅이 일어나기 너무 쉽다\n\n- 자잘한 개념들(option)\n최소 샘플 분할(min_samples_split)\n\n노드를 분할하기 위한 최소 샘플 수.\n과소적합(수를 줄임) 및 과적합(수를 늘림) 조절 가능\n\n가지치기(Pruning)\n\n트리의 불필요한 부분을 제거(아래로 뻗어나가는 가지 제거)하여 과적합 방지 및 모델 성능 향상에 도움.\n\n정보 이득(Information Gain)\n\n분할 전후의 엔트로피 차이를 측정(좀 많이 굉장히 어려운 개념임)\n높은 정보 이득은 더 좋은 분할을 의미\n\n지니 불순도(Gini Impurity)\n\n노드의 순도 측정 지표, 낮은 지니 불순도는 높은 클래스 순도를 의미\n\n\n결국 “트리를 어디까지 성장시킬래?”에 대한 이론적인 명확한 기술은 없다… Nonparametric Model이니까 정답이 없음…\n\n\n의사결정나무를 응용한 다양한 방법(너무 많아요, 진짜)들이 개발되었고, 모든 방법들의 원리를 세세하게 파헤치는 건 비효율적이다…\n그러한 다양한 방법들을 적당히 분류해보면 대체로 배깅, 랜덤포레스트, 부스팅 계열~(게다가 이중 여러가지를 포함하기도 함)~로 나뉜다. 앞으론 이 세 분류에 대한 공통적 아이디어를 파악해보도록 하자."
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "",
    "text": "sklearn.preprocessing을 이용하여 자료의 범위를 전처리해보자."
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#라이브러리-import",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#라이브러리-import",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.preprocessing"
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#minmaxscaler",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#minmaxscaler",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "2. MinMaxScaler",
    "text": "2. MinMaxScaler\n\nA. 모티브\n\n- 예제자료 : 학점, 토익 등이 취업에 미치는 정도\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과… (sklearn.linear_model.Linear_Regression())\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n그래서… * 토익이 중요해? 아니면 학점이 중요해? * 무엇이 얼만큼 중요해?\n- 모티브 : 토익과 gpa 모두 0~1 사이의 척도로 바꾸면 해석이 쉽지 않을까?"
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#b.-사용방법",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#b.-사용방법",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "### B. 사용방법",
    "text": "### B. 사용방법\n\nclass를 이용, object를 생성하는 방법(이전과 유사한 방법)\n\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit(df)\n\nscalr.transform(df)  ## 전처리의 경우에는 transform을 사용한다. .impute.SimpleImputer()에서도 그랬잖아?\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n역시 한번에 할 수도 있다.\n\n\nscalr.fit_transform(df)  ## 당연히 원래 자료를 훼손하진 않는다.\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\nsklearn.preprocessing.minmax_scale(df)  ## 한 번에 할 수도 있다.\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\n위처럼 할 수도 있는데, 이 경우는 scalr를 test셋에 적용시킬 수 없기 때문에 사용하지 않는다.\n\n\nC. 옳고 그른 방법론\n\n# 1 비효율적인 전환\n- 주어진 자료가 아래와 같이 train/test로 나뉘어있다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 5.0]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 3.5]).reshape(-1,1)\n\nX, XX\n\n(array([[1.],\n        [2.],\n        [3.],\n        [4.],\n        [5.]]),\n array([[1.5],\n        [2.5],\n        [3.5]]))\n\n\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit_transform(X), scalr.fit_transform(XX)\n\n(array([[0.  ],\n        [0.25],\n        [0.5 ],\n        [0.75],\n        [1.  ]]),\n array([[0. ],\n        [0.5],\n        [1. ]]))\n\n\n\n같은 값임에도 다르게 스케일을 변환시키는 것을 볼 수 있다.(X에선 5가 1인데, XX에선 3.5가 1이 됨.\n\n# 2 권장하는 스케일링 방법\n\nscalr = sklearn.preprocessing.MinMaxScaler()\n\nscalr.fit(X)\n\nscalr.transform(X), scalr.transform(XX)\n\n(array([[0.  ],\n        [0.25],\n        [0.5 ],\n        [0.75],\n        [1.  ]]),\n array([[0.125],\n        [0.375],\n        [0.625]]))\n\n\n\n더 합리적이다.\n\n# 3 변환값의 범위\n- 변환한 값이 무조건 0과 1 사이가 되는 것은 아니다.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n## XX의 5.0은 X에서의 최대값인 4.0을 초과한다.\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X)\n\nsclr.transform(X), sclr.transform(XX)\n\n(array([[0.        ],\n        [0.33333333],\n        [0.66666667],\n        [1.        ],\n        [0.83333333]]),\n array([[0.16666667],\n        [0.5       ],\n        [1.33333333]]))\n\n\n\n스케일링한 값이 1보다 커질 수 있다."
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#d.-아주아주-잘못된-스케일링-방법---정보누수",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#d.-아주아주-잘못된-스케일링-방법---정보누수",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "### D. 아주아주 잘못된 스케일링 방법 - 정보누수",
    "text": "### D. 아주아주 잘못된 스케일링 방법 - 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- train data와 test data를 합친다….????!!!??!?!??\n\nnp.concatenate([X, XX], axis = 0)\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친 데이터에서 스케일링….\n\nsklearn.preprocessing.MinMaxScaler().fit_transform(np.concatenate([X, XX], axis = 0))\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625],\n       [0.125],\n       [0.375],\n       [1.   ]])\n\n\n\n이렇게 전저리하는 것은 정보누수에 해당한다. 본래 test dataset은 알지 못한 상태인데 그것을 합칠 순 없다!\n대회에서 이런 일이 발생하면 cheating으로 간주되어 탈락된다.\n\n\n위에서 minmax_scale()로 처리하는 것은 전략적으로 비효율적인 문제이지 치팅과 관련된 치명적인 문제가 아니다. (만약 어떠한 경우에 minmax_scale 전처리 방식이 유리하다는 생각이 들면 사용해도 무방함)"
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#standardscaler",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#standardscaler",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "3. StandardScaler",
    "text": "3. StandardScaler\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n\n여기서 토익과 gpa가 미치는 영향을 비교하기 위해 각 값들을 표준화해보자.\n\n\nA. 사용법\n\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n\nMinMaxScaler도 마찬가지로 여러 열을 한번에 할 수 있다.\n\n- 원리\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식, 자유도는 0(모분산으로 취급)\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\n그냥 표준화하는 것"
  },
  {
    "objectID": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#비교",
    "href": "2023_MP/practice/A4. 연속형 자료의 범위 조정.html#비교",
    "title": "전처리 | 연속형 자료의 범위 조정",
    "section": "4. 비교",
    "text": "4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxSclaer:\n\n장점 : 원하는 범위 내로 데이터를 조정할 때 유용, 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력값을 조정하는 데 유용.\n단점 : 이상치에 매우 민감하다.\n\nStandardScaler:\n\n장점 : 이상치에 덜 민감함, 많은 통계적 기법들 - 선형 알고리즘에서 잘 작동함\n단점 : 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.\n\n\n\n단순히 MinMaxScaler는 데이터가 0~1 또는 -1~1사이의 범위에 있다고 가정한다.\n\n그래서 둘 중 어느 것을 선택해야 하는데???\n\n둘 중 이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 신경망의 경우 MinMaxScaler가 적합할 수 있다.\n\n결론적으로 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n\n결론"
  },
  {
    "objectID": "2023_MP/practice/B2. 의사결정나무 옵션.html",
    "href": "2023_MP/practice/B2. 의사결정나무 옵션.html",
    "title": "의사결정나무의 옵션 이해",
    "section": "",
    "text": "의사결정나무의 여러 옵션들에 대해서 알아보자!"
  },
  {
    "objectID": "2023_MP/practice/B2. 의사결정나무 옵션.html#라이브러리-imports",
    "href": "2023_MP/practice/B2. 의사결정나무 옵션.html#라이브러리-imports",
    "title": "의사결정나무의 옵션 이해",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport graphviz\n\n#-#\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "2023_MP/practice/B2. 의사결정나무 옵션.html#max_features",
    "href": "2023_MP/practice/B2. 의사결정나무 옵션.html#max_features",
    "title": "의사결정나무의 옵션 이해",
    "section": "2. max_features",
    "text": "2. max_features\n\n(말그대로) 설명변수 몇 개 쓸거야???\n\n# Data\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/insurance.csv')\ndf_train\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n# 적합 : max_features = 4\n\n## step 1\nX = pd.get_dummies(df_train.drop('charges', axis = 1), drop_first = True)\ny = df_train.charges\n\n## step 2\npredictr = sklearn.tree.DecisionTreeRegressor(max_features = 4)\n\n## step 3\npredictr.fit(X, y)\n\nDecisionTreeRegressor(max_features=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4)\n\n\n\n어떻게 적합했는지 프리 플랏으로 시각화하면…\n\n\nsklearn.tree.plot_tree(predictr, max_depth = 0, feature_names = X.columns.to_list());\n\n\n\n\n\npredictr.fit(X, y)\nsklearn.tree.plot_tree(predictr, max_depth = 0, feature_names = X.columns.to_list());\n\n\n\n\n\npredictr.fit(X, y)\nsklearn.tree.plot_tree(predictr, max_depth = 0, feature_names = X.columns.to_list());\n\n\n\n\n\nmax_features를 4로 제한했을 때, tree가 적합한 결과는 계속해서 달라진다.\n\n\nlen(X.columns)\n\n8\n\n\n\nmax_features = 4의 의미는 설명변수들 중 4개만 임의로 뽑아서 그중 최적의 변수와 최적의 c를 찾겠다는 의미이다. (정말 가중치 없이 랜덤으로 막 뽑아버린다.)\n이 경우는 smoker_yes가 가장 중요하여 이것이 뽑힌 절반 정도의 표본은 제일 위에 위치하게 된다."
  },
  {
    "objectID": "2023_MP/practice/B2. 의사결정나무 옵션.html#random_state",
    "href": "2023_MP/practice/B2. 의사결정나무 옵션.html#random_state",
    "title": "의사결정나무의 옵션 이해",
    "section": "3. random_state",
    "text": "3. random_state\n\n모두 같은 결과를 보게 만들어 재현을 쉽게 하고 싶어!!\n\n# 적합 : random_state = 42\n\n## step 1\nX = pd.get_dummies(df_train.drop('charges', axis = 1), drop_first = True)\ny = df_train.charges\n\n## step 2\npredictr = sklearn.tree.DecisionTreeRegressor(max_features = 4, random_state = 42)\n\n## step 3\npredictr.fit(X, y)\n\nDecisionTreeRegressor(max_features=4, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4, random_state=42)\n\n\n\n단순하다. train_test_split()이나, np.random.rand()나… 무작위로 뽑는 것에서 흔히 나타나는 random_state를 지정해주면 된다.\n\n- 백날 돌려봐서 같은 결과가 나오는지 확인해보자.\n\ngraphviz.Source(sklearn.tree.export_graphviz(\n    predictr,\n    max_depth = 1,\n    feature_names = X.columns.to_list()\n))\n\n\n\n\n\npredictr.fit(X, y)\ngraphviz.Source(sklearn.tree.export_graphviz(\n    predictr,\n    max_depth = 1,\n    feature_names = X.columns.to_list()\n))\n\n\n\n\n\n아둔한 중생아… 몇 번을 시도해도 같은 결과가 나올 것이다…"
  },
  {
    "objectID": "2023_MP/practice/B2. 의사결정나무 옵션.html#fitsample_weight",
    "href": "2023_MP/practice/B2. 의사결정나무 옵션.html#fitsample_weight",
    "title": "의사결정나무의 옵션 이해",
    "section": "4. .fit(sample_weight = [])",
    "text": "4. .fit(sample_weight = [])\n\n적합할 때에 사용하여 무게, 그러니까 가중치를 부여해주는 옵션이다. 특정 값을 중심으로 피팅을 해야 할 경우나, 값이 겹쳐있는 경우 유용한 옵션이다.\n\n# 예제 1 아래의 데이터를 생각해보자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\nplt.plot(X, y, 'o')\n\n\n\n\n\n이것을 의사결정나무로 적합한다면 어떤 \\(c\\)값을 첫번째로 택해야 할까? \\(\\to\\) 당연히 3.5정도가 되겠지…\n\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 1)\npredictr.fit(X, y)\ngraphviz.Source(sklearn.tree.export_graphviz(predictr))\n\n\n\n\n# 예제 2 유사하지만 다른 그림\n\n## 점, 5000배.\nX = np.array([1]*5000+[2]*5000+[3,4,5,6,7]).reshape(-1,1)\ny = np.array([10]*5000+[11]*5000+[12,20,21,22,23])\nplt.plot(X, y, 'o', alpha = 0.2)\n\n\n\n\n\n이 경우 산점도는 비슷하게 그려지지만, 1과 2에 5000개의 데이터들이 관측되었다. 따라서 1.5 근처에서 나누는 게 적합 점수가 높게 나올 것이다…\n\n\n## 실제로 적합해보면...\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 1)\npredictr.fit(X, y)\ngraphviz.Source(sklearn.tree.plot_tree(predictr));\n\n\n\n\n# 예제 3 가중치 부여\n\n처음의 데이터를 이용했음에도, 위와 결과가 동일하도록 만들 수 있다.\n\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\n\nplt.plot(X, y, 'o')\nplt.plot(X[:2], y[:2], 'o')\n\n\n\n\n\n1번과 2번 데이터를 맞추는 것이 다른 것들보다 5000배 정도 더 중요하다면???~(중력, 5000배)~\n\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 1)\npredictr.fit(X, y, sample_weight = [5000, 5000, 1,1,1,1,1])\ngraphviz.Source(sklearn.tree.export_graphviz(predictr))\n\n\n\n\n\n이런 식으로 똑같은 결과를 볼 수 있다. 이 때, 샘플의 사이즈만 다르다."
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html",
    "title": "Bagging | 의사결정나무",
    "section": "",
    "text": "ensemble의 Bagging을 이용해보자!"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#라이브러리-imports",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#라이브러리-imports",
    "title": "Bagging | 의사결정나무",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport sklearn.ensemble  ## 여러 트리 모듈을 이용하는 클래스를 포함\n\nimport matplotlib.animation  ## 그래프를 애니메이션으로 볼 예정\nimport IPython  ## IPython을 불러와서 산출...?\n\n#-#\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#기본-의사결정나무로-데이터-적합",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#기본-의사결정나무로-데이터-적합",
    "title": "Bagging | 의사결정나무",
    "section": "2. 기본 의사결정나무로 데이터 적합",
    "text": "2. 기본 의사결정나무로 데이터 적합\n- 기온에 따른 아이스크림 판매량 데이터…\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:80]\ntemp.sort()\neps = np.random.randn(80)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n0\n-4.1\n10.900261\n\n\n1\n-3.7\n14.002524\n\n\n2\n-3.0\n15.928335\n\n\n3\n-1.3\n17.673681\n\n\n4\n-0.5\n19.463362\n\n\n...\n...\n...\n\n\n75\n9.7\n50.813741\n\n\n76\n10.3\n42.304739\n\n\n77\n10.6\n45.662019\n\n\n78\n12.1\n48.739157\n\n\n79\n12.4\n46.007937\n\n\n\n\n80 rows × 2 columns\n\n\n\n\n## step 1\nX = df_train[['temp']]\ny = df_train['sales']\n\n## step 2\ntree = sklearn.tree.DecisionTreeRegressor()  ## 완전 퓨어하게, 하이퍼파라미터도 건드리지 말고.\n\n## step 3\ntree.fit(X, y)\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nplt.plot(X, y, 'o')\nplt.plot(X, tree.predict(X), '--')\n\n\n\n\n\n우리가 지금까지 먹어왔던 기본적인 DecisionTreeRegressor()이다. 오차항까지 그대로 따라가는 모습이 참으로 안타깝다…"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#배깅",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#배깅",
    "title": "Bagging | 의사결정나무",
    "section": "3. 배깅",
    "text": "3. 배깅\n- 같은 자료를 배깅을 이용해 적합한다면…\n\n## step 1\nX = df_train[['temp']]\ny = df_train['sales']\n\n## step 2\npredictr = sklearn.ensemble.BaggingRegressor()  ## 앙상블의 배깅 리그레서\n\n## step 3\npredictr.fit(X, y)\n\nBaggingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingRegressorBaggingRegressor()\n\n\n\nfig, ax = plt.subplots(1,2, figsize = (13,5))\nax[0].plot(X, y, 'o', label = 'True')\nax[0].plot(X, tree.predict(X), '--', label = 'prediction')\nax[0].set_title('Pure Tree')\nax[0].legend()\n\nax[1].plot(X, y, 'o', label = 'True')\nax[1].plot(X, predictr.predict(X), '--', label = 'prediction')\nax[1].set_title('Bagging')\nax[1].legend()\n\n&lt;matplotlib.legend.Legend at 0x207a4a6ee00&gt;\n\n\n\n\n\n\n뭔가 무지성적인 오차항 추종이 옅어졌다…?!"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#배깅의-원리에-대한-분석",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#배깅의-원리에-대한-분석",
    "title": "Bagging | 의사결정나무",
    "section": "4. 배깅의 원리에 대한 분석",
    "text": "4. 배깅의 원리에 대한 분석\n기본원리\n- 알고리즘\n\nn개의 샘플에서 n개를 무작위로 복원추출한다. 부트스트랩(Bootstrap)!!\n1번에서 뽑힌 샘플들을 이용하여 tree를 적합한다.\n상기 과정을 10번 반복하고, 10개의 tree의 평균값을 yhat으로 택한다.\n\n\nA. plot_tree 체크\n\n- 10개의 트리들의 리스트를 찾으려면?\n\ntrees = predictr.estimators_\ntrees\n\n[DecisionTreeRegressor(random_state=1644635363),\n DecisionTreeRegressor(random_state=1304269235),\n DecisionTreeRegressor(random_state=1794000214),\n DecisionTreeRegressor(random_state=1273087880),\n DecisionTreeRegressor(random_state=995922005),\n DecisionTreeRegressor(random_state=1372517728),\n DecisionTreeRegressor(random_state=1087222928),\n DecisionTreeRegressor(random_state=3687756),\n DecisionTreeRegressor(random_state=1772778467),\n DecisionTreeRegressor(random_state=92158766)]\n\n\n\n정말 말그대로 표본마다 각각 DecisionTreeRegressor()로 적합해준 것이다.\n\n- 재표본의 데이터셋을 찾으려면? (부트스트랩 샘플 인덱스)\n\npredictr.estimators_samples_\n\n[array([19, 10, 25, 29, 50,  7, 46, 31, 10, 39, 78, 14, 54, 79, 28, 35, 73,\n         0, 74, 72, 66, 36, 55, 24, 41, 11, 68, 65, 71, 36, 54, 41, 76, 34,\n         0, 59,  5,  7, 67, 61, 64, 21, 27, 26, 43, 55, 49, 23, 29, 27, 41,\n        14, 58,  5, 12, 40, 12, 38,  8, 19, 63,  4, 35, 75, 64,  9, 69, 17,\n        32, 15, 60, 55, 18, 55, 22, 73, 28, 48, 57, 63]),\n array([51, 70, 43, 33, 42, 76, 13, 32, 39,  6, 44, 32, 78, 38, 54,  4, 54,\n        78, 75,  1, 59, 58, 26, 41, 21, 45, 45, 63, 15,  0, 45, 43, 24, 50,\n        77, 26, 51, 53, 38,  6, 22,  5, 10, 32, 76,  7, 46, 70, 40, 28, 64,\n        69, 32, 49,  1,  1, 37,  7, 29, 29, 22,  1, 50, 56, 44, 52,  0, 30,\n        32, 73, 53, 69, 78, 46, 12,  3, 18, 60, 41, 70]),\n array([55,  8,  0, 71, 14, 74, 10,  7, 58, 10,  0, 50, 23, 61, 36, 66, 66,\n        52, 17,  6, 36,  3, 55, 13, 41,  5, 77, 21, 31, 14, 19, 59,  3, 25,\n        79, 39, 18, 24, 55,  5, 57, 19, 40, 15,  3, 75, 36, 25, 22, 13, 53,\n        55, 71, 28,  7,  1, 68, 48, 49, 77, 34, 35, 66, 41, 72, 45, 23, 63,\n        34, 50, 68, 47, 28, 20, 37, 23, 67, 17, 71, 64]),\n array([ 8, 24,  8, 32,  8, 37, 58, 59, 68, 32, 37, 16, 34, 55, 14,  2, 43,\n        39, 77, 16, 71, 24,  5, 71, 26, 43,  8, 12, 13, 10, 54,  3, 36, 23,\n        67,  3, 42, 48,  6, 34, 49, 30, 50, 33, 57, 54, 72, 56, 57, 62,  6,\n        36, 34, 75, 33, 66,  1, 39, 61, 61,  7, 49, 23, 35, 10, 67, 54, 74,\n        58, 23, 11, 42, 37,  1, 16, 79, 11,  6, 34, 44]),\n array([51, 62, 23,  0,  1, 67, 70, 71, 56, 31,  4, 38, 22, 10,  6, 31, 66,\n        19, 67, 72, 75, 17,  3, 21, 16, 44, 59,  8, 58, 27, 25, 17, 14, 17,\n        27,  6, 36, 77, 37, 46, 30,  1, 34,  7, 78, 23, 68, 22, 49, 26, 14,\n        38, 48,  3, 63,  5,  4, 71, 74, 32, 41, 59, 22, 37, 57, 71, 56, 30,\n        15, 28, 52, 42, 50, 79, 14, 15, 44, 37, 50, 76]),\n array([63, 61, 25, 22, 15, 19, 39,  5,  3, 48, 38, 68, 41, 48, 32, 45, 34,\n        52, 26,  2,  4,  7, 14, 59, 47, 79, 19, 48,  1, 66,  3,  4, 61, 65,\n        10,  1, 19, 55, 11, 60, 17, 71, 40,  0,  2, 27, 17, 67, 22, 74, 13,\n        71, 48, 37, 72, 58, 48, 44, 59, 32, 28, 53, 28, 39, 18,  0,  7,  6,\n        54,  0, 47, 47, 57, 66, 23, 66, 32, 77, 26, 34]),\n array([16, 22, 69, 63,  3, 32, 20, 66, 53,  5, 27,  1, 51,  1, 47, 41, 41,\n        33, 14,  7, 54, 27, 57, 75, 64,  8,  7,  8, 26, 41, 24, 44, 70, 65,\n        66, 42, 54, 76, 50, 67, 15, 62, 19, 42, 38, 71, 53, 45, 23, 72, 65,\n        24, 69, 47,  0, 75, 29, 34, 23, 63, 33, 77, 69, 18, 65,  2, 79, 33,\n        37, 58, 26, 55, 58, 63, 45, 17, 77, 41, 54, 13]),\n array([76, 31, 49, 79, 34, 63, 18, 30, 12, 35,  8, 46, 50, 49, 43, 13, 19,\n        68, 79,  4, 13, 22, 12, 76, 53, 52,  8, 31, 40, 39, 22, 53, 48, 15,\n        64, 19, 40,  1, 54, 77, 38, 28, 31, 71, 38, 42, 27,  6, 18, 42, 34,\n         0, 75, 37, 16, 58, 60, 62, 18,  2, 74,  6,  5, 65, 77, 44,  4,  9,\n        24, 15, 71, 59, 23, 37, 76, 68, 70, 65, 34, 73]),\n array([70, 27, 42, 10, 74, 26,  0, 51,  2, 12, 23, 28, 38, 30,  6, 32, 63,\n        73, 23, 68,  8, 79, 43, 68, 15, 44, 34, 55, 33,  0, 51, 66, 66, 47,\n        53, 76, 27,  3, 71, 10, 70, 30, 79,  1, 25, 49, 18,  0, 17, 65, 14,\n        27, 29, 76, 15, 12, 31, 26, 61, 42, 41, 21, 22, 38, 31, 11, 23, 13,\n        25, 10, 41, 23, 78, 73, 77, 47, 42, 32, 34,  5]),\n array([46, 24, 65, 62, 70, 15, 15, 33, 39, 44, 16, 33, 50, 24, 31, 36,  3,\n        69,  8, 34, 29, 72, 59, 30, 48, 63, 51, 45, 26, 74, 65, 48, 77, 26,\n        43, 55,  9, 28, 33, 35, 44, 25, 51,  6, 47, 56, 50, 72, 74, 47, 60,\n         8, 50,  9, 31, 10, 79, 45, 28, 61, 43, 55, 19, 73, 15, 19, 53, 46,\n        13, 70, 41, 57, 34, 62, 34, 59, 18, 30, 43, 16])]\n\n\n\n길이가 80인 array들의 리스트로 나타난다. 이것들은 데이터의 인덱스를 의미하는 것이므로 데이터프레임에 loc을 해주면 뽑아낼 수 있다…\n\n사실 엄밀히 말하자면 해당 데이터를 가지고 피팅을 하는 건 아니고, 인덱스의 수를 골라서 가중치를 부여하는 방식으로 들어가긴 한다…\n- 첫번째 트리(predictr.estimators_[0]) 재현\n\n## 호출하기 쉽도록 변수들을 어레이로 바꿔줌\nX_arr = np.array(X)\ny_arr = np.array(y)\n\n## 적합\ntree = sklearn.tree.DecisionTreeRegressor()\ntree.fit(X_arr[predictr.estimators_samples_[0]], y_arr[predictr.estimators_samples_[0]])\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(\n    tree,\n    max_depth = 1,\n    feature_names = X.columns.to_list()\n);\n\n\n\n\n\n## 작업결과\nsklearn.tree.plot_tree(\n    predictr.estimators_[0],\n    feature_names = X.columns.to_list(),\n    max_depth = 1\n);\n\n\n\n\n실제 작업된 결과와 동일한 것을 알 수 있다.\n\n다만, Bagging에서는 .fit(sample_weight)를 지정하여 가중치로 데이터의 수를 설정하기 때문에 samples의 사이즈는 다르다.\n\n- 엄밀하게 해보면…\n\n## 정확히는 sample_weight를 조정하는 것\n## 이런 식으로...\nlst = list(np.zeros(len(X)))\n\nfor i in predictr.estimators_samples_[0] :\n    lst[i] = lst[i] + 1\ntree.fit(X, y, sample_weight = lst)\n\nfig, ax = plt.subplots(1,2, figsize = (13, 5))\nsklearn.tree.plot_tree(\n    predictr.estimators_[0],\n    feature_names = X.columns.to_list(),\n    max_depth = 1,\n    ax = ax[0]\n)\nsklearn.tree.plot_tree(\n    tree,\n    feature_names = X.columns.to_list(),\n    max_depth = 1,\n    ax = ax[1]\n);\n\n\n\n\n\n이러니까 하나도 차이가 없지요잉?\n\n- tree와 predictr의 트리플랏 비교 (고정된 i)\n\ni = 4\n\nfig, ax = plt.subplots(2,1)\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[i],\n    feature_names = X.columns.to_list(),\n    max_depth = 1,\n    ax = ax[0]\n)\nax[0].set_title('Bagging Predictor')\n\ntree = sklearn.tree.DecisionTreeRegressor()\ntree.fit(X_arr[predictr.estimators_samples_[i]], y_arr[predictr.estimators_samples_[i]])  ## 가중치 없는 퓨어 트리모델\nsklearn.tree.plot_tree(\n    tree,\n    feature_names = X.columns.to_list(),\n    max_depth = 1,\n    ax = ax[1]\n)\nax[1].set_title('tree')\n\nText(0.5, 1.0, 'tree')\n\n\n\n\n\n- tree와 predictr간 트리플랏 비교(애니메이션)\n\nfig, ax = plt.subplots(2, 1)\nplt.close()\n\n\ndef func(frame) :\n    ax[0].clear()  ## 배깅으로 적합한 predictr\n    sklearn.tree.plot_tree(\n        predictr.estimators_[frame],\n        feature_names = X.columns.to_list(),\n        max_depth = 1,\n        ax = ax[0]\n    )\n    ax[0].set_title('Bagging Predictor')\n\n    ax[1].clear()  ## 퓨어 의사결정나무로 적합한 predictr\n    tree = sklearn.tree.DecisionTreeRegressor()\n    tree.fit(X_arr[predictr.estimators_samples_[frame]], y_arr[predictr.estimators_samples_[frame]])\n    sklearn.tree.plot_tree(\n        tree,\n        feature_names = X.columns.to_list(),\n        max_depth = 1,\n        ax = ax[1]\n    )\n    ax[1].set_title('Pure Tree')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = 10\n)\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n모든 프레임의 위 아래 값이 samples의 값만을 제외하고 동일한 모습이다. (샘플을 동일하게 만들고 싶다면 위에서 언급한 과정을 사용하라. 바꾸기 귀찮아서 생략하나, 충분히 수행할 수 있을 것이다…)"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#b.-resampling-fit",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#b.-resampling-fit",
    "title": "Bagging | 의사결정나무",
    "section": "## B. Resampling + Fit",
    "text": "## B. Resampling + Fit\n- estimator에 따라 달라지는 예측치\n\nsamples = predictr.estimators_samples_\ntrees = [sklearn.tree.DecisionTreeRegressor() for i in range(len(predictr.estimators_))]\nfor i in range(len(predictr.estimators_)) :\n    trees[i].fit(X_arr[predictr.estimators_samples_[i]], y_arr[predictr.estimators_samples_[i]])\n\ni = 0  ## i번째 estimator, 해당 값을 바꿔가며 그래프를 확인해보자.\nplt.plot(X, y, 'o', alpha = 0.2, color = 'grey', label = 'True')  ## 실제값\nplt.plot(X_arr[samples[i]], y_arr[samples[i]], 'o', alpha = 0.33, label = 'Resample')  ## 리샘플링된 값, true값과 겹침\nplt.plot(X, trees[i].predict(X), '--', label = 'Prediction')  ## X에 대한 예측값\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x207a991e8c0&gt;\n\n\n\n\n\n\n어떤 샘플이 뽑히냐에 따라 (i의 값이 달라질 때마다) 적합되는 모양이 달라진다.\n\n- 달라지는 양상을 애니메이션으로 시각화\n\nfig, ax = plt.subplots(1)\nplt.close()\n\n\ndef func(frame) :\n    ax.clear()\n    ax.plot(X, y, 'o', alpha = 0.2, color = 'grey', label = 'True')\n    ax.plot(X_arr[samples[frame]], y_arr[samples[frame]], 'o', alpha = 0.3, label = 'Resampled')\n    ax.plot(X, trees[frame].predict(X), '--', label = 'Prediction')\n    ax.legend()\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = 10\n)\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))  ## IPython에서 HTML을 인코딩해옴.\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nC. 앙상블결과 재현\n\n- 손코딩으로 직접 풀어보자…\n\nBagging으로 예측한 값\n\n\npredictr.predict(X)\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])\n\n\n\n열 개의 트리로 예측한 값의 평균\n\n\nnp.stack([tree.predict(X) for tree in predictr.estimators_]).shape, np.array([tree.predict(X) for tree in predictr.estimators_]).mean(axis = 0).shape\n\n((10, 80), (80,))\n\n\n\nnp.array([tree.predict(X) for tree in predictr.estimators_]).mean(axis = 0)  ## 행마다 평균을 내어 하나의 어레이로 만듦\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])\n\n\n\n위에서의 예측은 개별 트리의 예측값들의 평균으로 정리되었다는 것을 알 수 있다.\n\n- 최종결과물 (코드로 정리)\n\ndef ensemble(trees, i=None) :\n    if i is None :\n        i = len(trees)  ## i가 Null일 때, trees의 length로 설정\n    yhat = np.array([tree.predict(X) for tree in predictr.estimators_[:i+1]]).mean(axis = 0)  ## i+1까지 슬라이싱하는 거니까 i번째까지 뽑는다.\n    return yhat\n\n\ni의 의미… 몇 번째 트리까지만 결과값을 산출하는 데에 이용하겠음… 디폴트는 전부 다…(10개)\n\n\n예시 : 0번 트리의 예측값 평균만 활용\n\n\nensemble(trees, 0)  ## 0번 트리만 적용\n\narray([10.90026146, 10.90026146, 10.90026146, 19.46336233, 19.46336233,\n       20.31785349, 16.3076088 , 16.3076088 , 16.3076088 , 20.27763408,\n       20.27763408, 21.52796629, 21.52796629, 21.52796629, 18.34698175,\n       27.5369675 , 27.5369675 , 20.30881248, 20.30881248, 25.04963215,\n       25.04963215, 25.04963215, 32.42440294, 26.49340711, 26.49340711,\n       26.49340711, 26.40925726, 26.40925726, 29.55903213, 30.75418385,\n       29.70592592, 29.70592592, 31.45007539, 32.89828946, 32.89828946,\n       31.12503261, 25.9552363 , 33.12203011, 33.12203011, 30.60313283,\n       29.45886461, 29.45886461, 29.45886461, 30.60789344, 30.60789344,\n       30.60789344, 36.5245913 , 34.24458444, 34.24458444, 37.4829917 ,\n       37.4829917 , 37.4829917 , 37.4829917 , 31.13974993, 31.13974993,\n       31.13974993, 31.13974993, 36.58400962, 35.1723381 , 35.1723381 ,\n       39.75311187, 39.75311187, 39.75311187, 34.68877582, 44.47780794,\n       39.1744058 , 40.19626989, 42.86734269, 42.60143843, 40.80476673,\n       40.80476673, 42.1996627 , 38.72741866, 41.43992372, 45.95732063,\n       50.81374143, 42.30473921, 42.30473921, 48.7391566 , 46.00793717])\n\n\n- 9번 트리(마지막 트리)의 예측값까지 활용(전부 다)\n\ndisplay(str(ensemble(trees, 9)) == str(ensemble(trees)))\ndisplay(ensemble(trees, 9))\n\nTrue\n\n\narray([11.88782962, 14.05941305, 15.02231867, 18.03161729, 19.62619066,\n       19.86214551, 15.84293717, 15.95940294, 15.95940294, 20.30137042,\n       20.30137042, 22.51278676, 22.51278676, 23.68899036, 20.7954938 ,\n       26.45727462, 26.45727462, 20.48421278, 20.48421278, 25.08188452,\n       25.08188452, 25.08188452, 31.42611771, 25.99393577, 25.99393577,\n       25.99393577, 27.05912187, 27.05912187, 29.60439358, 29.94005816,\n       29.18760881, 29.18760881, 30.75340115, 30.82608162, 32.48384789,\n       31.03678302, 29.02978839, 31.17487146, 31.17487146, 31.05349512,\n       29.147739  , 29.147739  , 29.147739  , 30.40843883, 30.40843883,\n       33.53154643, 34.26668831, 33.20982041, 33.20982041, 36.82818648,\n       36.82818648, 34.66545508, 34.66545508, 34.24047203, 33.0829342 ,\n       33.0829342 , 35.29894866, 35.50366771, 35.47938512, 35.47938512,\n       38.8116606 , 38.8116606 , 37.74794717, 34.84063828, 39.73515434,\n       40.01130524, 40.05274675, 41.9980937 , 42.26869452, 40.81707653,\n       40.16985211, 41.5373848 , 39.69311797, 42.97563198, 45.99122302,\n       49.35681519, 43.64765096, 45.32629064, 47.10042494, 46.28105912])"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#d.-학습과정steps-시각화",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#d.-학습과정steps-시각화",
    "title": "Bagging | 의사결정나무",
    "section": "### D. 학습과정(steps) 시각화",
    "text": "### D. 학습과정(steps) 시각화\n- 고정된 i\n\ni = 0\n\nfig, ax = plt.subplots(1,4, figsize = (13, 3))\n## step 0 -- data import\nax[0].set_title('Step 0')\nax[0].plot(X, y, 'o', color = 'grey', alpha = 0.2)\n\n## step 1 -- Resampling\nax[1].set_title('Step 1 : ReSampling')\nax[1].plot(X, y, 'o', color = 'grey', alpha = 0.2)\nax[1].plot(X_arr[samples[i]], y_arr[samples[i]], 'o', alpha = 0.3)\n\n## step 2 -- fitting\nax[2].set_title('Step 3 : Fitting')\nax[2].plot(X, y, 'o', color = 'grey', alpha = 0.2)\nax[2].plot(X_arr[samples[i]], y_arr[samples[i]], 'o', alpha = 0.3)\nax[2].plot(X, trees[i].predict(X), '--')  ## 개별 tree의 적합\n\n## step 3 -- ensemble\nax[3].set_title('Step 4 : Ensemble')\nax[3].plot(X, y, 'o', color = 'grey', alpha = 0.2)\nax[3].plot(X, ensemble(trees, i), '--')  ## 적합한 트리들을 평균내어 적용\n\n\n\n\n\nBagging이 적합하는 과정 :\n\n\n자료를 받아온다.\n받아온 자료를 복원추출한다.\n복원추출한 자료를 의사결정나무로 적합한다.\n적합한 트리들의 개별 예측값들의 평균을 최종 예측값으로 제시한다.\n\n- 애니메이션화\n\ndef func(i) :\n    for a in ax:\n        a.clear()\n\n    ## step 0 -- data import\n    ax[0].set_title('Step 0')\n    ax[0].plot(X, y, 'o', color = 'grey', alpha = 0.2)\n\n    ## step 1 -- Resampling\n    ax[1].set_title('Step 1 : ReSampling')\n    ax[1].plot(X, y, 'o', color = 'grey', alpha = 0.2)\n    ax[1].plot(X_arr[samples[i]], y_arr[samples[i]], 'o', alpha = 0.3)\n    \n    ## step 2 -- fitting\n    ax[2].set_title('Step 3 : Fitting')\n    ax[2].plot(X, y, 'o', color = 'grey', alpha = 0.2)\n    ax[2].plot(X_arr[samples[i]], y_arr[samples[i]], 'o', alpha = 0.3)\n    ax[2].plot(X, trees[i].predict(X), '--')  ## 개별 tree의 적합\n    \n    ## step 3 -- ensemble\n    ax[3].set_title('Step 4 : Ensemble')\n    ax[3].plot(X, y, 'o', color = 'grey', alpha = 0.2)\n    ax[3].plot(X, ensemble(trees, i), '--', color = 'C1')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames = len(predictr.estimators_features_)\n)\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n최종 예측값에 관여하는 tree의 수가 많아질수록 예측값의 선이 완만해진다…"
  },
  {
    "objectID": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#어떻게-이런-생각을-했을까",
    "href": "2023_MP/practice/B3. 의사결정나무 - 배깅.html#어떻게-이런-생각을-했을까",
    "title": "Bagging | 의사결정나무",
    "section": "5. 어떻게 이런 생각을 했을까?",
    "text": "5. 어떻게 이런 생각을 했을까?\n# 모티브 : 끝자락의 예측값들을 조금 완만하게 만들고 싶었고, 이를 서로 다른 여러개의 tree를 만들어 해결하고자 하는데, 붓스트랩에서 영감을 받아 여러개의 트리를 만들었다.\n# 오차까지 적합하려고 하는 의사결정나무의 작동방식을 완화하려고 했음."
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html",
    "title": "오버피팅, 다중공선성",
    "section": "",
    "text": "오버피팅은 뭐고, 다중공선성은 왜 발생할까? 그리고 해결은 어떻게 할까?"
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#라이브러리-imports",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#라이브러리-imports",
    "title": "오버피팅, 다중공선성",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport sklearn"
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#언더라잉과-오차항",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#언더라잉과-오차항",
    "title": "오버피팅, 다중공선성",
    "section": "2. 언더라잉과 오차항",
    "text": "2. 언더라잉과 오차항\n- 만약 내가 원한다면, 관련이 있든 없든 무수히 많은 데이터를 모을 수 있다고 가정하자…\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\ndf_balance = pd.DataFrame((np.random.randn(500,5000)&gt;0.5).reshape(500,5000)*1,columns = ['X'+str(i) for i in range(5000)])\ndf_merged = pd.concat([df,df_balance],axis=1)\ndf_merged\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nX0\nX1\nX2\nX3\nX4\nX5\nX6\n...\nX4990\nX4991\nX4992\nX4993\nX4994\nX4995\nX4996\nX4997\nX4998\nX4999\n\n\n\n\n0\n135\n0.051535\n0\n1\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n1\n1\n1\n1\n1\n\n\n1\n935\n0.355496\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n485\n2.228435\n0\n1\n0\n1\n0\n0\n1\n0\n...\n0\n0\n1\n1\n0\n1\n0\n1\n0\n1\n\n\n3\n65\n1.179701\n0\n1\n1\n0\n0\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n4\n445\n3.962356\n1\n0\n0\n0\n0\n0\n1\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0\n0\n0\n1\n1\n1\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n496\n310\n2.601212\n1\n0\n1\n1\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n\n\n497\n225\n0.042323\n0\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n498\n320\n1.041416\n0\n1\n0\n0\n0\n1\n0\n1\n...\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n\n\n499\n375\n3.626883\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n\n\n500 rows × 5003 columns\n\n\n\n\nemployment의 예측과 상관이 없을 개인의 선호, balance_game을 가져왔다. (5000종류)\n\n\n## df_train, df_test =  sklearn.model_selection.train_test_split(test_size = 0.2) 이걸로 해도 된다.\n## step 1\nX = df_merged.drop(['employment'], axis = 1)[:400]\nXX = df_merged.drop(['employment'], axis = 1)[400:]\ny = df_merged.employment[:400]\nyy = df_merged.employment[400:]\n\n## step 2\npredictr = sklearn.linear_model.LogisticRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n(1.0, 0.79)\n\n\n\n쓸모없는 변수(y와 상관관계가 낮은 변수)를 사용해서 오버피팅되었다. train score가 상당히 높게 나왔다.(오차항까지 예측한 상황)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n\n# 1\nX = df.drop(['employment'], axis = 1)[:400]\nXX = df.drop(['employment'], axis = 1)[400:]\ny = df[['employment']][:400]\nyy = df[['employment']][400:]\n\n# 2\nprdtr = sklearn.linear_model.LogisticRegression()\n\n# 3\nprdtr.fit(X,y)\n\n# 4\nprdtr.score(X,y), prdtr.score(XX, yy)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n(0.8925, 0.83)\n\n\n\ntest 데이터에서 스코어가 더 높았다."
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#다중공선성",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#다중공선성",
    "title": "오버피팅, 다중공선성",
    "section": "3. 다중공선성",
    "text": "3. 다중공선성\n- 아래와 같은 가짜뉴스를 읽어보자.(ChatGPT를 이용하여 생성한 가짜뉴스)\n헤드라인: “텝스와 다른 영어 인증 시험들, 결국은 토익과 비슷한 결과를 보여준다?”\n본문:\n최근 몇 년 동안, 토익의 신뢰성에 대한 논란이 계속되어 왔습니다. 이러한 배경 속에서 텝스(TEPS), 토플(TOEFL) 등 여러 새로운 영어 능력 평가 시험이 등장하였습니다. 많은 학생들과 직장인들은 이러한 새로운 시험들이 토익보다 더 신뢰성 있고 현실적인 능력을 평가할 것이라는 기대감을 가지고 있었습니다.\n그러나 최근에 발표된 연구결과에 따르면, 텝스와 다른 영어 인증 시험들도 결국에는 토익과 매우 비슷한 성적 분포와 결과를 보여주었다고 합니다. 연구 팀은 여러 시험들간의 점수 분포와 성적의 상관관계를 분석한 결과, 대부분의 시험들이 실제 영어 능력에 대해 유사한 평가를 제공한다는 결론을 내렸습니다.\n“많은 사람들이 새로운 시험들이 더 현실적이거나 다양한 영어 능력을 평가할 것이라 기대했지만, 실제로는 모든 시험들이 비슷한 결과를 보여주었습니다.” 라며 연구 팀의 대표는 이렇게 언급하였습니다.\n이러한 연구결과는 영어 능력 평가 시험의 표준화와 신뢰성에 대한 논의를 새롭게 불러일으킬 것으로 보입니다.\n- 뉴스에 근거하여 아래의 가짜 자료를 생성했다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\nNaN\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\nNaN\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\nNaN\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\nNaN\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\nNaN\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\nNaN\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\nNaN\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\nNaN\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\nNaN\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\nNaN\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ntoeic0 ~ toeic499는 유사토익을 의미\n\n- 모르는 정보 : 사내 고용 법칙\n\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic * 1/100 + np.random.randn(500)\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n학점 1 증가는 토익 100점 증가와 비슷하다고 고려하고 있다.\n\n\nA. 이대로 분석 | 잘못됨\n\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.drop(['employment_score'], axis = 1)\ny = df_train.employment_score\nXX = df_test.drop(['employment_score'], axis = 1)\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.11705078212495712)\n\n\n\n두 점수가 큰 차이가 난다.(오차항까지 적합해버린 오버피팅의 상황)\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제로는 gpa는 1, toeic은 0.01, 나머지는 0이 되어야 하지만, 많이 다르다…"
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#b.-제대로-분석했다면",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#b.-제대로-분석했다면",
    "title": "오버피팅, 다중공선성",
    "section": "### B. 제대로 분석했다면?",
    "text": "### B. 제대로 분석했다면?\n- toeic과 gpa만이 유의미한 변수라는 걸 눈치챔. (아다리, 현실세계에선 일어날 수 없음)\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.loc[:, ['toeic', 'gpa']]\ny = df_train.employment_score\nXX = df_test.loc[:, ['toeic', 'gpa']]\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9133033622085311, 0.9127346436925985)\n\n\n\n스코어도 높음\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ntoeic    0.010063\ngpa      0.972163\ndtype: float64\n\n\n\n실제 계수값과 유사하도록 잘 추정됨\n\n\nC. 하다못해 toeic0와 gpa로 적합했다면???\n\n\n## step 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 42)\nX = df_train.loc[:, ['toeic0', 'gpa']]\ny = df_train.employment_score\nXX = df_test.loc[:, ['toeic0', 'gpa']]\nyy = df_test.employment_score\n\n## step 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## step 3\npredictr.fit(X, y)\n\n## step 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9120540945251211, 0.9115427614193155)\n\n\n\ns = pd.Series(predictr.coef_)\ns.set_axis(X.columns, axis = 0)\n\ntoeic0    0.010101\ngpa       0.981302\ndtype: float64\n\n\n\n굉장히 합리적이다!"
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#d.-고찰",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#d.-고찰",
    "title": "오버피팅, 다중공선성",
    "section": "### D. 고찰",
    "text": "### D. 고찰\n- 의문 : 왜 변수를 더 많이 넣었는데, 정보를 더 많이 제공해줬는데, 이상한 결과가 나올까???\n\n규칙을 찾으면 안될 것 (반응변수와의 상관관계가 없는 것) 에서 규칙을 찾고 있으니까 (오차항을 적합) 잘 될리가 없지…\n\n- 쓸모없는 변수?\n\n\n진짜 쓰레기, 쓰잘데기 없는 것(X1 = 부먹/찍먹, X2 = 민초/반민초…) -&gt; 애초에 이딴걸 가지고 y를 맞출 생각도 들지 않음…\n\n\n실제론 쓸모 있는데, 대체제가 있는 경우 -&gt; 대체제를 보고 y를 맞출 것 같기도 한데, 둘은 너무 비슷함…\n\n\n- 1과 2 모두 과대적합(overfitting)을 야기하고, 2와 같은 상황에서 발생하는 문제를 다중공선성(multiple linearity)이라고 한다.\n\n1은 corr(x_1, y), corr(x_2, y)가 낮게 나온다 -&gt; y와의 관계가 없다. 2는 corr(x_1, y), corr(x_2, y)는 높게 나오는데, corr(x_1, x_2)도 높게 나온다."
  },
  {
    "objectID": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#다중공선성의-특징",
    "href": "2023_MP/practice/A5. 오버피팅, 다중공선성.html#다중공선성의-특징",
    "title": "오버피팅, 다중공선성",
    "section": "4. 다중공선성의 특징",
    "text": "4. 다중공선성의 특징\n- 잘못된 분석을 재현하고, 계수를 해석해보자.\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n- 특이사항\n\ns.loc['toeic':].sum()\n\n0.010302732920633051\n\n\n\n비슷한 설명변수들의 회귀계수를 합하니까 0.01과 유사한 값이 나왔음…\n\n\nfig, ax = plt.subplots(3)\n\nfor i in range(3):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = i)\n    X = df_train.drop(['employment_score'], axis = 1)\n    XX = df_test.drop(['employment_score'], axis = 1)\n    y = df_train.employment_score\n    yy = df_test.employment_score\n\n    predictr = sklearn.linear_model.LinearRegression()\n\n    predictr.fit(X, y)\n\n    s = pd.Series(predictr.coef_)\n    ax[i].plot(s[1:], '-')\n    ax[i].set_title('sum of toeic coef = {}'.format(round(s[1:].sum(), 4)))\n\nfig.tight_layout()\n\n\n\n\n\n계수는 상당히 불안정하나, 그 합은 합리적인 값이 나온다.\n계수값의 해석이 용이하지 않다. 음의 계수값이 있다는 것은, 토익 유사한 시험의 점수를 올리면 취업이 오히려 안된다(…)라는 것과도 같다.\n\n\n이것의 해결은 직접 몇 개만 지우거나, 다중공선성을 해결하기 위해 패널티를 부여하는 모듈을 써서 해소 가능하다."
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "",
    "text": "의사결정나무를 이용하여 모형을 간단하게 적합해보자!"
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#라이브러리-imports",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#라이브러리-imports",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\nimport sklearn.tree\nimport seaborn as sns\nimport sklearn.model_selection"
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#다중공선성",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#다중공선성",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "2. 다중공선성",
    "text": "2. 다중공선성\n\nA. Data\n\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n유사 토익들이 매우 많은, 설명변수에 다중공선성이 존재하는 자료이다.\n\n- 근데 Lasso를 쓰지 않고도, 여러 설명변수를 배제하지 않고도, 자료를 쉽게 적합할 수 있는 방법이 있다면 믿겠는가???\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.8203054095383844)\n\n\n\n본래 모델은 트리 모형의 작동방식에 따라 정확도가 1이 나오지만, 예측 모델의 성능도 나름 나쁘지 않다.\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.linear_model.LassoCV(alphas = np.linspace(0.1, 2, 20))\n\n## 3\npredictr.fit(X, y)\n\n## 4\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.066e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.424e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.596e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.791e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.086e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.995e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.758e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.269e+00, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.408e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.055e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.412e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.745e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.516e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.655e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.318e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.655e+01, tolerance: 2.693e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.360e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.781e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.542e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.733e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.424e+00, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.040e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.248e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.404e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.849e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.897e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.468e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.599e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.833e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.540e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.155e+01, tolerance: 2.749e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.046e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.674e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.878e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.826e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.194e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.973e+00, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.167e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.597e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.071e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.339e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.314e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.767e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.678e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.100e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.054e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.692e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+01, tolerance: 2.668e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.383e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.403e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.355e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.533e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.173e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.488e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.664e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.037e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.565e+00, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.378e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.431e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.947e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.467e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.213e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.636e+01, tolerance: 2.723e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.980e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.452e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.598e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.233e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.944e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.593e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.450e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.626e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.864e+00, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.144e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.382e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.197e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.903e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.244e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.406e+01, tolerance: 2.545e-01\n  model = cd_fast.enet_coordinate_descent(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.041e+02, tolerance: 3.346e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. ]))\n\n\n\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9554916123291357, 0.8733838672032972)\n\n\n\nLasso로 적합한 결과가 더 스코어가 높긴 하지만, 트리 모형은 직관적이고 단순하다. 그리고 하이퍼파라미터도 바꿀 수 있잖아?\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n\nX = df_train.drop('employment_score', axis = 1)\ny = df_train.employment_score\nXX = df_test.drop('employment_score', axis = 1)\nyy = df_test.employment_score  ## 실제로는 알 수 없는 자료\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth = 5)\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(0.9435995918442013, 0.853062350997775)"
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#b.-평가",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#b.-평가",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### B. 평가",
    "text": "### B. 평가\n\nLasso가 좀 더 좋긴 한데, 의사결정나무도 공선성이 있는 상황에서 간단하게 사용가능하다.\n\n\nLasso는 엄청 발전된 모델이고, 의사결정나무는 아주 초기모델이라… 개선의 여지가 많다."
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#오버피팅",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#오버피팅",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "3. 오버피팅",
    "text": "3. 오버피팅\n\nA. 사전작업\n\n\n종속변수와 관련이 없는 변수들을 무작위로 생성하는 함수\n\n\ndef generating_df(n_balance):\n    global df\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\n## 물론 해당 함수에 df를 따로 입력하게 하여 length를 조절한 후 아무 df에 사용가능하도록 만들수도 있다.\ndef random_generation_df(df, n) :\n    df_random = pd.DataFrame(np.random.randn(len(df), n), columns = ['random'+str(i) for i in range(n)])\n    return pd.concat([df, df_random], axis = 1)\n\n\ngenerating_df(1)\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\n\n\n\n\n0\n135\n0.051535\n0\n0.039391\n\n\n1\n935\n0.355496\n0\n0.616426\n\n\n2\n485\n2.228435\n0\n0.985876\n\n\n3\n65\n1.179701\n0\n1.677899\n\n\n4\n445\n3.962356\n1\n-0.167288\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.936697\n\n\n496\n310\n2.601212\n1\n-0.184044\n\n\n497\n225\n0.042323\n0\n-0.805794\n\n\n498\n320\n1.041416\n0\n-0.483178\n\n\n499\n375\n3.626883\n1\n0.717244\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n## 1\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\nX = df_train.drop('employment', axis = 1)\ny = df_train.employment\nXX = df_test.drop('employment', axis = 1)\nyy = df_test.employment  ## 실제론 모름"
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#b.-분석",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#b.-분석",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### B. 분석",
    "text": "### B. 분석\n# 1 : 의사결정나무\n\n## 2\npredictr = sklearn.tree.DecisionTreeClassifier()  ## 범주형일 때 더 유용\n\n## 3\npredictr.fit(X, y)\n\n## 4\npredictr.score(X, y), predictr.score(XX, yy)\n\n(1.0, 0.82)\n\n\n\n오버피팅된 상황이다.\n\n# 2 : Lasso(L1 penalty)\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(penalty = 'l1', solver = 'liblinear')\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(employment_hat = predictr.predict(X))\ndf_test = df_test.assign(employment_hat = predictr.predict(XX))\n\n##-##\nprint('train score = ' + str(round(predictr.score(X, y), 4)))\nprint('test score = ' + str(round(predictr.score(XX, yy), 4)))\n\ntrain score = 0.8829\ntest score = 0.8733\n\n\n\n~역시 Lasso가 최고다~\n\n# 3 : Ridge(L2 penalty)\n\n## 2\npredictr = sklearn.linear_model.LogisticRegressionCV(penalty = 'l2')\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(employment_hat = predictr.predict(X))\ndf_test = df_test.assign(employment_hat = predictr.predict(XX))\n\n##-##\nprint('train score = ' + str(round(predictr.score(X, y), 4)))\nprint('test score  = ' + str(round(predictr.score(XX, yy), 4)))\n\ntrain score = 0.8886\ntest score  = 0.88\n\n\n\n~Ridge도 최고다~\n\n\nC. 설명변수들의 증가\n\n- 관련없는 변수들의 수가 커짐에 따라서 각 방법들의 train/test score는 어떻게 변화할까?\n\n데이터프레임과 predictor, 반응변수 열 이름을 넣어주면 fitting하고, 스코어를 배출하는 함수\n\n\ndef fitting_df(df, predictor, response) :\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size = 0.3, random_state = 202014107)\n    X = df_train.drop(response, axis = 1)\n    y = df_train[response]\n    XX = df_test.drop(response, axis = 1)\n    yy = df_test[response]\n\n    predictor.fit(X, y)\n\n    return predictor.score(X, y), predictor.score(XX, yy)\n\n\npredictor의 리스트와 필요없는 설명변수들이 가득찬 열의 개수를 정의\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(),\n             sklearn.linear_model.LogisticRegressionCV(penalty = 'l1', solver = 'liblinear'),\n             sklearn.linear_model.LogisticRegressionCV(penalty = 'l2')]\n\nn_balance_list = range(0, 5000, 50)\n\n\n그것들을 기반으로 세 변수들에게서 피팅하면서 나온 점수들을 원소로 하는 리스트를 컴프리헨션\n\n\nlst = [fitting_df(generating_df(n_balance), predictr, 'employment') for predictr, n_balance in [(predictr, n_balance) for n_balance in n_balance_list for predictr in predictrs]]\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nKeyboardInterrupt: \n\n\n\n좀 오래 걸릴 수밖에 없음…(이중으로 컴프리헨션 하는 게 훨씬 적합한 것 같긴 하다. 나중에 처리를 또 해야되니…)\n\n- 실험결과의 정리\n\narr = np.array(lst)\ntr = arr[:, :, 0]\ntst = arr[:, :, 1]\narr.shape\n\n\nar = np.array(lst)\narr = arr.reshape(100, 3, 2)\ntr = arr[:, :, 0]\ntst = arr[:, :, 1]\n\n\n전반적으로 관련없는 변수가 많아질수록 스코어가 떨어지기는 하는데, 뒤로 갈수록 tree의 점수는 다른 것들보다 감소폭이 적어 역전되는 상황이다.\n\n- 이를 시각화해보자.\n\npd.DataFrame(tr, columns = ['tree', 'lasso', 'ridge'])\n_.assign(dataset = 'train')\n\n\ntr_score = pd.DataFrame(tr, columns = ['tree', 'lasso', 'ridge'])\ntst_score = pd.DataFrame(tst, colunms = ['tree', 'lasso', 'ridge'])\n\nresult_df = pd.concat([tr_score, tst_score], axis = 0)"
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#이상치",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#이상치",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "4. 이상치",
    "text": "4. 이상치"
  },
  {
    "objectID": "2023_MP/practice/A8. 의사결정나무의 활용.html#a.-데이터",
    "href": "2023_MP/practice/A8. 의사결정나무의 활용.html#a.-데이터",
    "title": "의사결정나무의 활용 | 다중공선성, 오버피팅, 이상치",
    "section": "### A. 데이터",
    "text": "### A. 데이터\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.show()\n\n\n\n\n\n나머지 자료들은 모두 선형을 띄고 있는데, (-4.1, 200)이라는 이상치 하나가 모형의 설명을 어렵도록 만들고 있다.\n\n\nB. 분석\n\n# 1 일반적인 선형 회귀\n\n## 1\nX = df_train[['temp']]\ny = df_train.ice_sales\n\n## 2\npredictr = sklearn.linear_model.LinearRegression()\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(ice_sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.plot(df_train.temp, df_train.ice_sales_hat, '--', color = 'red')\nplt.show()\n\n\n\n\n\npredictr.coef_\n\narray([-0.64479089])\n\n\n\n기울기가 음수이다.\n- 늘 하던 것처럼 선형회귀로 적합했을 때, 해당 모형은 완전히 언더피팅된 것으로 보여진다…\n\n# 2 의사결정나무를 사용\n(주의!) Lasso와 Ridge는 공선성이 있는 모델에서만 잘 작동할 뿐, 그렇지 않은 경우 선형회귀와 유사하게 적합되니 사용하지 말것!(Ridge는 설명변수가 적을 때 특히 똑같게 적합하는 것 같다.)\n\n## 1\nX = df_train[['temp']]\ny = df_train.ice_sales\n\n## 2\npredictr = sklearn.tree.DecisionTreeRegressor()\n\n## 3\npredictr.fit(X, y)\n\n## 4\ndf_train = df_train.assign(ice_sales_hat = predictr.predict(X))\n\n\nplt.plot(df_train.temp, df_train.ice_sales, 'o')\nplt.plot(df_train.temp, df_train.ice_sales_hat, '--', color = 'red')\nplt.show()\n\n\n\n\n\npredictr.score(X, y)\n\n0.9992029367488545\n\n\n\n_df = df_train.drop(0, axis = 0)\nplt.plot(_df.temp, _df.ice_sales, 'o', label = 'observations')\nplt.plot(_df.temp, _df.ice_sales_hat, '--', color = 'red', label = 'prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nDecisionTreeRegressor의 경우 언더라잉만 적합하는 것을 넘어 오버피팅이 되게 하나, 결과는 언더피팅이 되는 경우보단 나름 합리적이다.\n적합에 관여한 구간 외의 값이 인풋으로 들어오면 해당 모형은 예측하지 못한다."
  },
  {
    "objectID": "2023_MP/practice/A2. 결측치.html",
    "href": "2023_MP/practice/A2. 결측치.html",
    "title": "결측치의 처리",
    "section": "",
    "text": "결측치를 시각화해보고, 계산해서 대치(impute)해보기도 하자!"
  },
  {
    "objectID": "2023_MP/practice/A2. 결측치.html#라이브러리-imports",
    "href": "2023_MP/practice/A2. 결측치.html#라이브러리-imports",
    "title": "결측치의 처리",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\n#!pip install missingno # missingno 라이브러리가 설치되어있지 않을 경우\n\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport sklearn.impute"
  },
  {
    "objectID": "2023_MP/practice/A2. 결측치.html#missingno의-활용",
    "href": "2023_MP/practice/A2. 결측치.html#missingno의-활용",
    "title": "결측치의 처리",
    "section": "2. missingno의 활용",
    "text": "2. missingno의 활용\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/msno.csv\")\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n0.383420\n1.385096\nNaN\n-0.545132\n-0.732395\n\n\n1\n1.084175\n0.080613\n-0.770527\n-0.272143\n-0.749881\n\n\n2\n1.142778\n1.258419\nNaN\n-0.072007\n-0.440757\n\n\n3\n0.307894\n0.521400\n0.446974\n0.329530\n-1.457388\n\n\n4\n0.237787\n0.132401\n-0.516630\n0.177995\n0.416182\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.041092\n-1.308165\n1.085820\n1.136210\nNaN\n\n\n996\n-1.286358\n1.547987\nNaN\n-0.174334\n-0.579486\n\n\n997\n0.710257\n1.764058\nNaN\n-0.353928\nNaN\n\n\n998\n-1.908729\n-0.804691\nNaN\nNaN\n-0.066739\n\n\n999\n0.650026\n2.206549\nNaN\n-0.919945\nNaN\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n결측치가 딱봐도 엄청 많아보인다. missingno는 그것을 시각화해준다.\n\n\nmsno.matrix(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n우측 노이즈와 같은 그래프에서 0에 있는 것은 해당 행에 데이터가 하나도 없다는 뜻이고, 5에 있는 것은 다섯개의 데이터가 해당 행에 존재한다는 것이다. 데이터셋이 다섯개니까 그 합이 그래프로 표기된다.\n\n\nmsno.heatmap(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nmsno.dendrogram(df)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n구조가 비슷한 자료들을 엮어놓는다.\n\n그럼 시각화를 했으니까, 이제 결측치를 처리해야겠지?"
  },
  {
    "objectID": "2023_MP/practice/A2. 결측치.html#숫자형-자료의-impute결측치를-대체하는-것",
    "href": "2023_MP/practice/A2. 결측치.html#숫자형-자료의-impute결측치를-대체하는-것",
    "title": "결측치의 처리",
    "section": "3. 숫자형 자료의 impute(결측치를 대체하는 것)",
    "text": "3. 숫자형 자료의 impute(결측치를 대체하는 것)\n- 주어진 자료\n\nA = [2.1, 1.9, 2.2, np.nan, 1.9]\nB = [0, 0, np.nan, 0, 0]\n\n\ndf = pd.DataFrame({'A' : A, 'B' : B})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\n결측치를 무엇으로 채워주면 좋을까?\n\n\n일단 평균으로 해보면 얼추 맞을 것 같다.\n\n\ndf2 = df\ndf2.loc[3, 'A'] = df2.A.mean()  ## mean과 같은 메소드는 결측치를 반영하지 않는다.\ndf2.loc[2, 'B'] = df2.B.mean()\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 근데 이게 엄청 많으면 언제 다 일일히 하고 있어? &gt; 자동으로 하려면?\n(방법1) | 평균으로 impute\n\nimputr = sklearn.impute.SimpleImputer()  ## SimpleImputer(strategy = 'mean')\nimputr\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\npredictr.fit하는 것처럼 결측치가 있는 열에 적합해야 한다.\n\n\nimputr.fit(df)\n\nSimpleImputer()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputerSimpleImputer()\n\n\n\npredictr.predict하는 것처럼 인풋시켜야 한다.\n\n\nimputr.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n\n위에서와 똑같은 결과를 산출했다.\n\n해당 과정은 imputr.fit_transform(df)로 한번에 시행할 수 있다.\n- 만약 평균이 아닌 다른 방식으로 결측치를 대체하고 싶다면…\n(방법 2) | median으로 impute\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'median')\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법 3) | 최빈값으로 대체\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법 4) | 정해진 상수값으로 대체\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 999)\nimputr.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])"
  },
  {
    "objectID": "2023_MP/practice/A2. 결측치.html#범주형-자료의-impute",
    "href": "2023_MP/practice/A2. 결측치.html#범주형-자료의-impute",
    "title": "결측치의 처리",
    "section": "4. 범주형 자료의 impute",
    "text": "4. 범주형 자료의 impute\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n(방법 1) | 최빈값을 이용\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'most_frequent')\nimputr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법 2) | 상수(지정값)로 대체함\n\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'G')\nA_ = pd.Series(imputr.fit_transform(df[['A']]).reshape(-1))\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'economy')\nB_ = pd.Series(imputr.fit_transform(df[['B']]).reshape(-1))\n\n\npd.concat([A_, B_], axis = 1).set_axis(['A','B'], axis = 1)\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\neconomy\n\n\n3\nY\nstat\n\n\n4\nG\nbio\n\n\n\n\n\n\n\n\n## 또는\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'G')\nA_ = imputr.fit_transform(df[['A']])\nimputr = sklearn.impute.SimpleImputer(strategy = 'constant', fill_value = 'economy')\nB_ = imputr.fit_transform(df[['B']])\n\npd.DataFrame(np.concatenate([A_,B_], axis = 1), columns = ['A','B'])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\neconomy\n\n\n3\nY\nstat\n\n\n4\nG\nbio\n\n\n\n\n\n\n\n\n일반적으로 연속형ㆍ숫자형 자료에는 평균, 범주형 자료에는 최빈값으로 대체한다."
  },
  {
    "objectID": "2023_MP/Titanic/A1. code-by-alexis-cook.html",
    "href": "2023_MP/Titanic/A1. code-by-alexis-cook.html",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "",
    "text": "Discussion 탭에서 가장 상위에 있는 안내 자료를 살펴보자."
  },
  {
    "objectID": "2023_MP/Titanic/A1. code-by-alexis-cook.html#라이브러리-import",
    "href": "2023_MP/Titanic/A1. code-by-alexis-cook.html#라이브러리-import",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier"
  },
  {
    "objectID": "2023_MP/Titanic/A1. code-by-alexis-cook.html#data-불러오기",
    "href": "2023_MP/Titanic/A1. code-by-alexis-cook.html#data-불러오기",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "2. Data 불러오기",
    "text": "2. Data 불러오기\n\ntrain_data = pd.read_csv(\"./data/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntest_data = pd.read_csv(\"./data/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS"
  },
  {
    "objectID": "2023_MP/Titanic/A1. code-by-alexis-cook.html#alexis의-코드-forecast",
    "href": "2023_MP/Titanic/A1. code-by-alexis-cook.html#alexis의-코드-forecast",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "3. Alexis의 코드 | forecast",
    "text": "3. Alexis의 코드 | forecast\n\nA. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]  ## 결측치가 많은 것들과 이상한 녀석들을 배제했다.\nX = pd.get_dummies(train_data[features])  ## 변수들 중 범주형 자료를 더미변수로 만든다.\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)  ## 예측값을 담아둔다. predictr.predict(XX)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n##output.to_csv('submission_AlexisCook.csv', index=False)  ##파일을 자꾸 만들어서 주석처리했다.\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nRandomForestClassifier 모듈을 사용하여 fitting 하였다.\n\n- 간단한 수정\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)  ## predict를 train data에 실시\n\n\n(predictions == y).mean()\n\n0.8159371492704826\n\n\n\nmodel.score(X, y)\n\n0.8159371492704826\n\n\n\nscore는 모델이 데이터와 맞는 정도를 내준다."
  },
  {
    "objectID": "2023_MP/Titanic/A1. code-by-alexis-cook.html#alexis-cook의-코드를-수정해보자",
    "href": "2023_MP/Titanic/A1. code-by-alexis-cook.html#alexis-cook의-코드를-수정해보자",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "### Alexis Cook의 코드를 수정해보자!",
    "text": "### Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\n## hyper parameter 조정\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nmodel.score(X, y)\n\n0.8170594837261503\n\n\n\n바꾼 게 더 좋은 것 같은데???\n\n- 이것도 제출 결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n\npd.read_csv(\"./data/test.csv\")[['PassengerId']].assign(Survived = predictions)#\\\n#.to_csv(\"AlexisCook수정_submission.csv\", index = False)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\nindex를 꼭 누락시켜야 한다."
  },
  {
    "objectID": "2023_MP/Titanic/A1. code-by-alexis-cook.html#제출결과의-비교",
    "href": "2023_MP/Titanic/A1. code-by-alexis-cook.html#제출결과의-비교",
    "title": "Kaggle | Alexis Cook의 코드",
    "section": "4. 제출결과의 비교",
    "text": "4. 제출결과의 비교\n\nhyper parameter를 조정해서 train score는 더 높아졌지만, 실제 test score는 더 낮아졌다.\n\n- overfitting된 경우 둘의 차이가 극명하게 난다."
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html",
    "href": "2023_MP/Titanic/A0. practice.html",
    "title": "Kaggle | 1st practice",
    "section": "",
    "text": "Kaggle의 competition에 대해 차근차근히 알아보고 첫 제출까지 해보도록 하자."
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html#라이브러리-imports",
    "href": "2023_MP/Titanic/A0. practice.html#라이브러리-imports",
    "title": "Kaggle | 1st practice",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport numpy as np\nimport pandas as pd\n\n\n# 캐글에 있는 노트북을 이용하면 가상 컴퓨터에 세 개의 파일들이 직접 들어온다.\n\ntr = pd.read_csv(\"./data/train.csv\")\ntr.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntst = pd.read_csv('./data/test.csv')\ntst.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS"
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html#kaggle-competition",
    "href": "2023_MP/Titanic/A0. practice.html#kaggle-competition",
    "title": "Kaggle | 1st practice",
    "section": "2. Kaggle Competition",
    "text": "2. Kaggle Competition\n\nA. 데이터 구경\n\n- 데이터의 설명을 빠르게 파악하는 방법\n1. 변수 위주로 kaggle 홈페이지에서 파악\n1. 구글 번역기 사용\n1. ChatGPT 이용\n\nChatGPT가 옳지 않은 소리를 할 때도 있지만, 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수 이름이 약어로 된 경우가 많은데, 이럴 경우 GPT가 유용하다."
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html#b.-메뉴",
    "href": "2023_MP/Titanic/A0. practice.html#b.-메뉴",
    "title": "Kaggle | 1st practice",
    "section": "### B. 메뉴",
    "text": "### B. 메뉴\n\nOverview(개요)\n\n\n경진대회 주최자가 경진 대회의 배경, 목표, 데이터셋 설명 등을 기술.\n\n\nData(데이터)\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 포함됨.\n\n\nCode(코드)\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 포함됨.\n\n\nDiscussion(토론)\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이뤄짐.\n\n\nLeaderboard(리더보드)\n\n\n경진대회 참가자들의 모델에 대한 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRule(규칙)\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 작업, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들 * Overview : martic, prize, timeline * Rules : matric, 외부데이터 사용 가능 여부, 하루 최대 제출 수, 최종 선택 가능한 솔루션 수(limit)\n- 대회의 유형 * Getting Started : 상을 제공하지 않음. 튜토리얼 전용. * Featured : 가장 일반적인 유형, 스폰서 회사의 비즈니스 관련 문제이므로 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야 하고 발표할 것을 요구받을 수 있음. * Analytics : 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html#데이터-분석",
    "href": "2023_MP/Titanic/A0. practice.html#데이터-분석",
    "title": "Kaggle | 1st practice",
    "section": "3. 데이터 분석",
    "text": "3. 데이터 분석\n\nA.test\n\n- 제출 결과는 리더보드에서 확인 가능\n- 답을 알 수 없고 제출해야 스코어만 확인할 수 있음"
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html#b.-train---스스로-풀어보고-채점할-수-있음",
    "href": "2023_MP/Titanic/A0. practice.html#b.-train---스스로-풀어보고-채점할-수-있음",
    "title": "Kaggle | 1st practice",
    "section": "### B. train - 스스로 풀어보고 채점할 수 있음",
    "text": "### B. train - 스스로 풀어보고 채점할 수 있음\n- train 데이터를 채점해보자.\n# accuracy의 계산\n\ndf = pd.DataFrame({'surv' : [1,0,1,1,0], 'sex' : ['f','m','f','m','m']})\n\n- surv+열과 sex열에서 sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\ndf.surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: surv, dtype: int64\n\n\n\ndf.sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: sex, dtype: object\n\n\n\n(df.sex == 'f')*1  ## bool이 원소인 list에 1을 곱해준다. f이면 1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: sex, dtype: int32\n\n\n- 결과를 정리하면 아래와 같다.\n\npd.DataFrame({'real' : df.surv, 'estimate' : (df.sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n\nprint((df.surv == (df.sex == 'f')*1).sum()/5)\n##print((df.surv == (df.sex == 'f')*1).mean()) ## 동일한 코드\n\n0.8\n\n\n- 실제 train 자료에 접목해서 여성만 생존한다고 하여 accuracy를 구해보자.\n\n(tr.Survived == (tr.Sex == 'female')*1).mean()\n\n0.7867564534231201\n\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()  ## True or False는 0, 1로도 구분되나보다.\n\n0.7867564534231201\n\n\n- 그러면 예측한 데이터프레임을 파일로 만들어서 보내보자.\n\ntst[['PassengerId']].assign(Survived = (tst.Sex == 'female')*1)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n\ntst[['PassengerId']].assign(Survived = (tst.Sex == 'female')*1).to_csv(\"gender_submission.csv\", index = False)\n\n\n해당 파일을 캐글에 업로드하면 submission이 완료된다.\n\n\nindex를 날려줘야 원하는 형식이 된다. (index = False를 하지 않으면 csv파일에 index의 숫자가 같이 저장된다…)"
  },
  {
    "objectID": "2023_MP/Titanic/A0. practice.html#개념",
    "href": "2023_MP/Titanic/A0. practice.html#개념",
    "title": "Kaggle | 1st practice",
    "section": "4. 개념",
    "text": "4. 개념\n- 캐글 대회는 시험과 비슷하다. * 캐글대회를 여는 사람은 보통 (1) 모의고사문제+답 (training set) (2) 실제시험문제 (test set)를 준다. * (1)의 자료에서는 문제(X,독립변수,설명변수)와 답(y,종속변수,반응변수)이 함께 주어진다. * (2)의 자료에서는 문제(X,독립변수,설명변수)만 주어진다. * 우리는 (1)을 이용하여 문제(X,독립변수,설명변수)와 답(y,종속변수,반응변수)사이의 관계를 찾아내는 훈련을 한다. * 그리고 그 훈련이 잘 되었는지를 평가하기 위해서 (2)를 풀어보고 그 결과를 제출한다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nR과 Python, 그 외 공부하는 데 필요했던 자료들이나 대회, 공모전, 연습했던 것들을 올립니다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "",
    "text": "주어진 자료에서 입학년도를 추가하고 싶다면 어떻게 해야 할까?"
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html#사전작업",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html#사전작업",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 설치\n\n\nimport numpy as np\nimport pandas as pd\n\n\n자료 받아오기 및 확인\n\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nstudent_id = [ '2023-12362', '2022-12471', '2023-12333', '2022-12400', '2022-12377',\n               '2022-12469', '2023-12314', '2022-12363', '2023-12445', '2023-12336',\n               '2023-12426', '2022-12380', '2023-12422', '2022-12488', '2022-12370',\n               '2023-12443', '2022-12463', '2023-12491', '2023-12340', '2022-12312' ]\ndf = pd.DataFrame({'student_id':student_id,'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf.head()\n\n\n\n\n\n\n\n\nstudent_id\natt\nrep\nmid\nfin\n\n\n\n\n0\n2023-12362\n65\n55\n50\n40\n\n\n1\n2022-12471\n95\n100\n50\n80\n\n\n2\n2023-12333\n65\n90\n60\n30\n\n\n3\n2022-12400\n55\n80\n75\n80\n\n\n4\n2022-12377\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n학번(student_id)에서 앞 네자리에 해당하는 숫자를 빼내어 새로운 열로 저장하면 좋을 것 같다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html#가공",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html#가공",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "2. 가공",
    "text": "2. 가공\n\n아래의 코드는 student_id 열을 '-'를 기준으로 앞뒤로 나누고 첫번째 것을 취한다. 숫자형으로 바꾼 뒤, 리스트로 산출한다.\n\n\n[int(i.split('-')[0]) for i in df.student_id]\n\n[2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2022]\n\n\n\nlambda를 이용해 가공할 수도 있다.\n\n\nlist(map((lambda x : int(x.split('-')[0])), df.student_id))\n\n[2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2023,\n 2022,\n 2023,\n 2022,\n 2022,\n 2023,\n 2022,\n 2023,\n 2023,\n 2022]\n\n\n\n첫번째 코드와 똑같은 결과를 산출한다."
  },
  {
    "objectID": "2023_DV/Solution Assemble/특정 열의 재가공.html#출력",
    "href": "2023_DV/Solution Assemble/특정 열의 재가공.html#출력",
    "title": "[문제 풀이] 데이터프레임 : 특정 열의 재가공",
    "section": "3. 출력",
    "text": "3. 출력\n\n상기의 코드를 df에 새로운 열 year에 삽입한다.\n\n\ndf.assign(year = [int(i.split('-')[0]) for i in df.student_id])\n\n\n\n\n\n\n\n\nstudent_id\natt\nrep\nmid\nfin\nyear\n\n\n\n\n0\n2023-12362\n65\n55\n50\n40\n2023\n\n\n1\n2022-12471\n95\n100\n50\n80\n2022\n\n\n2\n2023-12333\n65\n90\n60\n30\n2023\n\n\n3\n2022-12400\n55\n80\n75\n80\n2022\n\n\n4\n2022-12377\n80\n30\n30\n100\n2022\n\n\n5\n2022-12469\n75\n40\n100\n15\n2022\n\n\n6\n2023-12314\n65\n45\n45\n90\n2023\n\n\n7\n2022-12363\n60\n60\n25\n0\n2022\n\n\n8\n2023-12445\n95\n65\n20\n10\n2023\n\n\n9\n2023-12336\n90\n80\n80\n20\n2023\n\n\n10\n2023-12426\n55\n75\n35\n25\n2023\n\n\n11\n2022-12380\n95\n95\n45\n0\n2022\n\n\n12\n2023-12422\n95\n55\n15\n35\n2023\n\n\n13\n2022-12488\n50\n80\n40\n30\n2022\n\n\n14\n2022-12370\n50\n55\n15\n85\n2022\n\n\n15\n2023-12443\n95\n30\n30\n95\n2023\n\n\n16\n2022-12463\n50\n50\n45\n10\n2022\n\n\n17\n2023-12491\n65\n55\n15\n45\n2023\n\n\n18\n2023-12340\n70\n70\n40\n35\n2023\n\n\n19\n2022-12312\n90\n90\n80\n90\n2022\n\n\n\n\n\n\n\n완료\n-감사합니다-"
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html",
    "title": "데이터 크롤링 맛보기",
    "section": "",
    "text": "야후 파이낸셜을 이용하여 출산률 자료를 시각화해보자!"
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html#라이브러리-imports",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html#라이브러리-imports",
    "title": "데이터 크롤링 맛보기",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\n\n\n교수님 의견\n\n\n현존하는 시각화 툴 중에서 가장 뛰어난 것은 excel이라고 생각이 된다.\nbut, pandas를 더 잘 다루는 사람이 처리속도가 더 빠르다고 생각이 됨. 높은 숙련도를 가진다면 엑셀 사용자보다 더 높은 퍼포먼스를 보여줄 수 있을 것!"
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html#yfinance를-이용한-주식-자료-시각화",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html#yfinance를-이용한-주식-자료-시각화",
    "title": "데이터 크롤링 맛보기",
    "section": "2. yfinance를 이용한 주식 자료 시각화",
    "text": "2. yfinance를 이용한 주식 자료 시각화\n\nA. 크롤링 + 데이터 정리\n\n야후 파이낸셜에서 원하는 종목을 검색해서 그 코드를 가져와보았다…\nApple : AAPL\n삼성전자 : 005930.KS\n- 관심있는 데이터를 크롤링하는 코드\n\nsymbols = ['AMZN','AAPL','GOOG','MSFT','NFLX','NVDA','TSLA']    ## 관심있는 주식들\nstart = '2020-01-01'    ## 수집 시점\nend = '2023-11-06'    ## 수집 종점\ndf = yf.download(symbols,start,end)\n\n[*********************100%%**********************]  7 of 7 completed\n\n\n\ndf\n\n\n\n\n\n\n\n\nAdj Close\nClose\n...\nOpen\nVolume\n\n\n\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\nAAPL\nAMZN\nGOOG\n...\nNFLX\nNVDA\nTSLA\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.152664\n94.900497\n68.368500\n154.779510\n329.809998\n59.749290\n28.684000\n75.087502\n94.900497\n68.368500\n...\n326.100006\n59.687500\n28.299999\n135480400\n80580000\n28132000\n22622100\n4485800\n23753600\n142981500\n\n\n2020-01-03\n72.441475\n93.748497\n68.032997\n152.852264\n325.899994\n58.792946\n29.534000\n74.357498\n93.748497\n68.032997\n...\n326.779999\n58.775002\n29.366667\n146322800\n75288000\n23728000\n21116200\n3806900\n20538400\n266677500\n\n\n2020-01-06\n73.018684\n95.143997\n69.710503\n153.247330\n335.829987\n59.039501\n30.102667\n74.949997\n95.143997\n69.710503\n...\n323.119995\n58.080002\n29.364668\n118387200\n81236000\n34646000\n20813700\n5663100\n26263600\n151995000\n\n\n2020-01-07\n72.675278\n95.343002\n69.667000\n151.850067\n330.750000\n59.754280\n31.270666\n74.597504\n95.343002\n69.667000\n...\n336.470001\n59.549999\n30.760000\n108872000\n80898000\n30054000\n21634100\n4703200\n31485600\n268231500\n\n\n2020-01-08\n73.844345\n94.598503\n70.216003\n154.268814\n339.260010\n59.866348\n32.809334\n75.797501\n94.598503\n70.216003\n...\n331.489990\n59.939999\n31.580000\n132079200\n70160000\n30560000\n27746500\n7104500\n27710800\n467164500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.065933\n132.710007\n125.750000\n336.626770\n410.079987\n411.609985\n197.360001\n170.289993\n132.710007\n125.750000\n...\n402.350006\n410.869995\n209.279999\n51131000\n72485500\n24165600\n22828100\n5317100\n38802800\n136448200\n\n\n2023-10-31\n170.545319\n133.089996\n125.300003\n337.425140\n411.690002\n407.799988\n200.839996\n170.770004\n133.089996\n125.300003\n...\n409.239990\n404.500000\n196.119995\n44846000\n51589400\n21123400\n20265300\n3877600\n51796900\n118068300\n\n\n2023-11-01\n173.741104\n137.000000\n127.570000\n345.369019\n420.190002\n423.250000\n205.660004\n173.970001\n137.000000\n127.570000\n...\n414.769989\n408.839996\n204.039993\n56934900\n61529400\n26536600\n28158800\n4806100\n43759300\n121661700\n\n\n2023-11-02\n177.336380\n138.070007\n128.580002\n347.614471\n424.709991\n435.059998\n218.509995\n177.570007\n138.070007\n128.580002\n...\n421.170013\n433.279999\n212.970001\n77334800\n52236700\n24091700\n24348100\n4476000\n40917200\n125987600\n\n\n2023-11-03\n176.417572\n138.600006\n130.369995\n352.085388\n432.359985\n450.049988\n219.960007\n176.649994\n138.600006\n130.369995\n...\n428.760010\n440.200012\n221.149994\n79763700\n44007200\n19517900\n23624000\n3664800\n42385500\n119281000\n\n\n\n\n968 rows × 42 columns\n\n\n\n\nwide data로 들어가 있어서 작업이 필요한 모습이다.\n\n\ndf.stack()\n\n\n  \n    \n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\nAAPL\n73.249031\n75.087502\n75.150002\n73.797501\n74.059998\n135480400\n\n\nAMZN\n94.900497\n94.900497\n94.900497\n93.207497\n93.750000\n80580000\n\n\nGOOG\n68.368500\n68.368500\n68.406998\n67.077499\n67.077499\n28132000\n\n\nMSFT\n155.093689\n160.619995\n160.729996\n158.330002\n158.779999\n22622100\n\n\nNFLX\n329.809998\n329.809998\n329.980011\n324.779999\n326.100006\n4485800\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-11-03\nGOOG\n130.369995\n130.369995\n130.729996\n129.009995\n129.089996\n19517900\n\n\nMSFT\n352.799988\n352.799988\n354.390015\n347.329987\n349.630005\n23624000\n\n\nNFLX\n432.359985\n432.359985\n434.820007\n425.529999\n428.760010\n3664800\n\n\nNVDA\n450.049988\n450.049988\n453.089996\n437.230011\n440.200012\n42385500\n\n\nTSLA\n219.960007\n219.960007\n226.369995\n218.399994\n221.149994\n119281000\n\n\n\n\n\n6776 rows × 6 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 이 중 조정된 종가(Adj Close)만을 알아보고 싶다면?\n\ndf.stack().loc[:, 'Adj Close'].reset_index()\n## df.stack()['Adj Close'].reset_index()\n\n\n  \n    \n\n\n\n\n\n\nDate\nlevel_1\nAdj Close\n\n\n\n\n0\n2020-01-02\nAAPL\n73.249031\n\n\n1\n2020-01-02\nAMZN\n94.900497\n\n\n2\n2020-01-02\nGOOG\n68.368500\n\n\n3\n2020-01-02\nMSFT\n155.093689\n\n\n4\n2020-01-02\nNFLX\n329.809998\n\n\n...\n...\n...\n...\n\n\n6771\n2023-11-03\nGOOG\n130.369995\n\n\n6772\n2023-11-03\nMSFT\n352.799988\n\n\n6773\n2023-11-03\nNFLX\n432.359985\n\n\n6774\n2023-11-03\nNVDA\n450.049988\n\n\n6775\n2023-11-03\nTSLA\n219.960007\n\n\n\n\n\n6776 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 다른 방법(멀티 인덱스 이용)\n\ndf.columns\n\nMultiIndex([('Adj Close', 'AAPL'),\n            ('Adj Close', 'AMZN'),\n            ('Adj Close', 'GOOG'),\n            ('Adj Close', 'MSFT'),\n            ('Adj Close', 'NFLX'),\n            ('Adj Close', 'NVDA'),\n            ('Adj Close', 'TSLA'),\n            (    'Close', 'AAPL'),\n            (    'Close', 'AMZN'),\n            (    'Close', 'GOOG'),\n            (    'Close', 'MSFT'),\n            (    'Close', 'NFLX'),\n            (    'Close', 'NVDA'),\n            (    'Close', 'TSLA'),\n            (     'High', 'AAPL'),\n            (     'High', 'AMZN'),\n            (     'High', 'GOOG'),\n            (     'High', 'MSFT'),\n            (     'High', 'NFLX'),\n            (     'High', 'NVDA'),\n            (     'High', 'TSLA'),\n            (      'Low', 'AAPL'),\n            (      'Low', 'AMZN'),\n            (      'Low', 'GOOG'),\n            (      'Low', 'MSFT'),\n            (      'Low', 'NFLX'),\n            (      'Low', 'NVDA'),\n            (      'Low', 'TSLA'),\n            (     'Open', 'AAPL'),\n            (     'Open', 'AMZN'),\n            (     'Open', 'GOOG'),\n            (     'Open', 'MSFT'),\n            (     'Open', 'NFLX'),\n            (     'Open', 'NVDA'),\n            (     'Open', 'TSLA'),\n            (   'Volume', 'AAPL'),\n            (   'Volume', 'AMZN'),\n            (   'Volume', 'GOOG'),\n            (   'Volume', 'MSFT'),\n            (   'Volume', 'NFLX'),\n            (   'Volume', 'NVDA'),\n            (   'Volume', 'TSLA')],\n           )\n\n\n\n## df.loc[:, ('Adj Close', 'AAPL'):('Adj Close', 'TSLA')]\ndf.loc[:, 'Adj Close']\n\n\n  \n    \n\n\n\n\n\n\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.249031\n94.900497\n68.368500\n155.093689\n329.809998\n59.749298\n28.684000\n\n\n2020-01-03\n72.536888\n93.748497\n68.032997\n153.162460\n325.899994\n58.792942\n29.534000\n\n\n2020-01-06\n73.114883\n95.143997\n69.710503\n153.558350\n335.829987\n59.039497\n30.102667\n\n\n2020-01-07\n72.771027\n95.343002\n69.667000\n152.158279\n330.750000\n59.754265\n31.270666\n\n\n2020-01-08\n73.941635\n94.598503\n70.216003\n154.581909\n339.260010\n59.866344\n32.809334\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.289993\n132.710007\n125.750000\n337.309998\n410.079987\n411.609985\n197.360001\n\n\n2023-10-31\n170.770004\n133.089996\n125.300003\n338.109985\n411.690002\n407.799988\n200.839996\n\n\n2023-11-01\n173.970001\n137.000000\n127.570000\n346.070007\n420.190002\n423.250000\n205.660004\n\n\n2023-11-02\n177.570007\n138.070007\n128.580002\n348.320007\n424.709991\n435.059998\n218.509995\n\n\n2023-11-03\n176.649994\n138.600006\n130.369995\n352.799988\n432.359985\n450.049988\n219.960007\n\n\n\n\n\n968 rows × 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n멀티 인덱스 중 첫 번째 인덱스를 지정하여 뽑아낼 수 있다.\n\n- 애플(AAPL)만 뽑고 싶을 때\n\ndf.loc[:, 'AAPL']  ## 이건 안된다.\n\nKeyError: ignored\n\n\n\n하지만 뒤에껀 인식하지 못하는 모습.\n\n- 굳이 위와 같은 방법으로 쓰고 싶다면…(df.swaplevel())\n\ndf.stack().stack().swaplevel(i=1, j=2)    ## 인덱스의 순서를 바꿀 수 있음.\n\nDate                       \n2020-01-02  Adj Close  AAPL    7.315266e+01\n            Close      AAPL    7.508750e+01\n            High       AAPL    7.515000e+01\n            Low        AAPL    7.379750e+01\n            Open       AAPL    7.406000e+01\n                                   ...     \n2023-11-03  Close      TSLA    2.199600e+02\n            High       TSLA    2.263700e+02\n            Low        TSLA    2.184000e+02\n            Open       TSLA    2.211500e+02\n            Volume     TSLA    1.192810e+08\nLength: 40656, dtype: float64\n\n\n\ndf.stack().stack().swaplevel(i=1, j=2).unstack().unstack()\n\n\n  \n    \n\n\n\n\n\n\nAAPL\nAMZN\n...\nNVDA\nTSLA\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\nAdj Close\nClose\nHigh\nLow\n...\nHigh\nLow\nOpen\nVolume\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.249031\n75.087502\n75.150002\n73.797501\n74.059998\n135480400.0\n94.900497\n94.900497\n94.900497\n93.207497\n...\n59.977501\n59.180000\n59.687500\n23753600.0\n28.684000\n28.684000\n28.713333\n28.114000\n28.299999\n142981500.0\n\n\n2020-01-03\n72.536888\n74.357498\n75.144997\n74.125000\n74.287498\n146322800.0\n93.748497\n93.748497\n94.309998\n93.224998\n...\n59.457500\n58.525002\n58.775002\n20538400.0\n29.534000\n29.534000\n30.266666\n29.128000\n29.366667\n266677500.0\n\n\n2020-01-06\n73.114883\n74.949997\n74.989998\n73.187500\n73.447502\n118387200.0\n95.143997\n95.143997\n95.184502\n93.000000\n...\n59.317501\n57.817501\n58.080002\n26263600.0\n30.102667\n30.102667\n30.104000\n29.333332\n29.364668\n151995000.0\n\n\n2020-01-07\n72.771027\n74.597504\n75.224998\n74.370003\n74.959999\n108872000.0\n95.343002\n95.343002\n95.694504\n94.601997\n...\n60.442501\n59.097500\n59.549999\n31485600.0\n31.270666\n31.270666\n31.441999\n30.224001\n30.760000\n268231500.0\n\n\n2020-01-08\n73.941635\n75.797501\n76.110001\n74.290001\n74.290001\n132079200.0\n94.598503\n94.598503\n95.550003\n94.321999\n...\n60.509998\n59.537498\n59.939999\n27710800.0\n32.809334\n32.809334\n33.232666\n31.215334\n31.580000\n467164500.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.289993\n170.289993\n171.169998\n168.869995\n169.020004\n51131000.0\n132.710007\n132.710007\n133.000000\n128.559998\n...\n417.660004\n404.809998\n410.869995\n38802800.0\n197.360001\n197.360001\n210.880005\n194.669998\n209.279999\n136448200.0\n\n\n2023-10-31\n170.770004\n170.770004\n170.899994\n167.899994\n169.350006\n44846000.0\n133.089996\n133.089996\n133.570007\n131.710007\n...\n408.790009\n392.299988\n404.500000\n51796900.0\n200.839996\n200.839996\n202.800003\n194.070007\n196.119995\n118068300.0\n\n\n2023-11-01\n173.970001\n173.970001\n174.229996\n170.119995\n171.000000\n56934900.0\n137.000000\n137.000000\n137.350006\n133.710007\n...\n423.809998\n408.690002\n408.839996\n43759300.0\n205.660004\n205.660004\n205.990005\n197.850006\n204.039993\n121661700.0\n\n\n2023-11-02\n177.570007\n177.570007\n177.779999\n175.460007\n175.520004\n77334800.0\n138.070007\n138.070007\n138.809998\n136.470001\n...\n438.839996\n428.940002\n433.279999\n40917200.0\n218.509995\n218.509995\n219.199997\n211.449997\n212.970001\n125987600.0\n\n\n2023-11-03\n176.649994\n176.649994\n176.820007\n173.350006\n174.240005\n79763700.0\n138.600006\n138.600006\n139.490005\n137.449997\n...\n453.089996\n437.230011\n440.200012\n42385500.0\n219.960007\n219.960007\n226.369995\n218.399994\n221.149994\n119281000.0\n\n\n\n\n\n968 rows × 42 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n이러면 상호가 멀티인덱스의 앞에 생기게 되어 잘 된다.\n\n\ndf.swaplevel(i=0, j=1, axis = 1)\n\n\n  \n    \n\n\n\n\n\n\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\nAAPL\nAMZN\nGOOG\n...\nNFLX\nNVDA\nTSLA\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\n\n\n\nAdj Close\nAdj Close\nAdj Close\nAdj Close\nAdj Close\nAdj Close\nAdj Close\nClose\nClose\nClose\n...\nOpen\nOpen\nOpen\nVolume\nVolume\nVolume\nVolume\nVolume\nVolume\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.249031\n94.900497\n68.368500\n155.093689\n329.809998\n59.749298\n28.684000\n75.087502\n94.900497\n68.368500\n...\n326.100006\n59.687500\n28.299999\n135480400\n80580000\n28132000\n22622100\n4485800\n23753600\n142981500\n\n\n2020-01-03\n72.536888\n93.748497\n68.032997\n153.162460\n325.899994\n58.792942\n29.534000\n74.357498\n93.748497\n68.032997\n...\n326.779999\n58.775002\n29.366667\n146322800\n75288000\n23728000\n21116200\n3806900\n20538400\n266677500\n\n\n2020-01-06\n73.114883\n95.143997\n69.710503\n153.558350\n335.829987\n59.039497\n30.102667\n74.949997\n95.143997\n69.710503\n...\n323.119995\n58.080002\n29.364668\n118387200\n81236000\n34646000\n20813700\n5663100\n26263600\n151995000\n\n\n2020-01-07\n72.771027\n95.343002\n69.667000\n152.158279\n330.750000\n59.754265\n31.270666\n74.597504\n95.343002\n69.667000\n...\n336.470001\n59.549999\n30.760000\n108872000\n80898000\n30054000\n21634100\n4703200\n31485600\n268231500\n\n\n2020-01-08\n73.941635\n94.598503\n70.216003\n154.581909\n339.260010\n59.866344\n32.809334\n75.797501\n94.598503\n70.216003\n...\n331.489990\n59.939999\n31.580000\n132079200\n70160000\n30560000\n27746500\n7104500\n27710800\n467164500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.289993\n132.710007\n125.750000\n337.309998\n410.079987\n411.609985\n197.360001\n170.289993\n132.710007\n125.750000\n...\n402.350006\n410.869995\n209.279999\n51131000\n72485500\n24165600\n22828100\n5317100\n38802800\n136448200\n\n\n2023-10-31\n170.770004\n133.089996\n125.300003\n338.109985\n411.690002\n407.799988\n200.839996\n170.770004\n133.089996\n125.300003\n...\n409.239990\n404.500000\n196.119995\n44846000\n51589400\n21123400\n20265300\n3877600\n51796900\n118068300\n\n\n2023-11-01\n173.970001\n137.000000\n127.570000\n346.070007\n420.190002\n423.250000\n205.660004\n173.970001\n137.000000\n127.570000\n...\n414.769989\n408.839996\n204.039993\n56934900\n61529400\n26536600\n28158800\n4806100\n43759300\n121661700\n\n\n2023-11-02\n177.570007\n138.070007\n128.580002\n348.320007\n424.709991\n435.059998\n218.509995\n177.570007\n138.070007\n128.580002\n...\n421.170013\n433.279999\n212.970001\n77334800\n52236700\n24091700\n24348100\n4476000\n40917200\n125987600\n\n\n2023-11-03\n176.649994\n138.600006\n130.369995\n352.799988\n432.359985\n450.049988\n219.960007\n176.649994\n138.600006\n130.369995\n...\n428.760010\n440.200012\n221.149994\n79763700\n44007200\n19517900\n23624000\n3664800\n42385500\n119281000\n\n\n\n\n\n968 rows × 42 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n이렇게 바꿔도 됨(정리는 안 되어있는데, 일단 되긴 함.)\n\n- 제일 편한 거(타이디데이터로 만들고 정리)\n\ndf.stack().reset_index().rename({'level_1' : 'Subject'}, axis =1)  ## 타이디데이터로 만듦\n##.loc[lambda _df : _df.Subject == 'AAPL']\n\n\n  \n    \n\n\n\n\n\n\nDate\nSubject\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\n\n0\n2020-01-02\nAAPL\n73.249031\n75.087502\n75.150002\n73.797501\n74.059998\n135480400\n\n\n1\n2020-01-02\nAMZN\n94.900497\n94.900497\n94.900497\n93.207497\n93.750000\n80580000\n\n\n2\n2020-01-02\nGOOG\n68.368500\n68.368500\n68.406998\n67.077499\n67.077499\n28132000\n\n\n3\n2020-01-02\nMSFT\n155.093689\n160.619995\n160.729996\n158.330002\n158.779999\n22622100\n\n\n4\n2020-01-02\nNFLX\n329.809998\n329.809998\n329.980011\n324.779999\n326.100006\n4485800\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6771\n2023-11-03\nGOOG\n130.369995\n130.369995\n130.729996\n129.009995\n129.089996\n19517900\n\n\n6772\n2023-11-03\nMSFT\n352.799988\n352.799988\n354.390015\n347.329987\n349.630005\n23624000\n\n\n6773\n2023-11-03\nNFLX\n432.359985\n432.359985\n434.820007\n425.529999\n428.760010\n3664800\n\n\n6774\n2023-11-03\nNVDA\n450.049988\n450.049988\n453.089996\n437.230011\n440.200012\n42385500\n\n\n6775\n2023-11-03\nTSLA\n219.960007\n219.960007\n226.369995\n218.399994\n221.149994\n119281000\n\n\n\n\n\n6776 rows × 8 columns"
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html#b.-시각화",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html#b.-시각화",
    "title": "데이터 크롤링 맛보기",
    "section": "### B. 시각화",
    "text": "### B. 시각화\n- Adj Close를 기반으로 tidydata를 생성\n\ndf.loc[:,'Adj Close'].reset_index().set_index('Date').stack().reset_index().rename({'level_1' : 'Company', 0 : 'Price'}, axis = 1)\n\n\n  \n    \n\n\n\n\n\n\nDate\nCompany\nPrice\n\n\n\n\n0\n2020-01-02\nAAPL\n73.249031\n\n\n1\n2020-01-02\nAMZN\n94.900497\n\n\n2\n2020-01-02\nGOOG\n68.368500\n\n\n3\n2020-01-02\nMSFT\n155.093689\n\n\n4\n2020-01-02\nNFLX\n329.809998\n\n\n...\n...\n...\n...\n\n\n6771\n2023-11-03\nGOOG\n130.369995\n\n\n6772\n2023-11-03\nMSFT\n352.799988\n\n\n6773\n2023-11-03\nNFLX\n432.359985\n\n\n6774\n2023-11-03\nNVDA\n450.049988\n\n\n6775\n2023-11-03\nTSLA\n219.960007\n\n\n\n\n\n6776 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 시각화 : 판다스 데이터프레임 자체 함수를 이용\n\ndf.loc[:,'Adj Close'].reset_index().set_index('Date').stack().reset_index().rename({'level_1' : 'Company', 0 : 'Price'}, axis = 1)\\\n.plot.line(x = 'Date', y = 'Price', color = 'Company', backend = 'plotly')  ## plotly라는 녀석으로 그리고 싶음\n\n\n                                                \n\n\n- df.plot()은 라인 플랏을 디폴트로 한다.\n\ndf.loc[:,'Adj Close'].reset_index().set_index('Date').stack().reset_index().rename({'level_1' : 'Company', 0 : 'Price'}, axis = 1)\\\n.plot(x = 'Date', y = 'Price', color = 'Company', backend = 'plotly')  ## 얘도 똑같음, 디폴트가 line plot\n\n\n                                                \n\n\n\nplotly : 상당히 강력한 툴임, 아무튼 이미지적으로 지원하는 게 되게 많음.\n\n\n무슨 데이터이든 타이디데이터로 만들어두기만 하면 그래프로 그리는 것은 쉬운 과정이다. 그래프의 옵션같은 것들은 단순 암기나 검색, ChatGPT등을 통해 해결되는 부분이므로 데이터 가공 능력이 훨씬 중요하다."
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html#출산률-시각화",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html#출산률-시각화",
    "title": "데이터 크롤링 맛보기",
    "section": "3. 출산률 시각화",
    "text": "3. 출산률 시각화\n\nA. 크롤링 + 데이터 정리\n\n- 대한민국의 저출산 문제\nref: https://ko.wikipedia.org/wiki/대한민국의_저출산\n- 위의 url에서 3, 5번째 테이블을 읽고 싶다.\n\n3번째 테이블 : 시도별 출산율\n5번째 테이블 : 시도별 출생아 수\n\n\nurl을 긁어서 그대로 읽으면 된다.\n\n\ndf_lst = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%A0%80%EC%B6%9C%EC%82%B0')\n\n\ndf_lst\n\n[      연도    출산율\n 0   2002  1.178\n 1   2003  1.191\n 2   2004  1.164\n 3   2005  1.085\n 4   2006  1.132\n 5   2007  1.259\n 6   2008  1.192\n 7   2009  1.149\n 8   2010  1.226\n 9   2011  1.244\n 10  2012  1.297\n 11  2013  1.187\n 12  2014  1.205\n 13  2015  1.239\n 14  2016  1.172\n 15  2017  1.052\n 16  2018  0.977\n 17  2019  0.918\n 18  2020  0.837\n 19  2021  0.810,\n           지역  출생아 수(천명)  조출생률  합계출산율  인구(2021년 기준)\n 0         서울       47.4   5.0  0.642       9588711\n 1         부산       15.1   4.5  0.747       3369704\n 2         대구       11.2   4.6  0.807       2406296\n 3         대전        7.5   5.1  0.829       1457619\n 4         광주        7.3   5.1  0.811       1444787\n 5         인천       16.0   5.5  0.829       2936214\n 6        경기도       77.8   5.9  0.878      13479798\n 7       전라북도        8.2   4.5  0.909       1796331\n 8       경상남도       16.8   5.1  0.945       3329623\n 9       충청북도        8.6   5.4  0.983       1596303\n 10        울산        6.6   5.8  0.984       1128163\n 11      경상북도       12.9   4.9  1.003       2635896\n 12       제주도        4.0   6.0  1.021        674484\n 13      충청남도       11.9   5.7  1.029       2116452\n 14       강원도        7.8   5.1  1.036       1536175\n 15      전라남도        9.7   5.3  1.145       1844148\n 16        세종        3.5  10.0  1.277        361396\n 17  대한민국(전체)      272.4   5.3  0.837      51702100,\n    지역/연도[6]  2005 2006[7]  2007 2008[8] 2009[9]  2010  2011  2012  2013  2014  \\\n 0        서울  0.92    0.97  1.06    1.01    0.96  1.02  1.01  1.06  0.97  0.98   \n 1        부산  0.88    0.91  1.02    0.98    0.94  1.05  1.08  1.14  1.05  1.09   \n 2        대구  0.99    1.00  1.13    1.07    1.03  1.11  1.15  1.22  1.13  1.17   \n 3        인천  1.07    1.11  1.25    1.19    1.14  1.21  1.23  1.30  1.20  1.21   \n 4        광주  1.10    1.14  1.26    1.20    1.14  1.22  1.23  1.30  1.17  1.20   \n 5        대전  1.10    1.15  1.27    1.22    1.16  1.21  1.26  1.32  1.23  1.25   \n 6        울산  1.18    1.24  1.40    1.34    1.31  1.37  1.39  1.48  1.39  1.44   \n 7        세종     -       -     -       -       -     -     -  1.60  1.44  1.35   \n 8        경기  1.17    1.23  1.35    1.29    1.23  1.31  1.31  1.36  1.23  1.24   \n 9        강원  1.18    1.19  1.35    1.25    1.25  1.31  1.34  1.37  1.25  1.25   \n 10       충북  1.19    1.22  1.39    1.32    1.32  1.40  1.43  1.49  1.37  1.36   \n 11       충남  1.26    1.35  1.50    1.44    1.41  1.48  1.50  1.57  1.44  1.42   \n 12       전북  1.17    1.20  1.37    1.31    1.28  1.37  1.41  1.44  1.32  1.33   \n 13       전남  1.28    1.33  1.53    1.45    1.45  1.54  1.57  1.64  1.52  1.50   \n 14       경북  1.17    1.20  1.36    1.31    1.27  1.38  1.43  1.49  1.38  1.41   \n 15       경남  1.18    1.25  1.43    1.37    1.32  1.41  1.45  1.50  1.37  1.41   \n 16       제주  1.30    1.36  1.48    1.39    1.38  1.46  1.49  1.60  1.43  1.48   \n 17       전국  1.08    1.13  1.25    1.19    1.15  1.23  1.24  1.30  1.19  1.21   \n \n     2015  2016  2017  2018  2019  2020  2021  \n 0   1.00  0.94  0.84  0.76  0.72  0.64  0.63  \n 1   1.14  1.10  0.98  0.90  0.83  0.75  0.73  \n 2   1.22  1.19  1.07  0.99  0.93  0.81  0.78  \n 3   1.22  1.14  1.01  1.01  0.94  0.83  0.78  \n 4   1.21  1.17  1.05  0.97  0.91  0.81  0.90  \n 5   1.28  1.19  1.08  0.95  0.88  0.81  0.81  \n 6   1.49  1.42  1.26  1.13  1.08  0.99  0.94  \n 7   1.89  1.82  1.67  1.57  1.47  1.28  1.28  \n 8   1.27  1.19  1.07  1.00  0.94  0.88  0.85  \n 9   1.31  1.24  1.12  1.07  1.08  1.04  0.98  \n 10  1.41  1.36  1.24  1.17  1.05  0.98  0.95  \n 11  1.48  1.40  1.28  1.19  1.11  1.03  0.96  \n 12  1.35  1.25  1.15  1.04  0.97  0.91  0.85  \n 13  1.55  1.47  1.33  1.24  1.23  1.15  1.02  \n 14  1.46  1.40  1.26  1.17  1.09  1.00  0.97  \n 15  1.44  1.36  1.23  1.12  1.05  0.95  0.90  \n 16  1.48  1.43  1.31  1.22  1.15  1.02  0.95  \n 17  1.24  1.17  1.05  0.98  0.92  0.84  0.81  ,\n    지역/연도[6]  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019  2020\n 0        서울   9.2   9.0   9.3   8.4   8.4   8.4   7.7   6.7   6.0   5.6   5.0\n 1        부산   7.8   7.9   8.2   7.4   7.5   7.7   7.2   6.2   5.6   5.0   4.5\n 2        대구   8.3   8.3   8.6   7.8   7.8   7.9   7.4   6.6   5.9   5.4   4.6\n 3        인천   9.5   9.5   9.9   9.0   9.0   8.8   8.1   7.0   6.9   6.3   5.5\n 4        광주   9.7   9.6   9.9   8.7   8.7   8.5   7.9   6.9   6.3   5.8   5.1\n 5        대전   9.6   9.9  10.1   9.3   9.2   9.1   8.3   7.2   6.3   5.7   5.1\n 6        울산  10.2  10.3  10.7   9.9  10.0  10.1   9.4   8.1   7.1   6.6   5.8\n 7        세종     -     -  10.2   9.5   9.7  14.8  14.6  13.4  12.5  11.7  10.0\n 8        경기  10.5  10.4  10.5   9.3   9.2   9.2   8.4   7.4   6.9   6.4   5.9\n 9        강원   8.2   8.2   8.2   7.2   7.0   7.1   6.5   5.8   5.4   5.4   5.1\n 10       충북   9.6   9.6   9.8   8.8   8.5   8.6   8.1   7.2   6.7   5.9   5.4\n 11       충남   9.9   9.8  10.2   9.2   8.9   9.0   8.3   7.5   6.8   6.3   5.7\n 12       전북   8.7   8.7   8.7   7.8   7.7   7.6   6.8   6.1   5.5   4.9   4.5\n 13       전남   8.7   8.7   9.0   8.1   7.8   8.2   7.4   6.5   6.0   5.8   5.3\n 14       경북   8.9   9.1   9.2   8.3   8.2   8.3   7.7   6.7   6.0   5.5   4.9\n 15       경남   9.9   9.9  10.1   8.9   9.0   8.9   8.1   7.1   6.3   5.7   5.1\n 16       제주  10.0   9.9  10.4   9.1   9.3   9.2   8.7   7.8   7.3   6.8   6.0\n 17       전국   9.4   9.4   9.6   8.6   8.6   8.6   7.9   7.0   6.4   5.9   5.3,\n    지역/연도[6]    2010    2011        2012        2013        2014    2015  \\\n 0        서울   93266   91526   93914.000   84066.000   83711.000   83005   \n 1        부산   27415   27759   28673.000   25831.000   26190.000   26645   \n 2        대구   20557   20758   21472.000   19340.000   19361.000   19438   \n 3        인천   25752   20758   21472.000   25560.000   25786.000   25491   \n 4        광주   13979   13916   14392.000   12729.000   12729.000   12441   \n 5        대전   14314   14808   15279.000   14099.000   13962.000   13774   \n 6        울산   11432   11542   12160.000   11330.000   11556.000   11732   \n 7        세종       -       -    1054.000    1111.000    1344.000    2708   \n 8        경기  121753  122027  124746.000  112129.000     112.169  113495   \n 9        강원   12477   12408   12426.000   10980.000   10662.000   10929   \n 10       충북   14670   14804   15139.000   13658.000   13366.000   13563   \n 11       충남  20.242  20.398      20.448      18.628   18200.000   18604   \n 12       전북   16100   16175   16238.000   14555.000   14231.000   14087   \n 13       전남   16654   16612   16990.000   15401.000   14817.000   15061   \n 14       경북   23700   24250   24635.000   22206.000   22062.000   22310   \n 15       경남   32203   32536   33211.000   29504.000   29763.000   29537   \n 16       제주    5657    5628    5992.000    5328.000    5526.000    5600   \n 17       전국  470171  471265  484550.000  436455.000  435435.000  438420   \n \n           2016    2017    2018        2019    2020    2021  \n 0       75.536   65389   58074      53.673   47400   45531  \n 1    24906.000   21480   19152   17049.000   15100   14446  \n 2    18298.000   15946   14400   13233.000   11200   10661  \n 3    23609.000   20445   20087   18522.000   16000   14947  \n 4    11580.000   10120    9105    8364.000    7300    7956  \n 5    12436.000   10851    9337    8410.000    7500    7414  \n 6    10910.000    9381    8149    7539.000    6600    6127  \n 7     3297.000    3504    3703    3819.000    3500    3570  \n 8   105643.000   94088   83198      83.198   77800   76139  \n 9    10058.000    9958    8351    8283.000    7800    7357  \n 10   12742.000   11394   10586    9333.000    8600    8190  \n 11   17302.000   15670   14380   13228.000   11900   10984  \n 12   12698.000   11348   10001    8971.000    8200    7745  \n 13   13980.000   12354   11238   10832.000    9700    8430  \n 14   20616.000   17957   16079   14472.000   12900   12045  \n 15   27138.000   23849   21224   19250.000   16800   15562  \n 16    5494.000    5037    4781    4500.000    4000    3728  \n 17  406243.000  357771  326822  302676.000  272400  260562  ,\n             0        1        2        3        4\n 0          연도     2042     2050     2060     2067\n 1   0~14세 구성비     9.7%     9.0%     8.2%     8.2%\n 2  15~64세 구성비    55.1%    51.1%    48.0%    45.7%\n 3  65세 이상 구성비    35.2%    39.9%    43.8%    46.1%\n 4         총인구  4909만 명  4614만 명  4096만 명  3727만 명,\n             0        1        2        3        4\n 0          연도     2042     2050     2060     2067\n 1   0~14세 구성비     8.6%     7.9%     6.9%     6.5%\n 2  15~64세 구성비    55.1%    50.4%    46.5%    43.5%\n 3  65세 이상 구성비    36.3%    41.6%    46.6%    50.0%\n 4         총인구  4994만 명  4723만 명  4204만 명  3812만 명,\n         0                            1                    2  \\\n 0    인구유형                           정의                   범위   \n 1     총인구  일정 시점에 일정한 지역내에 살고 있는 모든 인구  국내 상주 내국인 국내 상주 외국인   \n 2  내국인 인구       90일 이상 국내거주 대한민국 국적 인구            국내 상주 내국인   \n \n                            3  \n 0                       관련통계  \n 1        주민등록인구 총조사인구 장래추계인구  \n 2  국회예산정책처(NABO) 내국인 인구 시범추계  ,\n                   0          1        2        3        4        5\n 0               NaN        NaN     2017     2020     2030     2040\n 1      통계청 내국인 인구전망      합계출산율    1.05명    0.90명    1.15명    1.27명\n 2      통계청 내국인 인구전망     내국인 인구  4994만 명  5005만 명  4980만 명  4857만 명\n 3      통계청 내국인 인구전망  65세 이상 비율    14.0%    16.1%    25.5%    34.3%\n 4  NABO 내국인 인구 시범추계      합계출산율    1.09명    0.87명    0.73명    0.73명\n 5  NABO 내국인 인구 시범추계     내국인 인구  4988만 명  5001만 명  4921만 명  4716만 명\n 6  NABO 내국인 인구 시범추계  65세 이상 비율    14.1%    15.9%    26.2%    36.9%,\n          0          1        2        3        4\n 0      NaN        NaN     2020     2030     2040\n 1   시나리오 1      합계출산율    0.87명    0.73명    0.73명\n 2   시나리오 1     내국인 인구  5001만 명  4921만 명  4716만 명\n 3   시나리오 1  65세 이상 비율    15.9%    26.2%    36.9%\n 4   시나리오 2      합계출산율    0.91명    0.72명    0.53명\n 5   시나리오 2     내국인 인구  5001만 명  4939만 명  4718만 명\n 6   시나리오 2  65세 이상 비율    15.9%    26.1%    36.9%\n 7   시나리오 3      합계출산율    0.87명    0.87명    0.87명\n 8   시나리오 3     내국인 인구  5001만 명  4945만 명  4764만 명\n 9   시나리오 3  65세 이상 비율    15.9%    26.1%    36.5%\n 10  시나리오 4      합계출산율    0.91명    1.15명    1.70명\n 11  시나리오 4     내국인 인구  5001만 명  4958만 명  4900만 명\n 12  시나리오 4  65세 이상 비율    15.9%    26.0%    35.5%,\n       0                                       1         2         3\n 0  시나리오                                    출산가정    기대수명가정    국제이동가정\n 1    S1                 출산율 2040년 1.80명 도달 후 지속  통계청 중위추계  통계청 중위추계\n 2    S2                 출산율 2040년 1.45명 도달 후 지속  통계청 중위추계  통계청 중위추계\n 3    S3  출산율 2040년 1.27명 도달 후 지속(통계청 중위 추계와 동일)  통계청 중위추계  통계청 중위추계\n 4    S4                 출산율 2021년 0.86명 도달 후 지속  통계청 중위추계  통계청 중위추계\n 5    S5                출산율 2061년 0.73명으로 감소세 지속  통계청 중위추계  통계청 중위추계\n 6  S3_1  출산율 2040년 1.27명 도달 후 지속(통계청 중위 추계와 동일)  통계청 고위추계  통계청 중위추계\n 7  S3_2  출산율 2040년 1.27명 도달 후 지속(통계청 중위 추계와 동일)  통계청 중위추계  통계청 고위추계,\n       0        1        2        3        4        5\n 0   총인구     2017     2020     2040     2050     2065\n 1    S1  5136만 명  5190만 명  5270만 명  5068만 명  4537만 명\n 2    S2  5136만 명  5186만 명  5175만 명  4898만 명  4233만 명\n 3    S3  5136만 명  5178만 명  5086만 명  4774만 명  4028만 명\n 4    S4  5136만 명  5178만 명  4946만 명  4547만 명  3673만 명\n 5    S5  5136만 명  5178만 명  4907만 명  4485만 명  3578만 명\n 6  S3_1  5136만 명  5180만 명  5133만 명  4847만 명  4113만 명\n 7  S3_2  5136만 명  5187만 명  5197만 명  4945만 명  4299만 명,\n       0       1       2       3       4       5\n 0   유소년    2017    2020    2040    2050    2065\n 1    S1  672만 명  642만 명  627만 명  584만 명  540만 명\n 2    S2  672만 명  640만 명  543만 명  474만 명  404만 명\n 3    S3  672만 명  630만 명  498만 명  424만 명  322만 명\n 4    S4  672만 명  630만 명  367만 명  287만 명  192만 명\n 5    S5  672만 명  630만 명  332만 명  252만 명  158만 명\n 6  S3_1  672만 명  630만 명  499만 명  425만 명  322만 명\n 7  S3_2  672만 명  630만 명  518만 명  452만 명  359만 명,\n       0        1        2        3        4        5\n 0  생산연령     2017     2020     2040     2050     2065\n 1    S1  3757만 명  3736만 명  2918만 명  2581만 명  2137만 명\n 2    S2  3757만 명  3736만 명  2907만 명  2522만 명  1970만 명\n 3    S3  3757만 명  3736만 명  2863만 명  2446만 명  1846만 명\n 4    S4  3757만 명  3736만 명  2854만 명  2357만 명  1621만 명\n 5    S5  3757만 명  3736만 명  2850만 명  2330만 명  1560만 명\n 6  S3_1  3757만 명  3736만 명  2871만 명  2455만 명  1855만 명\n 7  S3_2  3757만 명  3743만 명  2941만 명  2561만 명  2015만 명,\n       0       1       2        3        4        5\n 0    노인    2017    2020     2040     2050     2065\n 1    S1  707만 명  813만 명  1724만 명  1903만 명  1860만 명\n 2    S2  707만 명  813만 명  1724만 명  1903만 명  1860만 명\n 3    S3  707만 명  813만 명  1724만 명  1903만 명  1860만 명\n 4    S4  707만 명  813만 명  1724만 명  1903만 명  1860만 명\n 5    S5  707만 명  813만 명  1724만 명  1903만 명  1860만 명\n 6  S3_1  707만 명  814만 명  1763만 명  1966만 명  1936만 명\n 7  S3_2  707만 명  813만 명  1738만 명  1932만 명  1926만 명,\n        0      1      2      3      4      5\n 0  노인 비율   2017   2020   2040   2050   2065\n 1     S1  13.8%  15.7%  32.7%  37.6%  41.0%\n 2     S2  13.8%  15.7%  33.3%  38.9%  43.9%\n 3     S3  13.8%  15.7%  33.9%  39.9%  46.2%\n 4     S4  13.8%  15.7%  34.9%  41.9%  50.6%\n 5     S5  13.8%  15.7%  35.1%  42.4%  52.0%\n 6   S3_1  13.8%  15.7%  34.3%  40.6%  47.1%\n 7   S3_2  13.8%  15.7%  33.4%  39.1%  44.8%,\n              0       1          2      3              4\n 0   (유엔 중위 추계)  평균 출산율  65세 이상 비율  중위 연령            총인구\n 1    2020-2025   1.08명      15.8%  43.7세        5126만 명\n 2    2025-2030   1.09명      20.2%  46.5세        5133만 명\n 3    2030-2035   1.18명      24.7%  49.1세        5115만 명\n 4    2035-2040   1.25명      29.0%  51.4세        5068만 명\n 5    2040-2045   1.32명      32.9%  54.9세        4978만 명\n 6    2045-2050   1.39명      35.8%  56.5세        4848만 명\n 7    2050-2055   1.44명      38.1%  57.7세        4683만 명\n 8    2055-2060   1.48명      39.2%  58.5세        4485만 명\n 9    2060-2065   1.52명      40.9%  58.5세        4270만 명\n 10   2065-2070   1.56명      42.1%  58.3세        4056만 명\n 11   2070-2075   1.58명      41.9%  57.9세        3854만 명\n 12   2075-2080   1.61명      41.5%  57.1세        3666만 명\n 13   2080-2085   1.63명      41.3%  56.2세        3493만 명\n 14   2085-2090   1.64명      40.5%  55.7세        3335만 명\n 15   2090-2095   1.65명      39.5%  55.1세        3191만 명\n 16   2095-2100   1.67명      38.7%  54.6세  3062만 명 (2095,\n              0       1          2      3              4\n 0   (유엔 저위 추계)  평균 출산율  65세 이상 비율  중위 연령            총인구\n 1    2020-2025   0.83명      15.8%  43.7세        5126만 명\n 2    2025-2030   0.69명      20.4%  46.8세        5093만 명\n 3    2030-2035   0.68명      25.3%  49.8세        5012만 명\n 4    2035-2040   0.75명      30.1%  52.6세        4893만 명\n 5    2040-2045   0.82명      34.6%  55.2세        4740만 명\n 6    2045-2050   0.89명      38.1%  57.2세        4552만 명\n 7    2050-2055   0.94명      41.1%  59.0세        4333만 명\n 8    2055-2060   0.98명      43.1%  60.8세        4081만 명\n 9    2060-2065   1.02명      45.9%  62.5세        3804만 명\n 10   2065-2070   1.06명      48.5%  64.0세        3519만 명\n 11   2070-2075   1.08명      49.8%  64.9세        3241만 명\n 12   2075-2080   1.11명      51.0%  65.7세        2980만 명\n 13   2080-2085   1.13명      52.7%  66.6세        2737만 명\n 14   2085-2090   1.14명      53.7%  67.4세        2513만 명\n 15   2090-2095   1.15명      53.0%  67.4세        2303만 명\n 16   2095-2100   1.17명      51.6%  66.4세  2106만 명 (2095,\n              시행일                                           주요 개정 내용\n 0   2008. 2. 29.  ▪ 저출산ㆍ고령사회 정책에 관한 중요정책 결정의 신속성을 제고하고, 부처 중심의 책...\n 1    2012. 2. 5.  ▪ 매년 7월 11일을 인구의 날로 지정하고, 국가와 지자체는 인구의 날 취지에 적...\n 2  2012. 11. 24.  ▪ 저출산ㆍ고령사회 정책 관련 사업수행 체계를 일원화하고, 관련 부처 간의 정책적 ...\n 3   2014. 3. 18.     ▪ 국민 건강증진을 위한 시책을 강구하는데 있어 성별에 따른 특성도 반영하도록 개정,\n                    분야                   세부 분야  \\\n 0   출산과 양육에 유리한 환경 조성    결혼ㆍ출산ㆍ양육에 대한 사회책임 강화   \n 1   출산과 양육에 유리한 환경 조성  일ㆍ가정 양립 및 가족친화 사회문화 조성   \n 2   출산과 양육에 유리한 환경 조성             건전한 미래세대 육성   \n 3  고령사회 삶의 질 향상 기반 구축             노후소득보장체계 강화   \n 4  고령사회 삶의 질 향상 기반 구축       건강하고 보호받는 노후생활 보장   \n 5  고령사회 삶의 질 향상 기반 구축    노인의 사회참여와 노후준비 기반 조성   \n 6  고령사회 삶의 질 향상 기반 구축            고령친화 생활환경 조성   \n 7          미래 성장동력 확보  여성ㆍ고령자 등 잠재인력 활용 기반 구축   \n 8          미래 성장동력 확보      인적자원의 경쟁력 및 활용도 제고   \n 9          미래 성장동력 확보               고령친화사업 육성   \n \n                                                주요 과제   재정 규모 (2006-2010)  \n 0  ▪ 결혼ㆍ출산 주 연령층에 대한 결혼정보 제공 및 지원 ▪ 보육ㆍ교육비 지원의 선진...  19.1조 원 (총 96개 과제)  \n 1  ▪ 육아휴직제도 활성화 및 근로형태 유연화 ▪ 가족친화기업인증제 확산 ▪ 가족 가치...  19.1조 원 (총 96개 과제)  \n 2  ▪ 아동학대 예방 및 방임 아동 보호체계 구축 ▪ 지역사회 아동․청소년 방과후 서비...  19.1조 원 (총 96개 과제)  \n 3         ▪ 국민연금 내실화 및 사각지대 해소 ▪ 퇴직연금제도 확대, 개인연금 활성화    15조 원 (총 66개 과제)  \n 4  ▪ 노인질환에 대한 종합 지원 강화 ▪ 치매노인에 대한 종합적 관리 및 지원체계 구...    15조 원 (총 66개 과제)  \n 5                      ▪ 노인적합형 일자리 창출 ▪ 노인의 여가문화 활성화    15조 원 (총 66개 과제)  \n 6  ▪ 저소득 고령자를 위한 주택공급 확대 ▪ 고령자에게 편리한 교통환경 조성 ▪ 지역...    15조 원 (총 66개 과제)  \n 7  ▪ 여성의 직업능력 개발 및 취업 지원 ▪ 고령자 고용촉진 지원 강화 ▪ 우수 외국...   6.2조 원 (총 75개 과제)  \n 8           ▪ 수요자 중심의 직업능력개발 기회 확대 ▪ 산업현장의 안전, 보건 증진   6.2조 원 (총 75개 과제)  \n 9           ▪ 고령친화 제품 기술개발 촉진 ▪ 역모기지제도 및 자산운용산업의 활성화   6.2조 원 (총 75개 과제)  ,\n                   분야                  세부 분야  \\\n 0             저출산 대책         청년 일자리ㆍ주거대책 강화   \n 1             저출산 대책  난임 등 출생에 대한 사회적 책임 강화   \n 2             저출산 대책        맞춤형 돌봄 확대․교육 개혁   \n 3             저출산 대책        일ㆍ가정 양립 사각지대 해소   \n 4            고령사회 대책              노후소득보장 강화   \n 5            고령사회 대책         활기차고 안전한 노후 실현   \n 6            고령사회 대책  여성, 중ㆍ고령자, 외국인력 활용 확대   \n 7            고령사회 대책            고령친화경제로의 도약   \n 8   저출산․고령사회 대응기반 강화       민간ㆍ지역ㆍ정부 협력체계 강화   \n 9   저출산․고령사회 대응기반 강화            홍보ㆍ인식개선 활성화   \n 10  저출산․고령사회 대응기반 강화         중앙ㆍ지방의 추진기반 강화   \n \n                                                 주요 과제    재정 규모 (2016-2020)  \n 0   ▪ 노동 개혁을 통한 고용 창출과 일자리의 질 제고 ▪ 청년․예비부부 주거 지원 강...  108.4조 원 (총 47개 과제)  \n 1   ▪ 임신ㆍ출산 의료비 대폭 경감 ▪ 난임부부 종합지원체계 구축 ▪ 다문화 가정 맞춤...  108.4조 원 (총 47개 과제)  \n 2   ▪ 보육ㆍ돌봄 사각지대 해소 ▪ 지역사회 내 돌봄 여건 확충 ▪ 공교육의 역량 강화...  108.4조 원 (총 47개 과제)  \n 3            ▪ 일ㆍ가정 양립을 위한 기업 문화 확산 ▪ 육아기 근로시간 단축 활성화  108.4조 원 (총 47개 과제)  \n 4            ▪ 공적연금 강화, 주택ㆍ농지연금 대폭 확산 ▪ 퇴직ㆍ개인연금 확산ㆍ정착   89.1조 원 (총 78개 과제)  \n 5   ▪ 노인 의료비 부담 경감 ▪ 고령자 문화ㆍ여가 인프라 개선 ▪ 고령자를 위한 임대...   89.1조 원 (총 78개 과제)  \n 6   ▪ 경력단절여성 재취업 지원체계 강화 ▪ 중ㆍ고령자 취업 지원 활성화 ▪ 해외우수인...   89.1조 원 (총 78개 과제)  \n 7   ▪ 고령친화 관광산업 및 식품산업 육성 ▪ 대학구조개혁 추진 등 인구다운사이징 대비 강화   89.1조 원 (총 78개 과제)  \n 8              ▪ 사회 각 부문별 저출산 극복 운동 추진 ▪ 국민 참여 통로 다각화     0.0조 원 (총 9개 과제)  \n 9               ▪ 저출산 극복을 위한 인식개선ㆍ홍보 강화 ▪ 고비용 결혼문화 개선     0.0조 원 (총 9개 과제)  \n 10                  ▪ 저출산ㆍ고령사회 영향평가제도 도입 ▪ 저출산위 기능 강화     0.0조 원 (총 9개 과제)  ,\n                   분야                              세부 분야  \\\n 0             저출산 대책                      출산ㆍ양육비 부담 최소화   \n 1             저출산 대책                    아이와 함께하는 시간 최대화   \n 2             저출산 대책                   촘촘하고 안전한 돌봄체계 구축   \n 3             저출산 대책              모든 아동 존중과 포용적 가족문화 조성   \n 4             저출산 대책      2040세대 안정적인 삶의 기반(일ㆍ주거ㆍ교육) 조성   \n 5            고령사회 대책                   다층적 노후소득보장체계 내실화   \n 6            고령사회 대책                  신중년의 새로운 인생 출발 지원   \n 7            고령사회 대책                고령자의 다양한 사회참여 기회 확대   \n 8            고령사회 대책               지역사회 중심의 건강․돌봄 환경 구축   \n 9            고령사회 대책                  성숙한 노년기를 위한 기반 마련   \n 10  저출산․고령사회 대응기반 강화                         사회시스템 및 지역   \n 11  저출산․고령사회 대응기반 강화  인구 변화 대응 기반 강화 (총 10개 과제) 및 인식 개선   \n \n                                                 주요 과제   재정 규모 (2019-2020)  \n 0   ▪ 의료비 제로화, 아동수당 지급, 안전한 출산 ▪ 다자녀 지원 확대, 출산휴가급여...  77.3조 원 (총 96개 과제)  \n 1   ▪ 생애주기별 근로시간 단축, 남성 육아참여 확대 ▪ 일․생활 균형 환경 조성, 육...  77.3조 원 (총 96개 과제)  \n 2   ▪ 보육의 공공성 및 유치원 공공성 강화 ▪ 온종일 돌봄, 가정 내 돌봄 지원, 지...  77.3조 원 (총 96개 과제)  \n 3             ▪ 모든 아동 대상 차별 없는 보호 여건 마련 ▪ 포용적 가족문화 조성  77.3조 원 (총 96개 과제)  \n 4   ▪ 안정된 일자리, 차별 없는 일자리 조성 ▪ 청년ㆍ신혼부부 등 맞춤형 주거지원 강...  77.3조 원 (총 96개 과제)  \n 5   ▪ 다층적 노후보장소득체계 내실화, 공적연금 역할 강화 ▪ 사적연금 실효성 제고, ...    42조 원 (총 57개 과제)  \n 6   ▪ 인생 3모작 기반 구축을 통한 신중년 일자리 기회 확대 ▪ 활기찬 노후를 준비하...    42조 원 (총 57개 과제)  \n 7          ▪ 노인일자리 80만 개 창출․지원 ▪ 고령자 대상 여가 및 교육 기회 확대    42조 원 (총 57개 과제)  \n 8                        ▪ 의료․건강 관리, 돌봄․요양 및 주거․환경 조성    42조 원 (총 57개 과제)  \n 9     ▪ 존엄하게 삶을 마무리할 수 있는 기반 마련 ▪ 노인자살 예방을 위한 적극적인 대응    42조 원 (총 57개 과제)  \n 10  ▪ 인구구조 변화 대응 핵심분야 대책 마련 ▪ 지역 정책 패러다임 전환 및 인구 대...   0.0조 원 (총 10개 과제)  \n 11              ▪ 인구정책 추진체계 구축 ▪ 인식 개선을 위한 홍보 및 교육 강화   0.0조 원 (총 10개 과제)  ]\n\n\n\n모든 데이터프레임 형식의 자료를 다 가져온 상황\n\n- 두 번째 데이터프레임 호출(시도별 합계출산율)\n\ndf_lst[2]   ## 시도별 합계출산율\n\n\n  \n    \n\n\n\n\n\n\n지역/연도[6]\n2005\n2006[7]\n2007\n2008[8]\n2009[9]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n0.92\n0.97\n1.06\n1.01\n0.96\n1.02\n1.01\n1.06\n0.97\n0.98\n1.00\n0.94\n0.84\n0.76\n0.72\n0.64\n0.63\n\n\n1\n부산\n0.88\n0.91\n1.02\n0.98\n0.94\n1.05\n1.08\n1.14\n1.05\n1.09\n1.14\n1.10\n0.98\n0.90\n0.83\n0.75\n0.73\n\n\n2\n대구\n0.99\n1.00\n1.13\n1.07\n1.03\n1.11\n1.15\n1.22\n1.13\n1.17\n1.22\n1.19\n1.07\n0.99\n0.93\n0.81\n0.78\n\n\n3\n인천\n1.07\n1.11\n1.25\n1.19\n1.14\n1.21\n1.23\n1.30\n1.20\n1.21\n1.22\n1.14\n1.01\n1.01\n0.94\n0.83\n0.78\n\n\n4\n광주\n1.10\n1.14\n1.26\n1.20\n1.14\n1.22\n1.23\n1.30\n1.17\n1.20\n1.21\n1.17\n1.05\n0.97\n0.91\n0.81\n0.90\n\n\n5\n대전\n1.10\n1.15\n1.27\n1.22\n1.16\n1.21\n1.26\n1.32\n1.23\n1.25\n1.28\n1.19\n1.08\n0.95\n0.88\n0.81\n0.81\n\n\n6\n울산\n1.18\n1.24\n1.40\n1.34\n1.31\n1.37\n1.39\n1.48\n1.39\n1.44\n1.49\n1.42\n1.26\n1.13\n1.08\n0.99\n0.94\n\n\n7\n세종\n-\n-\n-\n-\n-\n-\n-\n1.60\n1.44\n1.35\n1.89\n1.82\n1.67\n1.57\n1.47\n1.28\n1.28\n\n\n8\n경기\n1.17\n1.23\n1.35\n1.29\n1.23\n1.31\n1.31\n1.36\n1.23\n1.24\n1.27\n1.19\n1.07\n1.00\n0.94\n0.88\n0.85\n\n\n9\n강원\n1.18\n1.19\n1.35\n1.25\n1.25\n1.31\n1.34\n1.37\n1.25\n1.25\n1.31\n1.24\n1.12\n1.07\n1.08\n1.04\n0.98\n\n\n10\n충북\n1.19\n1.22\n1.39\n1.32\n1.32\n1.40\n1.43\n1.49\n1.37\n1.36\n1.41\n1.36\n1.24\n1.17\n1.05\n0.98\n0.95\n\n\n11\n충남\n1.26\n1.35\n1.50\n1.44\n1.41\n1.48\n1.50\n1.57\n1.44\n1.42\n1.48\n1.40\n1.28\n1.19\n1.11\n1.03\n0.96\n\n\n12\n전북\n1.17\n1.20\n1.37\n1.31\n1.28\n1.37\n1.41\n1.44\n1.32\n1.33\n1.35\n1.25\n1.15\n1.04\n0.97\n0.91\n0.85\n\n\n13\n전남\n1.28\n1.33\n1.53\n1.45\n1.45\n1.54\n1.57\n1.64\n1.52\n1.50\n1.55\n1.47\n1.33\n1.24\n1.23\n1.15\n1.02\n\n\n14\n경북\n1.17\n1.20\n1.36\n1.31\n1.27\n1.38\n1.43\n1.49\n1.38\n1.41\n1.46\n1.40\n1.26\n1.17\n1.09\n1.00\n0.97\n\n\n15\n경남\n1.18\n1.25\n1.43\n1.37\n1.32\n1.41\n1.45\n1.50\n1.37\n1.41\n1.44\n1.36\n1.23\n1.12\n1.05\n0.95\n0.90\n\n\n16\n제주\n1.30\n1.36\n1.48\n1.39\n1.38\n1.46\n1.49\n1.60\n1.43\n1.48\n1.48\n1.43\n1.31\n1.22\n1.15\n1.02\n0.95\n\n\n17\n전국\n1.08\n1.13\n1.25\n1.19\n1.15\n1.23\n1.24\n1.30\n1.19\n1.21\n1.24\n1.17\n1.05\n0.98\n0.92\n0.84\n0.81\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf_lst[4]   ## 시도별 출생아 수\n\n\n  \n    \n\n\n\n\n\n\n지역/연도[6]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266\n91526\n93914.000\n84066.000\n83711.000\n83005\n75.536\n65389\n58074\n53.673\n47400\n45531\n\n\n1\n부산\n27415\n27759\n28673.000\n25831.000\n26190.000\n26645\n24906.000\n21480\n19152\n17049.000\n15100\n14446\n\n\n2\n대구\n20557\n20758\n21472.000\n19340.000\n19361.000\n19438\n18298.000\n15946\n14400\n13233.000\n11200\n10661\n\n\n3\n인천\n25752\n20758\n21472.000\n25560.000\n25786.000\n25491\n23609.000\n20445\n20087\n18522.000\n16000\n14947\n\n\n4\n광주\n13979\n13916\n14392.000\n12729.000\n12729.000\n12441\n11580.000\n10120\n9105\n8364.000\n7300\n7956\n\n\n5\n대전\n14314\n14808\n15279.000\n14099.000\n13962.000\n13774\n12436.000\n10851\n9337\n8410.000\n7500\n7414\n\n\n6\n울산\n11432\n11542\n12160.000\n11330.000\n11556.000\n11732\n10910.000\n9381\n8149\n7539.000\n6600\n6127\n\n\n7\n세종\n-\n-\n1054.000\n1111.000\n1344.000\n2708\n3297.000\n3504\n3703\n3819.000\n3500\n3570\n\n\n8\n경기\n121753\n122027\n124746.000\n112129.000\n112.169\n113495\n105643.000\n94088\n83198\n83.198\n77800\n76139\n\n\n9\n강원\n12477\n12408\n12426.000\n10980.000\n10662.000\n10929\n10058.000\n9958\n8351\n8283.000\n7800\n7357\n\n\n10\n충북\n14670\n14804\n15139.000\n13658.000\n13366.000\n13563\n12742.000\n11394\n10586\n9333.000\n8600\n8190\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604\n17302.000\n15670\n14380\n13228.000\n11900\n10984\n\n\n12\n전북\n16100\n16175\n16238.000\n14555.000\n14231.000\n14087\n12698.000\n11348\n10001\n8971.000\n8200\n7745\n\n\n13\n전남\n16654\n16612\n16990.000\n15401.000\n14817.000\n15061\n13980.000\n12354\n11238\n10832.000\n9700\n8430\n\n\n14\n경북\n23700\n24250\n24635.000\n22206.000\n22062.000\n22310\n20616.000\n17957\n16079\n14472.000\n12900\n12045\n\n\n15\n경남\n32203\n32536\n33211.000\n29504.000\n29763.000\n29537\n27138.000\n23849\n21224\n19250.000\n16800\n15562\n\n\n16\n제주\n5657\n5628\n5992.000\n5328.000\n5526.000\n5600\n5494.000\n5037\n4781\n4500.000\n4000\n3728\n\n\n17\n전국\n470171\n471265\n484550.000\n436455.000\n435435.000\n438420\n406243.000\n357771\n326822\n302676.000\n272400\n260562\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n세종시의 경우 중간에 지역이 추가된 경우라 결측치가 이상하게 들어가있음.\n\n\ndf = df_lst[4]\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18 entries, 0 to 17\nData columns (total 13 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   지역/연도[6]  18 non-null     object \n 1   2010      18 non-null     object \n 2   2011      18 non-null     object \n 3   2012      18 non-null     float64\n 4   2013      18 non-null     float64\n 5   2014      18 non-null     float64\n 6   2015      18 non-null     int64  \n 7   2016      18 non-null     float64\n 8   2017      18 non-null     int64  \n 9   2018      18 non-null     int64  \n 10  2019      18 non-null     float64\n 11  2020      18 non-null     int64  \n 12  2021      18 non-null     int64  \ndtypes: float64(5), int64(5), object(3)\nmemory usage: 2.0+ KB\n\n\n\n전부다 float이나 int여야 하는데, 0ㅡ1ㅡ2는 object임\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()  ## 첫 열은 적용시키기 애매하니 인덱스로 빼버림.\n\n\n  \n    \n\n\n\n\n\n\n지역\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266.000\n91526.000\n93914.000\n84066.000\n83711.000\n83005.0\n75.536\n65389.0\n58074.0\n53.673\n47400.0\n45531.0\n\n\n1\n부산\n27415.000\n27759.000\n28673.000\n25831.000\n26190.000\n26645.0\n24906.000\n21480.0\n19152.0\n17049.000\n15100.0\n14446.0\n\n\n2\n대구\n20557.000\n20758.000\n21472.000\n19340.000\n19361.000\n19438.0\n18298.000\n15946.0\n14400.0\n13233.000\n11200.0\n10661.0\n\n\n3\n인천\n25752.000\n20758.000\n21472.000\n25560.000\n25786.000\n25491.0\n23609.000\n20445.0\n20087.0\n18522.000\n16000.0\n14947.0\n\n\n4\n광주\n13979.000\n13916.000\n14392.000\n12729.000\n12729.000\n12441.0\n11580.000\n10120.0\n9105.0\n8364.000\n7300.0\n7956.0\n\n\n5\n대전\n14314.000\n14808.000\n15279.000\n14099.000\n13962.000\n13774.0\n12436.000\n10851.0\n9337.0\n8410.000\n7500.0\n7414.0\n\n\n6\n울산\n11432.000\n11542.000\n12160.000\n11330.000\n11556.000\n11732.0\n10910.000\n9381.0\n8149.0\n7539.000\n6600.0\n6127.0\n\n\n7\n세종\n0.000\n0.000\n1054.000\n1111.000\n1344.000\n2708.0\n3297.000\n3504.0\n3703.0\n3819.000\n3500.0\n3570.0\n\n\n8\n경기\n121753.000\n122027.000\n124746.000\n112129.000\n112.169\n113495.0\n105643.000\n94088.0\n83198.0\n83.198\n77800.0\n76139.0\n\n\n9\n강원\n12477.000\n12408.000\n12426.000\n10980.000\n10662.000\n10929.0\n10058.000\n9958.0\n8351.0\n8283.000\n7800.0\n7357.0\n\n\n10\n충북\n14670.000\n14804.000\n15139.000\n13658.000\n13366.000\n13563.0\n12742.000\n11394.0\n10586.0\n9333.000\n8600.0\n8190.0\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604.0\n17302.000\n15670.0\n14380.0\n13228.000\n11900.0\n10984.0\n\n\n12\n전북\n16100.000\n16175.000\n16238.000\n14555.000\n14231.000\n14087.0\n12698.000\n11348.0\n10001.0\n8971.000\n8200.0\n7745.0\n\n\n13\n전남\n16654.000\n16612.000\n16990.000\n15401.000\n14817.000\n15061.0\n13980.000\n12354.0\n11238.0\n10832.000\n9700.0\n8430.0\n\n\n14\n경북\n23700.000\n24250.000\n24635.000\n22206.000\n22062.000\n22310.0\n20616.000\n17957.0\n16079.0\n14472.000\n12900.0\n12045.0\n\n\n15\n경남\n32203.000\n32536.000\n33211.000\n29504.000\n29763.000\n29537.0\n27138.000\n23849.0\n21224.0\n19250.000\n16800.0\n15562.0\n\n\n16\n제주\n5657.000\n5628.000\n5992.000\n5328.000\n5526.000\n5600.0\n5494.000\n5037.0\n4781.0\n4500.000\n4000.0\n3728.0\n\n\n17\n전국\n470171.000\n471265.000\n484550.000\n436455.000\n435435.000\n438420.0\n406243.000\n357771.0\n326822.0\n302676.000\n272400.0\n260562.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18 entries, 0 to 17\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   지역      18 non-null     object \n 1   2010    18 non-null     float64\n 2   2011    18 non-null     float64\n 3   2012    18 non-null     float64\n 4   2013    18 non-null     float64\n 5   2014    18 non-null     float64\n 6   2015    18 non-null     float64\n 7   2016    18 non-null     float64\n 8   2017    18 non-null     float64\n 9   2018    18 non-null     float64\n 10  2019    18 non-null     float64\n 11  2020    18 non-null     float64\n 12  2021    18 non-null     float64\ndtypes: float64(12), object(1)\nmemory usage: 2.0+ KB\n\n\n\n지역만 object, 나머지는 float으로 잘 들어갔다."
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html#b.-시각화1-전국-출생아-수-시각화",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html#b.-시각화1-전국-출생아-수-시각화",
    "title": "데이터 크롤링 맛보기",
    "section": "### B. 시각화1 : 전국 출생아 수 시각화",
    "text": "### B. 시각화1 : 전국 출생아 수 시각화\n\n전국으로 따로 집계가 되어있긴 하지만, 따로 집계를 해서 산출\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\n\n\n  \n    \n\n\n\n\n\n\n지역\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266.000\n91526.000\n93914.000\n84066.000\n83711.000\n83005.0\n75.536\n65389.0\n58074.0\n53.673\n47400.0\n45531.0\n\n\n1\n부산\n27415.000\n27759.000\n28673.000\n25831.000\n26190.000\n26645.0\n24906.000\n21480.0\n19152.0\n17049.000\n15100.0\n14446.0\n\n\n2\n대구\n20557.000\n20758.000\n21472.000\n19340.000\n19361.000\n19438.0\n18298.000\n15946.0\n14400.0\n13233.000\n11200.0\n10661.0\n\n\n3\n인천\n25752.000\n20758.000\n21472.000\n25560.000\n25786.000\n25491.0\n23609.000\n20445.0\n20087.0\n18522.000\n16000.0\n14947.0\n\n\n4\n광주\n13979.000\n13916.000\n14392.000\n12729.000\n12729.000\n12441.0\n11580.000\n10120.0\n9105.0\n8364.000\n7300.0\n7956.0\n\n\n5\n대전\n14314.000\n14808.000\n15279.000\n14099.000\n13962.000\n13774.0\n12436.000\n10851.0\n9337.0\n8410.000\n7500.0\n7414.0\n\n\n6\n울산\n11432.000\n11542.000\n12160.000\n11330.000\n11556.000\n11732.0\n10910.000\n9381.0\n8149.0\n7539.000\n6600.0\n6127.0\n\n\n7\n세종\n0.000\n0.000\n1054.000\n1111.000\n1344.000\n2708.0\n3297.000\n3504.0\n3703.0\n3819.000\n3500.0\n3570.0\n\n\n8\n경기\n121753.000\n122027.000\n124746.000\n112129.000\n112.169\n113495.0\n105643.000\n94088.0\n83198.0\n83.198\n77800.0\n76139.0\n\n\n9\n강원\n12477.000\n12408.000\n12426.000\n10980.000\n10662.000\n10929.0\n10058.000\n9958.0\n8351.0\n8283.000\n7800.0\n7357.0\n\n\n10\n충북\n14670.000\n14804.000\n15139.000\n13658.000\n13366.000\n13563.0\n12742.000\n11394.0\n10586.0\n9333.000\n8600.0\n8190.0\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604.0\n17302.000\n15670.0\n14380.0\n13228.000\n11900.0\n10984.0\n\n\n12\n전북\n16100.000\n16175.000\n16238.000\n14555.000\n14231.000\n14087.0\n12698.000\n11348.0\n10001.0\n8971.000\n8200.0\n7745.0\n\n\n13\n전남\n16654.000\n16612.000\n16990.000\n15401.000\n14817.000\n15061.0\n13980.000\n12354.0\n11238.0\n10832.000\n9700.0\n8430.0\n\n\n14\n경북\n23700.000\n24250.000\n24635.000\n22206.000\n22062.000\n22310.0\n20616.000\n17957.0\n16079.0\n14472.000\n12900.0\n12045.0\n\n\n15\n경남\n32203.000\n32536.000\n33211.000\n29504.000\n29763.000\n29537.0\n27138.000\n23849.0\n21224.0\n19250.000\n16800.0\n15562.0\n\n\n16\n제주\n5657.000\n5628.000\n5992.000\n5328.000\n5526.000\n5600.0\n5494.000\n5037.0\n4781.0\n4500.000\n4000.0\n3728.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\n\n\n  \n    \n\n\n\n\n\n\n지역\n연도\n출생아 수\n\n\n\n\n0\n서울\n2010\n93266.0\n\n\n1\n서울\n2011\n91526.0\n\n\n2\n서울\n2012\n93914.0\n\n\n3\n서울\n2013\n84066.0\n\n\n4\n서울\n2014\n83711.0\n\n\n...\n...\n...\n...\n\n\n199\n제주\n2017\n5037.0\n\n\n200\n제주\n2018\n4781.0\n\n\n201\n제주\n2019\n4500.0\n\n\n202\n제주\n2020\n4000.0\n\n\n203\n제주\n2021\n3728.0\n\n\n\n\n\n204 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby('연도').aggregate({'출생아 수' : 'sum'}).reset_index()\n\n\n  \n    \n\n\n\n\n\n\n연도\n출생아 수\n\n\n\n\n0\n2010\n449949.242\n\n\n1\n2011\n445527.398\n\n\n2\n2012\n457813.448\n\n\n3\n2013\n417845.628\n\n\n4\n2014\n323378.169\n\n\n5\n2015\n438420.000\n\n\n6\n2016\n330782.536\n\n\n7\n2017\n358771.000\n\n\n8\n2018\n321845.000\n\n\n9\n2019\n165941.871\n\n\n10\n2020\n272300.000\n\n\n11\n2021\n260832.000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby('연도').aggregate({'출생아 수' : 'sum'}).reset_index()\\\n.plot.line(x = '연도', y = '출생아 수', backend = 'plotly')\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\2033454686.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\nC. 시각화2 : 시도별 출생아 수 시각화(line)\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.pivot_table(index = ['지역', '연도'], values = '출생아 수', aggfunc = 'sum').reset_index()  ## 똑같음(이미 타이디임)\n\n\n  \n    \n\n\n\n\n\n\n지역\n연도\n출생아 수\n\n\n\n\n0\n강원\n2010\n12477.0\n\n\n1\n강원\n2011\n12408.0\n\n\n2\n강원\n2012\n12426.0\n\n\n3\n강원\n2013\n10980.0\n\n\n4\n강원\n2014\n10662.0\n\n\n...\n...\n...\n...\n\n\n199\n충북\n2017\n11394.0\n\n\n200\n충북\n2018\n10586.0\n\n\n201\n충북\n2019\n9333.0\n\n\n202\n충북\n2020\n8600.0\n\n\n203\n충북\n2021\n8190.0\n\n\n\n\n\n204 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.plot.line(x = '연도', y = '출생아 수', color = '지역', backend = 'plotly')\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\2734995546.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\n두 가지 line plot 따로 장단점이 있음; 그래서 그 둘을 합쳐서 해보면 어떻게 될까."
  },
  {
    "objectID": "2023_DV/Review/B3. 주식과 yfinance.html#d.-시각화-3-시도별-출생아수-시각화area",
    "href": "2023_DV/Review/B3. 주식과 yfinance.html#d.-시각화-3-시도별-출생아수-시각화area",
    "title": "데이터 크롤링 맛보기",
    "section": "### D. 시각화 3 : 시도별 출생아수 시각화(area)",
    "text": "### D. 시각화 3 : 시도별 출생아수 시각화(area)\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역']).applymap(lambda x: float(x) if not x=='-' else 0)\\\n.drop('전국',axis=0)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.area(x='연도',y='출생아수',color='지역',backend='plotly')   ## plot.area()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\3490384095.py:2: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\n어떻게 변화하는지 전체를 시각화할 수 있음.\n누적된 정도를 파악한다.\n\n- 2014 경기, 2016 서울, 2019 서울ㆍ경기 : 출생아가 없었음. 왜 없었는가?\n\ndf\n\n\n  \n    \n\n\n\n\n\n\n지역/연도[6]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266\n91526\n93914.000\n84066.000\n83711.000\n83005\n75.536\n65389\n58074\n53.673\n47400\n45531\n\n\n1\n부산\n27415\n27759\n28673.000\n25831.000\n26190.000\n26645\n24906.000\n21480\n19152\n17049.000\n15100\n14446\n\n\n2\n대구\n20557\n20758\n21472.000\n19340.000\n19361.000\n19438\n18298.000\n15946\n14400\n13233.000\n11200\n10661\n\n\n3\n인천\n25752\n20758\n21472.000\n25560.000\n25786.000\n25491\n23609.000\n20445\n20087\n18522.000\n16000\n14947\n\n\n4\n광주\n13979\n13916\n14392.000\n12729.000\n12729.000\n12441\n11580.000\n10120\n9105\n8364.000\n7300\n7956\n\n\n5\n대전\n14314\n14808\n15279.000\n14099.000\n13962.000\n13774\n12436.000\n10851\n9337\n8410.000\n7500\n7414\n\n\n6\n울산\n11432\n11542\n12160.000\n11330.000\n11556.000\n11732\n10910.000\n9381\n8149\n7539.000\n6600\n6127\n\n\n7\n세종\n-\n-\n1054.000\n1111.000\n1344.000\n2708\n3297.000\n3504\n3703\n3819.000\n3500\n3570\n\n\n8\n경기\n121753\n122027\n124746.000\n112129.000\n112.169\n113495\n105643.000\n94088\n83198\n83.198\n77800\n76139\n\n\n9\n강원\n12477\n12408\n12426.000\n10980.000\n10662.000\n10929\n10058.000\n9958\n8351\n8283.000\n7800\n7357\n\n\n10\n충북\n14670\n14804\n15139.000\n13658.000\n13366.000\n13563\n12742.000\n11394\n10586\n9333.000\n8600\n8190\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604\n17302.000\n15670\n14380\n13228.000\n11900\n10984\n\n\n12\n전북\n16100\n16175\n16238.000\n14555.000\n14231.000\n14087\n12698.000\n11348\n10001\n8971.000\n8200\n7745\n\n\n13\n전남\n16654\n16612\n16990.000\n15401.000\n14817.000\n15061\n13980.000\n12354\n11238\n10832.000\n9700\n8430\n\n\n14\n경북\n23700\n24250\n24635.000\n22206.000\n22062.000\n22310\n20616.000\n17957\n16079\n14472.000\n12900\n12045\n\n\n15\n경남\n32203\n32536\n33211.000\n29504.000\n29763.000\n29537\n27138.000\n23849\n21224\n19250.000\n16800\n15562\n\n\n16\n제주\n5657\n5628\n5992.000\n5328.000\n5526.000\n5600\n5494.000\n5037\n4781\n4500.000\n4000\n3728\n\n\n17\n전국\n470171\n471265\n484550.000\n436455.000\n435435.000\n438420\n406243.000\n357771\n326822\n302676.000\n272400\n260562\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n경기 2013의 경우 소수점 존재, 서울 2016도, 등등… 정리가 좀 잘 안되어있음.\n\n\nE. 시각화 1,2,3 수정\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.applymap(lambda x: float(x) if not x=='-' else 0)\n\n\n  \n    \n\n\n\n\n\n\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n지역\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n서울\n93266.000\n91526.000\n93914.000\n84066.000\n83711.000\n83005.0\n75.536\n65389.0\n58074.0\n53.673\n47400.0\n45531.0\n\n\n부산\n27415.000\n27759.000\n28673.000\n25831.000\n26190.000\n26645.0\n24906.000\n21480.0\n19152.0\n17049.000\n15100.0\n14446.0\n\n\n대구\n20557.000\n20758.000\n21472.000\n19340.000\n19361.000\n19438.0\n18298.000\n15946.0\n14400.0\n13233.000\n11200.0\n10661.0\n\n\n인천\n25752.000\n20758.000\n21472.000\n25560.000\n25786.000\n25491.0\n23609.000\n20445.0\n20087.0\n18522.000\n16000.0\n14947.0\n\n\n광주\n13979.000\n13916.000\n14392.000\n12729.000\n12729.000\n12441.0\n11580.000\n10120.0\n9105.0\n8364.000\n7300.0\n7956.0\n\n\n대전\n14314.000\n14808.000\n15279.000\n14099.000\n13962.000\n13774.0\n12436.000\n10851.0\n9337.0\n8410.000\n7500.0\n7414.0\n\n\n울산\n11432.000\n11542.000\n12160.000\n11330.000\n11556.000\n11732.0\n10910.000\n9381.0\n8149.0\n7539.000\n6600.0\n6127.0\n\n\n세종\n0.000\n0.000\n1054.000\n1111.000\n1344.000\n2708.0\n3297.000\n3504.0\n3703.0\n3819.000\n3500.0\n3570.0\n\n\n경기\n121753.000\n122027.000\n124746.000\n112129.000\n112.169\n113495.0\n105643.000\n94088.0\n83198.0\n83.198\n77800.0\n76139.0\n\n\n강원\n12477.000\n12408.000\n12426.000\n10980.000\n10662.000\n10929.0\n10058.000\n9958.0\n8351.0\n8283.000\n7800.0\n7357.0\n\n\n충북\n14670.000\n14804.000\n15139.000\n13658.000\n13366.000\n13563.0\n12742.000\n11394.0\n10586.0\n9333.000\n8600.0\n8190.0\n\n\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604.0\n17302.000\n15670.0\n14380.0\n13228.000\n11900.0\n10984.0\n\n\n전북\n16100.000\n16175.000\n16238.000\n14555.000\n14231.000\n14087.0\n12698.000\n11348.0\n10001.0\n8971.000\n8200.0\n7745.0\n\n\n전남\n16654.000\n16612.000\n16990.000\n15401.000\n14817.000\n15061.0\n13980.000\n12354.0\n11238.0\n10832.000\n9700.0\n8430.0\n\n\n경북\n23700.000\n24250.000\n24635.000\n22206.000\n22062.000\n22310.0\n20616.000\n17957.0\n16079.0\n14472.000\n12900.0\n12045.0\n\n\n경남\n32203.000\n32536.000\n33211.000\n29504.000\n29763.000\n29537.0\n27138.000\n23849.0\n21224.0\n19250.000\n16800.0\n15562.0\n\n\n제주\n5657.000\n5628.000\n5992.000\n5328.000\n5526.000\n5600.0\n5494.000\n5037.0\n4781.0\n4500.000\n4000.0\n3728.0\n\n\n전국\n470171.000\n471265.000\n484550.000\n436455.000\n435435.000\n438420.0\n406243.000\n357771.0\n326822.0\n302676.000\n272400.0\n260562.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.applymap(lambda x: float(x) if not x=='-' else 0)\\\n.applymap(lambda x : x if str(x)[-1] == '0' else x*1000)\n\n\n  \n    \n\n\n\n\n\n\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n지역\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n서울\n93266.0\n91526.0\n93914.0\n84066.0\n83711.0\n83005.0\n75536.0\n65389.0\n58074.0\n53673.0\n47400.0\n45531.0\n\n\n부산\n27415.0\n27759.0\n28673.0\n25831.0\n26190.0\n26645.0\n24906.0\n21480.0\n19152.0\n17049.0\n15100.0\n14446.0\n\n\n대구\n20557.0\n20758.0\n21472.0\n19340.0\n19361.0\n19438.0\n18298.0\n15946.0\n14400.0\n13233.0\n11200.0\n10661.0\n\n\n인천\n25752.0\n20758.0\n21472.0\n25560.0\n25786.0\n25491.0\n23609.0\n20445.0\n20087.0\n18522.0\n16000.0\n14947.0\n\n\n광주\n13979.0\n13916.0\n14392.0\n12729.0\n12729.0\n12441.0\n11580.0\n10120.0\n9105.0\n8364.0\n7300.0\n7956.0\n\n\n대전\n14314.0\n14808.0\n15279.0\n14099.0\n13962.0\n13774.0\n12436.0\n10851.0\n9337.0\n8410.0\n7500.0\n7414.0\n\n\n울산\n11432.0\n11542.0\n12160.0\n11330.0\n11556.0\n11732.0\n10910.0\n9381.0\n8149.0\n7539.0\n6600.0\n6127.0\n\n\n세종\n0.0\n0.0\n1054.0\n1111.0\n1344.0\n2708.0\n3297.0\n3504.0\n3703.0\n3819.0\n3500.0\n3570.0\n\n\n경기\n121753.0\n122027.0\n124746.0\n112129.0\n112169.0\n113495.0\n105643.0\n94088.0\n83198.0\n83198.0\n77800.0\n76139.0\n\n\n강원\n12477.0\n12408.0\n12426.0\n10980.0\n10662.0\n10929.0\n10058.0\n9958.0\n8351.0\n8283.0\n7800.0\n7357.0\n\n\n충북\n14670.0\n14804.0\n15139.0\n13658.0\n13366.0\n13563.0\n12742.0\n11394.0\n10586.0\n9333.0\n8600.0\n8190.0\n\n\n충남\n20242.0\n20398.0\n20448.0\n18628.0\n18200.0\n18604.0\n17302.0\n15670.0\n14380.0\n13228.0\n11900.0\n10984.0\n\n\n전북\n16100.0\n16175.0\n16238.0\n14555.0\n14231.0\n14087.0\n12698.0\n11348.0\n10001.0\n8971.0\n8200.0\n7745.0\n\n\n전남\n16654.0\n16612.0\n16990.0\n15401.0\n14817.0\n15061.0\n13980.0\n12354.0\n11238.0\n10832.0\n9700.0\n8430.0\n\n\n경북\n23700.0\n24250.0\n24635.0\n22206.0\n22062.0\n22310.0\n20616.0\n17957.0\n16079.0\n14472.0\n12900.0\n12045.0\n\n\n경남\n32203.0\n32536.0\n33211.0\n29504.0\n29763.0\n29537.0\n27138.0\n23849.0\n21224.0\n19250.0\n16800.0\n15562.0\n\n\n제주\n5657.0\n5628.0\n5992.0\n5328.0\n5526.0\n5600.0\n5494.0\n5037.0\n4781.0\n4500.0\n4000.0\n3728.0\n\n\n전국\n470171.0\n471265.0\n484550.0\n436455.0\n435435.0\n438420.0\n406243.0\n357771.0\n326822.0\n302676.0\n272400.0\n260562.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.applymap(lambda x: float(x) if not x=='-' else 0)\\\n.applymap(lambda x : x if str(x)[-1] == '0' else x*1000)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.groupby('연도').agg({'출생아수' : 'sum'}).reset_index()\\\n.plot.line(x = '연도', y = '출생아수', backend = 'plotly')\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\2074312662.py:3: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\2074312662.py:4: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\n정상적으로 이해 가능함\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.applymap(lambda x: float(x) if not x=='-' else 0)\\\n.applymap(lambda x : x if str(x)[-1] == '0' else x*1000)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.line(x = '연도', y = '출생아수', color = '지역', backend = 'plotly')\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\3259186193.py:3: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\3259186193.py:4: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역']).applymap(lambda x: float(x) if not x=='-' else 0)\\\n.applymap(lambda x: x*1000 if x&lt;1000 else x)\\\n.drop('전국',axis=0)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.line(x='연도',y='출생아수',color='지역',backend='plotly')\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\2415151748.py:2: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_1244\\2415151748.py:3: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead."
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html",
    "href": "2023_DV/Review/B4. Plotly 시작.html",
    "title": "Plotly : pandas backend",
    "section": "",
    "text": "yfinance와 plotly를 이용하여 자료를 받고 시각화해보자!"
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#라이브러리-imports",
    "href": "2023_DV/Review/B4. Plotly 시작.html#라이브러리-imports",
    "title": "Plotly : pandas backend",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport plotly.io as pio\n\n\npd.options.plotting.backend = 'plotly'\npio.templates.default = 'plotly_white'\nprint(pio.templates)\n\nTemplates configuration\n-----------------------\n    Default template: 'plotly_white'\n    Available templates:\n        ['ggplot2', 'seaborn', 'simple_white', 'plotly',\n         'plotly_white', 'plotly_dark', 'presentation', 'xgridoff',\n         'ygridoff', 'gridon', 'none']\n\n\n\n\n기본적으로 산출되는 옵션을 바꿔준다. pandas의 디폴트 백엔드는 matplotlib이기 때문에, 이것을 plotly로 바꾸고 템플릿을 하얀색으로 바꿨다.\n\n- 간단하게 backend = plotly를 입력하지 않아도 되게 만들었음."
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#yfinance를-이용한-주식-자료-시각화",
    "href": "2023_DV/Review/B4. Plotly 시작.html#yfinance를-이용한-주식-자료-시각화",
    "title": "Plotly : pandas backend",
    "section": "2. yfinance를 이용한 주식 자료 시각화",
    "text": "2. yfinance를 이용한 주식 자료 시각화\n\nA. 크롤링 + 데이터 정리\n\n주식 종목에 따른 정보 가져오기(야후 파이낸셜에서 검색) :\nApple : AAPL\n삼성전자 : 005930.KS\n\n해당 코드를 이용하여 관심있는 데이터를 크롤링하려면?\n\n\nsymbols = ['AMZN','AAPL','GOOG','MSFT','NFLX','NVDA','TSLA']    ## 관심있는 주식들\nstart = '2020-01-01'    ## 장 시작\nend = '2023-11-06'    ## 장 종료\ndf = yf.download(symbols,start,end)\n\n[*********************100%%**********************]  7 of 7 completed\n\n\n\ndf\n\n\n\n\n\n\n\n\nAdj Close\nClose\n...\nOpen\nVolume\n\n\n\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\nAAPL\nAMZN\nGOOG\n...\nNFLX\nNVDA\nTSLA\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.152657\n94.900497\n68.368500\n155.093674\n329.809998\n59.749290\n28.684000\n75.087502\n94.900497\n68.368500\n...\n326.100006\n59.687500\n28.299999\n135480400\n80580000\n28132000\n22622100\n4485800\n23753600\n142981500\n\n\n2020-01-03\n72.441460\n93.748497\n68.032997\n153.162476\n325.899994\n58.792957\n29.534000\n74.357498\n93.748497\n68.032997\n...\n326.779999\n58.775002\n29.366667\n146322800\n75288000\n23728000\n21116200\n3806900\n20538400\n266677500\n\n\n2020-01-06\n73.018692\n95.143997\n69.710503\n153.558395\n335.829987\n59.039509\n30.102667\n74.949997\n95.143997\n69.710503\n...\n323.119995\n58.080002\n29.364668\n118387200\n81236000\n34646000\n20813700\n5663100\n26263600\n151995000\n\n\n2020-01-07\n72.675285\n95.343002\n69.667000\n152.158249\n330.750000\n59.754276\n31.270666\n74.597504\n95.343002\n69.667000\n...\n336.470001\n59.549999\n30.760000\n108872000\n80898000\n30054000\n21634100\n4703200\n31485600\n268231500\n\n\n2020-01-08\n73.844353\n94.598503\n70.216003\n154.581940\n339.260010\n59.866344\n32.809334\n75.797501\n94.598503\n70.216003\n...\n331.489990\n59.939999\n31.580000\n132079200\n70160000\n30560000\n27746500\n7104500\n27710800\n467164500\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.065933\n132.710007\n125.750000\n337.309998\n410.079987\n411.609985\n197.360001\n170.289993\n132.710007\n125.750000\n...\n402.350006\n410.869995\n209.279999\n51131000\n72485500\n24165600\n22828100\n5317100\n38802800\n136448200\n\n\n2023-10-31\n170.545319\n133.089996\n125.300003\n338.109985\n411.690002\n407.799988\n200.839996\n170.770004\n133.089996\n125.300003\n...\n409.239990\n404.500000\n196.119995\n44846000\n51589400\n21123400\n20265300\n3877600\n51796900\n118068300\n\n\n2023-11-01\n173.741104\n137.000000\n127.570000\n346.070007\n420.190002\n423.250000\n205.660004\n173.970001\n137.000000\n127.570000\n...\n414.769989\n408.839996\n204.039993\n56934900\n61529400\n26536600\n28158800\n4806100\n43759300\n121661700\n\n\n2023-11-02\n177.336380\n138.070007\n128.580002\n348.320007\n424.709991\n435.059998\n218.509995\n177.570007\n138.070007\n128.580002\n...\n421.170013\n433.279999\n212.970001\n77334800\n52236700\n24091700\n24348100\n4476000\n40917200\n125987600\n\n\n2023-11-03\n176.417572\n138.600006\n130.369995\n352.799988\n432.359985\n450.049988\n219.960007\n176.649994\n138.600006\n130.369995\n...\n428.760010\n440.200012\n221.149994\n79763700\n44007200\n19517900\n23624000\n3664800\n42385500\n119281000\n\n\n\n\n968 rows × 42 columns\n\n\n\n\nwide data로 들어가있는 것을 알 수 있다.\n\n- 그럼 필요한 정보(조정된 주가)만 가져와보자.\n\ndf.stack().loc[:, 'Adj Close'].reset_index()\n\n\n\n\n\n\n\n\nDate\nlevel_1\nAdj Close\n\n\n\n\n0\n2020-01-02\nAAPL\n73.152657\n\n\n1\n2020-01-02\nAMZN\n94.900497\n\n\n2\n2020-01-02\nGOOG\n68.368500\n\n\n3\n2020-01-02\nMSFT\n155.093674\n\n\n4\n2020-01-02\nNFLX\n329.809998\n\n\n...\n...\n...\n...\n\n\n6771\n2023-11-03\nGOOG\n130.369995\n\n\n6772\n2023-11-03\nMSFT\n352.799988\n\n\n6773\n2023-11-03\nNFLX\n432.359985\n\n\n6774\n2023-11-03\nNVDA\n450.049988\n\n\n6775\n2023-11-03\nTSLA\n219.960007\n\n\n\n\n6776 rows × 3 columns\n\n\n\n- 다른 방법(MultiIndex 이용)\n\ndf.columns\n\nMultiIndex([('Adj Close', 'AAPL'),\n            ('Adj Close', 'AMZN'),\n            ('Adj Close', 'GOOG'),\n            ('Adj Close', 'MSFT'),\n            ('Adj Close', 'NFLX'),\n            ('Adj Close', 'NVDA'),\n            ('Adj Close', 'TSLA'),\n            (    'Close', 'AAPL'),\n            (    'Close', 'AMZN'),\n            (    'Close', 'GOOG'),\n            (    'Close', 'MSFT'),\n            (    'Close', 'NFLX'),\n            (    'Close', 'NVDA'),\n            (    'Close', 'TSLA'),\n            (     'High', 'AAPL'),\n            (     'High', 'AMZN'),\n            (     'High', 'GOOG'),\n            (     'High', 'MSFT'),\n            (     'High', 'NFLX'),\n            (     'High', 'NVDA'),\n            (     'High', 'TSLA'),\n            (      'Low', 'AAPL'),\n            (      'Low', 'AMZN'),\n            (      'Low', 'GOOG'),\n            (      'Low', 'MSFT'),\n            (      'Low', 'NFLX'),\n            (      'Low', 'NVDA'),\n            (      'Low', 'TSLA'),\n            (     'Open', 'AAPL'),\n            (     'Open', 'AMZN'),\n            (     'Open', 'GOOG'),\n            (     'Open', 'MSFT'),\n            (     'Open', 'NFLX'),\n            (     'Open', 'NVDA'),\n            (     'Open', 'TSLA'),\n            (   'Volume', 'AAPL'),\n            (   'Volume', 'AMZN'),\n            (   'Volume', 'GOOG'),\n            (   'Volume', 'MSFT'),\n            (   'Volume', 'NFLX'),\n            (   'Volume', 'NVDA'),\n            (   'Volume', 'TSLA')],\n           )\n\n\n\n이렇게 멀티인덱스로 되어있는 경우, 앞의 것만으로도 호출할 수 있다. 예를들어…\n\n\ndf.loc[:, 'Adj Close']  ## 이건 된다.\n#df.loc[:, 'AAPL']  ## 이건 안된다.\n\n\n\n\n\n\n\n\nAAPL\nAMZN\nGOOG\nMSFT\nNFLX\nNVDA\nTSLA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.152657\n94.900497\n68.368500\n155.093674\n329.809998\n59.749290\n28.684000\n\n\n2020-01-03\n72.441460\n93.748497\n68.032997\n153.162476\n325.899994\n58.792957\n29.534000\n\n\n2020-01-06\n73.018692\n95.143997\n69.710503\n153.558395\n335.829987\n59.039509\n30.102667\n\n\n2020-01-07\n72.675285\n95.343002\n69.667000\n152.158249\n330.750000\n59.754276\n31.270666\n\n\n2020-01-08\n73.844353\n94.598503\n70.216003\n154.581940\n339.260010\n59.866344\n32.809334\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.065933\n132.710007\n125.750000\n337.309998\n410.079987\n411.609985\n197.360001\n\n\n2023-10-31\n170.545319\n133.089996\n125.300003\n338.109985\n411.690002\n407.799988\n200.839996\n\n\n2023-11-01\n173.741104\n137.000000\n127.570000\n346.070007\n420.190002\n423.250000\n205.660004\n\n\n2023-11-02\n177.336380\n138.070007\n128.580002\n348.320007\n424.709991\n435.059998\n218.509995\n\n\n2023-11-03\n176.417572\n138.600006\n130.369995\n352.799988\n432.359985\n450.049988\n219.960007\n\n\n\n\n968 rows × 7 columns\n\n\n\n- 굳이굳이 위와 같은 방식으로 AAPL만 추출하고 싶다면…\n\ndf.stack().stack().swaplevel(i = 1, j = 2)  ## 멀티인덱스(level)의 순서를 바꿔줌.\n\nDate                       \n2020-01-02  Adj Close  AAPL    7.315266e+01\n            Close      AAPL    7.508750e+01\n            High       AAPL    7.515000e+01\n            Low        AAPL    7.379750e+01\n            Open       AAPL    7.406000e+01\n                                   ...     \n2023-11-03  Close      TSLA    2.199600e+02\n            High       TSLA    2.263700e+02\n            Low        TSLA    2.184000e+02\n            Open       TSLA    2.211500e+02\n            Volume     TSLA    1.192810e+08\nLength: 40656, dtype: float64\n\n\n\ndf.stack().stack().swaplevel(i = 1, j = 2).unstack().unstack().loc[:, 'AAPL']\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.152657\n75.087502\n75.150002\n73.797501\n74.059998\n135480400.0\n\n\n2020-01-03\n72.441460\n74.357498\n75.144997\n74.125000\n74.287498\n146322800.0\n\n\n2020-01-06\n73.018692\n74.949997\n74.989998\n73.187500\n73.447502\n118387200.0\n\n\n2020-01-07\n72.675285\n74.597504\n75.224998\n74.370003\n74.959999\n108872000.0\n\n\n2020-01-08\n73.844353\n75.797501\n76.110001\n74.290001\n74.290001\n132079200.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.065933\n170.289993\n171.169998\n168.869995\n169.020004\n51131000.0\n\n\n2023-10-31\n170.545319\n170.770004\n170.899994\n167.899994\n169.350006\n44846000.0\n\n\n2023-11-01\n173.741104\n173.970001\n174.229996\n170.119995\n171.000000\n56934900.0\n\n\n2023-11-02\n177.336380\n177.570007\n177.779999\n175.460007\n175.520004\n77334800.0\n\n\n2023-11-03\n176.417572\n176.649994\n176.820007\n173.350006\n174.240005\n79763700.0\n\n\n\n\n968 rows × 6 columns\n\n\n\n\n이렇게 하면 된다.\n\n\ndf.swaplevel(i = 0, j = 1, axis = 1).loc[:, 'AAPL']\n\n\n\n\n\n\n\n\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n73.152657\n75.087502\n75.150002\n73.797501\n74.059998\n135480400\n\n\n2020-01-03\n72.441460\n74.357498\n75.144997\n74.125000\n74.287498\n146322800\n\n\n2020-01-06\n73.018692\n74.949997\n74.989998\n73.187500\n73.447502\n118387200\n\n\n2020-01-07\n72.675285\n74.597504\n75.224998\n74.370003\n74.959999\n108872000\n\n\n2020-01-08\n73.844353\n75.797501\n76.110001\n74.290001\n74.290001\n132079200\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-10-30\n170.065933\n170.289993\n171.169998\n168.869995\n169.020004\n51131000\n\n\n2023-10-31\n170.545319\n170.770004\n170.899994\n167.899994\n169.350006\n44846000\n\n\n2023-11-01\n173.741104\n173.970001\n174.229996\n170.119995\n171.000000\n56934900\n\n\n2023-11-02\n177.336380\n177.570007\n177.779999\n175.460007\n175.520004\n77334800\n\n\n2023-11-03\n176.417572\n176.649994\n176.820007\n173.350006\n174.240005\n79763700\n\n\n\n\n968 rows × 6 columns\n\n\n\n\n이것도 똑같다.(물론 컬럼 인덱스가 정렬이 안되어있는 게 다르긴한데, 결과는 똑같잖아?)\n\n- 솔직히 아래와 같이 하는 게 제일 맘편하다.\n\ndf.stack().reset_index().rename({'level_1' : 'Subject'}, axis = 1)  ## 바로 타이디데이터로\n\n\n\n\n\n\n\n\nDate\nSubject\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\n\n\n0\n2020-01-02\nAAPL\n73.152657\n75.087502\n75.150002\n73.797501\n74.059998\n135480400\n\n\n1\n2020-01-02\nAMZN\n94.900497\n94.900497\n94.900497\n93.207497\n93.750000\n80580000\n\n\n2\n2020-01-02\nGOOG\n68.368500\n68.368500\n68.406998\n67.077499\n67.077499\n28132000\n\n\n3\n2020-01-02\nMSFT\n155.093674\n160.619995\n160.729996\n158.330002\n158.779999\n22622100\n\n\n4\n2020-01-02\nNFLX\n329.809998\n329.809998\n329.980011\n324.779999\n326.100006\n4485800\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6771\n2023-11-03\nGOOG\n130.369995\n130.369995\n130.729996\n129.009995\n129.089996\n19517900\n\n\n6772\n2023-11-03\nMSFT\n352.799988\n352.799988\n354.390015\n347.329987\n349.630005\n23624000\n\n\n6773\n2023-11-03\nNFLX\n432.359985\n432.359985\n434.820007\n425.529999\n428.760010\n3664800\n\n\n6774\n2023-11-03\nNVDA\n450.049988\n450.049988\n453.089996\n437.230011\n440.200012\n42385500\n\n\n6775\n2023-11-03\nTSLA\n219.960007\n219.960007\n226.369995\n218.399994\n221.149994\n119281000\n\n\n\n\n6776 rows × 8 columns"
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#b.-시각화",
    "href": "2023_DV/Review/B4. Plotly 시작.html#b.-시각화",
    "title": "Plotly : pandas backend",
    "section": "### B. 시각화",
    "text": "### B. 시각화\n- tidydata를 생성\n\ndf.loc[:, 'Adj Close'].reset_index().set_index('Date').stack().reset_index().rename({'level_1' : 'Company', 0 : 'Price'}, axis = 1)\n\n\n\n\n\n\n\n\nDate\nCompany\nPrice\n\n\n\n\n0\n2020-01-02\nAAPL\n73.152657\n\n\n1\n2020-01-02\nAMZN\n94.900497\n\n\n2\n2020-01-02\nGOOG\n68.368500\n\n\n3\n2020-01-02\nMSFT\n155.093674\n\n\n4\n2020-01-02\nNFLX\n329.809998\n\n\n...\n...\n...\n...\n\n\n6771\n2023-11-03\nGOOG\n130.369995\n\n\n6772\n2023-11-03\nMSFT\n352.799988\n\n\n6773\n2023-11-03\nNFLX\n432.359985\n\n\n6774\n2023-11-03\nNVDA\n450.049988\n\n\n6775\n2023-11-03\nTSLA\n219.960007\n\n\n\n\n6776 rows × 3 columns\n\n\n\n- 시각화 : 데이터프레임 자체 메소드 활용\n\ndf.loc[:, 'Adj Close'].reset_index().set_index('Date').stack().reset_index().rename({'level_1' : 'Company', 0 : 'Price'}, axis = 1)\\\n.plot.line(x = 'Date', y = 'Price', color = 'Company', backend = 'plotly')  ## plot()은 라인이 디폴트긴 함.\n\n\n                                                \n\n\n\n깔쌈하죠?\n\n상당히 강력한 툴임, 이미지적으로 지원하는 게 되게 많음."
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#출산률-시각화",
    "href": "2023_DV/Review/B4. Plotly 시작.html#출산률-시각화",
    "title": "Plotly : pandas backend",
    "section": "3. 출산률 시각화",
    "text": "3. 출산률 시각화\n\nA. 크롤링 + 데이터 정리\n\n- 대한민국의 저출산 문제\n\nref : https://ko.wikipedia.org/wiki/대한민국의_저출산\n- 위의 url에서 3, 5번째 테이블만 읽고 싶다면 어떻게 해야 할까???\n\n3번째 테이블 : 시도별 출산률\n5번째 테이블 : 시도별 출생아 수\n\n1 데이터 긁어오기\n\ndf_lst = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%A0%80%EC%B6%9C%EC%82%B0')\n\n\n예, ㅈㄴ 단순합니다. 일단 판다스에서 자체적으로 html을 긁어올 수 있어요.\n\n\nlen(df_lst)  ## 22개의 테뷸러 데이터\n\n22\n\n\n\ndf_lst[2]  ## 시도별 합계출산률\n\n\n\n\n\n\n\n\n지역/연도[6]\n2005\n2006[7]\n2007\n2008[8]\n2009[9]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n0.92\n0.97\n1.06\n1.01\n0.96\n1.02\n1.01\n1.06\n0.97\n0.98\n1.00\n0.94\n0.84\n0.76\n0.72\n0.64\n0.63\n\n\n1\n부산\n0.88\n0.91\n1.02\n0.98\n0.94\n1.05\n1.08\n1.14\n1.05\n1.09\n1.14\n1.10\n0.98\n0.90\n0.83\n0.75\n0.73\n\n\n2\n대구\n0.99\n1.00\n1.13\n1.07\n1.03\n1.11\n1.15\n1.22\n1.13\n1.17\n1.22\n1.19\n1.07\n0.99\n0.93\n0.81\n0.78\n\n\n3\n인천\n1.07\n1.11\n1.25\n1.19\n1.14\n1.21\n1.23\n1.30\n1.20\n1.21\n1.22\n1.14\n1.01\n1.01\n0.94\n0.83\n0.78\n\n\n4\n광주\n1.10\n1.14\n1.26\n1.20\n1.14\n1.22\n1.23\n1.30\n1.17\n1.20\n1.21\n1.17\n1.05\n0.97\n0.91\n0.81\n0.90\n\n\n5\n대전\n1.10\n1.15\n1.27\n1.22\n1.16\n1.21\n1.26\n1.32\n1.23\n1.25\n1.28\n1.19\n1.08\n0.95\n0.88\n0.81\n0.81\n\n\n6\n울산\n1.18\n1.24\n1.40\n1.34\n1.31\n1.37\n1.39\n1.48\n1.39\n1.44\n1.49\n1.42\n1.26\n1.13\n1.08\n0.99\n0.94\n\n\n7\n세종\n-\n-\n-\n-\n-\n-\n-\n1.60\n1.44\n1.35\n1.89\n1.82\n1.67\n1.57\n1.47\n1.28\n1.28\n\n\n8\n경기\n1.17\n1.23\n1.35\n1.29\n1.23\n1.31\n1.31\n1.36\n1.23\n1.24\n1.27\n1.19\n1.07\n1.00\n0.94\n0.88\n0.85\n\n\n9\n강원\n1.18\n1.19\n1.35\n1.25\n1.25\n1.31\n1.34\n1.37\n1.25\n1.25\n1.31\n1.24\n1.12\n1.07\n1.08\n1.04\n0.98\n\n\n10\n충북\n1.19\n1.22\n1.39\n1.32\n1.32\n1.40\n1.43\n1.49\n1.37\n1.36\n1.41\n1.36\n1.24\n1.17\n1.05\n0.98\n0.95\n\n\n11\n충남\n1.26\n1.35\n1.50\n1.44\n1.41\n1.48\n1.50\n1.57\n1.44\n1.42\n1.48\n1.40\n1.28\n1.19\n1.11\n1.03\n0.96\n\n\n12\n전북\n1.17\n1.20\n1.37\n1.31\n1.28\n1.37\n1.41\n1.44\n1.32\n1.33\n1.35\n1.25\n1.15\n1.04\n0.97\n0.91\n0.85\n\n\n13\n전남\n1.28\n1.33\n1.53\n1.45\n1.45\n1.54\n1.57\n1.64\n1.52\n1.50\n1.55\n1.47\n1.33\n1.24\n1.23\n1.15\n1.02\n\n\n14\n경북\n1.17\n1.20\n1.36\n1.31\n1.27\n1.38\n1.43\n1.49\n1.38\n1.41\n1.46\n1.40\n1.26\n1.17\n1.09\n1.00\n0.97\n\n\n15\n경남\n1.18\n1.25\n1.43\n1.37\n1.32\n1.41\n1.45\n1.50\n1.37\n1.41\n1.44\n1.36\n1.23\n1.12\n1.05\n0.95\n0.90\n\n\n16\n제주\n1.30\n1.36\n1.48\n1.39\n1.38\n1.46\n1.49\n1.60\n1.43\n1.48\n1.48\n1.43\n1.31\n1.22\n1.15\n1.02\n0.95\n\n\n17\n전국\n1.08\n1.13\n1.25\n1.19\n1.15\n1.23\n1.24\n1.30\n1.19\n1.21\n1.24\n1.17\n1.05\n0.98\n0.92\n0.84\n0.81\n\n\n\n\n\n\n\n\ndf_lst[4]  ## 시도별 출생아 수\n\n\n\n\n\n\n\n\n지역/연도[6]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266\n91526\n93914.000\n84066.000\n83711.000\n83005\n75.536\n65389\n58074\n53.673\n47400\n45531\n\n\n1\n부산\n27415\n27759\n28673.000\n25831.000\n26190.000\n26645\n24906.000\n21480\n19152\n17049.000\n15100\n14446\n\n\n2\n대구\n20557\n20758\n21472.000\n19340.000\n19361.000\n19438\n18298.000\n15946\n14400\n13233.000\n11200\n10661\n\n\n3\n인천\n25752\n20758\n21472.000\n25560.000\n25786.000\n25491\n23609.000\n20445\n20087\n18522.000\n16000\n14947\n\n\n4\n광주\n13979\n13916\n14392.000\n12729.000\n12729.000\n12441\n11580.000\n10120\n9105\n8364.000\n7300\n7956\n\n\n5\n대전\n14314\n14808\n15279.000\n14099.000\n13962.000\n13774\n12436.000\n10851\n9337\n8410.000\n7500\n7414\n\n\n6\n울산\n11432\n11542\n12160.000\n11330.000\n11556.000\n11732\n10910.000\n9381\n8149\n7539.000\n6600\n6127\n\n\n7\n세종\n-\n-\n1054.000\n1111.000\n1344.000\n2708\n3297.000\n3504\n3703\n3819.000\n3500\n3570\n\n\n8\n경기\n121753\n122027\n124746.000\n112129.000\n112.169\n113495\n105643.000\n94088\n83198\n83.198\n77800\n76139\n\n\n9\n강원\n12477\n12408\n12426.000\n10980.000\n10662.000\n10929\n10058.000\n9958\n8351\n8283.000\n7800\n7357\n\n\n10\n충북\n14670\n14804\n15139.000\n13658.000\n13366.000\n13563\n12742.000\n11394\n10586\n9333.000\n8600\n8190\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604\n17302.000\n15670\n14380\n13228.000\n11900\n10984\n\n\n12\n전북\n16100\n16175\n16238.000\n14555.000\n14231.000\n14087\n12698.000\n11348\n10001\n8971.000\n8200\n7745\n\n\n13\n전남\n16654\n16612\n16990.000\n15401.000\n14817.000\n15061\n13980.000\n12354\n11238\n10832.000\n9700\n8430\n\n\n14\n경북\n23700\n24250\n24635.000\n22206.000\n22062.000\n22310\n20616.000\n17957\n16079\n14472.000\n12900\n12045\n\n\n15\n경남\n32203\n32536\n33211.000\n29504.000\n29763.000\n29537\n27138.000\n23849\n21224\n19250.000\n16800\n15562\n\n\n16\n제주\n5657\n5628\n5992.000\n5328.000\n5526.000\n5600\n5494.000\n5037\n4781\n4500.000\n4000\n3728\n\n\n17\n전국\n470171\n471265\n484550.000\n436455.000\n435435.000\n438420\n406243.000\n357771\n326822\n302676.000\n272400\n260562\n\n\n\n\n\n\n\n\n세종시의 경우 중간에 지역이 추가되어 결측치가 -로 들어가 있음\n\n2 데이터 처리\n\ndf = df_lst[4]\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18 entries, 0 to 17\nData columns (total 13 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   지역/연도[6]  18 non-null     object \n 1   2010      18 non-null     object \n 2   2011      18 non-null     object \n 3   2012      18 non-null     float64\n 4   2013      18 non-null     float64\n 5   2014      18 non-null     float64\n 6   2015      18 non-null     int64  \n 7   2016      18 non-null     float64\n 8   2017      18 non-null     int64  \n 9   2018      18 non-null     int64  \n 10  2019      18 non-null     float64\n 11  2020      18 non-null     int64  \n 12  2021      18 non-null     int64  \ndtypes: float64(5), int64(5), object(3)\nmemory usage: 2.0+ KB\n\n\n\n출생아 수이니까 지역빼고 전부 다 int64여야 할텐데, 1-2는 object임\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()  ## 어? 열 이름 영어로 안바꿔요???\n## applymap이 map으로 명령어가 바뀌었네... 근데 일단 이거 써야지 뭐.\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\615695314.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n지역\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266.000\n91526.000\n93914.000\n84066.000\n83711.000\n83005.0\n75.536\n65389.0\n58074.0\n53.673\n47400.0\n45531.0\n\n\n1\n부산\n27415.000\n27759.000\n28673.000\n25831.000\n26190.000\n26645.0\n24906.000\n21480.0\n19152.0\n17049.000\n15100.0\n14446.0\n\n\n2\n대구\n20557.000\n20758.000\n21472.000\n19340.000\n19361.000\n19438.0\n18298.000\n15946.0\n14400.0\n13233.000\n11200.0\n10661.0\n\n\n3\n인천\n25752.000\n20758.000\n21472.000\n25560.000\n25786.000\n25491.0\n23609.000\n20445.0\n20087.0\n18522.000\n16000.0\n14947.0\n\n\n4\n광주\n13979.000\n13916.000\n14392.000\n12729.000\n12729.000\n12441.0\n11580.000\n10120.0\n9105.0\n8364.000\n7300.0\n7956.0\n\n\n5\n대전\n14314.000\n14808.000\n15279.000\n14099.000\n13962.000\n13774.0\n12436.000\n10851.0\n9337.0\n8410.000\n7500.0\n7414.0\n\n\n6\n울산\n11432.000\n11542.000\n12160.000\n11330.000\n11556.000\n11732.0\n10910.000\n9381.0\n8149.0\n7539.000\n6600.0\n6127.0\n\n\n7\n세종\n0.000\n0.000\n1054.000\n1111.000\n1344.000\n2708.0\n3297.000\n3504.0\n3703.0\n3819.000\n3500.0\n3570.0\n\n\n8\n경기\n121753.000\n122027.000\n124746.000\n112129.000\n112.169\n113495.0\n105643.000\n94088.0\n83198.0\n83.198\n77800.0\n76139.0\n\n\n9\n강원\n12477.000\n12408.000\n12426.000\n10980.000\n10662.000\n10929.0\n10058.000\n9958.0\n8351.0\n8283.000\n7800.0\n7357.0\n\n\n10\n충북\n14670.000\n14804.000\n15139.000\n13658.000\n13366.000\n13563.0\n12742.000\n11394.0\n10586.0\n9333.000\n8600.0\n8190.0\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604.0\n17302.000\n15670.0\n14380.0\n13228.000\n11900.0\n10984.0\n\n\n12\n전북\n16100.000\n16175.000\n16238.000\n14555.000\n14231.000\n14087.0\n12698.000\n11348.0\n10001.0\n8971.000\n8200.0\n7745.0\n\n\n13\n전남\n16654.000\n16612.000\n16990.000\n15401.000\n14817.000\n15061.0\n13980.000\n12354.0\n11238.0\n10832.000\n9700.0\n8430.0\n\n\n14\n경북\n23700.000\n24250.000\n24635.000\n22206.000\n22062.000\n22310.0\n20616.000\n17957.0\n16079.0\n14472.000\n12900.0\n12045.0\n\n\n15\n경남\n32203.000\n32536.000\n33211.000\n29504.000\n29763.000\n29537.0\n27138.000\n23849.0\n21224.0\n19250.000\n16800.0\n15562.0\n\n\n16\n제주\n5657.000\n5628.000\n5992.000\n5328.000\n5526.000\n5600.0\n5494.000\n5037.0\n4781.0\n4500.000\n4000.0\n3728.0\n\n\n17\n전국\n470171.000\n471265.000\n484550.000\n436455.000\n435435.000\n438420.0\n406243.000\n357771.0\n326822.0\n302676.000\n272400.0\n260562.0\n\n\n\n\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index().info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 18 entries, 0 to 17\nData columns (total 13 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   지역      18 non-null     object \n 1   2010    18 non-null     float64\n 2   2011    18 non-null     float64\n 3   2012    18 non-null     float64\n 4   2013    18 non-null     float64\n 5   2014    18 non-null     float64\n 6   2015    18 non-null     float64\n 7   2016    18 non-null     float64\n 8   2017    18 non-null     float64\n 9   2018    18 non-null     float64\n 10  2019    18 non-null     float64\n 11  2020    18 non-null     float64\n 12  2021    18 non-null     float64\ndtypes: float64(12), object(1)\nmemory usage: 2.0+ KB\n\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\1650738614.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n지역만 object로 바뀌고 나머지는 float(???)으로 잘 들어간 모습."
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#b.-시각화-1-전국-출생아-수의-시각화",
    "href": "2023_DV/Review/B4. Plotly 시작.html#b.-시각화-1-전국-출생아-수의-시각화",
    "title": "Plotly : pandas backend",
    "section": "### B. 시각화 1 : 전국 출생아 수의 시각화",
    "text": "### B. 시각화 1 : 전국 출생아 수의 시각화\n- 전국으로 따로 집계가 되어있긴 하지만, 실습을 위해(?) 따로 집계를 해서 산출해보도록 하자.\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\2390378994.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n지역\n연도\n출생아 수\n\n\n\n\n0\n서울\n2010\n93266.0\n\n\n1\n서울\n2011\n91526.0\n\n\n2\n서울\n2012\n93914.0\n\n\n3\n서울\n2013\n84066.0\n\n\n4\n서울\n2014\n83711.0\n\n\n...\n...\n...\n...\n\n\n199\n제주\n2017\n5037.0\n\n\n200\n제주\n2018\n4781.0\n\n\n201\n제주\n2019\n4500.0\n\n\n202\n제주\n2020\n4000.0\n\n\n203\n제주\n2021\n3728.0\n\n\n\n\n204 rows × 3 columns\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby(by = '연도').aggregate({'출생아 수' : 'sum'}).reset_index()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\4269128748.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n연도\n출생아 수\n\n\n\n\n0\n2010\n449949.242\n\n\n1\n2011\n445527.398\n\n\n2\n2012\n457813.448\n\n\n3\n2013\n417845.628\n\n\n4\n2014\n323378.169\n\n\n5\n2015\n438420.000\n\n\n6\n2016\n330782.536\n\n\n7\n2017\n358771.000\n\n\n8\n2018\n321845.000\n\n\n9\n2019\n165941.871\n\n\n10\n2020\n272300.000\n\n\n11\n2021\n260832.000\n\n\n\n\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').applymap(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby(by = '연도').aggregate({'출생아 수' : 'sum'}).reset_index()\\\n.plot.line(x = '연도', y = '출생아 수', backend = 'plotly')\n\n## 그래프 표기에 한글이 너무나도 잘 나오기 때문에 자신감 있게 한글로 바꿔도 된다!!!\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_144\\2678187976.py:1: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n                                                \n\n\n\n출생아 수가 팍팍 튀는 구간이 있네용…\n\n\n시각화 2 : 시도별 출생아 수 시각화(line)\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').map(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.pivot_table(index = ['지역', '연도'], values = '출생아 수', aggfunc = 'sum').reset_index()  ## 이미 윗줄만으로 충분해...\n\n\n\n\n\n\n\n\n지역\n연도\n출생아 수\n\n\n\n\n0\n강원\n2010\n12477.0\n\n\n1\n강원\n2011\n12408.0\n\n\n2\n강원\n2012\n12426.0\n\n\n3\n강원\n2013\n10980.0\n\n\n4\n강원\n2014\n10662.0\n\n\n...\n...\n...\n...\n\n\n199\n충북\n2017\n11394.0\n\n\n200\n충북\n2018\n10586.0\n\n\n201\n충북\n2019\n9333.0\n\n\n202\n충북\n2020\n8600.0\n\n\n203\n충북\n2021\n8190.0\n\n\n\n\n204 rows × 3 columns\n\n\n\n\ndf.rename({'지역/연도[6]' : '지역'}, axis = 1).set_index('지역').map(lambda x : float(x) if x != '-' else 0).reset_index()\\\n.drop(17, axis = 0)\\\n.set_index('지역').stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.plot.line(x = '연도', y = '출생아 수', color = '지역', backend = 'plotly')\n\n\n                                                \n\n\n\n경기와 서울이 특정 년도에서 현저히 줄어드는 것을 알 수 있다.\n\n- 두 가지 line plot(전체, 지역별) 각각의 장단점이 있다;; 그 둘을 합쳐서 볼 순 없을까??"
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#d.-시각화-3-시도별-출생아-수-시각화area",
    "href": "2023_DV/Review/B4. Plotly 시작.html#d.-시각화-3-시도별-출생아-수-시각화area",
    "title": "Plotly : pandas backend",
    "section": "### D. 시각화 3 : 시도별 출생아 수 시각화(area)",
    "text": "### D. 시각화 3 : 시도별 출생아 수 시각화(area)\nplot.area()\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역']).map(lambda x: float(x) if not x=='-' else 0)\\\n.drop('전국',axis=0)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.area(x='연도',y='출생아수',color='지역',backend='plotly')   ## plot.area()\n\n\n                                                \n\n\n\n각각 어떻게 변화했는지와, 그 누적의 변화 정도를 시각화할 수 있다.\n\n- 그래서 자꾸 넘어가는 게, 2014 경기, 2016 서울, 2019 서울ㆍ경기 : 출생아가 없었음. 왜 없었어???\n\ndf\n\n\n\n\n\n\n\n\n지역/연도[6]\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\n0\n서울\n93266\n91526\n93914.000\n84066.000\n83711.000\n83005\n75.536\n65389\n58074\n53.673\n47400\n45531\n\n\n1\n부산\n27415\n27759\n28673.000\n25831.000\n26190.000\n26645\n24906.000\n21480\n19152\n17049.000\n15100\n14446\n\n\n2\n대구\n20557\n20758\n21472.000\n19340.000\n19361.000\n19438\n18298.000\n15946\n14400\n13233.000\n11200\n10661\n\n\n3\n인천\n25752\n20758\n21472.000\n25560.000\n25786.000\n25491\n23609.000\n20445\n20087\n18522.000\n16000\n14947\n\n\n4\n광주\n13979\n13916\n14392.000\n12729.000\n12729.000\n12441\n11580.000\n10120\n9105\n8364.000\n7300\n7956\n\n\n5\n대전\n14314\n14808\n15279.000\n14099.000\n13962.000\n13774\n12436.000\n10851\n9337\n8410.000\n7500\n7414\n\n\n6\n울산\n11432\n11542\n12160.000\n11330.000\n11556.000\n11732\n10910.000\n9381\n8149\n7539.000\n6600\n6127\n\n\n7\n세종\n-\n-\n1054.000\n1111.000\n1344.000\n2708\n3297.000\n3504\n3703\n3819.000\n3500\n3570\n\n\n8\n경기\n121753\n122027\n124746.000\n112129.000\n112.169\n113495\n105643.000\n94088\n83198\n83.198\n77800\n76139\n\n\n9\n강원\n12477\n12408\n12426.000\n10980.000\n10662.000\n10929\n10058.000\n9958\n8351\n8283.000\n7800\n7357\n\n\n10\n충북\n14670\n14804\n15139.000\n13658.000\n13366.000\n13563\n12742.000\n11394\n10586\n9333.000\n8600\n8190\n\n\n11\n충남\n20.242\n20.398\n20.448\n18.628\n18200.000\n18604\n17302.000\n15670\n14380\n13228.000\n11900\n10984\n\n\n12\n전북\n16100\n16175\n16238.000\n14555.000\n14231.000\n14087\n12698.000\n11348\n10001\n8971.000\n8200\n7745\n\n\n13\n전남\n16654\n16612\n16990.000\n15401.000\n14817.000\n15061\n13980.000\n12354\n11238\n10832.000\n9700\n8430\n\n\n14\n경북\n23700\n24250\n24635.000\n22206.000\n22062.000\n22310\n20616.000\n17957\n16079\n14472.000\n12900\n12045\n\n\n15\n경남\n32203\n32536\n33211.000\n29504.000\n29763.000\n29537\n27138.000\n23849\n21224\n19250.000\n16800\n15562\n\n\n16\n제주\n5657\n5628\n5992.000\n5328.000\n5526.000\n5600\n5494.000\n5037\n4781\n4500.000\n4000\n3728\n\n\n17\n전국\n470171\n471265\n484550.000\n436455.000\n435435.000\n438420\n406243.000\n357771\n326822\n302676.000\n272400\n260562\n\n\n\n\n\n\n\n\n경기 2014의 경우 인구수인데 소수점이 존재…(솔로몬???), 서울 2016… 등등. 사람이 쓰다보니 오탈자가 있다.\n\n\nE. 위에서의 시각화 수정\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)  ## 소수점이 존재할 경우 그대로, 아닐 경우 1000을 곱해줌.\n\n\n\n\n\n\n\n\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n지역\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n서울\n93266.0\n91526.0\n93914.0\n84066.0\n83711.0\n83005.0\n75536.0\n65389.0\n58074.0\n53673.0\n47400.0\n45531.0\n\n\n부산\n27415.0\n27759.0\n28673.0\n25831.0\n26190.0\n26645.0\n24906.0\n21480.0\n19152.0\n17049.0\n15100.0\n14446.0\n\n\n대구\n20557.0\n20758.0\n21472.0\n19340.0\n19361.0\n19438.0\n18298.0\n15946.0\n14400.0\n13233.0\n11200.0\n10661.0\n\n\n인천\n25752.0\n20758.0\n21472.0\n25560.0\n25786.0\n25491.0\n23609.0\n20445.0\n20087.0\n18522.0\n16000.0\n14947.0\n\n\n광주\n13979.0\n13916.0\n14392.0\n12729.0\n12729.0\n12441.0\n11580.0\n10120.0\n9105.0\n8364.0\n7300.0\n7956.0\n\n\n대전\n14314.0\n14808.0\n15279.0\n14099.0\n13962.0\n13774.0\n12436.0\n10851.0\n9337.0\n8410.0\n7500.0\n7414.0\n\n\n울산\n11432.0\n11542.0\n12160.0\n11330.0\n11556.0\n11732.0\n10910.0\n9381.0\n8149.0\n7539.0\n6600.0\n6127.0\n\n\n세종\n0.0\n0.0\n1054.0\n1111.0\n1344.0\n2708.0\n3297.0\n3504.0\n3703.0\n3819.0\n3500.0\n3570.0\n\n\n경기\n121753.0\n122027.0\n124746.0\n112129.0\n112169.0\n113495.0\n105643.0\n94088.0\n83198.0\n83198.0\n77800.0\n76139.0\n\n\n강원\n12477.0\n12408.0\n12426.0\n10980.0\n10662.0\n10929.0\n10058.0\n9958.0\n8351.0\n8283.0\n7800.0\n7357.0\n\n\n충북\n14670.0\n14804.0\n15139.0\n13658.0\n13366.0\n13563.0\n12742.0\n11394.0\n10586.0\n9333.0\n8600.0\n8190.0\n\n\n충남\n20242.0\n20398.0\n20448.0\n18628.0\n18200.0\n18604.0\n17302.0\n15670.0\n14380.0\n13228.0\n11900.0\n10984.0\n\n\n전북\n16100.0\n16175.0\n16238.0\n14555.0\n14231.0\n14087.0\n12698.0\n11348.0\n10001.0\n8971.0\n8200.0\n7745.0\n\n\n전남\n16654.0\n16612.0\n16990.0\n15401.0\n14817.0\n15061.0\n13980.0\n12354.0\n11238.0\n10832.0\n9700.0\n8430.0\n\n\n경북\n23700.0\n24250.0\n24635.0\n22206.0\n22062.0\n22310.0\n20616.0\n17957.0\n16079.0\n14472.0\n12900.0\n12045.0\n\n\n경남\n32203.0\n32536.0\n33211.0\n29504.0\n29763.0\n29537.0\n27138.0\n23849.0\n21224.0\n19250.0\n16800.0\n15562.0\n\n\n제주\n5657.0\n5628.0\n5992.0\n5328.0\n5526.0\n5600.0\n5494.0\n5037.0\n4781.0\n4500.0\n4000.0\n3728.0\n\n\n전국\n470171.0\n471265.0\n484550.0\n436455.0\n435435.0\n438420.0\n406243.0\n357771.0\n326822.0\n302676.0\n272400.0\n260562.0\n\n\n\n\n\n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)\\\n.stack().reset_index().rename({'level_1' : '연도', 0 : '출생아 수'}, axis = 1)\\\n.groupby('연도').aggregate({'출생아 수' : 'sum'}).reset_index()\\\n.plot.line(x = '연도', y = '출생아 수', backend = 'plotly')\n\n\n                                                \n\n\n\n사실 튀는 구간 따위는 없었다!!!(그치만 줄어들고 있는건 사실이었네…)\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.line(x = '연도', y = '출생아수', color = '지역', backend = 'plotly')\n\n\n                                                \n\n\n\ndf.rename({'지역/연도[6]':'지역'},axis=1)\\\n.set_index(['지역'])\\\n.map(lambda x: float(x) if not x=='-' else 0)\\\n.map(lambda x : x if x%1 == 0 else x*1000)\\\n.stack().reset_index().set_axis(['지역','연도','출생아수'],axis=1)\\\n.plot.area(x = '연도', y = '출생아수', color = '지역', backend = 'plotly')\n\n\n                                                \n\n\n\n정상적으로 이해할 수 있다."
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#여러가지-플랏",
    "href": "2023_DV/Review/B4. Plotly 시작.html#여러가지-플랏",
    "title": "Plotly : pandas backend",
    "section": "4. 여러가지 플랏",
    "text": "4. 여러가지 플랏\n\nplotly는 bar, line, scatter, hist, box, area형태의 플랏을 지원한다.(pie나 그런 것들은 지원안함!)"
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#a.-.plot.bar",
    "href": "2023_DV/Review/B4. Plotly 시작.html#a.-.plot.bar",
    "title": "Plotly : pandas backend",
    "section": "### A. .plot.bar()",
    "text": "### A. .plot.bar()\n- 예제 1 : 성별 별 합격률 시각화\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).reset_index().melt(id_vars='index').set_axis(['department','gender','result','count'],axis=1)\ndf  ## 파일에 index_column이 존재하고, 첫 행이 열이름인듯.\n\n\n\n\n\n\n\n\ndepartment\ngender\nresult\ncount\n\n\n\n\n0\nA\nmale\nfail\n314\n\n\n1\nB\nmale\nfail\n208\n\n\n2\nC\nmale\nfail\n204\n\n\n3\nD\nmale\nfail\n279\n\n\n4\nE\nmale\nfail\n137\n\n\n5\nF\nmale\nfail\n149\n\n\n6\nA\nmale\npass\n511\n\n\n7\nB\nmale\npass\n352\n\n\n8\nC\nmale\npass\n121\n\n\n9\nD\nmale\npass\n138\n\n\n10\nE\nmale\npass\n54\n\n\n11\nF\nmale\npass\n224\n\n\n12\nA\nfemale\nfail\n19\n\n\n13\nB\nfemale\nfail\n7\n\n\n14\nC\nfemale\nfail\n391\n\n\n15\nD\nfemale\nfail\n244\n\n\n16\nE\nfemale\nfail\n299\n\n\n17\nF\nfemale\nfail\n103\n\n\n18\nA\nfemale\npass\n89\n\n\n19\nB\nfemale\npass\n18\n\n\n20\nC\nfemale\npass\n202\n\n\n21\nD\nfemale\npass\n131\n\n\n22\nE\nfemale\npass\n94\n\n\n23\nF\nfemale\npass\n238\n\n\n\n\n\n\n\n\n어디서 많이 봤던 데이터, 집단 간 비교니까 바차트가 좋겠죠잉\n\n\ndf.pivot_table(index = 'gender', columns = 'result', values = 'count', aggfunc = 'sum')\\\n.assign(rate = lambda _df : _df['pass']/(_df.fail + _df['pass'])).reset_index()\\\n.assign(rate = lambda _df : _df.rate.apply(lambda x : round(x, 3)))\\\n.plot.bar(x = 'gender', y = 'rate', color = 'gender', text = 'rate', width = 600)\n\n\n                                                \n\n\n# 예제 2 : (성별, 학과) 별 지원자 수 시각화\n\ndf.pivot_table(index = ['gender', 'department'], columns = 'result', values = 'count', aggfunc = 'sum')\\\n.assign(rate = lambda _df : _df['pass']/(_df.fail + _df['pass'])).reset_index()\\\n.assign(rate = lambda _df : _df.rate.apply(lambda x : round(x, 2)))\\\n.plot.bar(x = 'gender', y = 'rate', color = 'gender', facet_col = 'department', text = 'rate', width = 800)\n\n\n                                                \n\n\n\nB. .plot.line()\n\n# 예제 1 : 핸드폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\n시계열 자료니까 라인차트로 그리면 좋겠다잉\n\n\ndf.melt(id_vars = ['Date']).set_axis(['날짜', '회사', '판매량'], axis = 1)\\\n.plot.line(x = '날짜', y = '판매량', color = '회사')  ## 어떻게 날짜로 잘 읽었네."
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#c.-.plot.scatter",
    "href": "2023_DV/Review/B4. Plotly 시작.html#c.-.plot.scatter",
    "title": "Plotly : pandas backend",
    "section": "### C. .plot.scatter()",
    "text": "### C. .plot.scatter()\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\\\n.loc[:,lambda df: df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda df: df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda df: df.Wage.str[1:].str.replace('K','000').astype(int))\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n가슴아픈 데이터, 연속형 변수 간 관계를 보고 싶으니 산점도가 적당\n\n\ndf.query('Position == \"DEFENDER\" or Position == \"FORWARD\"')\\\n.plot.scatter(x = 'ShotPower', y = 'StandingTackle',\n             color = 'Position', size = 'Wage', hover_data = ['Name', 'Age'],\n             opacity = 0.5, width = 800)  ## hover_data로 마우스를 가져다 대었을 때 표기되는 정보를 알 수 있으며, alpha대신 opacity 옵션으로 투명도 조절\n\n\n                                                \n\n\n\nD. .plot.box()\n\n# 예제 1 : 렛츠고! 전북고등학교!\n\ny1=[75,75,76,76,77,77,78,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,79,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들\n\n\ndf = pd.DataFrame({\n    'Class' : ['A']*len(y1) + ['B']*len(y2),\n    'Score' : y1+y2\n})\ndf\n\n\n\n\n\n\n\n\nClass\nScore\n\n\n\n\n0\nA\n75\n\n\n1\nA\n75\n\n\n2\nA\n76\n\n\n3\nA\n76\n\n\n4\nA\n77\n\n\n5\nA\n77\n\n\n6\nA\n78\n\n\n7\nA\n79\n\n\n8\nA\n79\n\n\n9\nA\n98\n\n\n10\nB\n76\n\n\n11\nB\n76\n\n\n12\nB\n77\n\n\n13\nB\n77\n\n\n14\nB\n78\n\n\n15\nB\n78\n\n\n16\nB\n79\n\n\n17\nB\n80\n\n\n18\nB\n80\n\n\n19\nB\n81\n\n\n\n\n\n\n\n\ndf.plot.box(x = 'Class', y = 'Score', color = 'Class',\n            points = 'all', width = 500, backend = 'plotly')  ## points를 'outlier'로 설정하면 \n\n\n                                                \n\n\n# 예제 2 : (년도, 시도) 별 전기 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_12860\\2652719362.py:9: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.plot.box(x = '시도', y = '에너지사용량(TOE)/전기', color = '시도', facet_row = '년도', width = 800, height = 1000, hover_data = ['지역', '연면적'], points = 'outliers')  ## outliers가 디폴트임"
  },
  {
    "objectID": "2023_DV/Review/B4. Plotly 시작.html#e.-.plot.hist",
    "href": "2023_DV/Review/B4. Plotly 시작.html#e.-.plot.hist",
    "title": "Plotly : pandas backend",
    "section": "### E. .plot.hist()",
    "text": "### E. .plot.hist()\n# 예제 1 : 타이타닉 (연령, 성별) 별 생존자\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n2.564949\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n3.401197\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n3.154870\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n3.401197\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n2.047693\n\n\n\n\n891 rows × 13 columns\n\n\n\n\ndf.Age.hist()\n\n\n                                                \n\n\n\n시리즈를 통해 아무 입력 없이 히스토그램을 그릴 수도 있다.\n\n\ndf.plot.hist(x = 'Age', color = 'Sex',\n            facet_row = 'Sex', facet_col = 'Survived')\n\n\n                                                \n\n\n\n성별 간 효과가 저연령에서는 옅어지는 것이 보인다.(생존률로 시각화해도 좋을듯)\n\n\ndf.plot.hist(x = 'Age', color = 'Survived', facet_col = 'Sex')\n\n\n                                                \n\n\n\ndf.loc[:, ['Age', 'Sex', 'Survived']].assign(Age_cut = lambda _df : pd.qcut(_df.Age, q = 10))\\\n.pivot_table(index = 'Age_cut', columns = 'Sex', values = 'Survived', aggfunc = 'mean')\\\n.stack().reset_index().rename({0 : 'Rate'}, axis = 1)\\\n.plot.bar(x = 'Sex', y = 'Rate', color = 'Sex', facet_col = 'Age_cut', width = 800)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotly\\express\\_core.py:2044: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n                                                \n\n\n\n세부조정은 알아서 하시길…\n\n\nF. .plot.area()\n\n# 예제 1 : 핸드폰 판매량\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\ndf.melt(id_vars = 'Date').set_axis(['날짜', '회사', '판매량'], axis = 1)\\\n.plot.area(x = '날짜', y = '판매량', color = '회사')\n\n\n                                                \n\n\n\n전체적인 판매량과 개별 판매량의 정도를 알 수 있음. (두 플랏을 한 번에!)\n\n# 예제 2 : 에너지 사용량\n\nurl = 'https://raw.githubusercontent.com/guebin/DV2022/main/posts/Energy/{}.csv'\nprov = ['Seoul', 'Busan', 'Daegu', 'Incheon',\n        'Gwangju', 'Daejeon', 'Ulsan', 'Sejongsi',\n        'Gyeonggi-do', 'Gangwon-do', 'Chungcheongbuk-do',\n        'Chungcheongnam-do', 'Jeollabuk-do', 'Jeollanam-do',\n        'Gyeongsangbuk-do', 'Gyeongsangnam-do', 'Jeju-do']\ndf = pd.concat([pd.read_csv(url.format(p+y)).assign(년도=y, 시도=p) for p in prov for y in ['2018', '2019', '2020', '2021']]).reset_index(drop=True)\\\n.assign(년도 = lambda df: df.년도.astype(int))\\\n.set_index(['년도','시도','지역']).applymap(lambda x: int(str(x).replace(',','')))\\\n.reset_index()\ndf.head()\n\nC:\\Users\\hollyriver\\AppData\\Local\\Temp\\ipykernel_12860\\2652719362.py:9: FutureWarning:\n\nDataFrame.applymap has been deprecated. Use DataFrame.map instead.\n\n\n\n\n\n\n\n\n\n\n년도\n시도\n지역\n건물동수\n연면적\n에너지사용량(TOE)/전기\n에너지사용량(TOE)/도시가스\n에너지사용량(TOE)/지역난방\n\n\n\n\n0\n2018\nSeoul\n종로구\n17929\n9141777\n64818\n82015\n111\n\n\n1\n2018\nSeoul\n중구\n10598\n10056233\n81672\n75260\n563\n\n\n2\n2018\nSeoul\n용산구\n17201\n10639652\n52659\n85220\n12043\n\n\n3\n2018\nSeoul\n성동구\n14180\n11631770\n60559\n107416\n0\n\n\n4\n2018\nSeoul\n광진구\n21520\n12054796\n70609\n130308\n0\n\n\n\n\n\n\n\n\ndf.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').apply(lambda x : x[-1]))\\\n.pivot_table(index = ['년도', '시도', '에너지종류'], values = '에너지사용량', aggfunc = 'sum')\\\n.reset_index().plot.area(x = '년도', y = '에너지사용량', facet_col = '에너지종류', color = '시도', width = 600)\n\n\n                                                \n\n\n\n## figure로 저장\nfig = df.set_index(['년도','시도','지역','건물동수','연면적']).stack().reset_index().rename({'level_5' : '에너지종류', 0 : '에너지사용량'}, axis = 1)\\\n.assign(에너지종류 = lambda _df : _df.에너지종류.str.split('/').str[-1])\\\n.pivot_table(index = ['에너지종류', '시도', '년도'], values = '에너지사용량', aggfunc = 'sum').reset_index()\\\n.plot.area(x = '년도', y = '에너지사용량', facet_col = '에너지종류', color = '시도', width = 600)\n\n## 아마도 xaxis의 범위를 한정하는 과정에서 xlabel의 범위가 조정되다보니 이리 되는듯.\nfig.update_layout(\n    xaxis_domain = [0.0, 0.25],\n    xaxis2_domain = [0.35, 0.60],\n    xaxis3_domain = [0.70, 0.95]\n)\n\n\n                                                \n\n\n\n겹칠땐 이렇게 하면 됩니다… 근데 좁아진 걸 보니 다르게 개선할 수도 있을 것 같기는 함…"
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html",
    "title": "Folium | 월드맵 시각화",
    "section": "",
    "text": "folium을 활용해 그리 대단하진 않지만, 엄청나보이는 차트를 만들어보자."
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#라이브러리-imports",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#라이브러리-imports",
    "title": "Folium | 월드맵 시각화",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np\nimport folium\nimport json\nimport requests"
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#코로플레스-맵choropleth-map",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#코로플레스-맵choropleth-map",
    "title": "Folium | 월드맵 시각화",
    "section": "2. 코로플레스 맵(Choropleth map)",
    "text": "2. 코로플레스 맵(Choropleth map)\n- 코로플레스 맵의 예시\n\n\n대충 정의하면, coropleth = polygon + y라고 볼 수 있다.(좌표묶음과 측정값)"
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#folium-기본",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#folium-기본",
    "title": "Folium | 월드맵 시각화",
    "section": "3. folium 기본",
    "text": "3. folium 기본\n- 개념\n\nMap Object를 생성(fig)\nMap Object에 이것저것 추가(geom 추가)\n\n\nA. folium.Map()\n\n# 예시 1 : 기본 월드맵\n\nm = folium.Map()\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n세계지도를 불러온다. GIS가 탑재된듯.\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [37, 127], zoom_start = 10\n)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n옵션을 지정하여 시작위치와 초기 줌 정도, 스크롤에 따른 줌인-아웃 기능 비활성화 여부 등을 변경할 수 있다.\n좌표 정보는 구글맵스에서 지역을 더블클릭하여 가져올 수 있다.\nzoom은 18이 최대이며, 그 이상은 18의 값으로 고정된다."
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#b.-.folium.marker",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#b.-.folium.marker",
    "title": "Folium | 월드맵 시각화",
    "section": "### B. .folium.Marker()",
    "text": "### B. .folium.Marker()\n# 예제 1 : Map에 Marker를 추가\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.846737, 127.129374], zoom_start = 18\n)\n\nmarker = folium.Marker(location = [35.846737, 127.129374]).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nMap개체에 좌표정보가 포함된 Marker를 추가하는 형태\n\n# 예제 2 : poligon, 통계학과 대학원생의 산책경로\n## 통계학과 대학원생의 산책경로..\n[35.8471, 127.1291]\n[35.8468, 127.1289]\n[35.84635, 127.1291]\n[35.84635, 127.1297]\n[35.8468, 127.12995]\n[35.8474, 127.1300]\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.8468,127.1294],  ## 분수대\n    zoom_start = 18\n)\n\nfolium.Marker(location = [35.8471, 127.1291]).add_to(m)\nfolium.Marker(location = [35.8468, 127.1289]).add_to(m)\nfolium.Marker(location = [35.84635, 127.1291]).add_to(m)\nfolium.Marker(location = [35.84635, 127.1297]).add_to(m)\nfolium.Marker(location = [35.8468, 127.12995]).add_to(m)\nfolium.Marker(location = [35.8474, 127.1300]).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nC. folium.Polygon()\n\n# 예제 1 폴리곤으로 한번에…\n\nm = folium.Map(\n    scrollWheelZoom=False,\n    location = [35.8468,127.1294], # 분수대\n    zoom_start=18,\n)\nfolium.Polygon(\n    locations = [[35.8471, 127.1291],\n                 [35.8468, 127.1289],\n                 [35.84635, 127.1291],\n                 [35.84635, 127.1297],\n                 [35.8468, 127.12995],\n                 [35.8474, 127.1300]],  ## 이중 리스트(2차원)\n    fill=True   ## 속까지 채워줌\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n# 예제 2 : 2개의 폴리곤을 추가(3중 리스트)\n\npoly = np.array([[35.8471, 127.1291],\n                 [35.8468, 127.1289],\n                 [35.84635, 127.1291],\n                 [35.84635, 127.1297],\n                 [35.8468, 127.12995],\n                 [35.8474, 127.1300]])\n\nlat, lon = poly.T\npoly2 = np.stack([lat, lon+0.001], axis = 1)\nnp.stack([poly, poly2], axis = 0)\n\narray([[[ 35.8471 , 127.1291 ],\n        [ 35.8468 , 127.1289 ],\n        [ 35.84635, 127.1291 ],\n        [ 35.84635, 127.1297 ],\n        [ 35.8468 , 127.12995],\n        [ 35.8474 , 127.13   ]],\n\n       [[ 35.8471 , 127.1301 ],\n        [ 35.8468 , 127.1299 ],\n        [ 35.84635, 127.1301 ],\n        [ 35.84635, 127.1307 ],\n        [ 35.8468 , 127.13095],\n        [ 35.8474 , 127.131  ]]])\n\n\n\nm = folium.Map(\n    scrollWheelZoom=False,\n    location = [35.8468,127.1294], # 분수대\n    zoom_start=18,\n)\n\nfolium.Polygon(\n    locations = np.stack([poly, poly2], axis = 0),\n    fill = True\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n삼중 리스트를 잘 넣는다면 코로플레스맵처럼 지도를 행정구역별로 나눌 수 있지 않을까??? \\(\\rightarrow\\) 근데 이 노가다를 다행히도 누군가 해놨다."
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#south-korea-github",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#south-korea-github",
    "title": "Folium | 월드맵 시각화",
    "section": "4. South Korea github",
    "text": "4. South Korea github"
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#a.-유저-소개",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#a.-유저-소개",
    "title": "Folium | 월드맵 시각화",
    "section": "### A. 유저 소개",
    "text": "### A. 유저 소개\n\nsouthkorea라는 깃허브 유저가 있는데, 이중 southkorea-maps라는 레포지토리에는…\nkostat/2018/json/이란 폴더가 있고, 아래의 파일들이 있음.\n\nskorea-municipalities-2018-geo.json # &lt;-- 이 파일에 관심있음.\nskorea-municipalities-2018-topo-simple.json\nskorea-municipalities-2018-topo.json\nskorea-provinces-2018-geo.json # &lt;-- 이 파일에 관심있음.\nskorea-provinces-2018-topo-simple.json\nskorea-provinces-2018-topo.json\nskorea-submunicipalities-2018-geo.json\nskorea-submunicipalities-2018-topo-simple.json\nskorea-submunicipalities-2018-topo.json\n이중 관심있는 아래의 두 파일\nskorea-municipalities-2018-geo.json\nskorea-provinces-2018-geo.json\n\n이게 행정구역을 의미하는 폴리곤을 노가다로 정리해 둔 것임.\n\n\nB. json 파일 다운로드\n\n\nglobal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json').text)\nlocal_dict = json.loads(requests.get('https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-municipalities-2018-geo.json').text)\n\n## json파일을 로드할 때 사용하는 코드. 그래서 json과 requests를 import했지! 보면 text로 가져온 것을 알 수 있다.\n\n\nglobal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nlocal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nglobal_dict['type'], global_dict['name'], global_dict['crs']\n\n('FeatureCollection',\n 'sido',\n {'type': 'name', 'properties': {'name': 'urn:ogc:def:crs:OGC:1.3:CRS84'}})\n\n\n\n각각 이러한 키값을 가지고 있는데… 위 셋은 딱히 지역이나 정보와 관련된 것은 없는 것 같다. 그러니까 얘네들은 뭔가 해야될 때 건드리지 않는 게 좋다."
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#c.-json-파일의-구조",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#c.-json-파일의-구조",
    "title": "Folium | 월드맵 시각화",
    "section": "### C. json 파일의 구조",
    "text": "### C. json 파일의 구조\n\n#global_dict['features']  ## 굉장히 긴 딕셔너리\n\n\n#global_dict['features'][0]  ## 첫 번째 행정구역의 정보(딕셔너리)\n\n\n#global_dict['features'][0]['geometry']  ## 첫번째 행정구역의 폴리곤 정보(또 딕셔너리...)\n\n\nglobal_dict['features'][0]['geometry']['coordinates']  ## 우리가 원하는 3중 리스트!!(스압주의)\n\n[[[127.02214829071995, 37.6997208220174],\n  [127.02531549179129, 37.69958052666555],\n  [127.0268980715665, 37.700251138801015],\n  [127.02702219174589, 37.70111540651664],\n  [127.02768734803148, 37.700937381678756],\n  [127.02899106854076, 37.69965827866671],\n  [127.02928557925306, 37.69929162349923],\n  [127.02960141563474, 37.69817519597225],\n  [127.02965442206329, 37.69780663241529],\n  [127.02972931239567, 37.69627243691011],\n  [127.03016631976621, 37.69519820573226],\n  [127.03097248066267, 37.693556837990705],\n  [127.0324142885382, 37.691839038874726],\n  [127.03577652734599, 37.69237420164817],\n  [127.03676285064714, 37.69263992022006],\n  [127.03790670521317, 37.69327193541787],\n  [127.03999748131439, 37.69468439213186],\n  [127.04110878733236, 37.69529978284375],\n  [127.0418126234525, 37.69538228560054],\n  [127.04307253543809, 37.69523090725608],\n  [127.04481711632289, 37.6929362514031],\n  [127.045094291979, 37.692385199610435],\n  [127.04862943554828, 37.69406226137355],\n  [127.04880746336237, 37.6932813039853],\n  [127.04975102873895, 37.68920713537357],\n  [127.04982002382117, 37.68797700395555],\n  [127.04998471076341, 37.68761035012039],\n  [127.05064314534968, 37.68650780387472],\n  [127.05092067338259, 37.686158552402],\n  [127.05108569439875, 37.686055160768575],\n  [127.05136797709446, 37.685921581506264],\n  [127.0517981363431, 37.6858148464868],\n  [127.05180416049991, 37.68706675968194],\n  [127.05206063036638, 37.687282882739005],\n  [127.05214429018166, 37.687352392956214],\n  [127.05222706432502, 37.68742077669241],\n  [127.05469544726837, 37.68872271744571],\n  [127.05610252968587, 37.68920437206933],\n  [127.05817261245335, 37.689669926579896],\n  [127.05839034056493, 37.689685868260845],\n  [127.05966013532905, 37.69033198280304],\n  [127.06220905346507, 37.6928052648238],\n  [127.06236779518714, 37.693024235595644],\n  [127.06237354393551, 37.693135159327554],\n  [127.06239478737797, 37.693546495381035],\n  [127.06260174913736, 37.69424379234776],\n  [127.06273214287815, 37.694675910354626],\n  [127.06305928833302, 37.694799063961284],\n  [127.0633863131918, 37.694921883623124],\n  [127.06630814019205, 37.69444172108839],\n  [127.06854802331812, 37.69400040092273],\n  [127.0691509700167, 37.69368950125125],\n  [127.0708788225368, 37.693700048667225],\n  [127.07087906859705, 37.693700062181264],\n  [127.07265760034454, 37.69379950124051],\n  [127.07313786558306, 37.69397574643172],\n  [127.07339823304241, 37.69410087667406],\n  [127.07378269914281, 37.69449368954549],\n  [127.07393007343649, 37.69464423236862],\n  [127.07448636301702, 37.695009628645295],\n  [127.07490680588823, 37.69521968047455],\n  [127.0781962787199, 37.695986510360626],\n  [127.08110478631578, 37.696137457947785],\n  [127.08384328792428, 37.69426889016338],\n  [127.08387832248333, 37.69279212349614],\n  [127.08390501861447, 37.69178245187781],\n  [127.08503096598021, 37.69054991893467],\n  [127.08517719556258, 37.69038984858866],\n  [127.08845357891715, 37.68990263950211],\n  [127.09055188620752, 37.68957025151863],\n  [127.09238424524798, 37.689933184218404],\n  [127.09303672225575, 37.689966466063815],\n  [127.09509714278617, 37.68938485003124],\n  [127.09599596058983, 37.68907104690658],\n  [127.09611844498184, 37.68777746180145],\n  [127.09629522668848, 37.687169760670066],\n  [127.09642514740469, 37.68626418153678],\n  [127.09642311415676, 37.685637441138354],\n  [127.09580983621922, 37.68464039310604],\n  [127.09428757876928, 37.683418538095395],\n  [127.09291905210979, 37.6815515024995],\n  [127.09194656333966, 37.67919029908044],\n  [127.09194153856279, 37.67869251579676],\n  [127.09218959069077, 37.67803010766824],\n  [127.09247959854748, 37.67764303460199],\n  [127.092614356114, 37.677474561385274],\n  [127.0939627740268, 37.675617232268024],\n  [127.09644725778618, 37.66968910695227],\n  [127.0962373655161, 37.66882941453695],\n  [127.09497929397519, 37.66760002717652],\n  [127.09458063193003, 37.666172300680316],\n  [127.0946800144686, 37.66492127931861],\n  [127.09541063101342, 37.663889644590746],\n  [127.09414211770347, 37.66329882532109],\n  [127.09217357563625, 37.66079254873924],\n  [127.09120691221572, 37.659334556715166],\n  [127.09113228726125, 37.658243024751805],\n  [127.09157431798847, 37.657772436849626],\n  [127.09235164609564, 37.65549186279581],\n  [127.09286741474956, 37.654453659625155],\n  [127.09356843248374, 37.653318729010714],\n  [127.0939157426287, 37.65296173000377],\n  [127.09400788402047, 37.65272853044388],\n  [127.09404075508206, 37.65254014144715],\n  [127.09403529660045, 37.652418519610805],\n  [127.09399991212685, 37.652295226453205],\n  [127.09326587472968, 37.65061772855914],\n  [127.09246587944706, 37.64970976752811],\n  [127.0926944997674, 37.64857999351487],\n  [127.09438136905902, 37.64495591656824],\n  [127.09456996976148, 37.644570600805835],\n  [127.09755891099569, 37.64396483085298],\n  [127.09890806335117, 37.6440343984249],\n  [127.10159264246101, 37.64477235815439],\n  [127.10280013821811, 37.64520492267967],\n  [127.1028823886103, 37.64540897697184],\n  [127.10386737257883, 37.64548852460528],\n  [127.10411721532803, 37.64548033184725],\n  [127.10657637866768, 37.64537730972521],\n  [127.10769224246286, 37.64496664676661],\n  [127.10786269687772, 37.64469282241332],\n  [127.10928861344723, 37.64285888886917],\n  [127.10989832951653, 37.64248752395126],\n  [127.11125728398851, 37.64237533392683],\n  [127.11142380118652, 37.64208686345102],\n  [127.11178551212458, 37.64073538398819],\n  [127.11113544340976, 37.639943933494294],\n  [127.11090335225738, 37.638262475561504],\n  [127.11155819260641, 37.63803293665921],\n  [127.11163126044404, 37.63797762400025],\n  [127.11163252401063, 37.63797657539423],\n  [127.1116335605215, 37.63797538002365],\n  [127.11163434149125, 37.637974066637646],\n  [127.11248319338027, 37.63648912158805],\n  [127.11220344878163, 37.63264276723643],\n  [127.1116323201253, 37.631503284414286],\n  [127.111371172448, 37.631227883211594],\n  [127.11082549334053, 37.630781845942806],\n  [127.10888476082644, 37.62930151924797],\n  [127.10596351597135, 37.62733070951351],\n  [127.10432790803095, 37.62327070428881],\n  [127.10406621959581, 37.62165311388996],\n  [127.10490909745644, 37.62156734430207],\n  [127.10538285122183, 37.62090189640023],\n  [127.1056619895515, 37.6204162469155],\n  [127.10555591433918, 37.62037072519622],\n  [127.11130504706806, 37.62069184201602],\n  [127.11214286452346, 37.620307017764965],\n  [127.11502106963971, 37.61953760329908],\n  [127.11619333227809, 37.61894778380488],\n  [127.11725242287622, 37.61674187052708],\n  [127.11699384437024, 37.616455741862225],\n  [127.11687861029367, 37.61632493344969],\n  [127.11667194632817, 37.61601542319483],\n  [127.11670352036289, 37.61571891606276],\n  [127.11679649663584, 37.614954676556906],\n  [127.11708589521979, 37.61396022396543],\n  [127.11737381655063, 37.61224525812902],\n  [127.1172872225345, 37.611180500418115],\n  [127.11687784452862, 37.609530700257416],\n  [127.11669806436957, 37.60884925145508],\n  [127.11809512146557, 37.60547242425238],\n  [127.11809557127602, 37.60544383142748],\n  [127.1180703665001, 37.604938459230944],\n  [127.11804701133192, 37.60460229805868],\n  [127.11558445031194, 37.601718190575795],\n  [127.11409999472093, 37.60012853384475],\n  [127.11447064798934, 37.599054605066726],\n  [127.11548924282775, 37.59765428196658],\n  [127.11572670052439, 37.597314488762194],\n  [127.11644313933067, 37.59619235770955],\n  [127.11687748428929, 37.59549705077813],\n  [127.11666448808528, 37.594016545777],\n  [127.11569009390017, 37.59362528500345],\n  [127.11552687588426, 37.593560121449116],\n  [127.11548546147779, 37.593548610635445],\n  [127.11541325076173, 37.593530949622995],\n  [127.1150282067279, 37.59349020815677],\n  [127.11445400975263, 37.59343304169271],\n  [127.1142683890762, 37.593416046920936],\n  [127.11366535163782, 37.593356363149674],\n  [127.11333737292749, 37.593259818471644],\n  [127.11241171452632, 37.59185636470543],\n  [127.1101545340761, 37.58743032790922],\n  [127.109944225286, 37.58595741365269],\n  [127.10973285600105, 37.58524413527232],\n  [127.109376495115, 37.58409232681838],\n  [127.10896665929798, 37.58337922727966],\n  [127.1074133022487, 37.58249485444903],\n  [127.10525566107893, 37.58156565577623],\n  [127.10344046907751, 37.58059529802311],\n  [127.1031362307321, 37.58021462295139],\n  [127.10289434724014, 37.57990679295722],\n  [127.10290869795878, 37.57978627410625],\n  [127.10302529385626, 37.578906983060165],\n  [127.10231819140941, 37.57804309180647],\n  [127.10195757011611, 37.577552361766436],\n  [127.10114351387524, 37.5760709289373],\n  [127.10091888215331, 37.574204101343454],\n  [127.10088786785404, 37.57376349428088],\n  [127.10166464576407, 37.57240065940598],\n  [127.10312974060275, 37.572279426510896],\n  [127.10366088935352, 37.57191791188504],\n  [127.10367642912719, 37.57190732992747],\n  [127.10423020073478, 37.571387655728294],\n  [127.10385348290774, 37.5704937526072],\n  [127.10272467888672, 37.56706421441915],\n  [127.10167082530147, 37.56340484757312],\n  [127.10120468941865, 37.56157990217243],\n  [127.10116259287368, 37.561313298804556],\n  [127.10112209698956, 37.56105064062629],\n  [127.10113406515579, 37.561003040117356],\n  [127.10127747386707, 37.5604659926051],\n  [127.10139885421326, 37.56025021143431],\n  [127.10154158761175, 37.55999837150702],\n  [127.10169403565163, 37.55974146071346],\n  [127.10180078997355, 37.55956709194242],\n  [127.10185922758541, 37.55949382793663],\n  [127.10216726500754, 37.55927810196558],\n  [127.10492928845024, 37.556421585556336],\n  [127.10540370353208, 37.556335336136144],\n  [127.1054039797918, 37.556335335256705],\n  [127.1060404466977, 37.55636800208074],\n  [127.10639619161522, 37.55646172320076],\n  [127.10714458157308, 37.55710610125959],\n  [127.10998660563237, 37.55834211535795],\n  [127.11167162292871, 37.55872797074785],\n  [127.11296064655822, 37.558794346205815],\n  [127.11522556193233, 37.55676018833242],\n  [127.11564563403175, 37.55769987998584],\n  [127.11730331913004, 37.559425641957304],\n  [127.11736706467859, 37.55947936004973],\n  [127.12295557080333, 37.56348990218229],\n  [127.12807612442845, 37.56590930955854],\n  [127.12861049420515, 37.56616043544185],\n  [127.13087184119757, 37.56687794943424],\n  [127.13375309980111, 37.56779331042001],\n  [127.1342663695222, 37.56795394418317],\n  [127.13474102736858, 37.56802464198976],\n  [127.13766579387027, 37.568424209460574],\n  [127.14560001479188, 37.568431169360885],\n  [127.1489495440468, 37.56843431854424],\n  [127.15124335260579, 37.569795301642884],\n  [127.15319599288145, 37.57096985010494],\n  [127.1549810601263, 37.57204353719504],\n  [127.15689916857828, 37.572954115769065],\n  [127.16084531250948, 37.575652662972885],\n  [127.1614681179371, 37.57604621474144],\n  [127.16200668497996, 37.57638652892725],\n  [127.16255503715276, 37.576730120691614],\n  [127.16674716243453, 37.578975666143656],\n  [127.1686475082068, 37.57899747794731],\n  [127.1686477364252, 37.57899709190847],\n  [127.17025143915063, 37.579016497229034],\n  [127.17183744214854, 37.57926199037442],\n  [127.17434341757382, 37.58002897085931],\n  [127.1745646848708, 37.58009734641977],\n  [127.17507432498863, 37.58025539696954],\n  [127.17510636726472, 37.58026661233748],\n  [127.17565914212791, 37.58056537625033],\n  [127.17585094748326, 37.580669273596094],\n  [127.17601761614299, 37.58076165809506],\n  [127.17637876684248, 37.58095906115711],\n  [127.17651851047951, 37.58103431126574],\n  [127.17715482863876, 37.58120032803303],\n  [127.17714288110588, 37.579858265364955],\n  [127.17706037157764, 37.579544339856604],\n  [127.17698860354076, 37.579416335558484],\n  [127.1769358967764, 37.57934067705333],\n  [127.17688336105913, 37.579265297509494],\n  [127.17674096763471, 37.57911572057686],\n  [127.17668903282423, 37.57906934265248],\n  [127.17559141963727, 37.57845491709445],\n  [127.17544227008392, 37.57836278564491],\n  [127.17541541545809, 37.57829863026236],\n  [127.17540428164837, 37.5782080868865],\n  [127.17540423462499, 37.57820770097899],\n  [127.17536098677607, 37.57784747971302],\n  [127.17532131705451, 37.577363342392985],\n  [127.17532434111249, 37.57729990460691],\n  [127.17532755327359, 37.57723513694318],\n  [127.17568308790581, 37.57489869852943],\n  [127.17570202935693, 37.57483166387621],\n  [127.1761038656408, 37.57413533821314],\n  [127.17613152481712, 37.57408856274329],\n  [127.1763084845669, 37.57397427178013],\n  [127.1764980410476, 37.573801389354294],\n  [127.17694363989052, 37.57329364424182],\n  [127.17773113162198, 37.57218791533225],\n  [127.17781926600982, 37.57205770863517],\n  [127.17913286165616, 37.56912369470184],\n  [127.17914903579462, 37.569085099484376],\n  [127.17920371766037, 37.56894649028298],\n  [127.17922017374693, 37.568873819289145],\n  [127.1793312074897, 37.56826577378065],\n  [127.17937794706329, 37.56798582974494],\n  [127.17943875427594, 37.56760084598443],\n  [127.17946467033734, 37.566583016887506],\n  [127.1801941808996, 37.56458787465877],\n  [127.18099458554735, 37.56332274641403],\n  [127.18118573247567, 37.5630169666324],\n  [127.18127812880661, 37.562830429531225],\n  [127.18151410938351, 37.5622675112996],\n  [127.18190992016416, 37.56124879949777],\n  [127.18199542158489, 37.56099244596784],\n  [127.18204081632643, 37.56040549758009],\n  [127.18202084650781, 37.55994782824462],\n  [127.18202079834414, 37.55994743873002],\n  [127.18198419478763, 37.55965551058688],\n  [127.181787374278, 37.55882911648127],\n  [127.18165662833978, 37.557998302305066],\n  [127.18135146515526, 37.553258048765485],\n  [127.1813403273481, 37.55296496478539],\n  [127.18142169421999, 37.552757324685345],\n  [127.18159477250546, 37.552592350538426],\n  [127.18170384487837, 37.552507995663994],\n  [127.182926745995, 37.55023535046519],\n  [127.18284450852022, 37.5494290906802],\n  [127.1826736926933, 37.547747320441374],\n  [127.18265679525105, 37.54752440129038],\n  [127.18276107519915, 37.546499426131824],\n  [127.18279599113094, 37.54638983933193],\n  [127.18353917154177, 37.54517039495601],\n  [127.18169282664165, 37.546444749492956],\n  [127.18125643812643, 37.54657971583249],\n  [127.17991053630526, 37.54657443190896],\n  [127.17933128170148, 37.54656742148427],\n  [127.17608601659725, 37.54561946546925],\n  [127.17571742052262, 37.54520770406822],\n  [127.17538324376515, 37.54520199786245],\n  [127.17463760452904, 37.54525912602242],\n  [127.17447709977255, 37.545277660809106],\n  [127.1743146633164, 37.54529732395536],\n  [127.17390569685948, 37.545600032434535],\n  [127.17157169030703, 37.545385773320305],\n  [127.1651681782903, 37.544631798899346],\n  [127.1631627309092, 37.54499101097656],\n  [127.16009589899915, 37.54151453079874],\n  [127.15958338038794, 37.54102822393515],\n  [127.15671195122711, 37.53757774554748],\n  [127.15603276390642, 37.53734241425835],\n  [127.15402132902574, 37.534514497157076],\n  [127.15356680162624, 37.533665193070505],\n  [127.15374715099493, 37.53160008846154],\n  [127.15353910604473, 37.53043671974301],\n  [127.15323227758938, 37.52923709895612],\n  [127.15316377616772, 37.529108804926516],\n  [127.14947629508237, 37.52502054146565],\n  [127.14779743367326, 37.52214947334571],\n  [127.14593249219926, 37.521955801698766],\n  [127.14578093977661, 37.521941375307804],\n  [127.14480065945018, 37.51938973774546],\n  [127.14509193634692, 37.51683494851859],\n  [127.14532885964844, 37.516607158321115],\n  [127.14538965833438, 37.51651332725961],\n  [127.14543404002671, 37.51642852537821],\n  [127.14544427334503, 37.51633278212625],\n  [127.14543916612115, 37.51606474208284],\n  [127.14485430106286, 37.51567802502981],\n  [127.14475438292835, 37.515645484717986],\n  [127.14460355182592, 37.515606249070395],\n  [127.1433741159653, 37.51557760277298],\n  [127.14210518808247, 37.51466010401819],\n  [127.14023773642688, 37.50985352338447],\n  [127.13996581242264, 37.50864088493049],\n  [127.14001384551528, 37.50852453676255],\n  [127.14032052088919, 37.508273591129395],\n  [127.14135345423192, 37.50694790778293],\n  [127.14147949264665, 37.50677882190354],\n  [127.14151665030897, 37.506708103021836],\n  [127.14151309866016, 37.50670219734203],\n  [127.14126852582045, 37.50641867264958],\n  [127.14117786173351, 37.50631882301717],\n  [127.1409971469675, 37.506163894207944],\n  [127.14081284937815, 37.50597856844039],\n  [127.14113191237908, 37.50546377662881],\n  [127.14223298799321, 37.504884417170004],\n  [127.14342402698134, 37.50441363645751],\n  [127.14392479947718, 37.50440092444333],\n  [127.14410326532739, 37.5043964831474],\n  [127.14550250136601, 37.50330570568455],\n  [127.14595206598852, 37.5032268868512],\n  [127.14769492141562, 37.50321206574927],\n  [127.14860243574782, 37.50372282317089],\n  [127.1490451129403, 37.50400073373371],\n  [127.15082434872058, 37.50406156817608],\n  [127.15165372960253, 37.503395752670144],\n  [127.1521091406241, 37.50297029811745],\n  [127.15323083173114, 37.50267941781355],\n  [127.15643795942692, 37.50185900576393],\n  [127.1565550135419, 37.50189404497823],\n  [127.15772923925586, 37.50317895196306],\n  [127.1588661178982, 37.50239019657405],\n  [127.16050427114095, 37.501094512438115],\n  [127.16121949144342, 37.50039527422741],\n  [127.16139030513257, 37.5002010498375],\n  [127.16143108867095, 37.50002247972045],\n  [127.16142016695896, 37.49970545993054],\n  [127.16135443347227, 37.499537459426215],\n  [127.16125092773778, 37.49939089854903],\n  [127.16110733163866, 37.49927532319294],\n  [127.15986640646732, 37.495706834034365],\n  [127.15996541511976, 37.49527986027531],\n  [127.16001826923471, 37.494707646707646],\n  [127.16002621602986, 37.49437455452455],\n  [127.1597454988479, 37.49363667082564],\n  [127.15970484975863, 37.49354690503217],\n  [127.15965196338816, 37.493434358962624],\n  [127.1595169833914, 37.49327460353943],\n  [127.15931845606401, 37.49313662914927],\n  [127.15764946367052, 37.490182673876625],\n  [127.15763380331077, 37.4901717886922],\n  [127.15740206846664, 37.48908854265012],\n  [127.15043020214158, 37.4852488932469],\n  [127.15021754524786, 37.48510537954825],\n  [127.14992170977906, 37.484905729020184],\n  [127.14868305306886, 37.484043192462515],\n  [127.148579027746, 37.483889863822014],\n  [127.14822520162613, 37.48336825471925],\n  [127.1477703794897, 37.48262416785541],\n  [127.14766978415436, 37.48242576594467],\n  [127.14760689985782, 37.48226916996039],\n  [127.14758794988897, 37.482221953125226],\n  [127.14752532591987, 37.48201386379114],\n  [127.14748230260066, 37.48180264116629],\n  [127.14747861659086, 37.481773480024934],\n  [127.14745907220343, 37.48158945671828],\n  [127.14739684802731, 37.480585692947166],\n  [127.14728211415827, 37.478739628360124],\n  [127.14715916521881, 37.47729511320696],\n  [127.14714533170121, 37.47722179897961],\n  [127.14584352066795, 37.477273068635235],\n  [127.145796878535, 37.47727491723534],\n  [127.14575646485959, 37.47727651768649],\n  [127.1443704748871, 37.47733114309174],\n  [127.14364342913493, 37.47525741763574],\n  [127.14361885800092, 37.47517065376269],\n  [127.14360185852455, 37.475082746354325],\n  [127.14359251006233, 37.47499412650607],\n  [127.1435733636672, 37.47468530404496],\n  [127.14356722898532, 37.474586308344385],\n  [127.14352658956736, 37.473930418522116],\n  [127.1394881494977, 37.474089616658645],\n  [127.13833405279085, 37.47413506191677],\n  [127.13766388419639, 37.474147919140066],\n  [127.13684251991336, 37.47415318717528],\n  [127.13677761017136, 37.47415548714804],\n  [127.1363969537117, 37.47417048239313],\n  [127.13546828445813, 37.47424021251808],\n  [127.1352873110148, 37.47425705042134],\n  [127.13312242962854, 37.4745798598],\n  [127.13285605187727, 37.47462645866824],\n  [127.13262354347293, 37.472395357539355],\n  [127.13282098466541, 37.46919569189179],\n  [127.13282095574344, 37.4691942063499],\n  [127.1327996032295, 37.468393664276086],\n  [127.13087554387387, 37.46775756923912],\n  [127.13083001167365, 37.46774525764664],\n  [127.13078324278253, 37.46773607352042],\n  [127.13073584632706, 37.46773009808275],\n  [127.13068790133586, 37.46772735681814],\n  [127.13063981486039, 37.46772786001192],\n  [127.13059198261257, 37.46773162422421],\n  [127.1305447100502, 37.46773862246795],\n  [127.13018892567416, 37.4678033697074],\n  [127.12933858733388, 37.467959210768726],\n  [127.12930302575272, 37.467966655370866],\n  [127.1266812987817, 37.468621775910755],\n  [127.12644686764371, 37.468750108913085],\n  [127.12602455140079, 37.4691058782371],\n  [127.12570131939023, 37.46931823345992],\n  [127.12551817321834, 37.46943048351345],\n  [127.12537453641595, 37.46951707676626],\n  [127.12531488674534, 37.46955289992616],\n  [127.12520301623321, 37.46961974560557],\n  [127.12510091762285, 37.469613652978204],\n  [127.12494334079366, 37.4696031234281],\n  [127.12487834302604, 37.469597279961135],\n  [127.12485136253184, 37.469523255232446],\n  [127.1248975970592, 37.46938552451682],\n  [127.12504582809731, 37.468979630189054],\n  [127.12516820516782, 37.46875144004594],\n  [127.1251820284445, 37.468352452585854],\n  [127.12519937283692, 37.467833231267484],\n  [127.12498822426089, 37.46724835864308],\n  [127.12482726884033, 37.46700159707097],\n  [127.12475930593294, 37.466916352653705],\n  [127.1244880965488, 37.4666435226522],\n  [127.12420970259667, 37.466517104307265],\n  [127.12420688119097, 37.46652048627629],\n  [127.12414326804209, 37.466495215148015],\n  [127.1237669717385, 37.46633342476096],\n  [127.12320225935616, 37.46597867005132],\n  [127.12185946757938, 37.46513395264438],\n  [127.11747485026633, 37.46220061045074],\n  [127.11747077309776, 37.462194983538076],\n  [127.11711994009005, 37.46167753791065],\n  [127.11698929578043, 37.46148450859329],\n  [127.11693965937852, 37.46090030945957],\n  [127.11689967841377, 37.4604011395989],\n  [127.1168948964395, 37.45864042153465],\n  [127.11669702911543, 37.45862325785094],\n  [127.11623941228295, 37.4585961143597],\n  [127.11589291262797, 37.45859194472567],\n  [127.11549606922648, 37.45869791695614],\n  [127.11547664981921, 37.458704404910726],\n  [127.11395536575394, 37.45951676647907],\n  [127.1136342600001, 37.45973246531095],\n  [127.11353635596203, 37.45980154776803],\n  [127.11341975702304, 37.45990076930371],\n  [127.11276328611841, 37.46053025046058],\n  [127.11290865142644, 37.46064192929295],\n  [127.11318486414389, 37.46075823127989],\n  [127.11329182742885, 37.46083640990825],\n  [127.11333484694086, 37.46090535023642],\n  [127.11333484534424, 37.46090566030155],\n  [127.11333387525471, 37.46108724567093],\n  [127.1122298035669, 37.46150499909431],\n  [127.11182027181094, 37.46164419121139],\n  [127.10868615916183, 37.4621564211832],\n  [127.10688007579863, 37.462369502165004],\n  [127.10667208681129, 37.46241051180783],\n  [127.10646723536935, 37.46242392987725],\n  [127.10616731423701, 37.462406454624805],\n  [127.10434135861443, 37.46217434553484],\n  [127.10447230410983, 37.46102952693268],\n  [127.10394160853244, 37.460050593707535],\n  [127.10381093870869, 37.459927656301566],\n  [127.09930610906709, 37.45667755614937],\n  [127.09823746163788, 37.456371246967336],\n  [127.09522278311739, 37.45639422742016],\n  [127.09431215563166, 37.45622477015542],\n  [127.09377352285104, 37.45607736437658],\n  [127.09354036220854, 37.455888055203964],\n  [127.08882127538375, 37.44974941955953],\n  [127.08843214262606, 37.44899484298624],\n  [127.08832682104673, 37.44862776075761],\n  [127.08832676671251, 37.44862736036078],\n  [127.08836930483001, 37.44724005811312],\n  [127.08816128515241, 37.44538498355397],\n  [127.08785507296487, 37.44489387071301],\n  [127.08777816075089, 37.44487515146447],\n  [127.08730388244146, 37.44464171021886],\n  [127.082904590745, 37.442173808672536],\n  [127.08243181881421, 37.441579468344386],\n  [127.08209733924632, 37.44135078689038],\n  [127.08144410364704, 37.441230999973904],\n  [127.08017160102241, 37.441051093956524],\n  [127.07908335144711, 37.44129874337731],\n  [127.07817890647885, 37.44155161511739],\n  [127.07760188111618, 37.44170909672176],\n  [127.07601057612212, 37.44213893609793],\n  [127.07475598627195, 37.442218846646846],\n  [127.0744976898156, 37.44223280045726],\n  [127.07224980537977, 37.442199811541435],\n  [127.07186137425094, 37.44102970185969],\n  [127.07221171709271, 37.439382901540675],\n  [127.07376019338288, 37.437789433842276],\n  [127.07379537723341, 37.43765707199904],\n  [127.07384351098383, 37.43740786000359],\n  [127.07356006757476, 37.436963727095595],\n  [127.0730242304579, 37.43640683285397],\n  [127.0709099786286, 37.43309491379506],\n  [127.06961542955781, 37.430611692111725],\n  [127.06569800697741, 37.42915810241767],\n  [127.06114498957349, 37.42998528229182],\n  [127.05978112053222, 37.4295872657291],\n  [127.05345832004603, 37.42880858729806],\n  [127.0503615218898, 37.42982129068007],\n  [127.04957975136693, 37.4302701432368],\n  [127.04736923949098, 37.430701820429285],\n  [127.04722475567219, 37.432040442932944],\n  [127.04644166111552, 37.43299046057295],\n  [127.04590560884242, 37.43348142450798],\n  [127.04418573191268, 37.4350554539952],\n  [127.04109031701444, 37.437769124917864],\n  [127.04087975167322, 37.437878441821276],\n  [127.04048048713241, 37.43808242753594],\n  [127.04005277677203, 37.43823714507124],\n  [127.03956828753749, 37.438195909531515],\n  [127.03922186091357, 37.43816448538617],\n  [127.03903241753622, 37.43815131864844],\n  [127.03706776525239, 37.43827496782511],\n  [127.03697772841022, 37.43829639401592],\n  [127.03557372599246, 37.439004097845434],\n  [127.03532026694523, 37.439966564131865],\n  [127.03529507001076, 37.44007215137235],\n  [127.03520801081689, 37.440437652690015],\n  [127.03516906728176, 37.4406057551936],\n  [127.03517186217657, 37.44094194757337],\n  [127.03528263969284, 37.44103476257073],\n  [127.0360722545635, 37.44155099433042],\n  [127.03608762765222, 37.44156253268867],\n  [127.03643099253159, 37.441832734480634],\n  [127.03665445379359, 37.442080160034884],\n  [127.03748294023711, 37.44326670427758],\n  [127.03793486290014, 37.44417883691202],\n  [127.03803531906024, 37.444516404127626],\n  [127.03818353178544, 37.44501754331141],\n  [127.03823039750196, 37.44518026909942],\n  [127.03820566265372, 37.4458588524557],\n  [127.03781196836518, 37.44892239759809],\n  [127.03777449207378, 37.449201719792974],\n  [127.03774440117567, 37.44941881885998],\n  [127.03696804004046, 37.450691157558936],\n  [127.03640970411149, 37.45149687786112],\n  [127.03577642448006, 37.452195909168545],\n  [127.03626740809199, 37.45426216962762],\n  [127.03712770180721, 37.45520768138208],\n  [127.03706742872744, 37.45547434330387],\n  [127.0367406508785, 37.456443867860756],\n  [127.03492422096939, 37.46017540450221],\n  [127.03453266613107, 37.46333522021434],\n  [127.0346601133906, 37.463976592257936],\n  [127.03467091241455, 37.464090458173466],\n  [127.03460314189319, 37.464173939752115],\n  [127.0331405892598, 37.46501738910679],\n  [127.03275757043367, 37.46519318169501],\n  [127.03192176568467, 37.465518883534436],\n  [127.03119554711617, 37.465626061203594],\n  [127.03055508280168, 37.46551416328518],\n  [127.03039682015434, 37.46548632407364],\n  [127.02992820559366, 37.46535158628146],\n  [127.02958338917796, 37.4642881854868],\n  [127.02873126984468, 37.46202826888523],\n  [127.02815696813059, 37.460621993714916],\n  [127.02634678296988, 37.458104359241986],\n  [127.02620458102675, 37.4579878159816],\n  [127.02599541833983, 37.4578169581426],\n  [127.02543481238588, 37.45756450281699],\n  [127.025277616827, 37.4574938687448],\n  [127.02150295539224, 37.45622583459711],\n  [127.01969201000381, 37.45578660642505],\n  [127.01557484735889, 37.454916014104],\n  [127.0150649965327, 37.45483188354524],\n  [127.0148735746752, 37.454841752950394],\n  [127.01450961789199, 37.45486122860588],\n  [127.0107251186711, 37.45577244670041],\n  [127.00923267412264, 37.45722091033481],\n  [127.00704092418567, 37.460215469835134],\n  [127.00496875820613, 37.463186899888576],\n  [127.0044696722443, 37.46407327656836],\n  [127.00476218583773, 37.46492302457033],\n  [127.00490546550544, 37.46584260252905],\n  [127.00452135791603, 37.46679064066523],\n  [127.00424565849995, 37.46714344410636],\n  [127.00367535892853, 37.467720386588596],\n  [127.00273482189448, 37.46712461108707],\n  [127.00196279723106, 37.467087449562925],\n  [127.00047686511023, 37.46706294825534],\n  [126.99742674489028, 37.46724223517637],\n  [126.99717099538091, 37.467214070232885],\n  [126.99698554452699, 37.46717971600085],\n  [126.99675187621652, 37.467072433303954],\n  [126.99628192995874, 37.46665064295208],\n  [126.99719359563045, 37.464184165933666],\n  [126.99725312948726, 37.46401973479701],\n  [126.99737077162193, 37.46368749255192],\n  [126.99676862985268, 37.461873352884574],\n  [126.9942899320261, 37.461421317893326],\n  [126.99315292901082, 37.46127623511324],\n  [126.9916230699418, 37.46044156303114],\n  [126.98999453840017, 37.4594629780069],\n  [126.98863550944588, 37.458165845862126],\n  [126.98678106962474, 37.45740326454205],\n  [126.98238921265964, 37.45591710269429],\n  [126.97775711976738, 37.455199116892956],\n  [126.97457956131598, 37.454412875477026],\n  [126.97175282603096, 37.45173733547508],\n  [126.97056939304684, 37.44945270098554],\n  [126.96959356922594, 37.44909597687449],\n  [126.96866171986065, 37.44875446455986],\n  [126.967662979455, 37.44838054424455],\n  [126.96428976779832, 37.44626723711946],\n  [126.9639418980544, 37.4452458892869],\n  [126.96393361850694, 37.445214631166266],\n  [126.96432082073919, 37.44346229477368],\n  [126.96463404099109, 37.44203964730629],\n  [126.9638033064139, 37.44078692549053],\n  [126.96373604591848, 37.44074688094471],\n  [126.96311396054732, 37.44037652510398],\n  [126.96298353095135, 37.44029960855669],\n  [126.96294292915762, 37.44028045754587],\n  [126.95899886229834, 37.43912694137169],\n  [126.95636819846372, 37.43875401134443],\n  [126.94928410957398, 37.438250273989894],\n  [126.94815539463404, 37.43863037558223],\n  [126.9456762727248, 37.43730530031926],\n  [126.9450906377534, 37.437088221251784],\n  [126.94022016050745, 37.43571237211725],\n  [126.9393643055451, 37.43602581803675],\n  [126.93857452478132, 37.43642042942645],\n  [126.93774638275855, 37.4373857576777],\n  [126.93727268446924, 37.4386322720711],\n  [126.93725777476023, 37.43892734300138],\n  [126.93724856528063, 37.439167511901296],\n  [126.93724711351074, 37.43921284001876],\n  [126.93740216615096, 37.43937426800926],\n  [126.93769358300823, 37.43968442708137],\n  [126.937713857788, 37.43972075748547],\n  [126.93786405751602, 37.44019753476193],\n  [126.9377458076725, 37.440354020623566],\n  [126.93735357677096, 37.440871598648926],\n  [126.93668810996004, 37.44169594611689],\n  [126.93664692478835, 37.441745758401595],\n  [126.9348979978904, 37.443209481797815],\n  [126.9344508816705, 37.44325822606353],\n  [126.93055407014207, 37.44546848090059],\n  [126.93051744833456, 37.445548423023716],\n  [126.9303939359546, 37.44581837200393],\n  [126.93032633869163, 37.4459686910352],\n  [126.93016316403606, 37.4463779831893],\n  [126.93035671199829, 37.44733422226026],\n  [126.92839869373803, 37.450212485076534],\n  [126.9283570058542, 37.44989868951273],\n  [126.9283079026723, 37.44954867104572],\n  [126.9282796601699, 37.4493526845579],\n  [126.92399602716448, 37.446210224228935],\n  [126.92325327636715, 37.44577274628548],\n  [126.92289722984206, 37.445173349643994],\n  [126.92294662379582, 37.44487295199244],\n  [126.9229034040661, 37.444673007921715],\n  [126.92276280882136, 37.44404108321409],\n  [126.9220070215115, 37.443250757259776],\n  [126.92069895346346, 37.44151996843528],\n  [126.91866310009635, 37.43998262017672],\n  [126.91538923818742, 37.43966802735873],\n  [126.91389137436857, 37.43930708270779],\n  [126.91318980966076, 37.43909539212985],\n  [126.91230851419965, 37.43857946078951],\n  [126.91143164721281, 37.43762316839669],\n  [126.91120280868347, 37.43720121439982],\n  [126.91120000044674, 37.437187408644185],\n  [126.91121640637944, 37.4370463602423],\n  [126.91128427742785, 37.43654325238066],\n  [126.9113332747712, 37.4361713431048],\n  [126.91002834759448, 37.43432412191302],\n  [126.90990399563053, 37.43422744640446],\n  [126.90967840112272, 37.43405973794322],\n  [126.90940570432362, 37.43386468248096],\n  [126.90939564142113, 37.43386044896999],\n  [126.90859344653741, 37.433691729024154],\n  [126.90752668322209, 37.43362021944715],\n  [126.90663449172253, 37.43359694340164],\n  [126.90582547214655, 37.43399615235654],\n  [126.90582519408719, 37.43399615186017],\n  [126.90522457694115, 37.433997069199144],\n  [126.90298757638999, 37.434067620178844],\n  [126.90186087029541, 37.436397524817494],\n  [126.89914461684712, 37.43866819351907],\n  [126.89897791331421, 37.43870182579147],\n  [126.89958451332427, 37.44036666498753],\n  [126.8982570102593, 37.44320652479412],\n  [126.89578235329736, 37.44563339801652],\n  [126.89564123458763, 37.44829320383536],\n  [126.8946038195621, 37.451159154897915],\n  [126.89398300003057, 37.45271733991926],\n  [126.89182284732061, 37.45228350409839],\n  [126.8910486017461, 37.45224245321772],\n  [126.88978012488182, 37.45227307520445],\n  [126.88964213513057, 37.45231546387733],\n  [126.88956055650301, 37.45243222840465],\n  [126.88952054982705, 37.45262112176969],\n  [126.88950365869798, 37.4527010699145],\n  [126.88953577253639, 37.452946634786336],\n  [126.88964770418707, 37.45331924221095],\n  [126.88966648039558, 37.453521707197815],\n  [126.88965276996692, 37.45359855847977],\n  [126.889592047684, 37.45370296489857],\n  [126.88876417143807, 37.45476564219567],\n  [126.88828498935065, 37.45516865405293],\n  [126.8863169029548, 37.45628901611006],\n  [126.88626468031083, 37.456362515796506],\n  [126.88618548846422, 37.456522645677246],\n  [126.88588075182155, 37.45750839016862],\n  [126.8853564731724, 37.45953456857106],\n  [126.88534006096899, 37.45963704166101],\n  [126.88534960239055, 37.45986258161532],\n  [126.88539936108317, 37.46000916976716],\n  [126.88543071619748, 37.46009095992281],\n  [126.88544367776255, 37.46012453265269],\n  [126.88551853776474, 37.46025525216233],\n  [126.885650375358, 37.46043079769516],\n  [126.88607198185638, 37.460859466843345],\n  [126.886128084154, 37.460899787651186],\n  [126.88620134657874, 37.46091646478202],\n  [126.88625378849405, 37.46092666112505],\n  [126.88747359786943, 37.4610655269108],\n  [126.88749655889849, 37.46106301666018],\n  [126.88768203443473, 37.46103729223392],\n  [126.88785888520992, 37.460997475157],\n  [126.88795325015386, 37.46095448958872],\n  [126.8883358698455, 37.46078084535764],\n  [126.88863061689209, 37.46078817357594],\n  [126.88876919238663, 37.46083250900126],\n  [126.88887551772156, 37.460946920406975],\n  [126.88853249141253, 37.46133205164132],\n  [126.88810791069389, 37.461806645801666],\n  [126.88712180563425, 37.462904361685275],\n  [126.88704339763547, 37.46290231182216],\n  [126.88692384788376, 37.46289178120236],\n  [126.8866437804029, 37.46286673117086],\n  [126.88624491347673, 37.46279820029081],\n  [126.88485800472498, 37.46254315202208],\n  [126.88285845675665, 37.46426078649855],\n  [126.88457429409604, 37.46564823572887],\n  [126.88399444693059, 37.46666861463424],\n  [126.88153185190849, 37.46929115820678],\n  [126.88020447481996, 37.47093497073136],\n  [126.87987759363838, 37.4714213189637],\n  [126.87978452249763, 37.471539058789205],\n  [126.87955796413078, 37.47182967743504],\n  [126.87896535335604, 37.47259125928221],\n  [126.87873771587651, 37.4728841366197],\n  [126.87845148258745, 37.47325521436153],\n  [126.87801141557026, 37.47382773802628],\n  [126.87758404673444, 37.47451796201532],\n  [126.87614354086422, 37.47684468784561],\n  [126.87598571313652, 37.47710383526691],\n  [126.87532045038951, 37.478372692072554],\n  [126.87527082721985, 37.47846810142021],\n  [126.87495854231086, 37.47907199778555],\n  [126.87428469764085, 37.480380256044086],\n  [126.87413691190955, 37.480757112415716],\n  [126.87337901472617, 37.48243356052865],\n  [126.8727442969783, 37.482424995238794],\n  [126.87277987743597, 37.482794985827105],\n  [126.87278663474126, 37.48348878210105],\n  [126.8717971958008, 37.48499743368912],\n  [126.87176262543268, 37.485269390497095],\n  [126.87244469888192, 37.48611004050176],\n  [126.87267382382166, 37.486220095922036],\n  [126.87273456256477, 37.486240154483085],\n  [126.87289532395594, 37.486251871041624],\n  [126.87304222324997, 37.486197970915136],\n  [126.87313717688521, 37.486151898173034],\n  [126.87455644442305, 37.48536674989791],\n  [126.8749436128769, 37.485699557362956],\n  [126.87626185595225, 37.48723955123044],\n  [126.8766887354242, 37.4878526882681],\n  [126.87676002659364, 37.48811743166551],\n  [126.87680025485933, 37.48835707247939],\n  [126.87679035662485, 37.48857077467621],\n  [126.87666836775517, 37.488731698886916],\n  [126.87638215065101, 37.488941163019874],\n  [126.87604346872821, 37.489047234427446],\n  [126.8758168244155, 37.48902812727765],\n  [126.87471601721661, 37.48857729667705],\n  [126.87307774871593, 37.48825285739754],\n  [126.87295540342764, 37.48830058919606],\n  [126.87292252364195, 37.48831350594885],\n  [126.87274847564402, 37.488430729714324],\n  [126.87266911360857, 37.48855396331768],\n  [126.87241371655864, 37.48933079643515],\n  [126.8724404006732, 37.48953552394017],\n  [126.87244516069696, 37.48954200832903],\n  [126.87267624752327, 37.48985141040301],\n  [126.87277059260437, 37.4899587830918],\n  [126.87283922397087, 37.490014335559685],\n  [126.87336843875514, 37.490386572775556],\n  [126.87357693309845, 37.490481684306566],\n  [126.87375273922521, 37.490475679558024],\n  [126.8739332173435, 37.490428855349435],\n  [126.87402067996526, 37.490316033035164],\n  [126.87409064170969, 37.49021447138926],\n  [126.87414915935193, 37.48998055733015],\n  [126.87411047167724, 37.48977722861176],\n  [126.87432244123868, 37.48969551876888],\n  [126.874579194672, 37.48976928586574],\n  [126.87465124038863, 37.48989887276378],\n  [126.8748395606655, 37.49074546038744],\n  [126.87453459446087, 37.4910683660723],\n  [126.87437268165921, 37.49122136264536],\n  [126.87413007357357, 37.491337668238565],\n  [126.87395185020526, 37.49130790942899],\n  [126.87225463784726, 37.490602443243475],\n  [126.8715772336594, 37.490209200924156],\n  [126.8715477661439, 37.49008528105915],\n  [126.8715056835045, 37.489998794005494],\n  [126.8713423702246, 37.48983362446104],\n  [126.87125961324185, 37.48977074005055],\n  [126.87108027252202, 37.48968325006296],\n  [126.87094589711485, 37.48963721815786],\n  [126.87065821481464, 37.48956087360729],\n  [126.87042098603534, 37.48953583190591],\n  [126.8703347531482, 37.489544746411035],\n  [126.87022812471946, 37.48959615733273],\n  [126.87017041588264, 37.489665641527296],\n  [126.87013621956086, 37.48971909518386],\n  [126.86987690273945, 37.490296013812674],\n  [126.86936626773986, 37.49201860453653],\n  [126.86936892693092, 37.49271744174477],\n  [126.86937342007144, 37.49277207332995],\n  [126.86950267732847, 37.4943284018372],\n  [126.86758862651642, 37.49481588988599],\n  [126.86761040833085, 37.49468809006178],\n  [126.86619406839125, 37.49253860319941],\n  [126.86451819511568, 37.491203026084236],\n  [126.86302813884646, 37.49087608634184],\n  [126.8628110904766, 37.490828236574096],\n  [126.8627337568918, 37.490804494216775],\n  [126.86258684437263, 37.49075871779353],\n  [126.86238768631601, 37.49067907388865],\n  [126.86217497834068, 37.49057746081463],\n  [126.86208955762203, 37.49053119281495],\n  [126.86195048469753, 37.490447969739456],\n  [126.86183032272321, 37.490367864585316],\n  [126.86176820935094, 37.490326119958326],\n  [126.86162303620054, 37.49020374479842],\n  [126.861481767076, 37.490067301502876],\n  [126.86138442651429, 37.489968076240665],\n  [126.86111908480686, 37.48966817910177],\n  [126.86102581461058, 37.48955826657109],\n  [126.86101648222878, 37.489547269155715],\n  [126.86068610548242, 37.48916817605563],\n  [126.86018595731015, 37.48859544427551],\n  [126.85925601817621, 37.48753114874059],\n  [126.8585450417844, 37.48672107555025],\n  [126.85822542932799, 37.486360288148504],\n  [126.85811613511434, 37.48623739417916],\n  [126.85797103348565, 37.48608461582477],\n  [126.85783430290624, 37.485983086909044],\n  [126.85763012230208, 37.485867113767235],\n  [126.85748360128834, 37.48580696465288],\n  [126.85744917364828, 37.48579453432356],\n  [126.8572904242174, 37.48576055327166],\n  [126.85721499765847, 37.48575426248749],\n  [126.85708640467882, 37.48574368934753],\n  [126.85676809597386, 37.485721054345184],\n  [126.85655188308645, 37.48570671540534],\n  [126.85653942998213, 37.48570626540688],\n  [126.85349858643376, 37.482621834752926],\n  [126.85270046935516, 37.481818616044556],\n  [126.85261962553658, 37.481786686082536],\n  [126.85258624296549, 37.48177876052663],\n  [126.85206621905958, 37.48166492424135],\n  [126.851751885319, 37.48160370879597],\n  [126.85166783206732, 37.48158868170002],\n  [126.85161377601116, 37.481583540277235],\n  [126.85099047011765, 37.481527845736494],\n  [126.85084315206339, 37.48152259001683],\n  [126.85070182006966, 37.48152550894454],\n  [126.85057300416253, 37.48153941821166],\n  [126.85041837279233, 37.481566818390434],\n  [126.8493324275512, 37.48184331552959],\n  [126.84698612892635, 37.48189443252598],\n  [126.8466017682575, 37.48167122516655],\n  [126.8465568595493, 37.4816333299397],\n  [126.84651717642556, 37.48159893200954],\n  [126.84642038315994, 37.48150701100949],\n  [126.84640821872374, 37.481495453254205],\n  [126.84626305142321, 37.48130323673433],\n  [126.84621918975228, 37.481240109255275],\n  [126.84604937012155, 37.480925377389326],\n  [126.84600724781465, 37.480798339922416],\n  [126.84598678074923, 37.480695539848334],\n  [126.845976830455, 37.48064175143651],\n  [126.8459342194971, 37.4803147980667],\n  [126.8454343751056, 37.47446358358097],\n  [126.84536592690858, 37.47385949919483],\n  [126.84536055354526, 37.47381246555105],\n  [126.84503323897347, 37.47355891752733],\n  [126.84419346893853, 37.47401499804191],\n  [126.84395991392603, 37.474482912719694],\n  [126.84311200124603, 37.47486984773489],\n  [126.8430773430002, 37.4748849961075],\n  [126.84295255134103, 37.47492481992884],\n  [126.84278163936918, 37.47497161415357],\n  [126.8384196728936, 37.47536790968778],\n  [126.83825923862072, 37.475374710633375],\n  [126.83610875738073, 37.474675546160455],\n  [126.83461359077172, 37.47487025950699],\n  [126.83305167563579, 37.47724609250874],\n  [126.8320789692601, 37.477567839756105],\n  [126.8317480949545, 37.47765099536511],\n  [126.83080142694622, 37.47742382313479],\n  [126.83045751616729, 37.47733913587807],\n  [126.82962156724689, 37.476895297577286],\n  [126.82955329242202, 37.47662461753497],\n  [126.8295367634638, 37.476595599289965],\n  [126.82920434453489, 37.47619529596771],\n  [126.82818195209177, 37.47603415313276],\n  [126.82787204443441, 37.4759892067256],\n  [126.82410830374371, 37.476352189043084],\n  [126.8195759381957, 37.47635542376256],\n  [126.8194233714162, 37.47632759999209],\n  [126.81930759652315, 37.47628940790379],\n  [126.81929647232971, 37.47628516514027],\n  [126.81914316116901, 37.476205532322275],\n  [126.81832133653424, 37.475298753226035],\n  [126.81766855225743, 37.473238400442405],\n  [126.81465604938846, 37.47476019094166],\n  [126.81472979756464, 37.475057306435204],\n  [126.81496487102987, 37.47586011888277],\n  [126.81522179414826, 37.476251063198596],\n  [126.81529553572993, 37.47636211907153],\n  [126.81641141908979, 37.477536748815474],\n  [126.81706064565485, 37.478024854451064],\n  [126.81721035612148, 37.478135181855585],\n  [126.81732392065949, 37.478139295676314],\n  ...]]\n\n\n\n값이 굉장히 긴 것 같지만 위의 것들에 비하면 새발의 피다… 그래도 체감은 해봐야지(coordinate : 좌표계라는 뜻)\n\n\nnp.array(global_dict['features'][0]['geometry']['coordinates']).shape\n\n(1, 1498, 2)\n\n\n\n서울의 다각형을 의미하는 것 같음!(확인해보자)\n\n\nlon, lat = np.array(global_dict['features'][0]['geometry']['coordinates'])[:,:,:].T\n\n\nlon, lat = np.array(global_dict['features'][0]['geometry']['coordinates'])[0,:,:].T\nnp.stack([lat, lon], axis = 1).shape\n\n(1498, 2)\n\n\n\nnp.stack([lat, lon], axis = 1).shape\n\n(1498, 2, 1)\n\n\n\n위도-경도로 저장된 것을 경도-위도로 바꿔줬다.(folium에선 이렇게 인식하니까…)\n\n- 이게 어떤 원리나면…\n\n[0, :, :] 첫번째 원소만 선택 &gt; 멀티 리스트를 깨줌\nT &gt; 전치시켜서 두 개의 리스트로 만듦\nlon, lat에 각자 저장함(순서를 바꾸기 위함)\nnp.stack([lat, lon], axis = 1) &gt; 열로 스택을 쌓아줌(차원을 늘림)\n\n\nm = folium.Map(\n    location = [37.55,127],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Polygon(\n    locations = np.stack([lat,lon],axis=1).tolist(),    ## tolist()는 안해도 됨\n    fill = True\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n일상언어에선 (latitude, longitude) 순서로 표현하는 경우가 다반수이다. 위도, 경도로 나타내지만, 컴퓨터 언어에서 표현의 혼동을 줄이기 위해 \\(x\\), \\(y\\)축에 대응하여 (longitude, latitude)로 표현하기도 한다."
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#folium.choropleth를-이용한-시각화",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#folium.choropleth를-이용한-시각화",
    "title": "Folium | 월드맵 시각화",
    "section": "5. folium.Choropleth()를 이용한 시각화",
    "text": "5. folium.Choropleth()를 이용한 시각화\n\nA. folium.Choropleth() 소개\n\n- folium.Choropleth()는 아래와 같은 방식으로 그림을 그린다고 생각하면 편리하다.\n\njson 파일을 바탕으로 폴리곤을 그린다, 폴리곤에 이름을 붙인다.\ndf = [폴리곤의 이름, 통계값(y)]와 같은 형식으로 정리된 데이터프레임을 바탕으로 각 폴리곤에 대응하는 y값을 색깔로 매핑한다.\n\n\n즉, 폴리곤 데이터가 포함된 녀석과 표시될 색생정보가 들어갈 녀석. 데이터프레임이 두 개가 필요함."
  },
  {
    "objectID": "2023_DV/Review/B5. Folium과 월드맵.html#b.-polygon-시각화",
    "href": "2023_DV/Review/B5. Folium과 월드맵.html#b.-polygon-시각화",
    "title": "Folium | 월드맵 시각화",
    "section": "### B. Polygon 시각화",
    "text": "### B. Polygon 시각화\n# 예제 1 : 전국의 행정구역 시각화(global)\n\nm = folium.Map(\n    location = [36, 128],\n    zoom_start = 7,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = global_dict\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n# 예제 2 : 전국의 행정구역 시각화(local)\n\nm = folium.Map(\n    location = [36, 128],\n    zoom_start = 6,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data = local_dict\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n폴리곤을 그릴 수 있는 딕셔너리 타입의 정보가 들어가고, 그것은 json파일에 온전히 포함되어 있다. 그니까 넣어주기만 하면 구역은 일단 알아서 그려줌…\n\n# 예제 3 : 전국의 행정구역 시각화(덕진구/완산구)\n- 먼저 전주에 해당하는 폴리곤만 추출해보자.(features만 뽑으면 됨)\n\nlocal_jeonju = local_dict.copy()  ## 딕셔너리를 바꿀 때 혹시라도 문제가 생기지 않도록...\n\n\nprint('dictionary formation \\n &gt; '+str(local_jeonju['features'][15]['properties']))\nprint('\\n'+'length of features \\n &gt; '+str(len(local_jeonju['features'])))\n\ndictionary formation \n &gt; {'name': '강서구', 'base_year': '2018', 'name_eng': 'Gangseo-gu', 'code': '11160'}\n\nlength of features \n &gt; 250\n\n\n- 해당 위치에 지역명이 들어가 있는 것을 알 수 있다. 여기서 정확히 하려면 code를 입력해줘야 하겠지만… 일단은 name을 위주로 해보자.\n\n[dic['properties']['name'] for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]\n\n['전주시완산구', '전주시덕진구']\n\n\n\n잘 추출되는 것 같음\n\n\n#[dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]\n\n\n너무길어… 자를래… 하지만 잘 된 것 같음.\n\n- 그래서 이게 무엇이냐?\n\nprint(type(local_dict['features']), type(local_dict['features'][0]))\nprint(type([dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]),\n     type([dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')][0]))\n\n&lt;class 'list'&gt; &lt;class 'dict'&gt;\n&lt;class 'list'&gt; &lt;class 'dict'&gt;\n\n\n- local_dict['features']는 딕셔너리를 원소로 하는 리스트인데, 추출한 리스트는 그것의 부분집합임!\n\n따라서 이 값을 local_dict['features']에 대신 넣어준다면??\n\n\nlocal_jeonju['features'] = [dic for dic in local_jeonju['features'] if (dic['properties']['name'] == '전주시덕진구') or (dic['properties']['name'] == '전주시완산구')]\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start = 11\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이렇게 전주시의 행정구역만 감싼 모습이 보인다.\n\n\nC. (Polygon, Value) 시각화(\\(\\star \\star \\star\\))\n\n# 예제 1 : 덕진구 vs 완산구\n덕진구와 완산구의 전기 사용량이 아래와 같이 정리되었다고 하자.\n\ndf = pd.DataFrame({\n    'key':['전주시덕진구', '전주시완산구'],\n    'elec_use':[20,30]\n})\ndf\n\n\n\n\n\n\n\n\nkey\nelec_use\n\n\n\n\n0\n전주시덕진구\n20\n\n\n1\n전주시완산구\n30\n\n\n\n\n\n\n\n- 그러면…\n\nlocal_jeonju['features'][0]['properties']\n\n{'name': '전주시완산구',\n 'base_year': '2018',\n 'name_eng': 'Jeonjusiwansangu',\n 'code': '35011'}\n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju,   ## json 파일로 폴리곤을 그린다.\n    key_on = 'properties.name',   ## ['properties']['name']의 값으로 폴리곤에 이름을 붙인다.\n    data = df,   ## 이름과 y값의 정보를 가져온다.\n    columns = ['key', 'elec_use']   ## 어떤 열이 (1)이름 (2)y값의 정보인지 알려준다.\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이렇게 아름답게 셰이딩이 된다.\n\n\n이해를 위해 필요한 약간의 직관\n\n코로플레스 맵을 그리기 위해서는 항상 두 개의 데이터(geo_data, data)를 연결해야 한다.\n두 개의 데이터를 연결하기 위해서는 공유 가능한 연결의 매개체(key_on)가 필요하다.\n코로플레스 맵의 연결 매개체는 ‘전주시완산구’, ’전주시덕진구’와 같은 지역명이다.\n\n\n# 예제 2 : 덕진구 vs 완산구 | key_on에 대한 이해를 돕기 위해 만든 억지예제\n덕진구와 완산구 전기 사용량이 입력한 사람에 따라 다른 이름으로 저장이 되어 아래와 같이 정리되었다고 하자.\n\ndf = pd.DataFrame({\n    'key':['전주시덕진구', 'Jeonjusiwansangu'],\n    'elec_use':[20,30]\n})\ndf\n\n\n\n\n\n\n\n\nkey\nelec_use\n\n\n\n\n0\n전주시덕진구\n20\n\n\n1\nJeonjusiwansangu\n30\n\n\n\n\n\n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\n\nfolium.Choropleth(\n    geo_data = local_jeonju,\n    key_on = 'properties.name',   ## 한글로 된 이름만 포함함\n    data = df,\n    columns = ['key', 'elec_use']  ## 순서가 뒤바뀌면 안됨\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n깨졌음.\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,\n    key_on='properties.name_eng',\n    data=df,\n    columns=['key','elec_use']\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n이번엔 한글부분이 깨졌음\n\n# 예제 3 : code 열을 새롭게 할당하고 code열로 key_on\n\ndf.assign(code = ['35012','30511'])\n\n\n\n\n\n\n\n\nkey\nelec_use\ncode\n\n\n\n\n0\n전주시덕진구\n20\n35012\n\n\n1\nJeonjusiwansangu\n30\n30511\n\n\n\n\n\n\n\n\nm = folium.Map(\n    location = [35.84195368311022, 127.1155556693179],\n    zoom_start=11,\n    scrollWheelZoom = False\n)\nfolium.Choropleth(\n    geo_data=local_jeonju,\n    key_on='properties.code',   ## code로 폴리곤에 이름을 붙여준다.\n    data=df.assign(code = ['35012','35011']),\n    columns=['code','elec_use']   ## key 값을 code로 바꾼 모습\n).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n코드는 작성자에 따라 달라질 일이 없기 때문에(오탈자라도 넣지 않는 이상…) 확실하게 가져올 수 있다.\n\n# 예제 4 : 대한민국 인구수 시각화 코로플레스(global)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv')\ndf\n\n\n\n\n\n\n\n\n행정구역(시군구)별\n총인구수 (명)\n\n\n\n\n0\n서울특별시\n9532428\n\n\n1\n부산광역시\n3356311\n\n\n2\n대구광역시\n2390721\n\n\n3\n인천광역시\n2945009\n\n\n4\n광주광역시\n1442454\n\n\n5\n대전광역시\n1454228\n\n\n6\n울산광역시\n1122566\n\n\n7\n세종특별자치시\n368276\n\n\n8\n경기도\n13549577\n\n\n9\n강원도\n1537717\n\n\n10\n충청북도\n1596948\n\n\n11\n충청남도\n2118977\n\n\n12\n전라북도\n1789770\n\n\n13\n전라남도\n1834653\n\n\n14\n경상북도\n2627925\n\n\n15\n경상남도\n3318161\n\n\n16\n제주특별자치도\n676569\n\n\n\n\n\n\n\n\n먹기 좋게 데이터가 저장되어 있다.\n\n\nprint('dictionary formation \\n : '+str(global_dict['features'][0]['properties']))\nprint('\\n length of features \\n : '+str(len(global_dict['features'])))\n\ndictionary formation \n : {'name': '서울특별시', 'base_year': '2018', 'name_eng': 'Seoul', 'code': '11'}\n\n length of features \n : 17\n\n\n\n뭐 바꿀 건 따로 없어보이니 그대로 넣어버리면…\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [36, 128],\n    zoom_start = 6\n)\n\nfolium.Choropleth(\n    geo_data = global_dict,\n    key_on = 'properties.name',  ## 한글이름임\n    data = df,\n    columns = ['행정구역(시군구)별', '총인구수 (명)']\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n깔쌈하게 잘 됐음.\n\n# 예제 5 : 대한민국 인구수 시각화 코로플레스(local)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-muni.csv')\ndf\n\n\n\n\n\n\n\n\n행정구역(시군구)별\n총인구수 (명)\n\n\n\n\n0\n종로구\n145346\n\n\n1\n중구\n122781\n\n\n2\n용산구\n223713\n\n\n3\n성동구\n287174\n\n\n4\n광진구\n340814\n\n\n...\n...\n...\n\n\n269\n함양군\n38475\n\n\n270\n거창군\n61242\n\n\n271\n합천군\n43029\n\n\n272\n제주시\n493225\n\n\n273\n서귀포시\n183344\n\n\n\n\n274 rows × 2 columns\n\n\n\n\n또 예쁘장(?)하게 나온 데이터\n\n\nm = folium.Map(\n    scrollWheelZoom = False,\n    location = [36, 128],\n    zoom_start = 6\n)\n\nfolium.Choropleth(\n    geo_data = local_dict,\n    key_on = 'properties.name',  ## 한글이름임\n    data = df,\n    columns = ['행정구역(시군구)별', '총인구수 (명)']\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n시꺼멓게 표시된 부분이 있다. 이것이 깨진 부분에 해당함;;;\n\n\n[dic['properties']['name'] for dic in local_dict['features'] if not dic['properties']['name'] in list(df['행정구역(시군구)별'])]\n\n['수원시장안구',\n '수원시권선구',\n '수원시팔달구',\n '수원시영통구',\n '성남시수정구',\n '성남시중원구',\n '성남시분당구',\n '안양시만안구',\n '안양시동안구',\n '안산시상록구',\n '안산시단원구',\n '고양시덕양구',\n '고양시일산동구',\n '고양시일산서구',\n '용인시처인구',\n '용인시기흥구',\n '용인시수지구',\n '청주시상당구',\n '청주시서원구',\n '청주시흥덕구',\n '청주시청원구',\n '천안시동남구',\n '천안시서북구',\n '전주시완산구',\n '전주시덕진구',\n '포항시남구',\n '포항시북구',\n '창원시의창구',\n '창원시성산구',\n '창원시마산합포구',\n '창원시마산회원구',\n '창원시진해구']\n\n\n\n[i for i in df['행정구역(시군구)별'] if '수원' in str(i)]\n\n['수원시']\n\n\n\n애초에 수원시 자체를 구로 나누지 않았으니 없을 수밖에;;; 이런 경우에는… 아직 잘 모르겠다. 합칠 수 있는 방법이 따로 있으려나?"
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "",
    "text": "FIFA23 선수 데이터에서 결측치를 처리하고 여러 열을 가공하여 시각화해보자!"
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#라이브러리-import",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#라이브러리-import",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport pandas as pd\nimport numpy as np\nfrom plotnine import *"
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#학습할-코드",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#학습할-코드",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "2. 학습할 코드",
    "text": "2. 학습할 코드\n\nA. dropna()\n\n- 행에서 결측치가 하나라도 있으면 제거한다.\n\ndf = pd.DataFrame({\n    'A': [1,2,3,np.nan,5,6,7],\n    'B': [11,np.nan,33,np.nan,55,66,77], \n    'C': [111,222,333,np.nan,555,666,np.nan]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n1\n2.0\nNaN\n222.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n6\n7.0\n77.0\nNaN\n\n\n\n\n\n\n\n\n이러한 데이터가 있다고 할 때…\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n\n\n\n\n\n\n결측치가 하나라도 있는 행은 모두 드롭된다. (원본 데이터 손상 X)"
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#b.-_",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#b.-_",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### B. _",
    "text": "### B. _\n- 파이썬에서 가장 최근 콘솔에 띄워진 결과는 _로 불러올 수 있다.\n\na = [1,2,3]\na + [4] \n\n[1, 2, 3, 4]\n\n\n\n_\n\n[1, 2, 3, 4]\n\n\n\n_ + [5]\n\n[1, 2, 3, 4, 5]\n\n\n\n_.pop()  ## 마지막 요소를 리턴하고 그 요소는 삭제\n\n5\n\n\n\n_ + 1\n\n6\n\n\n\n리스트에서 숫자까지… _의 다사다난한 모험"
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#fifa23-시각화-문제",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#fifa23-시각화-문제",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "3. FIFA23 시각화 문제",
    "text": "3. FIFA23 시각화 문제\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBest Position\nBest Overall Rating\nRelease Clause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n\n\n5 rows × 65 columns\n\n\n\n포지션별로 선수들의 능력치(ShotPower,SlidingTackle)와 급여(Wage)를 시각화하고 싶다. 아래의 세부지침에 맞추어 포지션별 ShotPower와 SlidingTackle의 산점도를 그려라.\n\ndf.Position\n\n0        &lt;span class=\"pos pos18\"&gt;CAM\n1        &lt;span class=\"pos pos11\"&gt;LDM\n2         &lt;span class=\"pos pos24\"&gt;RS\n3        &lt;span class=\"pos pos13\"&gt;RCM\n4          &lt;span class=\"pos pos7\"&gt;LB\n                    ...             \n16705    &lt;span class=\"pos pos29\"&gt;RES\n16706    &lt;span class=\"pos pos29\"&gt;RES\n16707    &lt;span class=\"pos pos29\"&gt;RES\n16708    &lt;span class=\"pos pos28\"&gt;SUB\n16709    &lt;span class=\"pos pos28\"&gt;SUB\nName: Position, Length: 16710, dtype: object\n\n\n세부지침\nA. Column의 이름에서 공백을 제거하라.\nB. 결측치가 50%이상인 컬럼을 찾고 이를 제거하라. 그 뒤에 .dropna()를 사용하여 결측치가 포함된 행을 제거하라.\nC. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.\n\nposition_dict = {\n    'GOALKEEPER':{'GK'},\n    'DEFENDER':{'CB','RCB','LCB','RB','LB','RWB','LWB'},\n    'MIDFIELDER':{'CM','RCM','LCM','CDM','RDM','LDM','CAM','RAM','LAM','RM','LM'},\n    'FORWARD':{'ST','CF','RF','LF','RW','LW','RS','LS'},\n    'SUB':{'SUB'},\n    'RES':{'RES'}\n}\n\nD. df.Wage를 적절하게 변환하라.\nE. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.\n시각화 예시"
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#fifa23-시각화---풀이",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#fifa23-시각화---풀이",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "4. FIFA23 시각화 - 풀이",
    "text": "4. FIFA23 시각화 - 풀이\n\nA. Column의 이름에서 공백을 제거하라.\n\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Jersey Number', 'Joined', 'Loaned From', 'Contract Valid Until',\n       'Height', 'Weight', 'Crossing', 'Finishing', 'HeadingAccuracy',\n       'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy',\n       'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility',\n       'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength',\n       'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision',\n       'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle',\n       'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'Best Position', 'Best Overall Rating', 'Release Clause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.columns.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'JerseyNumber',\n       'Joined', 'LoanedFrom', 'ContractValidUntil', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'BestPosition', 'BestOverallRating', 'ReleaseClause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\ndf.set_axis(df.columns.str.replace(' ', ''), axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'JerseyNumber',\n       'Joined', 'LoanedFrom', 'ContractValidUntil', 'Height', 'Weight',\n       'Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys',\n       'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n       'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance',\n       'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots',\n       'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties',\n       'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving',\n       'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes',\n       'BestPosition', 'BestOverallRating', 'ReleaseClause',\n       'DefensiveAwareness'],\n      dtype='object')\n\n\n\n공백이 없어진 것을 확인할 수 있다.\n\n\n\nB. 결측치가 50%이상인 컬럼을 찾고 이를 제거하라. 그 뒤에 .dropna()를 사용하여 결측치가 포함된 행을 제거하라.\n\n\ndf_b = df.set_axis(df.columns.str.replace(' ', ''), axis = 1)\n\n\ndf_b.loc[:, df_b.isna().mean() &gt;= 0.5].columns\n\nIndex(['LoanedFrom', 'Marking'], dtype='object')\n\n\n\n위 두 개의 컬럼이 결측치가 50% 이상이다.\n\n\ndf_b.loc[:, df_b.isna().mean() &lt; 0.5].dropna()\n##df_b.loc[:, [s &lt; 0.5 for s in df_b.isna().mean()]].dropna()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n따라서 해당 조건에 반대를 슬라이싱하는 방식으로 해당 컬럼을 제거하였다."
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#c.-position_dict를-이용하여-df.position을-적절하게-변환하라.-변환된-값을-df.position에-저장하라.",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#c.-position_dict를-이용하여-df.position을-적절하게-변환하라.-변환된-값을-df.position에-저장하라.",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### C. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.",
    "text": "### C. position_dict를 이용하여 df.Position을 적절하게 변환하라. 변환된 값을 df.Position에 저장하라.\n\nposition_dict\n\n{'GOALKEEPER': {'GK'},\n 'DEFENDER': {'CB', 'LB', 'LCB', 'LWB', 'RB', 'RCB', 'RWB'},\n 'MIDFIELDER': {'CAM',\n  'CDM',\n  'CM',\n  'LAM',\n  'LCM',\n  'LDM',\n  'LM',\n  'RAM',\n  'RCM',\n  'RDM',\n  'RM'},\n 'FORWARD': {'CF', 'LF', 'LS', 'LW', 'RF', 'RS', 'RW', 'ST'},\n 'SUB': {'SUB'},\n 'RES': {'RES'}}\n\n\n\ndf_c = df_b.loc[:, df_b.isna().mean() &lt; 0.5].dropna()\ndf_c.Position\n\n0        &lt;span class=\"pos pos18\"&gt;CAM\n1        &lt;span class=\"pos pos11\"&gt;LDM\n2         &lt;span class=\"pos pos24\"&gt;RS\n3        &lt;span class=\"pos pos13\"&gt;RCM\n4          &lt;span class=\"pos pos7\"&gt;LB\n                    ...             \n16703    &lt;span class=\"pos pos29\"&gt;RES\n16704    &lt;span class=\"pos pos29\"&gt;RES\n16706    &lt;span class=\"pos pos29\"&gt;RES\n16707    &lt;span class=\"pos pos29\"&gt;RES\n16708    &lt;span class=\"pos pos28\"&gt;SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n뒤의 &gt;를 제외한 문자열을 추출해서 바꿔줘야 할 것 같다.\n\n\ndf_c.assign(Position = df_c.Position.str.split('&gt;').str[-1]).Position\n\n0        CAM\n1        LDM\n2         RS\n3        RCM\n4         LB\n        ... \n16703    RES\n16704    RES\n16706    RES\n16707    RES\n16708    SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n뒤의 문자열만 추출\n\n- 무지성으로 쥐어짜내본 아이디어\n\nx = df_c.Position.str.split('&gt;').str[-1][2]\nlst = [i != 0 for i in [key if x in value else 0 for key, value in position_dict.items()]];lst\n\n[False, False, False, True, False, False]\n\n\n\n[i != 0 for i in [key if x in value else 0 for key, value in position_dict.items()]].index(True)\n\n3\n\n\n\nlst = [[key if i in value else np.nan if i == np.nan else 1 for key, value in position_dict.items()] for i in df_c.Position.str.split('&gt;').str[-1]]\n\n\nvalue와 같은 값이면 key, 아니면 1, 결측치면 np.nan을 넣어줘봤음.\n\n\ndef cutting_1(lst):\n    for i in lst:\n        if i != 1:\n            return i\n\nPosition_s = pd.Series(lst).apply(cutting_1); Position_s\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n14393           RES\n14394           RES\n14395           RES\n14396           RES\n14397           SUB\nLength: 14398, dtype: object\n\n\n\n잘 된듯~(근데 앞에서 .dropna()를 안해서 쓸데없는 코드까지 작성해버렸다.)~\n\n\nlst_2 = list(map(lambda x : [key for key, value in position_dict.items() if x in value], df_c.Position.str.split('&gt;').str[-1]))\n## [i[0] for i in lst_2] : 왜인지 안됨\n## [i.pop() for i in lst_2] : 이건 뭐 객체 저장인지 뭔지 문제라는데, 다른 변수에 copy()해서 넣어봐도 안됨\n\n\ndf_c.reset_index(drop = True).assign(Position = Position_s).Position\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n14393           RES\n14394           RES\n14395           RES\n14396           RES\n14397           SUB\nName: Position, Length: 14398, dtype: object\n\n\n\n잘못된 코드\n\n\ndf_c.assign(Position = Position_s).Position  ## 문제가 있는 코드, reset_index(drop = True)를 안해줘서 인덱스가 꼬임\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n16703           NaN\n16704           NaN\n16706           NaN\n16707           NaN\n16708           NaN\nName: Position, Length: 14398, dtype: object\n\n\n\n뭘 잘못했는 지 알겠지? index가 달라서 값이 엮이지가 않잖아…\n\n- 매우 간단한 교수님의 해법\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda _df: _df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns\n\n\n\n\n아마도 모든 문제의 원흉은 dropna()를 하지 않은 너에게 있었다. (결측치가 있으면 작동이 힘든가봄)\n\n\nD. df.Wage를 적절하게 변환하라.\n\n\ndf.Wage\n\n0        €250K\n1        €140K\n2        €135K\n3        €350K\n4         €45K\n         ...  \n16705      €1K\n16706     €550\n16707     €700\n16708     €500\n16709       €0\nName: Wage, Length: 16710, dtype: object\n\n\n\n시각화 해주려면 숫자형 자료여야 하는데, 범주형으로 들어가있다.\n\n- 앞의 유로를 없애고, K는 1000을 곱해주자.\n\ndf_d = df_c.reset_index(drop = True).assign(Position = Position_s)\n\n\ndf_d.Wage.str.replace('€','').str.replace('K','000')\n##[int(i) if i[-1] != 'K' else int(i.replace('K',''))*1000 for i in df_d.Wage.str.replace('€','')]와 동일\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n14393       650\n14394       950\n14395       550\n14396       700\n14397       500\nName: Wage, Length: 14398, dtype: object\n\n\n\ndf_d.assign(Wage = df_d.Wage.str.replace('€','').str.replace('K','000').astype(int)).Wage\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n14393       650\n14394       950\n14395       550\n14396       700\n14397       500\nName: Wage, Length: 14398, dtype: int32\n\n\n\n잘 된 것을 볼 수 있다."
  },
  {
    "objectID": "2023_DV/Review/A7. 실습_FIFA23.html#e.-positiondefender-or-positionforward에-해당하는-관측치를-고른-뒤-x축에-shotpower-y축에-slidingtackle을-시각화하라.-이때-position은-color로-구분하고-wage는-size와-alpha로-구분하라.",
    "href": "2023_DV/Review/A7. 실습_FIFA23.html#e.-positiondefender-or-positionforward에-해당하는-관측치를-고른-뒤-x축에-shotpower-y축에-slidingtackle을-시각화하라.-이때-position은-color로-구분하고-wage는-size와-alpha로-구분하라.",
    "title": "데이터 시각화 실습 : FIFA23 선수 데이터",
    "section": "### E. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.",
    "text": "### E. Position==“DEFENDER” or Position==“FORWARD”에 해당하는 관측치를 고른 뒤 x축에 ShotPower, y축에 SlidingTackle을 시각화하라. 이때 Position은 color로 구분하고 Wage는 size와 alpha로 구분하라.\n\ndf_e = df_d.assign(Wage = df_d.Wage.str.replace('€','').str.replace('K','000').astype(int))\n\n\ndf_e.loc[(df_e.Position == \"DEFENDER\") | (df_e.Position == \"FORWARD\")]\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n11\n155862\nSergio Ramos\n35\nhttps://cdn.sofifa.com/players/155/862/22_60.png\nSpain\nhttps://cdn.sofifa.com/flags/es.png\n88\n88\nParis Saint-Germain\nhttps://cdn.sofifa.com/teams/73/30.png\n...\n91.0\n11.0\n8.0\n9.0\n7.0\n11.0\nCB\n88.0\n€44.4M\n84.0\n\n\n12\n197445\nD. Alaba\n29\nhttps://cdn.sofifa.com/players/197/445/22_60.png\nAustria\nhttps://cdn.sofifa.com/flags/at.png\n84\n84\nReal Madrid CF\nhttps://cdn.sofifa.com/teams/243/30.png\n...\n82.0\n5.0\n7.0\n14.0\n15.0\n9.0\nCB\n84.0\n€72.8M\n86.0\n\n\n20\n210514\nJoão Cancelo\n27\nhttps://cdn.sofifa.com/players/210/514/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n86\n87\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n80.0\n6.0\n9.0\n15.0\n14.0\n14.0\nRB\n86.0\n€137.6M\n79.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13109\n203430\nG. Ray\n27\nhttps://cdn.sofifa.com/players/203/430/22_60.png\nWales\nhttps://cdn.sofifa.com/flags/gb-wls.png\n59\n60\nExeter City\nhttps://cdn.sofifa.com/teams/143/30.png\n...\n59.0\n13.0\n10.0\n12.0\n9.0\n8.0\nCB\n60.0\n€420K\n58.0\n\n\n13124\n187154\nN. Canavan\n30\nhttps://cdn.sofifa.com/players/187/154/22_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.com/flags/ie.png\n63\n63\nBradford City\nhttps://cdn.sofifa.com/teams/1804/30.png\n...\n62.0\n6.0\n10.0\n11.0\n14.0\n6.0\nCB\n63.0\n€700K\n63.0\n\n\n13183\n263968\nK. Sow\n18\nhttps://cdn.sofifa.com/players/263/968/22_60.png\nSwitzerland\nhttps://cdn.sofifa.com/flags/ch.png\n54\n76\nFC Lausanne-Sport\nhttps://cdn.sofifa.com/teams/1862/30.png\n...\n55.0\n6.0\n9.0\n13.0\n7.0\n14.0\nCB\n56.0\n€796K\n54.0\n\n\n13238\n263022\nM. Rosenfelder\n18\nhttps://cdn.sofifa.com/players/263/022/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n57\n71\nSC Freiburg II\nhttps://cdn.sofifa.com/teams/110691/30.png\n...\n60.0\n10.0\n10.0\n8.0\n10.0\n11.0\nCB\n59.0\n€726K\n58.0\n\n\n13405\n261062\nLee Han Beom\n19\nhttps://cdn.sofifa.com/players/261/062/22_60.png\nKorea Republic\nhttps://cdn.sofifa.com/flags/kr.png\n53\n72\nFC Seoul\nhttps://cdn.sofifa.com/teams/982/30.png\n...\n53.0\n7.0\n14.0\n5.0\n6.0\n15.0\nCB\n55.0\n€431K\n52.0\n\n\n\n\n3298 rows × 63 columns\n\n\n\n\ntidydata = df_e.loc[(df_e.Position == \"DEFENDER\") | (df_e.Position == \"FORWARD\")]\n\n\nfig = ggplot(tidydata)\npoint = geom_point(aes(x = 'ShotPower', y = 'SlidingTackle', color = 'Position', size = 'Wage', alpha = 'Wage'), position = 'jitter')\n\nfig + point\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n해치웠나…?\n\n결론\n\n- 데이터의 결측치를 반드시 먼저 처리하고 하자!(dropna()의 필요성)\n- pop()을 사용하기 전에는 결측치를 반드시 모두 없애자!\n- 데이터를 가공하여 순서가 바뀐 경우 왠만해선 인덱스를 초기화해주자!(reset_index()의 필요성)"
  },
  {
    "objectID": "2023_DV/Review/A4. Pandas의 기본.html",
    "href": "2023_DV/Review/A4. Pandas의 기본.html",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "",
    "text": "pandas에서 행과 열을 선택하는 기술에 대해서 알아보도록 하자!"
  },
  {
    "objectID": "2023_DV/Review/A4. Pandas의 기본.html#라이브러리-import",
    "href": "2023_DV/Review/A4. Pandas의 기본.html#라이브러리-import",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "2023_DV/Review/A4. Pandas의 기본.html#pandas-행과-열의-선택",
    "href": "2023_DV/Review/A4. Pandas의 기본.html#pandas-행과-열의-선택",
    "title": "Pandas 기본기 | 행과 열의 선택",
    "section": "2. pandas : 행과 열의 선택",
    "text": "2. pandas : 행과 열의 선택\n- 같은 자료, 다른 두 형태의 데이터프레임\n\ndf = pd.DataFrame({'date': ['12/30','12/31','01/01','01/02','01/03'], 'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]})\ndf\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n얘는 인덱스는 그저 숫자의 의미이고\n\n\nts = pd.DataFrame({'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]}, index=['12/30','12/31','01/01','01/02','01/03'])\nts  ## 중요한 코드는 아님, 근데 그냥 index 지정해주는 거잖아\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n12/31\n95\n100\n50\n80\n\n\n01/01\n65\n90\n60\n30\n\n\n01/02\n55\n80\n75\n80\n\n\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n얘는 인덱스에 시계열적 표현이 있다.\n\n\nts.reset_index()  ## 결국 이렇게 하는 게 다루기 편하다.\n\n\n  \n    \n\n\n\n\n\n\nindex\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n안바꾸고 냅두는 경우, index가 time seat를 의미하는 경우에는 안바꾸기도 한다. ~근데 위는 왜 다시 바꾼거~\n\n\nA. 열의 선택\n\n1번째 방법 : df.ㆍ\n! 치명적인 단점 : 변수 이름에 공백 등이 있으면 불러올 수 없음.\n\n\ndf.X1   ## df 또한 하나의 object이므로\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\n2번째 방법 : df['ㆍ'], df[['ㆍ']]\n\n\ndf['X1']  ## df를 일종의 딕셔너리처럼 취급하는 방법\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\nSeries로 불러온다.\n\ndictionary?\n\ndct = dict({'date': ['12/30','12/31','01/01','01/02','01/03'], 'X1': [65,95,65,55,80], 'X2': [55,100,90,80,30], 'X3': [50,50,60,75,30], 'X4': [40,80,30,80,100]})\ndct['X1']\n\n[65, 95, 65, 55, 80]\n\n\n\ndf.keys()\n\nIndex(['date', 'X1', 'X2', 'X3', 'X4'], dtype='object')\n\n\n\ndct.keys()\n\ndict_keys(['date', 'X1', 'X2', 'X3', 'X4'])\n\n\n\nkey와 value가 있는 것처럼 column의 한 값(key)에 대한 데이터(value)가 있는 모습이다.\n\n\ndf[['X1']]  ## 프레임으로 산출\n\n\n  \n    \n\n\n\n\n\n\nX1\n\n\n\n\n0\n65\n\n\n1\n95\n\n\n2\n65\n\n\n3\n55\n\n\n4\n80\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf[['X1', 'X2']]  ## 2개 이상 산출 가능\n\n\n  \n    \n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n65\n55\n\n\n1\n95\n100\n\n\n2\n65\n90\n\n\n3\n55\n80\n\n\n4\n80\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n3번째 방법 : df.iloc[:, ㆍ] &gt; 통째로 np.array와 같다고 보면 된다.\n\n\ndf.iloc[:, 0] ## numpy에서 행렬을 다루는 것과 완전히 같게 사용 가능.\n\n0    12/30\n1    12/31\n2    01/01\n3    01/02\n4    01/03\nName: date, dtype: object\n\n\n\n#df.iloc[:,0] ## int - 0번째 행 | [0]이면 데이터프레임으로\n#df.iloc[:,-2:] # int:int, -2번째 행부터 -1번째 행까지(뒤에서 두 개)\n#df.iloc[:,1::2] # int:int:int - 스트라이딩, 1번째(두번째) 행부터 2개 단위로 추출\n#df.iloc[:,[0,2]] # [int,int] - 특정 행(0, 2번째)을 데이터프레임의 형태로 반환\n#df.iloc[:,[True,True,False,False,False]] # bool의 list\n#df.iloc[:,range(2)] # range, 앞에서 2개\n\n\n4번째 방법 : df.loc[:, ㆍ] &gt; 완전히 새로운 방법\n\n\n#df.loc[:,'X1'] # str - 시리즈 | ['X1']이면 데이터프레임으로\n#df.loc[:,'X1':'X3'] # 'str':'str' -- 칼럼이름으로 슬라이싱 **\n#df.loc[:,'X1'::2] # 'str'::int -- 칼럼이름으로 스트라이딩 **\n#df.loc[:,['X1','X4']] # [str,str] - 특정 행만 데이터프레임으로\n#df.loc[:,[True,False,False,True,False]] # bool의 list\n\n\"\"\"\n그냥 왠만해선 다 됨\n\"\"\"\n\n- 'date'행 부터\n\ndf.loc[:, 'date':]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- True가 입력된 행만\n\ndf.loc[:, [True, False, True, False, True]]   ## columns의 이름에 어떤 조건을 걸어서 True에 해당하는 열만 산출 가능\n\n\n  \n    \n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 2칸씩 띄며 스트라이딩\n\ndf.loc[:, ::2]  ## 2 간격으로 스트라이딩\n\n\n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n\n\n\nB. 행의 선택\n\n방법1 : df[]\n\n\ndf[:2]    ## int:int -- 슬라이싱 // df.iloc[:2, :], df.iloc[:2] 와 같음\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf[::2]  ## 스트라이딩, df.iloc[::2]와 같음\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n# df[:2] # int:int -- 슬라이싱 // df.iloc[:2,:], df.iloc[:2] 와 같음\n# df[::2] # int:int -- 스트라이딩\n# ts['12/30':'01/02'] # str:str -- 슬라이싱 &gt; 인덱스가 문자열 등일 경우\n# ts['12/31'::2] # str:str -- 스트라이딩\n# df[['12' in date for date in df.date]] # [bool,bool] `12`가 데이터에 포함되어 있을 경우\n# df[df.X1 &lt; 70] # pd.Series([bool,bool])\n\n\n방법2 : df.iloc[]\n\n\n# df.iloc[0] # int  df.iloc[0, :]에서 생략된 표현\n# df.iloc[-2:] # int:int -- 슬라이싱\n# df.iloc[1::2] # int:int -- 스트라이딩\n# df.iloc[[0]] # [int]\n# df.iloc[[0,1]] # [int,int]\n# df.iloc[['12' in date for date in df.date]] # [bool,bool] 이 경우는 []와 동일하다.\n# df.iloc[range(2)] # range\n\n- 해당 방법은 리스트나 어레이의 원소를 다루는 것과 완전히 동일해서… 아래를 참고하면 된다.\n\nlst = [[1,2,3], [3,4,5]]\nlst[0]\n\n[1, 2, 3]\n\n\n\nary = np.array(lst)\nary[0]\n\narray([1, 2, 3])\n\n\n\n방법3 : df.loc[]\n\n\n# df.loc[0] # int \n# ts.loc['12/30'] # str \n# df.loc[:2] # int:int \n# ts.loc[:'01/02'] # str:str \n# df.loc[[0,1]] # [int,int]\n# ts.loc[['12/30','01/01']] # [str,str]\n# df.loc[['12' in date for date in df.date]] # [bool,bool] 이 경우는 []와 동일하다.\n# df.loc[df.X1&gt;70] # pd.Series([bool,bool]) \n\n\ndf.loc[:2]  ## character와 비슷한 형식이기 때문에 2까지 포함이 된다.\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[df.X1 &gt; 70]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[df.X1 &gt; 70]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n위처럼 튜플로 넣을 수도 있다. 근데 iloc의 경우 위와 같은 코드로 입력하면 오류가 난다.\n\n\n## df.iloc[df.X1 &gt; 70] &gt; 오류, bool을 받을 수 있으나, 튜플의 형태로 들어가면 반환하지 않는다.\ndf.iloc[list(df.X1 &gt; 70)]\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nD. 교수님 방식\n-가장 안전한 코드\n\ndf.loc[:,:] ## 해당 코드를 써놓고 시작, generally\n\n\n  \n    \n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n하나의 col을 뽑으려 할 때\n\n\n# df.X1       ## 제일 간단함. 게다가 눌러보면 변수 이름들이 나옴\n# df['X1']\n# df[['X1']]\n\n\nrow를 슬라이싱\n\n\n# df[:5]\n# ts[:'01/02']  # 시계열일 경우\n\n\n조건에 맞는 row를 뽑을 때 좋은 코드\n\n\n# df[df.X1&lt;60]  ## 이게 좋기는 한데, True, False를 직접 만들어야 하는 경우도 많음.\n# df.loc[['12' in date for date in df.date]]\n\n\n['12' in date for date in df.date]\n\n[True, True, False, False, False]\n\n\n\n하나의 row를 뽑으려 할 때 좋은 코드\n\n\n# df.iloc[0]\n# df.loc[0]\n\n\nts.iloc[[0]]\n# ts.iloc[0]의 경우 오류가 남(인덱스 이름이 숫자열이 아님\n\n\n  \n    \n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\n\n(row,col)을 뽑으려 할 때 좋은 코드\n\n\n# df.X1[0]    ## &lt;- pd.Series를 뽑고 인덱스로 접근\n# df['X1'][0]\n\n\n# df.iloc[0,0]\n# df.loc[0,'X1']\n\n*위의 상황 이외에는 df.loc[:,:]를 사용하는 것이 유리하다.\n\n\n요약\n\n알아두면 좋은 규칙\n.iloc[] 와 .iloc[,:]는 완전히 동등하다.\n.loc[] 와 .loc[,:]는 완전히 동등하다.\n결과를 pd.Series 형태가 아닌 pd.DataFrame 형태로 얻고 싶다면 [[?]]를 사용하면 된다.\n\n\n\nROW\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[3,:]\n\n\nint:int\nX\nO\nO\n\\(\\Delta\\)\ndf[3:5]\n\n\n[int,int]\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[idx,:]\n\n\nstr\nX\nX\nX\nO\nts.loc['time1',:]\n\n\nstr:str\nX\nO\nX\nO\nts.loc['time1':'time2',:]\n\n\n[str,str]\nX\nX\nX\nO\n안할 듯\n\n\n[bool,bool]\nX\nO\nO\nO\ndf[filtered_idx]\n\n\npd.Series([bool,bool])\nX\nO\nX\nO\ndf[df.X1&gt;20]\n\n\n\n\n\nCOL\n\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\ntarget\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\ncol\nX\nX\nO\nX\ndf.iloc[:,0]\n\n\nint:int\ncol\nX\nX\nO\nX\ndf.iloc[:,0:2]\n\n\n[int,int]\ncol\nX\nX\nO\nX\ndf.iloc[:,idx]\n\n\nstr\ncol\nO\nO\nX\nO\ndf.loc[:,'X1']\n\n\nstr:str\ncol\nX\nX\nX\nO\ndf.loc[:,'X1':'X4']\n\n\n[str,str]\ncol\nX\nO\nX\nO\ndf.loc[:,colname_list]\n\n\n[bool,bool]\ncol\nX\nX\nO\nO\ndf.loc[:,bool_list]"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "",
    "text": "열 이름 변경, 열 추가, 리스트 컴프리헨션, 결측치 파악, query, 매핑 등등… 해당 내용은 왠만해선 다 알아두는 게 좋다."
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#import",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#import",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "1. import",
    "text": "1. import\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-기본기능",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-기본기능",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "2. Pandas 기본기능",
    "text": "2. Pandas 기본기능\n\nA. 열의 이름 변경\n\n\ndf = pd.DataFrame(np.random.randn(3, 2))\ndf\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n-0.000655\n0.686701\n\n\n1\n0.591774\n0.842045\n\n\n2\n-0.027722\n-0.703161\n\n\n\n\n\n\n\n- 방법1 : df.columns에 대입\n\ndf.columns = ['A', 'B']\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n-0.000655\n0.686701\n\n\n1\n0.591774\n0.842045\n\n\n2\n-0.027722\n-0.703161\n\n\n\n\n\n\n\n- 방법2 : df.set_axis() \\(\\star\\star\\star\\)\n\ndf2 = pd.DataFrame(np.random.randn(5,3))\ndf2\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.200618\n-0.567175\n-0.249051\n\n\n1\n0.805185\n-0.479624\n0.797904\n\n\n2\n-1.278647\n-0.061503\n1.048704\n\n\n3\n0.308626\n-3.294418\n0.326681\n\n\n4\n1.585979\n-1.200001\n0.386765\n\n\n\n\n\n\n\n\ndf2 = df2.set_axis(['A','B','C'], axis = 1)\ndf2\n\n#df2.set_axis(['a','b','c','d,',e'], axis = 0)으로 하면 인덱스가 바뀐다.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n0.200618\n-0.567175\n-0.249051\n\n\n1\n0.805185\n-0.479624\n0.797904\n\n\n2\n-1.278647\n-0.061503\n1.048704\n\n\n3\n0.308626\n-3.294418\n0.326681\n\n\n4\n1.585979\n-1.200001\n0.386765\n\n\n\n\n\n\n\n- 방법3 : df.rename()\n\ndf3 = pd.DataFrame(np.random.randn(5,3))\ndf3\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.202540\n0.265273\n1.855420\n\n\n1\n-0.422516\n-0.954117\n-0.050532\n\n\n2\n-0.010961\n-1.681503\n-1.613766\n\n\n3\n0.855199\n0.773191\n1.149413\n\n\n4\n0.310184\n-0.063591\n-0.572836\n\n\n\n\n\n\n\n\ndf3.rename({0 : 'A'}, axis = 1) ## dictionary 형태로 지정, 특정 열만 바꿈\n##df3.rename(columns = {0 : 'A'})와 동일\n\n\n\n\n\n\n\n\nA\n1\n2\n\n\n\n\n0\n0.202540\n0.265273\n1.855420\n\n\n1\n-0.422516\n-0.954117\n-0.050532\n\n\n2\n-0.010961\n-1.681503\n-1.613766\n\n\n3\n0.855199\n0.773191\n1.149413\n\n\n4\n0.310184\n-0.063591\n-0.572836"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-행의-이름-변경",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-행의-이름-변경",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. 행의 이름 변경",
    "text": "### B. 행의 이름 변경\n- 방법1 : df.index에 대입\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.108275\n-0.802206\n-3.011323\n\n\n1\n-1.437775\n-1.868590\n-0.079212\n\n\n\n\n\n\n\n\ndf.index = ['a', 'b']\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\na\n0.108275\n-0.802206\n-3.011323\n\n\nb\n-1.437775\n-1.868590\n-0.079212\n\n\n\n\n\n\n\n- 방법2 : df.set_axis() \\(\\star\\star\\star\\)\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-0.179379\n0.684650\n1.678079\n\n\n1\n0.487614\n-1.358992\n-0.661587\n\n\n\n\n\n\n\n\ndf.set_axis(['1','2'], axis = 0)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n1\n-0.179379\n0.684650\n1.678079\n\n\n2\n0.487614\n-1.358992\n-0.661587\n\n\n\n\n\n\n\n- 방법3 : df.rename()\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-0.051285\n1.185885\n0.841335\n\n\n1\n0.118555\n1.527457\n0.544870\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.rename({0 : 'A'}, axis = 0)    ## default = 0\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nA\n-0.051285\n1.185885\n0.841335\n\n\n1\n0.118555\n1.527457\n0.544870\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 방법 4 : df.set_index() &gt; 임의의 열을 행이름으로 지정, 이미 있던 열 하나를 인덱스로 잡고 싶을 시 사용\n\ndf = pd.DataFrame({'id':['2020-43052','2021-43053'], 'X1':[1,2],'X2':[2,3]})\ndf\n\n\n\n\n\n\n\n\nid\nX1\nX2\n\n\n\n\n0\n2020-43052\n1\n2\n\n\n1\n2021-43053\n2\n3\n\n\n\n\n\n\n\n\ndf.set_index('id')\n\n\n\n\n\n\n\n\nX1\nX2\n\n\nid\n\n\n\n\n\n\n2020-43052\n1\n2\n\n\n2021-43053\n2\n3\n\n\n\n\n\n\n\n\n# A~B에 대한 연습문제\n\n- 데이터 load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n# 예제1 : 열의 이름을 출력하고, 열의 이름중 공백()이 있을 경우 언더바(_) 로 바꾸자.\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n- 방법1 : df.columns에 직접 대입\n\ndf_ = df\ndf_.columns = [i.replace(' ', '_') for i in df_.columns]\n\n\ndf_.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n- 방법2 : set_axis() 이용 \\(\\star\\star\\star\\)\n\ndf_ = df\ndf_.set_axis([col.replace(' ', '_') for col in df_.columns], axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n- 방법 3 : rename() 이용(안중요함)\n\ntemp3 = df\n\ndic = {i:i.replace(' ','_') for i in df.columns}\ntemp3.rename(dic, axis = 1).columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n# 예제2: ID를 row-index로 지정하라.\n\ndf.ID\n\n0        209658\n1        212198\n2        224334\n3        192985\n4        224232\n          ...  \n17655    269526\n17656    267946\n17657    270567\n17658    256624\n17659    256376\nName: ID, Length: 17660, dtype: int64\n\n\n- 방법1 : 직접지정\n\ndf_ = df\ndf_.index = df.ID\ndf_.index\n\nIndex([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961, 208333,\n       210514,\n       ...\n       256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567, 256624,\n       256376],\n      dtype='int64', name='ID', length=17660)\n\n\n- 방법2 : set_axis() \\(\\star\\star\\star\\)\n\ndf_ = df\ndf_ = df_.set_axis(df.ID)   ## default : axis = 0, df_.set_axis(df.ID, axis = 0)과 동일\ndf_.index\n\nInt64Index([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961,\n            208333, 210514,\n            ...\n            256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567,\n            256624, 256376],\n           dtype='int64', name='ID', length=17660)\n\n\n- 방법3 : set_index()\n\n이 경우 해당 열을 나중에 따로 드랍하지 않아도 됨\n\n\ndf_ = df\ndf_ = df_.set_index('ID')\ndf_.index\n\nIndex([209658, 212198, 224334, 192985, 224232, 212622, 197445, 187961, 208333,\n       210514,\n       ...\n       256879, 269546, 267647, 253186, 267461, 269526, 267946, 270567, 256624,\n       256376],\n      dtype='int64', name='ID', length=17660)"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#c.-df.t-데이터프레임을-전치",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#c.-df.t-데이터프레임을-전치",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. df.T | 데이터프레임을 전치",
    "text": "### C. df.T | 데이터프레임을 전치\ndf.T를 이용하여 데이터를 살피면 편리함\n- data load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n- df.T : 데이터프레임을 전치(transition)한다.\n\ndf.T.loc[:,:3]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\nID\n209658\n212198\n224334\n192985\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\nK. De Bruyne\n\n\nAge\n27\n27\n30\n31\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nhttps://cdn.sofifa.net/players/192/985/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\nBelgium\n\n\nFlag\nhttps://cdn.sofifa.net/flags/de.png\nhttps://cdn.sofifa.net/flags/pt.png\nhttps://cdn.sofifa.net/flags/ar.png\nhttps://cdn.sofifa.net/flags/be.png\n\n\nOverall\n87\n86\n85\n91\n\n\nPotential\n88\n87\n85\n91\n\n\nClub\nFC Bayern München\nManchester United\nSevilla FC\nManchester City\n\n\nClub Logo\nhttps://cdn.sofifa.net/teams/21/30.png\nhttps://cdn.sofifa.net/teams/11/30.png\nhttps://cdn.sofifa.net/teams/481/30.png\nhttps://cdn.sofifa.net/teams/10/30.png\n\n\nValue\n€91M\n€78.5M\n€46.5M\n€107.5M\n\n\nWage\n€115K\n€190K\n€46K\n€350K\n\n\nSpecial\n2312\n2305\n2303\n2303\n\n\nPreferred Foot\nRight\nRight\nLeft\nRight\n\n\nInternational Reputation\n4.0\n3.0\n2.0\n4.0\n\n\nWeak Foot\n4.0\n3.0\n3.0\n5.0\n\n\nSkill Moves\n3.0\n4.0\n3.0\n4.0\n\n\nWork Rate\nHigh/ Medium\nHigh/ High\nHigh/ High\nHigh/ High\n\n\nBody Type\nUnique\nUnique\nStocky (170-185)\nUnique\n\n\nReal Face\nYes\nYes\nNo\nYes\n\n\nPosition\n&lt;span class=\"pos pos28\"&gt;SUB\n&lt;span class=\"pos pos15\"&gt;LCM\n&lt;span class=\"pos pos7\"&gt;LB\n&lt;span class=\"pos pos13\"&gt;RCM\n\n\nJoined\nJul 1, 2018\nJan 30, 2020\nSep 14, 2020\nAug 30, 2015\n\n\nLoaned From\nNaN\nNaN\nNaN\nNaN\n\n\nContract Valid Until\n2026\n2026\n2024\n2025\n\n\nHeight\n189cm\n179cm\n172cm\n181cm\n\n\nWeight\n82kg\n69kg\n69kg\n70kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n€198.9M\n\n\nKit Number\n8.0\n8.0\n19.0\n17.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n- 출력옵션 조정\n\npd.options.display.max_rows = 10\ndisplay(df.T.iloc[:, :3])\npd.reset_option('display.max_rows')   ## 디폴트 옵션으로 변경\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\n209658\n212198\n224334\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\n\n\nAge\n27\n27\n30\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\n\n\n...\n...\n...\n...\n\n\nHeight\n189cm\n179cm\n172cm\n\n\nWeight\n82kg\n69kg\n69kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n\n\nKit Number\n8.0\n8.0\n19.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\n\n\n\n\n29 rows × 3 columns\n\n\n\n\n여기선 설명을 위해 줄이는 옵션을 사용했지만, 보통은 늘려서 사용함."
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#d.-df.dtypes-sdtype-데이터의-타입-산출",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#d.-df.dtypes-sdtype-데이터의-타입-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### D. df.dtypes, s,dtype | 데이터의 타입 산출",
    "text": "### D. df.dtypes, s,dtype | 데이터의 타입 산출\n- df.dtypes\n\n데이터프레임 각 열에 저장된 데이터들의 타입을 알려준다.\n\n\ndf.dtypes\n\nID                            int64\nName                         object\nAge                           int64\nPhoto                        object\nNationality                  object\nFlag                         object\nOverall                       int64\nPotential                     int64\nClub                         object\nClub Logo                    object\nValue                        object\nWage                         object\nSpecial                       int64\nPreferred Foot               object\nInternational Reputation    float64\nWeak Foot                   float64\nSkill Moves                 float64\nWork Rate                    object\nBody Type                    object\nReal Face                    object\nPosition                     object\nJoined                       object\nLoaned From                  object\nContract Valid Until         object\nHeight                       object\nWeight                       object\nRelease Clause               object\nKit Number                  float64\nBest Overall Rating          object\ndtype: object\n\n\n\nobject : string이라고 생각해도 무방. 범주형 자료.\n\n- s.dtype Series에 붙여 사용\n\ndf.Name.dtype   ## 한 행의 데이터 타입만을 산출\n\ndtype('O')\n\n\n\n다양한 활용이 가능\n\n\ndf.Name.dtype == np.object_\n\nTrue\n\n\n\ndf.Age.dtype == np.int64\n\nTrue\n\n\n\ndf['International Reputation'].dtype == np.float64\n\nTrue\n\n\n\nbool을 산출하니까 컴프리헨션에 조건문 걸어서 해도 되고… 활용의 여지가 넓다.\n\n# 예제: df에서 int64 자료형만 출력\n- 풀이 1 : 표를 보고 직접 뽑음\n\npd.Series(list(df.dtypes))\n\n0       int64\n1      object\n2       int64\n3      object\n4      object\n5      object\n6       int64\n7       int64\n8      object\n9      object\n10     object\n11     object\n12      int64\n13     object\n14    float64\n15    float64\n16    float64\n17     object\n18     object\n19     object\n20     object\n21     object\n22     object\n23     object\n24     object\n25     object\n26     object\n27    float64\n28     object\ndtype: object\n\n\n\ndf.iloc[:, [0,2,6,7,12]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns\n\n\n\n- 풀이 2 : 리스트 컴프리핸션 이용\n\ndf.loc[:, [o == np.int64 for o in df.dtypes]]\n##df.loc[:, [o == 'int64' for o in df.dtypes]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#e.-df.sort_values-데이터들을-정렬",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#e.-df.sort_values-데이터들을-정렬",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### E. df.sort_values() | 데이터들을 정렬",
    "text": "### E. df.sort_values() | 데이터들을 정렬\n- 예시1 : 순서대로 정렬\n\ndf.sort_values('Age')   ## 나이가 어린 순서대로 오름차순 정렬 / 인덱스 개판\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n13712\n271072\nE. Topcu\n16\nhttps://cdn.sofifa.net/players/271/072/23_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.net/flags/ie.png\n48\n58\nDrogheda United\nhttps://cdn.sofifa.net/teams/1572/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 8, 2022\nNaN\n2022\n183cm\n65kg\n€175K\n20.0\nNaN\n\n\n13078\n259442\n22 R. van den Berg\n16\nhttps://cdn.sofifa.net/players/259/442/22_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n60\n81\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 24, 2020\nNaN\n2024\n190cm\n73kg\n€1.8M\n33.0\nNaN\n\n\n11257\n266205\n22 Y. Koré\n16\nhttps://cdn.sofifa.net/players/266/205/22_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n59\n74\nParis FC\nhttps://cdn.sofifa.net/teams/111817/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nAug 11, 2022\nNaN\n2025\n187cm\n75kg\n€1.1M\n34.0\nNaN\n\n\n11278\n261873\n21 H. Kumagai\n16\nhttps://cdn.sofifa.net/players/261/873/21_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n52\n70\nVegalta Sendai\nhttps://cdn.sofifa.net/teams/112836/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 16, 2021\nNaN\n2023\n174cm\n64kg\n€375K\n48.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시 2 : 내림차순으로 정렬\n\ndf.sort_values('Age', ascending = False)  ## default : ascending = True\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17360\n261023\n21 H. Broun\n16\nhttps://cdn.sofifa.net/players/261/023/21_60.png\nScotland\nhttps://cdn.sofifa.net/flags/gb-sct.png\n52\n72\nKilmarnock\nhttps://cdn.sofifa.net/teams/82/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 17, 2020\nNaN\n2022\n182cm\n70kg\n€523K\n40.0\nNaN\n\n\n15536\n263639\n22 M. Pavel\n16\nhttps://cdn.sofifa.net/players/263/639/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n51\n69\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2021\nNaN\n2023\n178cm\n66kg\n€277K\n77.0\nNaN\n\n\n11398\n256405\n21 W. Essanoussi\n16\nhttps://cdn.sofifa.net/players/256/405/21_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n59\n75\nVVV-Venlo\nhttps://cdn.sofifa.net/teams/100651/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2019\nNaN\n2022\n178cm\n70kg\n€1.1M\n24.0\nNaN\n\n\n15030\n270594\nT. Walczak\n16\nhttps://cdn.sofifa.net/players/270/594/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n54\n68\nWisła Płock\nhttps://cdn.sofifa.net/teams/1569/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 7, 2021\nNaN\n2023\n191cm\n88kg\n€494K\n99.0\nNaN\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시 3 : 능력치가 좋은 순서대로 정렬\n\ndf.sort_values(by = 'Overall', ascending = False)    ## 수가 높을수록 위로 가니까, by 생략해도 됨.\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n41\n188545\nR. Lewandowski\n33\nhttps://cdn.sofifa.net/players/188/545/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n91\n91\nFC Barcelona\nhttps://cdn.sofifa.net/teams/241/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 18, 2022\nNaN\n2025\n185cm\n81kg\n€172.2M\n9.0\nNaN\n\n\n124\n165153\nK. Benzema\n34\nhttps://cdn.sofifa.net/players/165/153/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n91\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos21\"&gt;CF\nJul 9, 2009\nNaN\n2023\n185cm\n81kg\n€131.2M\n9.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n56\n158023\nL. Messi\n35\nhttps://cdn.sofifa.net/players/158/023/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n91\n91\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos23\"&gt;RW\nAug 10, 2021\nNaN\n2023\n169cm\n67kg\n€99.9M\n30.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15513\n266751\n22 Jung Ho Yeon\n20\nhttps://cdn.sofifa.net/players/266/751/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n45\n53\nGwangJu FC\nhttps://cdn.sofifa.net/teams/112258/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 20, 2022\nNaN\n2026\n180cm\n73kg\n€145K\n23.0\nNaN\n\n\n16215\n268279\n22 J. Looschen\n24\nhttps://cdn.sofifa.net/players/268/279/22_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n44\n47\nSV Meppen\nhttps://cdn.sofifa.net/teams/110597/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMar 19, 2022\nNaN\n2026\n178cm\n78kg\n€92K\n42.0\nNaN\n\n\n16042\n255283\n20 Kim Yeong Geun\n22\nhttps://cdn.sofifa.net/players/255/283/20_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n44\n49\nGyeongnam FC\nhttps://cdn.sofifa.net/teams/111588/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 9, 2020\nNaN\n2020\n174cm\n71kg\n€53K\n43.0\nNaN\n\n\n14634\n269038\n22 Zhang Wenxuan\n16\nhttps://cdn.sofifa.net/players/269/038/22_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n44\n59\nGuangzhou FC\nhttps://cdn.sofifa.net/teams/111839/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 1, 2022\nNaN\n2022\n175cm\n70kg\n€239K\n29.0\nNaN\n\n\n17618\n168933\n07 I. Paskov\n33\nhttps://cdn.sofifa.net/players/168/933/07_60.png\nBulgaria\nhttps://cdn.sofifa.net/flags/bg.png\n43\n42\nNaN\nhttps://cdn.sofifa.net/flags/bg.png\n...\nNaN\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n184cm\n79kg\nNaN\n24.0\nNaN\n\n\n\n\n\n17660 rows × 29 columns"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#f.-df.info-행별-information-산출",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#f.-df.info-행별-information-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### F. df.info() | 행별 information 산출",
    "text": "### F. df.info() | 행별 information 산출\n시험에는 절대 안 낼 거지만 매우 중요한 것\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17660 entries, 0 to 17659\nData columns (total 29 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   ID                        17660 non-null  int64  \n 1   Name                      17660 non-null  object \n 2   Age                       17660 non-null  int64  \n 3   Photo                     17660 non-null  object \n 4   Nationality               17660 non-null  object \n 5   Flag                      17660 non-null  object \n 6   Overall                   17660 non-null  int64  \n 7   Potential                 17660 non-null  int64  \n 8   Club                      17449 non-null  object \n 9   Club Logo                 17660 non-null  object \n 10  Value                     17660 non-null  object \n 11  Wage                      17660 non-null  object \n 12  Special                   17660 non-null  int64  \n 13  Preferred Foot            17660 non-null  object \n 14  International Reputation  17660 non-null  float64\n 15  Weak Foot                 17660 non-null  float64\n 16  Skill Moves               17660 non-null  float64\n 17  Work Rate                 17660 non-null  object \n 18  Body Type                 17622 non-null  object \n 19  Real Face                 17622 non-null  object \n 20  Position                  17625 non-null  object \n 21  Joined                    16562 non-null  object \n 22  Loaned From               694 non-null    object \n 23  Contract Valid Until      17299 non-null  object \n 24  Height                    17660 non-null  object \n 25  Weight                    17660 non-null  object \n 26  Release Clause            16509 non-null  object \n 27  Kit Number                17625 non-null  float64\n 28  Best Overall Rating       21 non-null     object \ndtypes: float64(4), int64(5), object(20)\nmemory usage: 3.9+ MB\n\n\n\ndata들의 현황을 한눈에 파악하기 좋다.\n\n\ndf.iloc[:, [28]].sort_values('Best Overall Rating')\n\n\n  \n    \n\n\n\n\n\n\nBest Overall Rating\n\n\n\n\n13299\n&lt;span class=\"bp3-tag p p-54\"&gt;54&lt;/span&gt;\n\n\n14366\n&lt;span class=\"bp3-tag p p-56\"&gt;56&lt;/span&gt;\n\n\n16779\n&lt;span class=\"bp3-tag p p-58\"&gt;58&lt;/span&gt;\n\n\n16968\n&lt;span class=\"bp3-tag p p-58\"&gt;58&lt;/span&gt;\n\n\n16835\n&lt;span class=\"bp3-tag p p-59\"&gt;59&lt;/span&gt;\n\n\n...\n...\n\n\n17655\nNaN\n\n\n17656\nNaN\n\n\n17657\nNaN\n\n\n17658\nNaN\n\n\n17659\nNaN\n\n\n\n\n\n17660 rows × 1 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.loc[:, ['Best Overall Rating']].isna().sum()\n\nBest Overall Rating    17639\ndtype: int64\n\n\n\n결측치가 매우 많은 것을 볼 수 있다."
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#g.-df.isna-각-원소가-결측치인지-아닌지-산출",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#g.-df.isna-각-원소가-결측치인지-아닌지-산출",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### G. df.isna()| 각 원소가 결측치인지 아닌지 산출",
    "text": "### G. df.isna()| 각 원소가 결측치인지 아닌지 산출\n- 예시 1 : 열별로 결측치 카운트\n\ndf.isna()   ## NaN 값이 있다면 True 산출\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17656\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17657\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17658\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17659\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\ndf.isna().sum(axis = 0)   ## default : axis = 0\n\nID                              0\nName                            0\nAge                             0\nPhoto                           0\nNationality                     0\nFlag                            0\nOverall                         0\nPotential                       0\nClub                          211\nClub Logo                       0\nValue                           0\nWage                            0\nSpecial                         0\nPreferred Foot                  0\nInternational Reputation        0\nWeak Foot                       0\nSkill Moves                     0\nWork Rate                       0\nBody Type                      38\nReal Face                      38\nPosition                       35\nJoined                       1098\nLoaned From                 16966\nContract Valid Until          361\nHeight                          0\nWeight                          0\nRelease Clause               1151\nKit Number                     35\nBest Overall Rating         17639\ndtype: int64\n\n\n\narr = np.array([(True, False), (True, False), (False, True)])\narr\n\narray([[ True, False],\n       [ True, False],\n       [False,  True]])\n\n\n\narr.shape\n\n(3, 2)\n\n\n\narr.sum(axis = 0) ## 열별로 합\n\narray([2, 1])\n\n\n\narr.sum(axis = 1)   ## 행별로 합\n\narray([1, 1, 1])\n\n\n- 예시2 : 결측치가 50% 이상인 열 출력\n\ntype(df.isna().mean() &gt; 0.5)  ## 이 값 자체가 시리즈이므로 리스트로 넣으면 안된다.\n\npandas.core.series.Series\n\n\n\ndf.loc[:, df.isna().mean() &gt; 0.5]  ## [df.isna().mean() &gt; 0.5]는 에러뜸\n\n\n\n\n\n\n\n\nLoaned From\nBest Overall Rating\n\n\n\n\n0\nNaN\nNaN\n\n\n1\nNaN\nNaN\n\n\n2\nNaN\nNaN\n\n\n3\nNaN\nNaN\n\n\n4\nNaN\nNaN\n\n\n...\n...\n...\n\n\n17655\nNaN\nNaN\n\n\n17656\nNaN\nNaN\n\n\n17657\nNaN\nNaN\n\n\n17658\nNaN\nNaN\n\n\n17659\nNaN\nNaN\n\n\n\n\n17660 rows × 2 columns"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#h.-df.drop-특정-행이나-열을-drop",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#h.-df.drop-특정-행이나-열을-drop",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### H. df.drop() | 특정 행이나 열을 drop",
    "text": "### H. df.drop() | 특정 행이나 열을 drop\n- 예시 1 : [0,1,2,3] 행을 drop\n\ndf.drop([0,1,2,3])\n## df.drop([0,1,2,3], axis = 0)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n5\n212622\nJ. Kimmich\n27\nhttps://cdn.sofifa.net/players/212/622/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n89\n90\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos9\"&gt;RDM\nJul 1, 2015\nNaN\n2025\n177cm\n75kg\n€182M\n6.0\nNaN\n\n\n6\n197445\nD. Alaba\n30\nhttps://cdn.sofifa.net/players/197/445/23_60.png\nAustria\nhttps://cdn.sofifa.net/flags/at.png\n86\n86\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos6\"&gt;LCB\nJul 1, 2021\nNaN\n2026\n180cm\n78kg\n€113.8M\n4.0\nNaN\n\n\n7\n187961\n22 Paulinho\n32\nhttps://cdn.sofifa.net/players/187/961/22_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n83\n83\nAl Ahli\nhttps://cdn.sofifa.net/teams/112387/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJul 22, 2021\nNaN\n2024\n183cm\n80kg\n€48.5M\n15.0\nNaN\n\n\n8\n208333\nE. Can\n28\nhttps://cdn.sofifa.net/players/208/333/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n82\n82\nBorussia Dortmund\nhttps://cdn.sofifa.net/teams/22/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nFeb 18, 2020\nNaN\n2024\n186cm\n86kg\n€51.9M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17656 rows × 29 columns\n\n\n\n- 예시 2 : ['Name', 'Age']열을 drop\n\ndf.drop(columns = ['Name', 'Age'])\n## df.drop(['Name', 'Age'], axis = 1)\n\n\n  \n    \n\n\n\n\n\n\nID\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\nValue\nWage\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n€91M\n€115K\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n€78.5M\n€190K\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n€46.5M\n€46K\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n€107.5M\n€350K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n€89.5M\n€110K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n€70K\n€2K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n\n17660 rows × 27 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n결국에 axis 옵션만 기억하면 다른 parameter를 기억하지 않아도 된다.\n\n\n# G~H 에 대한 연습문제\n# 예제: 결측치가 50퍼 이상인 열을 제외하라.\n- 풀이 1 : 무지성 직접 제외\n\ndf.isna().mean()  ## := df.isna().sum() / len(df). Series\n\nID                          0.000000\nName                        0.000000\nAge                         0.000000\nPhoto                       0.000000\nNationality                 0.000000\nFlag                        0.000000\nOverall                     0.000000\nPotential                   0.000000\nClub                        0.011948\nClub Logo                   0.000000\nValue                       0.000000\nWage                        0.000000\nSpecial                     0.000000\nPreferred Foot              0.000000\nInternational Reputation    0.000000\nWeak Foot                   0.000000\nSkill Moves                 0.000000\nWork Rate                   0.000000\nBody Type                   0.002152\nReal Face                   0.002152\nPosition                    0.001982\nJoined                      0.062174\nLoaned From                 0.960702\nContract Valid Until        0.020442\nHeight                      0.000000\nWeight                      0.000000\nRelease Clause              0.065176\nKit Number                  0.001982\nBest Overall Rating         0.998811\ndtype: float64\n\n\n\ndf.drop(columns=['Loaned From','Best Overall Rating'])\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns\n\n\n\n- 풀이 2 : 이성적인 풀이\n\ndf.loc[:, df.isna().mean() &lt; 0.5]  ## 시리즈니까 리스트로 묶지 말것!!\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-missing",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-missing",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "3. Pandas : missing",
    "text": "3. Pandas : missing"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-numpy",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-numpy",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. Numpy",
    "text": "### A. Numpy\n- 발생 : np.nan\n\nnp.nan\n\nnan\n\n\n\narr = np.array([1,2,3,np.nan])\narr\n\narray([ 1.,  2.,  3., nan])\n\n\n\narr.mean()\n\nnan\n\n\n\nprint(type(np.nan))  ## np.nan 자체는 일종의 float로 취급된다.\nprint(type(arr[0]))\n\n&lt;class 'float'&gt;\n&lt;class 'numpy.float64'&gt;"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-pandas",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-pandas",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. Pandas",
    "text": "### B. Pandas\n- 발생 : np.nan, pd.NA\n\npd.Series([np.nan,1,2,3])\n\n0    NaN\n1    1.0\n2    2.0\n3    3.0\ndtype: float64\n\n\n\npd.Series([pd.NA, 1, 2, 3])\n\n0    &lt;NA&gt;\n1       1\n2       2\n3       3\ndtype: object\n\n\n\n위 두 개의 코드는 동일하다고 봐도 무방하다.\n\n- pd.Serise에 NaN 또는 &lt;NA&gt;가 있다면 연산할 때 제외함\n\nprint(f'np.nan을 넣은 시리즈의 평균 : {pd.Series([np.nan,1,2,3]).mean()} = pd.NA를 넣은 시리즈의 평균 : {pd.Series([pd.NA,1,2,3]).mean()}')\n\nnp.nan을 넣은 시리즈의 평균 : 2.0 = pd.NA를 넣은 시리즈의 평균 : 2.0\n\n\n\ns1 = pd.Series([np.nan,1,2,3])\ntype(s1[0])\n\nnumpy.float64\n\n\n\ns2 = pd.Series([pd.NA, 1,2,3])\ntype(s2[0])\n\npandas._libs.missing.NAType\n\n\n\nmissing은 그냥 NaN이라고 보자.\n\n- 검출(\\(\\star\\))(중요한가?)\n\ns1.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\ns2.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\npd.isna(s1[0]), pd.isnull(s1[0])  ## 결측치가 있느냐?\n\n(True, True)\n\n\n\npd.isna(s2[0]), pd.isnull(s2[0])  ## 결측치가 있느냐?\n\n(True, True)\n\n\n\nid(pd.isna), id(pd.isnull) # 같은함수\n\n(135146401797248, 135146401797248)\n\n\n\nid를 찍었을 때 같다면 같은 함수이다."
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-query",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "5. Pandas : query",
    "text": "5. Pandas : query\n\n개 간단하고 쉽지만 고점은 낮은 데이터 처리방식\n\n\nts = pd.DataFrame(np.random.normal(size=(20,4)),columns=list('ABCD'),index=pd.date_range('20221226',periods=20)).assign(E=['A']*10+['B']*10)\nts\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-02\n-1.136712\n0.595607\n-1.938775\n0.201931\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-06\n-0.095612\n-0.692796\n0.456627\n-0.395918\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-08\n-0.939328\n0.745621\n0.632724\n0.032088\nB\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n2023-01-10\n-0.339565\n-0.460976\n0.320097\n0.482333\nB\n\n\n2023-01-11\n-0.117493\n-1.964531\n-1.867120\n2.325713\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB\n\n\n2023-01-14\n-1.069801\n-0.659982\n-0.368828\n1.286645\nB"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-기본-query",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-기본-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. 기본 query",
    "text": "### A. 기본 query\n- 예시1: A&gt;0 and B&lt;0\n\nts.query('A&gt;0 and B&lt;0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n\n\n\n\n\n- 예시2: A&lt;B&lt;C\n\nts.query('A&lt;B&lt;C')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-14\n-1.069801\n-0.659982\n-0.368828\n1.286645\nB\n\n\n\n\n\n\n\n- 예시3: (A+B/2) &gt; 0\n\nts.query('(A+B)/2 &gt; 0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB\n\n\n\n\n\n\n\n- 예시4: (A+B/2) &gt; 0 and E=='A'\n\nts.query('(A+B)/2 &gt; 0 and E == \"A\"')   ## string 조건을 넣어주고 싶으면 다른 따옴표로 구분하면 된다.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-03\n0.118754\n0.119941\n-0.828199\n-1.356401\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n\n\n\n\n\n\n그냥 스트링으로 된 것들 중에는 생각헀던 건 왠만해선 다 된다."
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-외부변수를-이용",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-외부변수를-이용",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. 외부변수를 이용",
    "text": "### B. 외부변수를 이용\n- 예시1: A &gt; mean(A)\n\nmean = ts.A.mean()\nmean\n\n0.14414224086779243\n\n\n\n#ts.query('A &gt; np.mean(A)')   ## 이건 안됨\n#ts.query('A &gt; A.mean()')      ## 이건 되긴 함\n\n#ts.query('A &gt; mean')    ## column 중 하나인지 뭔지 헷갈림, 그래서 안됨\n\nts.query('A &gt; @mean')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-30\n2.573588\n-0.005872\n0.868897\n0.932830\nA\n\n\n2022-12-31\n0.146699\n-0.306216\n1.241642\n-1.008297\nA\n\n\n2023-01-01\n1.105096\n-0.492485\n0.865509\n-0.383760\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-05\n0.621326\n0.150997\n-0.479691\n0.810434\nB\n\n\n2023-01-07\n1.117905\n0.431402\n-0.235017\n0.897339\nB\n\n\n2023-01-09\n1.158532\n-2.312485\n-1.292257\n-1.325453\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n\n\n\n\n\n\nA.mean()보다 작은 값들을 산출했다.\n\n\nvalue = np.percentile(ts.B, 77)  ## ts.B에서 77백분위수에 해당하는 숫자\nts.query('B &gt; @value')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2023-01-02\n-1.136712\n0.595607\n-1.938775\n0.201931\nA\n\n\n2023-01-04\n0.673908\n1.199221\n1.454181\n-0.370048\nA\n\n\n2023-01-08\n-0.939328\n0.745621\n0.632724\n0.032088\nB\n\n\n2023-01-12\n0.574654\n0.984037\n0.641058\n0.264561\nB\n\n\n2023-01-13\n-0.252865\n0.519606\n0.373864\n0.520175\nB"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#c.-index로-query",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#c.-index로-query",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. Index로 query",
    "text": "### C. Index로 query\n- 예시: (2022년 12월30일 보다 이전 날짜) \\(\\cup\\) (2023년 1월10일)\n\nts.query('index &lt; \"2022-12-30\" or index == \"2023-01-10\"')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-1.027712\n-0.590487\n1.580671\n-0.315109\nA\n\n\n2022-12-27\n0.640362\n-0.520975\n-0.607376\n-0.560362\nA\n\n\n2022-12-28\n0.625888\n-1.711870\n0.573349\n0.040879\nA\n\n\n2022-12-29\n-1.494778\n-0.333769\n0.028889\n0.984416\nA\n\n\n2023-01-10\n-0.339565\n-0.460976\n0.320097\n0.482333\nB"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#d.-열의-이름에-공백이-있을-경우",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#d.-열의-이름에-공백이-있을-경우",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### D. 열의 이름에 공백이 있을 경우",
    "text": "### D. 열의 이름에 공백이 있을 경우\n열의 이름에 공백이 있으면 백틱을 이용하면 된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n\ndf.query('`Skill Moves` &gt; 4')  ## `를 사용\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n19\n193082\nJ. Cuadrado\n34\nhttps://cdn.sofifa.net/players/193/082/23_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n83\n83\nJuventus\nhttps://cdn.sofifa.net/teams/45/30.png\n...\nYes\n&lt;span class=\"pos pos3\"&gt;RB\nJul 1, 2017\nNaN\n2023\n179cm\n72kg\n€23M\n11.0\nNaN\n\n\n27\n189509\nThiago\n31\nhttps://cdn.sofifa.net/players/189/509/23_60.png\nSpain\nhttps://cdn.sofifa.net/flags/es.png\n86\n86\nLiverpool\nhttps://cdn.sofifa.net/teams/9/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nSep 18, 2020\nNaN\n2024\n174cm\n70kg\n€102.7M\n6.0\nNaN\n\n\n44\n232411\nC. Nkunku\n24\nhttps://cdn.sofifa.net/players/232/411/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n86\n89\nRB Leipzig\nhttps://cdn.sofifa.net/teams/112172/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n175cm\n73kg\n€166.9M\n12.0\nNaN\n\n\n62\n233927\nLucas Paquetá\n24\nhttps://cdn.sofifa.net/players/233/927/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n82\n87\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nOct 1, 2020\nNaN\n2025\n180cm\n72kg\n€90.9M\n10.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4516\n253755\nTalles Magno\n20\nhttps://cdn.sofifa.net/players/253/755/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n71\n83\nNew York City FC\nhttps://cdn.sofifa.net/teams/112828/30.png\n...\nNo\n&lt;span class=\"pos pos16\"&gt;LM\nMay 18, 2021\nNaN\n2026\n186cm\n70kg\n€7.7M\n43.0\nNaN\n\n\n4643\n246548\nO. Sahraoui\n21\nhttps://cdn.sofifa.net/players/246/548/23_60.png\nNorway\nhttps://cdn.sofifa.net/flags/no.png\n67\n78\nVålerenga Fotball\nhttps://cdn.sofifa.net/teams/920/30.png\n...\nNo\n&lt;span class=\"pos pos27\"&gt;LW\nMay 15, 2019\nNaN\n2023\n170cm\n65kg\n€3.3M\n10.0\nNaN\n\n\n4872\n251570\nR. Cherki\n18\nhttps://cdn.sofifa.net/players/251/570/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n73\n88\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 7, 2019\nNaN\n2023\n176cm\n71kg\n€17.7M\n18.0\nNaN\n\n\n5361\n225712\nD. Bahamboula\n27\nhttps://cdn.sofifa.net/players/225/712/23_60.png\nCongo\nhttps://cdn.sofifa.net/flags/cg.png\n63\n63\nLivingston FC\nhttps://cdn.sofifa.net/teams/621/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 9, 2022\nNaN\n2024\n185cm\n70kg\n€875K\n7.0\nNaN\n\n\n10452\n212455\n17 H. Mastour\n18\nhttps://cdn.sofifa.net/players/212/455/17_60.png\nMorocco\nhttps://cdn.sofifa.net/flags/ma.png\n65\n76\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\n&lt;a href=\"/team/47/ac-milan/\"&gt;AC Milan&lt;/a&gt;\nJun 30, 2017\n175cm\n63kg\nNaN\n98.0\nNaN\n\n\n\n\n65 rows × 29 columns"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-할당",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-할당",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "5. Pandas : 할당",
    "text": "5. Pandas : 할당\n아래와 같은 자료를 조건에 맞게 가공해서 새로운 열을 추가해보자.\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\ndf = pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-df.assign",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-df.assign",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. df.assign()",
    "text": "### A. df.assign()\n- 예시: total = att*0.1 + rep*0.2 + mid*0.35 + fin*0.35 를 계산하여 할당\n\ndf.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35)\n## 원래 데이터 손상시키지 않음\n\ndf_total = df.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35)\ndf_total\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n49.00\n\n\n1\n95\n100\n50\n80\n75.00\n\n\n2\n65\n90\n60\n30\n56.00\n\n\n3\n55\n80\n75\n80\n75.75\n\n\n4\n80\n30\n30\n100\n59.50\n\n\n5\n75\n40\n100\n15\n55.75\n\n\n6\n65\n45\n45\n90\n62.75\n\n\n7\n60\n60\n25\n0\n26.75\n\n\n8\n95\n65\n20\n10\n33.00\n\n\n9\n90\n80\n80\n20\n60.00\n\n\n10\n55\n75\n35\n25\n41.50\n\n\n11\n95\n95\n45\n0\n44.25\n\n\n12\n95\n55\n15\n35\n38.00\n\n\n13\n50\n80\n40\n30\n45.50\n\n\n14\n50\n55\n15\n85\n51.00\n\n\n15\n95\n30\n30\n95\n59.25\n\n\n16\n50\n50\n45\n10\n34.25\n\n\n17\n65\n55\n15\n45\n38.50\n\n\n18\n70\n70\n40\n35\n47.25\n\n\n19\n90\n90\n80\n90\n86.50"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-df.eval",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-df.eval",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. df.eval()",
    "text": "### B. df.eval()\n- A에서와 동일한 할당\n\ndf.eval('total = att*0.1 + rep*0.2 + mid*0.3 + fin*0.4')    ## query를 쓰는 것처럼, 원본 데이터를 변화시키지 않음\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0\n\n\n\n\n\n\n\n\nbut, 사칙연산과 같은 기초연산 수준에서만 잘 작동한다."
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#c.-dfcolname-xxx",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#c.-dfcolname-xxx",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### C. df[colname] = xxx",
    "text": "### C. df[colname] = xxx\n\n별로 안쓰는 방법\n\n\ndf['total'] = df.att*0.1 + df.rep*0.2 + df.mid*0.3 + df.fin*0.4   ## 원래의 데이터프레임을 손상시킨다.\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-transform-columnstarstarstar",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#pandas-transform-columnstarstarstar",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "6. Pandas : transform column(\\(\\star\\star\\star\\))",
    "text": "6. Pandas : transform column(\\(\\star\\star\\star\\))"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-lambda",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#a.-lambda",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### A. lambda",
    "text": "### A. lambda\n- 예시1: \\(x \\to x+2\\)\n\n\"\"\"\ndef f(x) :\n  return x + 2\n\n해당 코드와 동일하다.\n\"\"\"\n\nf = lambda x: x+2\nprint(f(3))\n\nprint((lambda x : x+2)(3))    ## (lambda x : x+2) 자체가 함수이므로, 뒤에 변수만 지정해주면 된다.\n\n5\n5\n\n\n- 예시2: \\(x,y \\to x+y\\)\n\n(lambda x,y : x+y)(1,3)\n\n4\n\n\n- 예시3: ‘2023-09’ \\(\\to\\) 9 (format : int)\n\n(lambda x : int(x[-2:]))('2023-09')   ## -1번째(뒤에서 두번째 원소까지 추출)\n\n9\n\n\n- 예시4: ‘2023-09’ \\(\\to\\) (2023, 9) (format : tuple)\n\n(lambda x : (int(x[:4]), int(x[-2:])))('2023-09')\n\n(2023, 9)\n\n\n- 예시5: 문자열이 ‘cat’이면 1 ’dog’ 이면 0 // ’cat이면 1 ’cat’이 아니면 0\n\n(lambda x : 1 if x == 'cat' else 0)('cat')\n## (lambda x : pd.Series(x == 'cat').sum())('cat') ## 이것도 된다.\n\n1"
  },
  {
    "objectID": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-map",
    "href": "2023_DV/Review/A5. Pandas, 데이터프레임 핸들링.html#b.-map",
    "title": "Pandas 기본기 | 데이터프레임 핸들링",
    "section": "### B. map",
    "text": "### B. map\n\n함수의 output들을 엮는다. 매핑하는 거\n\n:- 개념: map(f,[x1,x2,...xn])=[f(x1),f(x2),...,f(xn)]\n- 예시1: x-&gt;x+1을 [1,2,3]에 적용\n\nf = lambda x: x+1\nlist(map(f,[1,2,3]))\n\n[2, 3, 4]\n\n\n\nlist(map(lambda x : x + 1, [1,2,3]))\n\n[2, 3, 4]\n\n\n\n매핑하는 것 자체는 수나 리스트가 아니기 때문에 리스트로 엮어줘야 값을 알 수 있다.\n\n- 예시2 df.Height열 변환하기 (xxxcm 라고 적혀있는 것을 cm 없애고 키만 뽑기)\ns.str.replace('cm', '')\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height[:5]\ns\n\n0    189cm\n1    179cm\n2    172cm\n3    181cm\n4    172cm\nName: Height, dtype: object\n\n\n\nlist(map(lambda x : int(x[:-2]), s))\n\n[189, 179, 172, 181, 172]\n\n\n- 풀이 1 : map 이용\n\ndf.assign(Height = list(map(lambda x: int(x.replace('cm','')), df.Height)))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 풀이 2 : 사실 수틀리면 컴프리헨션 쓰면 된다.\n\ndf.assign(Height = [int(s.replace('cm', '')) for s in df.Height])\n\n\n  \n    \n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n\n17660 rows × 29 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n# 예시4 – df.Position 열에 아래와 같은 변환을 수행하고, 변환된 열을 할당하라.\n\n\n\nbefore\nafter\n\n\n\n\n&lt;span class=\"pos pos28\"&gt;SUB\nSUB\n\n\n&lt;span class=\"pos pos15\"&gt;LCM\nLCM\n\n\n&lt;span class=\"pos pos7\"&gt;LB\nLB\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n\n- 풀이 1\n\n데이터를 불러와서…\n\n\nlist(map(lambda x : x.str.split('&gt;')[-1] if x.isna() == False else pd.NA, df.Position))\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\n\n\n저장된 꼬라지를 보면…\n\n\nx = df.Position[0]\nx\n\n'&lt;span class=\"pos pos28\"&gt;SUB'\n\n\n\n게다가 결측치까지 있네???\n\n\ndf.Position.isna().sum()\n\n35\n\n\n\n결측치 처리 + 데이터 변환\n\n\ndf.assign(Position = list(map(lambda x : x.split('&gt;')[-1] if not pd.isna(x) else pd.NA, df.Position))).Position\n\n0        SUB\n1        LCM\n2         LB\n3        RCM\n4        RCM\n        ... \n17655    RES\n17656    RES\n17657    RES\n17658    RES\n17659    RES\nName: Position, Length: 17660, dtype: object\n\n\n- (풀이2) – 수틀리면 리스트컴프리헨션\n\nf = lambda x: x.split(\"&gt;\")[-1] if not pd.isna(x) else pd.NA\n\n\ndf.assign(Position = [f(s) for s in df.Position]).Position\n\n0        SUB\n1        LCM\n2         LB\n3        RCM\n4        RCM\n        ... \n17655    RES\n17656    RES\n17657    RES\n17658    RES\n17659    RES\nName: Position, Length: 17660, dtype: object\n\n\n\nmapping하는 게 조금 더 자연스럽고 한번에 쓸 수 있다. ~(어차피 이미 람다로 함수 만들었잖아?)~"
  },
  {
    "objectID": "2023_DV/Review/B1. merge와 concat.html",
    "href": "2023_DV/Review/B1. merge와 concat.html",
    "title": "Tidydata 심화 | groupby()",
    "section": "",
    "text": "groupby() 메소드의 심화"
  },
  {
    "objectID": "2023_DV/Review/B1. merge와 concat.html#라이브러리-imports",
    "href": "2023_DV/Review/B1. merge와 concat.html#라이브러리-imports",
    "title": "Tidydata 심화 | groupby()",
    "section": "1. 라이브러리 imports",
    "text": "1. 라이브러리 imports\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "2023_DV/Review/B1. merge와 concat.html#groupby",
    "href": "2023_DV/Review/B1. merge와 concat.html#groupby",
    "title": "Tidydata 심화 | groupby()",
    "section": "2. groupby",
    "text": "2. groupby\n\nA. df.groupby\n\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ndf\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\ng = df.groupby(by = 'department')\ng\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000029243C3B250&gt;\n\n\n\nset(dir(g)) & {'__iter__'} # g는 반복문을 돌리기 유리하게 설계되어 있음\n\n{'__iter__'}\n\n\n\ng에 적용할 수 있는 메소드(dir(g))중에 '__iter__'라는 게 있다. 즉, g는 for문을 돌리려고 만든 오브젝트이다.\n\n\n[i for i in g]  ## list(g)\n\n[('A',\n    department  gender  count\n  0          A    male      1\n  1          A  female      2),\n ('B',\n    department  gender  count\n  2          B    male      3\n  3          B  female      1)]\n\n\n\n두 개의 튜플이 나온다. 튜플의 원소는 그룹화하는 열과 sub_dataframe과 같은 형식이다.\n\n\ndct = {k:df for k, df in g}  ## 딕셔너리 컴프리헨션\ndct\n\n{'A':   department  gender  count\n 0          A    male      1\n 1          A  female      2,\n 'B':   department  gender  count\n 2          B    male      3\n 3          B  female      1}\n\n\n\ndisplay(dct['A'])\ndisplay(dct['B'])\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\n딕셔너리의 원소가 sub-dataframe"
  },
  {
    "objectID": "2023_DV/Review/B1. merge와 concat.html#b.-g의-이용법",
    "href": "2023_DV/Review/B1. merge와 concat.html#b.-g의-이용법",
    "title": "Tidydata 심화 | groupby()",
    "section": "### B. g의 이용법",
    "text": "### B. g의 이용법\n- g를 이용하여 원래의 df를 복원하라.\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ng = df.groupby('department')\n\n\npd.concat([df for _,df in g])\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n\n단순히 묶어주기만 해도 이렇게 나온다.\n\n- g를 이용하여 아래와 동일한 기능을 하는 코드를 작성하라. (agg함수 사용 금지)\n\ndf = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\ndf.groupby('department').agg({'count':'sum'})\n\n\n\n\n\n\n\n\ncount\n\n\ndepartment\n\n\n\n\n\nA\n3\n\n\nB\n4\n\n\n\n\n\n\n\n\nlist(g)\n\n[('A',\n    department  gender  count\n  0          A    male      1\n  1          A  female      2),\n ('B',\n    department  gender  count\n  2          B    male      3\n  3          B  female      1)]\n\n\n\npd.DataFrame(pd.Series({i:sum(j['count']) for i, j in g}))  ## 리스트로 먼저 묶어줘야 함.\n##pd.DataFrame(pd.Series({k:df['count'].sum() for k,df in g})) 이게 더 직관적\n\n\n\n\n\n\n\n\n0\n\n\n\n\nA\n3\n\n\nB\n4\n\n\n\n\n\n\n\n- 이 데이터프레임을 class를 기준으로 그룹핑하여 sub-dataframe을 만들고, score가 높은 순서로 정렬하는 코드를 작성하라.\n\ndf = pd.DataFrame({'class':['A']*5+['B']*5, 'id':[0,1,2,3,4]*2, 'score':[60,20,40,60,90,20,30,90,95,95]})\ndf\n\n\n\n\n\n\n\n\nclass\nid\nscore\n\n\n\n\n0\nA\n0\n60\n\n\n1\nA\n1\n20\n\n\n2\nA\n2\n40\n\n\n3\nA\n3\n60\n\n\n4\nA\n4\n90\n\n\n5\nB\n0\n20\n\n\n6\nB\n1\n30\n\n\n7\nB\n2\n90\n\n\n8\nB\n3\n95\n\n\n9\nB\n4\n95\n\n\n\n\n\n\n\n\ng = df.groupby('class')\n\npd.concat([df.sort_values('score', ascending = False) for i, df in g], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nclass\nid\nscore\n\n\n\n\n0\nA\n4\n90\n\n\n1\nA\n0\n60\n\n\n2\nA\n3\n60\n\n\n3\nA\n2\n40\n\n\n4\nA\n1\n20\n\n\n5\nB\n3\n95\n\n\n6\nB\n4\n95\n\n\n7\nB\n2\n90\n\n\n8\nB\n1\n30\n\n\n9\nB\n0\n20"
  },
  {
    "objectID": "2023_DV/Review/B1. merge와 concat.html#merge",
    "href": "2023_DV/Review/B1. merge와 concat.html#merge",
    "title": "Tidydata 심화 | groupby()",
    "section": "3. merge",
    "text": "3. merge\n\nA. 가장 빈번하게 사용되는 상황\n\n- 예시 : big 데이터프레임에 groupby+agg를 사용하여 small 데이터프레임이 생긴 경우\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'department':['A','B'], 'count_sum':[3,4]})\n## big.groupby('department').aggregate({'count':'sum'}).rename({'count' : 'count_sum'}, axis = 1)\n\n\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ncount_sum\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\ndepartment | gender | count –&gt; big\ndepartment | count_sum –&gt; small\n\ndepartment | gender | count | count_sum\n\n이러한 작업을 하고 싶을 때, pd.merge()를 사용하면 된다.\n\n\npd.merge(big, small)\n## big.merge(small)\n## small.merge(big)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n사실 아래가 정확한 코드이다.\n\npd.merge(big, small, on = 'department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n공통부분인 department에 따라 데이터프레임이 병합됨\n\n\n두 데이터프레임은 IndexLabel에 대하여 서로 다른 정보를 각각 정리한 상황\n두 데티어프레임에서 공통인 열(IndexLabel(을 찾고, 이것을 기준으로 데이터의 정보를 병합한다.\n\n\n\nB. 여러가지 파라메터\n\n# on\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = big.groupby('department').agg({'count':'sum'}).reset_index()\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndepartment\ncount\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n- 잘못된 코드\n\npd.merge(big, small)  ## department와 count가 겹친다. 이름은 겹치지만 count는 의미가 다름\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n\n\n\n\n\n\n연결고리로 이해\n\n‘A’, 1\n‘A’, 2\n‘B’, 3\n‘B’, 1\n\n‘A’, 3\n‘B’, 4\n두 데이터프레임의 연결고리가 없다. 따라서 아무것도 산출할 수 없다…\n- 제대로 쓴 코드\n\npd.merge(big, small, on = 'department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ncount_y\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n- 열의 이름을 살리면서…\n\npd.merge(big, small.rename({'count' : 'count_sum'}, axis = 1))  ## 공통부분을 없애줌\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n어차피 이름을 바꿔야 하니, 처음부터 양식에 맞게 해주는 게 더 좋을 수 있음…\n\n사실 아래 둘은 같은 코드이다.\n\npd.merge(big,small,on='department')\npd.merge(big,small,left_on='department', right_on='department')\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ncount_y\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n# left_on, right_on\n\nbig = pd.DataFrame({'department':['A','A','B','B'], 'gender':['male','female','male','female'],'count':[1,2,3,1]})\nsmall = pd.DataFrame({'dept':['A','B'], 'count':[3,4]})\ndisplay(\"big\",big)\ndisplay(\"small\",small)\n\n'big'\n\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\n\n\n\n\n0\nA\nmale\n1\n\n\n1\nA\nfemale\n2\n\n\n2\nB\nmale\n3\n\n\n3\nB\nfemale\n1\n\n\n\n\n\n\n\n'small'\n\n\n\n\n\n\n\n\n\ndept\ncount\n\n\n\n\n0\nA\n3\n\n\n1\nB\n4\n\n\n\n\n\n\n\n\n공통부분은 count이고, 오히려 다른 부분은 모두 공통되지 않은 상황\n\n\npd.merge(big, small)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ndept\n\n\n\n\n0\nB\nmale\n3\nA\n\n\n\n\n\n\n\n\ncount로 합치면서 지랄이 난다.\n\n- department, dept를 기준으로 병합…\n\npd.merge(big, small, left_on = 'department', right_on = 'dept')\n## 왼쪽 데이터프레임에선 department, 오른쪽 데이터프레임에선 dept를 기준으로 삼음...\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount_x\ndept\ncount_y\n\n\n\n\n0\nA\nmale\n1\nA\n3\n\n\n1\nA\nfemale\n2\nA\n3\n\n\n2\nB\nmale\n3\nB\n4\n\n\n3\nB\nfemale\n1\nB\n4\n\n\n\n\n\n\n\n- 더 직관적이고 편하게…\n\npd.merge(big, small.rename({'dept' : 'department', 'count' : 'count_sum'}, axis = 1))\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount\ncount_sum\n\n\n\n\n0\nA\nmale\n1\n3\n\n\n1\nA\nfemale\n2\n3\n\n\n2\nB\nmale\n3\n4\n\n\n3\nB\nfemale\n1\n4\n\n\n\n\n\n\n\n\n강제로 한 열만 이름이 같도록 해서 공통부분을 지정해줬다.(훨씬 편하지요옹?)\n\n# how\n\ndf1 = pd.DataFrame({\n    'dept':['통계','수학','과학','IAB'], \n    'count':[20,30,25,50]\n})\ndf2 = pd.DataFrame({\n    'dept':['통계','수학','과학','신설학과'], \n    'desc':['통계학과는...','수학과는...','과학학과는...','이 학과는 내년에 신설될 예정이고...']\n})\ndisplay(\"df1\",df1)\ndisplay(\"df2\",df2)\n\n'df1'\n\n\n\n\n\n\n\n\n\ndept\ncount\n\n\n\n\n0\n통계\n20\n\n\n1\n수학\n30\n\n\n2\n과학\n25\n\n\n3\nIAB\n50\n\n\n\n\n\n\n\n'df2'\n\n\n\n\n\n\n\n\n\ndept\ndesc\n\n\n\n\n0\n통계\n통계학과는...\n\n\n1\n수학\n수학과는...\n\n\n2\n과학\n과학학과는...\n\n\n3\n신설학과\n이 학과는 내년에 신설될 예정이고...\n\n\n\n\n\n\n\n공통의 열인 dept, 서로 다른 정보인 count와 desc\n\non, left_on, right_on을 사용할 필요가 없다.\n\n\npd.merge(df1, df2)  ## IAB, 신설학과가 미아됨\n\n\n\n\n\n\n\n\ndept\ncount\ndesc\n\n\n\n\n0\n통계\n20\n통계학과는...\n\n\n1\n수학\n30\n수학과는...\n\n\n2\n과학\n25\n과학학과는...\n\n\n\n\n\n\n\n\n근데 겹치지 않는 것이 없어졌음…\n\n겹치지 않는 자료를 처리하는 방식은 4가지 경우로 나누어진다.\n\n#pd.merge(df1, df2, how = 'inner')  ## 보수적으로, 자료가 없으면 없앰, default\n#pd.merge(df1, df2, how = 'left')  ## 왼쪽 거 기준으로(첫 번째)\n#pd.merge(df1, df2, how = 'right')  ## 오른쪽 거 기준으로(두 번째)\npd.merge(df1, df2, how = 'outer')  ## 개방적으로, 자료를 전부 보전\n\n\n\n\n\n\n\n\ndept\ncount\ndesc\n\n\n\n\n0\n통계\n20.0\n통계학과는...\n\n\n1\n수학\n30.0\n수학과는...\n\n\n2\n과학\n25.0\n과학학과는...\n\n\n3\nIAB\n50.0\nNaN\n\n\n4\n신설학과\nNaN\n이 학과는 내년에 신설될 예정이고..."
  },
  {
    "objectID": "2023_DV/Review/B1. merge와 concat.html#concat-merge를-이용한-데이터-병합",
    "href": "2023_DV/Review/B1. merge와 concat.html#concat-merge를-이용한-데이터-병합",
    "title": "Tidydata 심화 | groupby()",
    "section": "4. concat, merge를 이용한 데이터 병합",
    "text": "4. concat, merge를 이용한 데이터 병합\n\ndf_course2023 = pd.DataFrame({\n    'name':['최규빈']*3+['최혜미']*2+['이영미']+['양성준'],\n    'year':[2023]*7,\n    'course':['파이썬프로그래밍', '데이터시각화', '기계학습활용','수리통계1', '수리통계2','회귀분석1','통계수학']})\ndf_course2023\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n\n\n1\n최규빈\n2023\n데이터시각화\n\n\n2\n최규빈\n2023\n기계학습활용\n\n\n3\n최혜미\n2023\n수리통계1\n\n\n4\n최혜미\n2023\n수리통계2\n\n\n5\n이영미\n2023\n회귀분석1\n\n\n6\n양성준\n2023\n통계수학\n\n\n\n\n\n\n\n\ndf_course2024 = pd.DataFrame({\n    'name':['최규빈','이영미','이영미','양성준','최혜미'],\n    'year':[2024]*5,\n    'course':['기계학습활용','수리통계1', '수리통계2','회귀분석1','통계수학']})\ndf_course2024\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2024\n기계학습활용\n\n\n1\n이영미\n2024\n수리통계1\n\n\n2\n이영미\n2024\n수리통계2\n\n\n3\n양성준\n2024\n회귀분석1\n\n\n4\n최혜미\n2024\n통계수학\n\n\n\n\n\n\n\n\ndf_score = pd.DataFrame({\n    'name':['최규빈','최규빈','이영미','이영미','양성준','양성준','최혜미','최혜미'],\n    'year':[2023,2024]*4,\n    'score':[1, 1.2, 5,5,5,5,5,5]})\ndf_score\n\n\n\n\n\n\n\n\nname\nyear\nscore\n\n\n\n\n0\n최규빈\n2023\n1.0\n\n\n1\n최규빈\n2024\n1.2\n\n\n2\n이영미\n2023\n5.0\n\n\n3\n이영미\n2024\n5.0\n\n\n4\n양성준\n2023\n5.0\n\n\n5\n양성준\n2024\n5.0\n\n\n6\n최혜미\n2023\n5.0\n\n\n7\n최혜미\n2024\n5.0\n\n\n\n\n\n\n\n\ndf_sex = pd.DataFrame({'name':['최규빈','이영미','양성준','최혜미'],\n                        'sex':['male','female','male','female']})\ndf_sex\n\n\n\n\n\n\n\n\nname\nsex\n\n\n\n\n0\n최규빈\nmale\n\n\n1\n이영미\nfemale\n\n\n2\n양성준\nmale\n\n\n3\n최혜미\nfemale\n\n\n\n\n\n\n\n주어진 정보를 바탕으로, 4개의 데이터프레임을 결합하라.\n- 풀이\ndf_course2023  ## 7개 강의목록\ndf_course2024  ## 5개 강의목록\ndf_score  ## 점수\ndf_sex  ## 교수님 성별\n\n위 두개는 열이 똑같다. (concat)\n\n\n아래 두 개는 다른 정보이다. (merge)\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n\n\n1\n최규빈\n2023\n데이터시각화\n\n\n2\n최규빈\n2023\n기계학습활용\n\n\n3\n최혜미\n2023\n수리통계1\n\n\n4\n최혜미\n2023\n수리통계2\n\n\n5\n이영미\n2023\n회귀분석1\n\n\n6\n양성준\n2023\n통계수학\n\n\n7\n최규빈\n2024\n기계학습활용\n\n\n8\n이영미\n2024\n수리통계1\n\n\n9\n이영미\n2024\n수리통계2\n\n\n10\n양성준\n2024\n회귀분석1\n\n\n11\n최혜미\n2024\n통계수학\n\n\n\n\n\n\n\n\n단순히 두 개의 데이터프레임을 합쳤다.\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True).merge(df_score)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\nscore\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n1.0\n\n\n1\n최규빈\n2023\n데이터시각화\n1.0\n\n\n2\n최규빈\n2023\n기계학습활용\n1.0\n\n\n3\n최혜미\n2023\n수리통계1\n5.0\n\n\n4\n최혜미\n2023\n수리통계2\n5.0\n\n\n5\n이영미\n2023\n회귀분석1\n5.0\n\n\n6\n양성준\n2023\n통계수학\n5.0\n\n\n7\n최규빈\n2024\n기계학습활용\n1.2\n\n\n8\n이영미\n2024\n수리통계1\n5.0\n\n\n9\n이영미\n2024\n수리통계2\n5.0\n\n\n10\n양성준\n2024\n회귀분석1\n5.0\n\n\n11\n최혜미\n2024\n통계수학\n5.0\n\n\n\n\n\n\n\n\n연도별로 점수를 넣어줬다. name과 year만 겹치므로 알아서 합쳐진다.\n\n\npd.concat([df_course2023, df_course2024], axis = 0).reset_index(drop = True).merge(df_score).merge(df_sex)\n\n\n\n\n\n\n\n\nname\nyear\ncourse\nscore\nsex\n\n\n\n\n0\n최규빈\n2023\n파이썬프로그래밍\n1.0\nmale\n\n\n1\n최규빈\n2023\n데이터시각화\n1.0\nmale\n\n\n2\n최규빈\n2023\n기계학습활용\n1.0\nmale\n\n\n3\n최규빈\n2024\n기계학습활용\n1.2\nmale\n\n\n4\n최혜미\n2023\n수리통계1\n5.0\nfemale\n\n\n5\n최혜미\n2023\n수리통계2\n5.0\nfemale\n\n\n6\n최혜미\n2024\n통계수학\n5.0\nfemale\n\n\n7\n이영미\n2023\n회귀분석1\n5.0\nfemale\n\n\n8\n이영미\n2024\n수리통계1\n5.0\nfemale\n\n\n9\n이영미\n2024\n수리통계2\n5.0\nfemale\n\n\n10\n양성준\n2023\n통계수학\n5.0\nmale\n\n\n11\n양성준\n2024\n회귀분석1\n5.0\nmale\n\n\n\n\n\n\n\n\nname이 겹치므로 알아서 합쳐진다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html",
    "href": "2023_DV/Review/A3. Plotnine.html",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "",
    "text": "plotnine : R에서의 문법을 이용하여 그래프를 그려보자!"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#라이브러리-import",
    "href": "2023_DV/Review/A3. Plotnine.html#라이브러리-import",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "1. 라이브러리 import",
    "text": "1. 라이브러리 import\n\n##!pip install plotnine   ## plotnine이 구축되지 않은 경우 설치해야 한다.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n\n\nimport plotnine\n\n\nplotnine.options.dpi= 150\nplotnine.options.figure_size = (6, 5)\n\n\n간단한 그래프 설정이다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#mpg-data",
    "href": "2023_DV/Review/A3. Plotnine.html#mpg-data",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "2. mpg data",
    "text": "2. mpg data\n\nA. read data\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/mpg.csv')\ndf\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#b.-descriptions",
    "href": "2023_DV/Review/A3. Plotnine.html#b.-descriptions",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. descriptions",
    "text": "### B. descriptions\n\ndf.columns\n\nIndex(['manufacturer', 'model', 'displ', 'year', 'cyl', 'trans', 'drv', 'cty',\n       'hwy', 'fl', 'class'],\n      dtype='object')\n\n\n- 각 행들이 어떤 의미를 가지는 지 Chat GPT에게 분석을 요청해봤다.\n\n- 그렇단다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#mpg의-시각화-2차원",
    "href": "2023_DV/Review/A3. Plotnine.html#mpg의-시각화-2차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "3. mpg의 시각화 : 2차원",
    "text": "3. mpg의 시각화 : 2차원\n\nA. x=displ, y=hwy\n\n- 예시 1 : 정직하게 메뉴얼대로…\n\n파라미터를 직접 지정해주는 경우\n\n\nggplot(data = df) + geom_point(mapping = aes(x = 'displ', y = 'hwy')) ## aes : dictionary와 유사하다고 생각하면 된다.\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n파라미터 생략\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#b.-rpy2-코랩-아닌-경우-실습-금지",
    "href": "2023_DV/Review/A3. Plotnine.html#b.-rpy2-코랩-아닌-경우-실습-금지",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. rpy2 : 코랩 아닌 경우 실습 금지",
    "text": "### B. rpy2 : 코랩 아닌 경우 실습 금지\n- R에서도 거의 똑같은 문법으로 그릴 수 있음\n\n#import rpy2\n#%load_ext rpy2.ipython\n\n\n#%%R\n#library(tidyverse)\n#df = mpg\n#ggplot(df)+geom_point(aes(x=displ,y=hwy))"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#mpg의-시각화-3차원",
    "href": "2023_DV/Review/A3. Plotnine.html#mpg의-시각화-3차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "4. mpg의 시각화 : 3차원",
    "text": "4. mpg의 시각화 : 3차원\n\nA. x=displ, y=hwy, shape=class\n\n\nset(df['class'])  ## 중복되지 않은 값이 어느 것이 있는 지 산출\n## df['class'].unique() : 이건 array로 산출된다. 동일한 코드\n\n{'2seater', 'compact', 'midsize', 'minivan', 'pickup', 'subcompact', 'suv'}\n\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', shape = 'class'))   ## class를 shape로 구분 &gt; 불편함\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#b.-xdispl-yhwy-colorclass",
    "href": "2023_DV/Review/A3. Plotnine.html#b.-xdispl-yhwy-colorclass",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. x=displ, y=hwy, color=class",
    "text": "### B. x=displ, y=hwy, color=class\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n모양까지 class별로 달랐으면 좋겠다.\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'))\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n전체적으로 포인트의 사이즈를 키우고 싶다.\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'), size = 5)  ## 외부 파라미터\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n너무 커서 겹치니까 투명도 조정\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'class', shape = 'class'), size = 5, alpha = 0.5)\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\ngeom_point()에서 내부 aes()에 넣은 값들은 값들을 구분하도록 되며, 외부에 입력된 값은 전체 개체들을 바꿔버린다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#mpg의-시각화-4차원-5차원",
    "href": "2023_DV/Review/A3. Plotnine.html#mpg의-시각화-4차원-5차원",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "5. mpg의 시각화 : 4차원, 5차원",
    "text": "5. mpg의 시각화 : 4차원, 5차원\n\nset(df['drv'])  ## 4륜구동, 전륜구동(front), 후륜구동(r)\n\n{'4', 'f', 'r'}\n\n\n\nA. drive metiod에 더 중점\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy', color = 'drv', shape = 'class'), size = 4, alpha = 0.5)\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n4륜구동이 연비가 낮은 걸 확인할 수 있다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#b.-5차원-시각화",
    "href": "2023_DV/Review/A3. Plotnine.html#b.-5차원-시각화",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 5차원 시각화",
    "text": "### B. 5차원 시각화\n\nset(df['cyl'])  ## 실린더 수, 4,5,6,8\n\n{4, 5, 6, 8}\n\n\n\nggplot(df) + geom_point(aes(x='displ',y='hwy',color='drv',shape='class', size = 'cyl'), alpha = 0.5)  ## 외부 파라미터에 size는 제거\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n여기까지가 기본적인 사용 방법이다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#객체지향적-시각화",
    "href": "2023_DV/Review/A3. Plotnine.html#객체지향적-시각화",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "6. 객체지향적 시각화",
    "text": "6. 객체지향적 시각화\n- ggplot의 정체는 뭐지?\n\ntype(ggplot)\n\ntype\n\n\n\nclass. 어떤 물체를 만들어내는 함수와 비슷. matplotlib에서의 plt.figure()와 유사하다고 보면 된다.\n\n- 그럼 geom_point는 정체가 뭐지?\n\ntype(geom_point)  ## class, 생성함수.\n\nplotnine.utils.Registry\n\n\n\ngeom은 그림, 그래프라고 보면 된다. ’fig.add_axes()후 추가된ax`에 그래프를 그리는 것과 유사"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#a.-fig-geom_point-geom_smooth",
    "href": "2023_DV/Review/A3. Plotnine.html#a.-fig-geom_point-geom_smooth",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### A. fig + geom_point + geom_smooth",
    "text": "### A. fig + geom_point + geom_smooth\n\nfig  = ggplot(df)\npoint = geom_point(aes(x = 'displ', y = 'hwy'))\n\n\npoint ## 아무것도 나오지 않음\n\n&lt;plotnine.geoms.geom_point.geom_point at 0x121c8015390&gt;\n\n\n\nfig + point\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n두 개체를 합치니 피규어에 그래프가 들어가버린 형태가 되었다.\n\n\ngeom_smooth() | 산점도가 아닌 직선 그래프를 그려준다.\n\n\nsmooth = geom_smooth(aes(x = 'displ', y = 'hwy'))\n\n\nfig + smooth  ## ggplot(df) + geom_smooth(aes(x = 'displ', y = 'hwy')), 추세선 산출\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n그럼 셋을 합쳐보면…\n\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\nggplot(df) + geom_point(aes(x = 'displ', y = 'hwy')) + geom_smooth(aes(x = 'displ', y = 'hwy'))과 동일하다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#b.-시각화-개선",
    "href": "2023_DV/Review/A3. Plotnine.html#b.-시각화-개선",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 시각화 개선",
    "text": "### B. 시각화 개선\n\ngeom_point()를 개선\n\n\npoint_better = geom_point(aes(x='displ',y='hwy',color='drv',size='cyl'),alpha=0.5)  ## 색상과 크기로 구분\n\n\nfig + point_better\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\ngeom_smooth() 개선\n\n\nsmooth_better = geom_smooth(aes(x = 'displ',  y = 'hwy', color = 'drv'), linetype = 'dashed')  ## 차종별로 추세선\n\n\nfig + smooth_better\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\nassemble\n\n\nfig + smooth_better + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#c.-다양한-조합",
    "href": "2023_DV/Review/A3. Plotnine.html#c.-다양한-조합",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### C. 다양한 조합",
    "text": "### C. 다양한 조합\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\nfig + smooth_better + point_better\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n전체 추세선 추가\n\n\nfig + smooth_better + point_better + geom_smooth(aes(x = 'displ', y = 'hwy'), color = 'white', linetype = 'dashed', size = 3)\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#아이스크림을-많이-먹으면-걸리는-병---인과관계와-상관관계",
    "href": "2023_DV/Review/A3. Plotnine.html#아이스크림을-많이-먹으면-걸리는-병---인과관계와-상관관계",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "7. 아이스크림을 많이 먹으면 걸리는 병 - 인과관계와 상관관계",
    "text": "7. 아이스크림을 많이 먹으면 걸리는 병 - 인과관계와 상관관계"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#a.-교회의-수와-범죄-아이스크림과-소아마비",
    "href": "2023_DV/Review/A3. Plotnine.html#a.-교회의-수와-범죄-아이스크림과-소아마비",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### A. 교회의 수와 범죄, 아이스크림과 소아마비",
    "text": "### A. 교회의 수와 범죄, 아이스크림과 소아마비\n\n\n\n\n교회의 개수\n범죄건수\n\n\n\n\n전주\n100\n20\n\n\n부산\n1000\n200\n\n\n서울\n5000\n1000\n\n\n\n\n결론(?) : 교회가 많을 수록 범죄도 많아진다???\n\n\n배경없이 숫자만 비교할 경우, 상관관계를 인과관계로 착각할 수도 있다.\n인구에 대한 인과를 착각\n\n- 내용요약\n\n여름 → 수영장 → 소아마비\n여름 → 아이스크림\n아이스크림과 소아마비는 상관관계가 높다. 따라서 아이스크림 성분 중에서 소아마비를 유발하는 유해물질이 있을 것이다(?)\n\n\n다른 변인을 통제하고(인구가 동일한 지역), 비교하려는 대상만 차이를 부여해야 한다."
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#b.-기상자료",
    "href": "2023_DV/Review/A3. Plotnine.html#b.-기상자료",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### B. 기상자료",
    "text": "### B. 기상자료\n- 기상자료 다운로드\n\ntemp=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()\n## 판다스 데이터의 4번째 열만 가져와 numpy.array로 만든다.\n\n\nplt.plot(temp)    ## 이럴 때는 ggplot보다 matplotlib가 훨씬 편하다.\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#c.-숨은-진짜-상황-1-온도---아이스크림-판매량",
    "href": "2023_DV/Review/A3. Plotnine.html#c.-숨은-진짜-상황-1-온도---아이스크림-판매량",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### C. 숨은 진짜 상황 1 : 온도 -> 아이스크림 판매량",
    "text": "### C. 숨은 진짜 상황 1 : 온도 -&gt; 아이스크림 판매량\n-아래와 같은 관계를 가정하자. \\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\text{오차}\\]\n\nnp.random.seed(1)   ## 결과가 같도록 시드 설정\nicecream = 20 + 2 * temp + np.random.randn(len(temp))*10  ## N(0, 10^2)\nplt.plot(temp, icecream, 'o', alpha = 0.5)\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#d.-숨은-진짜-상황-2-온도---소아마비-반응수치",
    "href": "2023_DV/Review/A3. Plotnine.html#d.-숨은-진짜-상황-2-온도---소아마비-반응수치",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### D. 숨은 진짜 상황 2 : 온도 -> 소아마비 반응수치",
    "text": "### D. 숨은 진짜 상황 2 : 온도 -&gt; 소아마비 반응수치\n- 아래와 같은 관계를 가정하자. \\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\text{오차}\\]\n\nnp.random.seed(2)\n\ndisease = 30 + 0.5 * temp + np.random.randn(len(temp))*1  ## N(0,1)\nplt.plot(temp, disease, 'o', alpha = 0.5)\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#e.-우리가-관측한-상황온도는-은닉되어-있음",
    "href": "2023_DV/Review/A3. Plotnine.html#e.-우리가-관측한-상황온도는-은닉되어-있음",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### E. 우리가 관측한 상황(온도는 은닉되어 있음)",
    "text": "### E. 우리가 관측한 상황(온도는 은닉되어 있음)\n\nplt.plot(icecream, disease, 'o', alpha=0.3)\nplt.show()\n\n\n\n\n\nnp.corrcoef(icecream,disease)\n\narray([[1.        , 0.86298975],\n       [0.86298975, 1.        ]])\n\n\n여름만 뽑아서 플랏한다면?\n\nplt.plot(icecream,disease,'o',alpha=0.3)\nplt.plot(icecream[temp&gt;25], disease[temp&gt;25],'o') ## 기온이 25도 이상, 즉, 여름(아마도)\nplt.show()"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#f.-ggplot으로-온도구간을-세분화하여-시각화하자.",
    "href": "2023_DV/Review/A3. Plotnine.html#f.-ggplot으로-온도구간을-세분화하여-시각화하자.",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "### F. ggplot으로 온도구간을 세분화하여 시각화하자.",
    "text": "### F. ggplot으로 온도구간을 세분화하여 시각화하자.\n- 데이터를 데이터프레임으로\n\ndf = pd.DataFrame({'temp' : temp, 'ice' : icecream, 'dis' : disease})\ndf\n\n\n\n\n\n\n\n\ntemp\nice\ndis\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n\n\n1\n1.4\n16.682436\n30.643733\n\n\n2\n2.6\n19.918282\n29.163804\n\n\n3\n2.0\n13.270314\n32.640271\n\n\n4\n2.5\n33.654076\n29.456564\n\n\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n\n\n652\n20.4\n76.554679\n38.920443\n\n\n653\n18.3\n68.666079\n39.882650\n\n\n654\n12.8\n42.771364\n36.613159\n\n\n655\n6.7\n30.736731\n34.902513\n\n\n\n\n656 rows × 3 columns\n\n\n\n- 구간별로 나눈 변수를 추가 : pd.cut(df, bins = int)\n\ndf.assign(temp_cut = pd.cut(df.temp, bins = 5))   ## 온도를 4구간으로 분할한다\n\n\n\n\n\n\n\n\ntemp\nice\ndis\ntemp_cut\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n(-3.92, 4.56]\n\n\n1\n1.4\n16.682436\n30.643733\n(-3.92, 4.56]\n\n\n2\n2.6\n19.918282\n29.163804\n(-3.92, 4.56]\n\n\n3\n2.0\n13.270314\n32.640271\n(-3.92, 4.56]\n\n\n4\n2.5\n33.654076\n29.456564\n(-3.92, 4.56]\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n(13.04, 21.52]\n\n\n652\n20.4\n76.554679\n38.920443\n(13.04, 21.52]\n\n\n653\n18.3\n68.666079\n39.882650\n(13.04, 21.52]\n\n\n654\n12.8\n42.771364\n36.613159\n(4.56, 13.04]\n\n\n655\n6.7\n30.736731\n34.902513\n(4.56, 13.04]\n\n\n\n\n656 rows × 4 columns\n\n\n\n\ncut_df = df.assign(temp_cut = pd.cut(df.temp, bins = 7))\n\nfig = ggplot(cut_df)\npoint = geom_point(aes(x = 'ice', y = 'dis', color = 'temp_cut'), alpha = 0.2)\nsmooth = geom_smooth(aes(x = 'ice', y = 'dis', color = 'temp_cut'), linetype = 'dashed')\n\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n실제로 보니 상관관계가 없어보인다.\n\n\n진짜 아이스크림을 먹고 배탈이 났다면?\n\nnp.random.seed(1)\nicecream_sales = 30 + 2 * temp + np.random.randn(len(temp))*10\n\n\nnp.random.seed(2)\ndisease = 30 + 0 * temp + 0.15 * icecream + np.random.randn(len(temp))*1  ## temp, 온도가 미치는 영향을 제로로\n\n\ndf2 = pd.DataFrame({'temp' : temp, 'ice' : icecream_sales, 'dis' : disease})\ndf2.assign(temp_cut = pd.cut(df2.temp, bins = 7))\n\n\n\n\n\n\n\n\ntemp\nice\ndis\ntemp_cut\n\n\n\n\n0\n-0.5\n45.243454\n34.869760\n(-6.343, -0.286]\n\n\n1\n1.4\n26.682436\n32.446099\n(-0.286, 5.771]\n\n\n2\n2.6\n29.918282\n30.851546\n(-0.286, 5.771]\n\n\n3\n2.0\n23.270314\n33.630818\n(-0.286, 5.771]\n\n\n4\n2.5\n43.654076\n33.254676\n(-0.286, 5.771]\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n78.839992\n40.009905\n(17.886, 23.943]\n\n\n652\n20.4\n86.554679\n40.203645\n(17.886, 23.943]\n\n\n653\n18.3\n78.666079\n41.032562\n(17.886, 23.943]\n\n\n654\n12.8\n52.771364\n36.628863\n(11.829, 17.886]\n\n\n655\n6.7\n40.736731\n36.163023\n(5.771, 11.829]\n\n\n\n\n656 rows × 4 columns\n\n\n\n\nfig = ggplot(df2.assign(temp_cut = pd.cut(df2.temp,bins=7)))\npoint = geom_point(aes(x='ice',y='dis',color='temp_cut'),alpha=0.2)\nsmooth = geom_smooth(aes(x='ice',y='dis',color='temp_cut'),linetype='dashed')\nfig + point + smooth\n\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\nC:\\Users\\hollyriver\\anaconda3\\envs\\py\\lib\\site-packages\\plotnine\\stats\\smoothers.py:330: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n&lt;Figure Size: (900 x 750)&gt;\n\n\n\n무친 인과관계"
  },
  {
    "objectID": "2023_DV/Review/A3. Plotnine.html#결론",
    "href": "2023_DV/Review/A3. Plotnine.html#결론",
    "title": "Plotnine : R에서 비롯한 패키지",
    "section": "8. 결론",
    "text": "8. 결론\n\n아이스크림 먹어도 소아마비 안걸려!\n\n\n온도라는 흑막(은닉변수)을 잘 찾았고, 결과적으로 온도 -&gt; 아이스크림 판매량 & 소아마비라는 합리적인 진리를 얻을 수 있었다.\n\n\n고려할 흑막이 온도뿐이라는 보장이 있나?\n\n\n이론적으로는 모든 은닉변수들을 통제하였을 경우에도 corr(X,Y)의 절댓값이 1에 가깝다면 그때는 인과성이 있음이라고 주장할 수 있다.(이 경우에도 둘 중 어느것이 원인인지 파악하는 것은 불가)\n즉, 모든 은닉변수를 제거하면 상관성 = 인과성이다.\n\n\n모든 흑막을 제거하는 건 사실상 불가능하지 않나?\n\n\n실험계획을 잘 하면 흑막을 제거한 효과가 있음(무작위 추출 등)\n인과추론 : 실험계획이 사실상 불가능한 경우가 있음 -&gt; 모인 데이터에서 최대한 흑막2ㆍ3ㆍ4ㆍㆍㆍ등이 비슷한 그룹끼리 “매칭”을 시킨 뒤, 그룹간 corr을 구하여 규명한다!\n\n\n데이터의 수가 방대해지면서 가능해졌다."
  },
  {
    "objectID": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html",
    "href": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "",
    "text": "파이썬을 이용하여 간단한 그래프를 그려보고, 이미지를 이퀼라이징하는 방법을 알아보도록 하자."
  },
  {
    "objectID": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html#사전작업",
    "href": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html#사전작업",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "1. 사전작업",
    "text": "1. 사전작업\n\n라이브러리 import\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n##--------이퀼라이징을 위한 라이브러리--------\n#!pip install opencv-python\nimport cv2\n\n##--------parameter 설정--------\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (3,2)\nmatplotlib.rcParams['figure.dpi'] = 150 ## 450 : 300\n\n\n오늘 알아볼 함수들\n\nplt.boxplot()      ## 박스플롯 생성\nnp.random.randn()  ## 정규분포 하 확률변수 추출(default : 표준정규분포에서 1개 추출)\nnp.random.seed()   ## 시드 생성\nplt.hist()         ## 히스토그램 생성\ncv2.imread()       ## 이미지를 행렬로 읽어들임\nplt.imshow()       ## 행렬로 저장된 이미지를 시각화\ncv2.equalizeHist() ## 히스토그램 이퀼라이징\n\n!wget link         ## 파일 다운로드(리눅스)"
  },
  {
    "objectID": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html#플롯",
    "href": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html#플롯",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "2. 플롯",
    "text": "2. 플롯\n\nA. Boxplot\n전북고등학교에는 10명의 학생이 있는 두 개의 학급이 있고, 각 학생들이 받은 점수는 아래와 같다.\n\ny1 = [75,75,76,76,77,77,78,79,79,98]\ny2 = [76,76,77,77,78,78,79,80,80,81]\n\n\ny1_frame = pd.DataFrame(y1)\ny1_frame.describe()\n\n\n  \n    \n\n\n\n\n\n\n0\n\n\n\n\ncount\n10.000000\n\n\nmean\n79.000000\n\n\nstd\n6.831301\n\n\nmin\n75.000000\n\n\n25%\n76.000000\n\n\n50%\n77.000000\n\n\n75%\n78.750000\n\n\nmax\n98.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 1반의 평균은 \\(79\\)\n\ny2_frame = pd.DataFrame(y2)\ny2_frame.describe()\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n10.00000\n\n\nmean\n78.20000\n\n\nstd\n1.75119\n\n\nmin\n76.00000\n\n\n25%\n77.00000\n\n\n50%\n78.00000\n\n\n75%\n79.75000\n\n\nmax\n81.00000\n\n\n\n\n\n\n\n- 2반의 평균은 \\(78.2\\)이다.\n그렇다면 1반(y1)과 2반(y2), 두 반을 지도하는 선생님 중 어떤 선생님이 우수할까?\n\n아마도… : 평균을 중심으로 분석할 시 y1이 더 잘 지도했다고 판단할 수 있다.\n반론 : 평균은 A반이 더 높으나, 편차또한 더 크다. 고득점을 받는 한 학생(outlier)을 제외하면 전체적으로 B반 학생들이 시험을 더 잘 보았다고 해석할 수 있다.\n\n단순한 평균비교보다 학생들이 받은 점수의 분포를 비교하는 것이 중요.\n따라서 그 분포를 알아보기 위해 Boxplot을 그려보자.\n\n\nmatplotlib로 boxplot 그리기\n\nplt.boxplot(y1);  ## 세미콜론을 붙이면 결과값만 출력한다.\n\n\n\n\n\nplt.boxplot(y2);\n\n\n\n\n\nplt.boxplot([y1,y2]); ## 2차원의 리스트를 넣어 여러 개를 동시에 출력시킬 수도 있다.\n\n##np.array([y1, y2]).shape ## &gt; (2, 10)\n\n\n\n\n위처럼 하나의 outlier를 배제한다면, 나머지의 분포는 2반이 더 높게 위치함을 알 수 있다.\n\n박스플롯의 장점 : 단순히 평균만 제공하는 것보다 데이터를 파악하고 직관을 얻기에 유용하다.\n박스플롯이 이용되는 범위 : 초기 자료 분포를 파악하기 용이, 두 개 이상의 방법을 비교\n\n\n\nB. Histogram\n- 중심경향치(평균, 중앙값)만 가지고 집단을 비교할 순 없다.\n이전의 자료도 결과론적으로 중앙값이 더 타당해 보이나, 이것을 근거로 B반이 공부를 더 잘했다는 주장도 비합리적이다.\n\n단순 평균비교로 이러한 질문에 답을 하기 어려움.\n박스플롯으로 전체분포를 파악해도 어떤 반이 공부를 더 잘한다는 기준을 잡기 애매함.\n\nBut!\n특수한 경우에는 두 반 중에 누가 더 공부를 잘하냐는 질문에 명확히 대답할 수 있다.\n정규분포 전북고등학교 : 평균은 좋은 측정값인가?\n\nnp.random.seed(43052)\ny1 = np.random.randn(10000)   ## random.randn, standard normal distribution\ny2 = np.random.randn(10000) + 0.5\n\n- 두 반의 성적은 모두 표준정규분포를 따르는데, 2반의 성적이 일괄적으로 0.5가 높은 상황\n\nnp.mean(y1), np.mean(y2)\n\n(-0.011790879905079434, 0.4979147460611458)\n\n\n\nplt.boxplot([y1,y2]);\n\n\n\n\n\n분포의 모양이 거의 비슷한데, 중앙값(평균)이 2반이 더 높으므로 성적이 더 높다고 말할 수 있다. &gt; 게다가 평균적으로 0.5점 정도 더 공부를 잘한다고 대답할 수 있다!\n\n근데, 위와 같은 경우는 정규분포에서 뽑힌 랜덤샘플이라 분포의 모양이 같다고 하긴 했는데… 실제 데이터를 확인할 때는 박스플롯으로 하긴 어려워보인다.\n따라서!\n히스토그램을 그려 확인해보자\nplt.hist(array, bins = int, range = list)\n\nplt.hist([y1, y2], bins = 100);\n\n\n\n\n\n둘의 분포는 비슷하지만, 2반(주황색)이 조금 더 높은 수준에서 자리함을 알 수 있다."
  },
  {
    "objectID": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html#equalization",
    "href": "2023_DV/Review/A0. Introduction(그래프, 이미지 이퀼라이징).html#equalization",
    "title": "Introduction | 그래프, 이미지 이퀼라이징",
    "section": "3. Equalization",
    "text": "3. Equalization\n히스토그램이나 이미지를 눈으로 보기 쉽도록 이퀼라이징해보자!\n이미지 자료 다운로드\n\n#!pip install wget\nimport wget\n\nwget.download(\"https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\")\nimg = cv2.imread(\"Unequalized_Hawkes_Bay_NZ.jpg\")\n\n##--------리눅스 환경 충족 시--------\n##!wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\n##img = cv2.imread('https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg')\n##!rm Unequalized_Hawkes_Bay_NZ.jpg\n\n## 파일을 들여오고 인식한 뒤 삭제하는 코드이다.\n\nRequirement already satisfied: wget in c:\\users\\hollyriver\\anaconda3\\envs\\ssk\\lib\\site-packages (3.2)\n100% [............................................................................] 110895 / 110895\n\n\n\nplt.imshow(img) ## image show\n\n&lt;matplotlib.image.AxesImage at 0x184b83fab00&gt;\n\n\n\n\n\n\nplt.imshow()를 통해서 이미지를 가져왔다!\n\n근데, img는 어떤 값으로 저장된 걸까?\n\nA. 사실 이미지는 숫자열이었다!\n\n_img1 = np.array([0,30,90,120,150,180,210,240,255]).reshape(3,-1)  ## 3행 3열로 변경\n_img1\n\narray([[  0,  30,  90],\n       [120, 150, 180],\n       [210, 240, 255]])\n\n\n\nplt.imshow(_img1, cmap = 'gray')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n_img2 = np.array([0,20,40,60,80,100,120,140,160]).reshape(3,3)\n_img2\n\narray([[  0,  20,  40],\n       [ 60,  80, 100],\n       [120, 140, 160]])\n\n\n\nplt.imshow(_img2, cmap = 'gray', vmin = 0, vmax = 255)  ## vmin, vmax를 설정해주지 않으면 가장 큰 값이 max(white)가 된다\nplt.colorbar()\nplt.show()\n\n\n\n\n255에 가까울 수록 하얀색, 0에 가까울 수록 검정색인 이미지로 변환된 것을 볼 수 있다. 숫자만으로 이뤄진 행렬이 이미지가 된 것이다!\n크게, 더 크게 해보자!\n\n_img3 = np.concatenate([_img1,_img2], axis = 1)  ## 열로 병합, default는 행으로 병합\n_img3\n\narray([[  0,  30,  90,   0,  20,  40],\n       [120, 150, 180,  60,  80, 100],\n       [210, 240, 255, 120, 140, 160]])\n\n\n\nplt.imshow(_img3, cmap = 'gray')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\nB. RGB값을 더한 그림 그리기\n\n먼저, RGB값에 해당하는 수를 각각 array로 지정해주자.\n\n\nr = np.array(\n    [[255, 255, 255,  0,   0],\n     [255, 255, 255,  0,   0],\n     [255, 255, 255,  0,   0],\n     [  0,   0,   0,  0,   0],\n     [  0,   0,   0,  0,   0]]\n)\ng = np.array(\n    [[  0,   0, 255, 255, 255],\n     [  0,   0, 255, 255, 255],\n     [  0,   0, 255, 255, 255],\n     [  0,   0,   0,   0,   0],\n     [  0,   0,   0,   0,   0]]\n)\nb = np.array(\n    [[  0,   0,   0,   0,   0],\n     [  0,   0,   0,   0,   0],\n     [255, 255, 255, 255, 255],\n     [255, 255, 255, 255, 255],\n     [255, 255, 255, 255, 255]]\n)\nz = np.array(\n    [[ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0],\n     [ 0,  0,  0,  0,  0]]\n)\n\n\n그리고 합쳐서 RGB값을 할당해준다.\n\n\nred = np.stack([r,z,z], axis = -1)\ngreen = np.stack([z,g,z], axis = -1)\nblue = np.stack([z,z,b], axis = -1)\n\n\ntemp = np.stack([r,g,b], axis = -1);temp\n\narray([[[255,   0,   0],\n        [255,   0,   0],\n        [255, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[255,   0,   0],\n        [255,   0,   0],\n        [255, 255,   0],\n        [  0, 255,   0],\n        [  0, 255,   0]],\n\n       [[255,   0, 255],\n        [255,   0, 255],\n        [255, 255, 255],\n        [  0, 255, 255],\n        [  0, 255, 255]],\n\n       [[  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255]],\n\n       [[  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255],\n        [  0,   0, 255]]])\n\n\n\n원소 하나 당 3개의 값이 할당된 것을 알 수 있다.\n\n\nnp.stack([], axis = -1) 크기가 동일한 행렬들의 각 원소들을 리스트화 하여 원소로 저장한다.\n\n\nplt.imshow(red+green+blue)\nplt.show()\n\n\n\n\nR, G, B를 같은 비율로 섞으면 다시 흑백이미지가 된다.\n\narr2 = np.array([[10, 40], [80, 60]])\narr2\n\narray([[10, 40],\n       [80, 60]])\n\n\n\narr3 = np.stack([arr2, arr2, arr2], axis = -1)  ## rgb값이 각각 동일\nplt.imshow(arr3)\nplt.show()\n\n\n\n\n\nimg.shape ## 원소의 리스트 수가 3이라는 것으로 rgb가 포함되어 있음을 추측할 수 있음.\n\n(683, 1024, 3)\n\n\n\n\nC. 히스토그램 이퀼라이징\n그래서 이퀼라이징은 뭐냐고!\n\nimg에서 추출해온 행렬로 아래와 같은 히스토그램을 만들어보자.\n\n\nr = img[:, :, 0]  ## 첫 번째 원소\ng = img[:, :, 1]  ## 두 번째 원소\nb = img[:, :, 2]  ## 세 번째 원소\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255])\nplt.show()\n\n\n\n\n\n120~200 사이에 값이 몰려있음\n120~200의 분포된 모양은 그대로 유지하면서 range를 0~255까지 늘린다면?\n\n\nrr = cv2.equalizeHist(r)\ngg = cv2.equalizeHist(g)\nbb = cv2.equalizeHist(b)\n\n\ncv2 라이브러리의 equalizeHist() 사용하면 행렬의 모든 원소들의 분포 정도를 고르게(0~255) 바꾼다.\n\n\nplt.hist(r.reshape(-1),bins=255, range=[0,255],label='befor');\nplt.hist(rr.reshape(-1),bins=255,range=[0,255],label='after');  ## cv2.equalizeHist() 사용\nplt.legend()\nplt.show()\n\n\n\n\n그렇다면 이것을 응용하여 위에서의 이미지를 이퀼라이징하면?\n- 이퀼라이징된 각 원소들을 다시 이어붙여 하나의 이미지로 만들어본다.\n\nimg2 = np.stack([rr,gg,bb], axis = -1)  ## axis = -1 &gt; z축(원소 내에서 확장)으로 추가\nimg2.shape\n\n(683, 1024, 3)\n\n\n\nplt.imshow(img2)\nplt.show()\n\n\n\n\n\nplt.imshow(np.concatenate([img,img2], axis = 1))\nplt.show()\n\n\n\n\n\n이렇게, 이미지를 조금 더 구별하기 쉽도록 바꿀 수 있다."
  },
  {
    "objectID": "2023_DV.html",
    "href": "2023_DV.html",
    "title": "2023 DV",
    "section": "",
    "text": "전북대학교 2023년 2학기 통계학부 최규빈 교수님의 “데이터시각화(Data Visualization)” 강의를 듣고 그 내용을 나름대로 정리한 내용입니다.\n\n:::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\n라이브러리 imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n라이브러리 imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n데이터 크롤링 맛보기\n\n\n\n\n\n\n\nyfinance\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nFolium | 월드맵 시각화\n\n\n\n\n\n\n\nfolium\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n훌륭한 시각화 2\n\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPlotly : pandas backend\n\n\n\n\n\n\n\nplotly\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nTidydata 심화 실습\n\n\n\n\n\n\n\npandas\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nTidydata 심화 | groupby()\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nTidydata 만들기\n\n\n\n\n\n\n\npandas\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n집단 간 비교 : 심슨의 역설\n\n\n\n\n\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n데이터 시각화 실습 : FIFA23 선수 데이터\n\n\n\n\n\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPandas 사용 팁\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPandas 기본기 | 데이터프레임 핸들링\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPandas 기본기 | 행과 열의 선택\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPlotnine : R에서 비롯한 패키지\n\n\n\n\n\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nSeaborn | 데이터프레임 친화적 패키지\n\n\n\n\n\n\n\nseaborn\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nPlot | 꺾은선, 산점도, 객체지향화\n\n\n\n\n\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction | 그래프, 이미지 이퀼라이징\n\n\n\n\n\n\n\nmatplotlib\n\n\ncv2\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n[문제 풀이] 데이터프레임 : 특정 열의 재가공\n\n\n\n\n\n\n\nQuiz\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\n강신성\n\n\n\n\n\n\n  \n\n\n\n\n[문제 풀이] 특정 단어를 포함하는 열 선택\n\n\n\n\n\n\n\nQuiz\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\n강신성\n\n\n\n\n:::\n\n\n\nNo matching items\n\n:::\n:::"
  }
]